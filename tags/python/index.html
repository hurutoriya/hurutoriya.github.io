<!doctype html><html lang=ja dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>python | hurutoriya</title><meta name=keywords content><meta name=description content="Shunya Ueta's blog"><meta name=author content="Shunya Ueta"><link rel=canonical href=https://shunyaueta.com/tags/python/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><link rel=icon href=https://shunyaueta.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shunyaueta.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shunyaueta.com/favicon-32x32.png><link rel=apple-touch-icon href=https://shunyaueta.com/apple-touch-icon.png><link rel=mask-icon href=https://shunyaueta.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://shunyaueta.com/tags/python/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-JMJRQJT0Q3"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JMJRQJT0Q3")</script><meta property="og:title" content="python"><meta property="og:description" content="Shunya Ueta's blog"><meta property="og:type" content="website"><meta property="og:url" content="https://shunyaueta.com/tags/python/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="python"><meta name=twitter:description content="Shunya Ueta's blog"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://shunyaueta.com/ accesskey=h title="hurutoriya (Alt + H)">hurutoriya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://shunyaueta.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://shunyaueta.com/about title=About><span>About</span></a></li><li><a href=https://searchengineeringnewsletter.substack.com/ title=Newsletter><span>Newsletter</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://shunyaueta.com/index.xml title=RSS><span>RSS</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>python</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2022-10-23-2344/images/01.gif alt=デモアプリの動画></figure><header class=entry-header><h2>Elasticsearchの近似近傍探索を使って、ドラえもんのひみつ道具検索エンジンを作ってみた</h2></header><div class=entry-content><p>Elasticsearch 8 系から使用可能になった近似近傍探索1を使って、ドラえもんのひみつ道具の自然言語検索ができる検索エンジンを作ってみた。
デモ動画のように、検索したいひみつ道具を説明する文章することで近しいひみつ道具が検索されます。
コードは GitHub に公開してあるので、興味のある方は手元で、動かして遊ぶことが出来ます。 poetry と Docker さえあれば動くようになっています。
hurutoriya/doraemon-himitsu-dogu-search: Doraemon Himitsu Dogu Japanese semantic search based on Elascticsearch ANN
システムの概要図はこんな感じ
所感 ドラえもんのひみつ道具のデータセットを今回１から作ったが、パースと前処理がめんどくさくてここが一番手間がかかった。が、工夫しないと出来なかったので、一番楽しいところでもあった。 文章の特徴抽出は、sonoisa/sentence-bert-base-ja-mean-tokens-v2 · Hugging Faceを使わせていただき、驚くほど簡単に実現できた。 実際はもっと精度を高めるには、fine tune などがいいのだろうが、システム側を作ることに注力したかったので今回は割愛 デモアプリの構築は streamlit を使って 20m くらいで作れたので、相変わらず便利すぎて愛用している。今回の検索エンジンは CLI から実行もできるが、こうやってデモアプリがあったほうがそれっぽくて気持ちいい。 インデキシング時にトーカナイザーのことなど全く考えずに特徴ベクトルだけインデキシングして、それで検索が成り立つというのは新鮮。閾値設定しなければゼロヒット問題にも直面しないので、できることの幅は広がりそう。 Elasticsearch の近似近傍探索は、今回ベクトル同士の近似近傍探索しかやっていないが、それもインデキシング、クエリ部分は公式ドキュメントを見れば事足りたので変にハマることはなかった。 クエリ部分はこれだけで書けた。
1 2 3 4 5 6 7 8 9 10 query = { "knn": { "field": "vector", "query_vector": sentence_embeddings[0], "k": 10, "num_candidates": 100, }, "fields": ["name", "description"], } result = es....</p></div><footer class=entry-footer><span title='2022-10-23 23:44:13 +0900 +0900'>10月 23, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Elasticsearchの近似近傍探索を使って、ドラえもんのひみつ道具検索エンジンを作ってみた" href=https://shunyaueta.com/posts/2022-10-23-2344/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Python で zip関数を使う際に、２つの配列が同じ大きさを想定する場合は 3.10 から導入された strict=True を使おう</h2></header><div class=entry-content><p>Python で２つの配列を for 文で扱いたい場合によく使うのが zip() です。
zip()を使った for 文では暗黙的に同じ大きさが要求されると思っていたが、実際には以下のように２つの配列の大きさが異なっていてもエラーが出ないことに気が付かず、困ったことがあった。
1 2 3 4 5 6 7 8 9 10 In [1]: a = [1,2,3,4] In [2]: b = [1,2,3] In [3]: for i,j in zip(a,b): ...: print(i,j) ...: 1 1 2 2 3 3 てっきり、大きい配列の要素を参照時にエラーが発生するかと思ったら、そんなことはなかった。
assert とかで事前にコケるようにしておくとか必要そう。 もしくは、両者の配列のサイズが同じことを明示的に確認するのが吉。
また蛇足だが、Stackoverflow では意図的に異なる大きさの配列を上手く循環させつつ回したい場合の対処法も書いてあり勉強になった。1
2022-10-27: 追記
@ftnext さんから以下の情報2を教えてもらいました。
小さい方を読み切ったら for を抜けるの予想と違いますよね。 3.10 から zip に strict 引数が追加されており、True を指定すれば長さが異なると ValueError を送出するようになったんです！ https://docs.python.org/ja/3/library/functions.html#zip… また長い方に合わせたいときは zip_longest が標準ライブラリの itertools にありますー...</p></div><footer class=entry-footer><span title='2022-10-17 11:56:22 +0900 +0900'>10月 17, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Python で zip関数を使う際に、２つの配列が同じ大きさを想定する場合は 3.10 から導入された strict=True を使おう" href=https://shunyaueta.com/posts/2022-10-17-1156/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる</h2></header><div class=entry-content><p>2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1
Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow’s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX)....</p></div><footer class=entry-footer><span title='2022-08-18 19:38:29 +0900 +0900'>8月 18, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる" href=https://shunyaueta.com/posts/2022-08-18-1938/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2022-08-10-1717/images/1.png alt="poetry show の実行結果"></figure><header class=entry-header><h2>poetry show でパッケージ名に (!) が付与されている意味</h2></header><div class=entry-content><p>poetry show は、poetry の設定ファイルの pyproject.tomlに記載された利用可能なパッケージ名を表示してくれる。
例えば、ターミナルで poetry install を行う前に、poetry showを行うと以下のような結果がでる。
そして、grep で上記の結果を表示させてみると
1 2 > poetry show | grep aiohttp aiohttp (!) 3.8.1 Async http client/server framework (asyncio) と パッケージ名に (!)が付与されている。
この(!)ってそもそもどんな意味なのか気になったので調べてみました。
Poetry のコードを直接読んでみると、 test_show_basic_with_not_installed_packages_non_decoratedのテストケースが今回の事例にマッチしており、わかりやすかった。 意味としては、「インストールされたパッケージに対する show コマンドを非装飾モードで結果を出力」へのテストだ。 状況としては、cachyとpendulumを poetry add して、 cachyのみを poetry install している。
1 2 3 4 5 6 7 8 9 10 poetry.package.add_dependency(Factory.create_dependency("cachy", "^0.1.0")) poetry.package.add_dependency(Factory.create_dependency("pendulum", "^2.0.0")) cachy_010 = get_package("cachy", "0.1.0") cachy_010.description = "Cachy package" pendulum_200 = get_package("pendulum", "2....</p></div><footer class=entry-footer><span title='2022-08-10 17:17:29 +0900 +0900'>8月 10, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to poetry show でパッケージ名に (!) が付与されている意味" href=https://shunyaueta.com/posts/2022-08-10-1717/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>社内でデータ分析結果を可視化・共有する際に Google Colab が便利</h2></header><div class=entry-content><p>社内でデータ分析のレポートを書く際は Google Colab がとても便利な事に気がついた。
Google Bigquery でデータを抽出、Google Sheets で可視化 従来だと、自分がやっていた方法として、
Google BQ などで分析対象結果のデータを抽出 その結果を Google Spread Sheet として保存して、Google Sheets の機能で可視化。元の SQL のコードは、別シートを作ってそこに貼り付けておく。 利点としては、一度データを抽出した後は、Google Sheets で二次加工が簡単にできる点がとても便利。 また、 Google Sheet を共有後に Produc Manager が出したい数値を、Product Manager 自身が Google Sheets を元にさっと計算することもできる。
だが、二次加工が便利なのはいいが、大抵の可視化ってパターンが決まっているかつ二次加工の状況が必ず発生するわけではないので、SQL 取得とその可視化を一気通貫でできないかなと考えていた。
なにか良い方法無いかなと思っている矢先に、別のチームの同僚が、Google Colab を使って、BQ を dataframe として保存後 matplotlib で可視化しているのを見かけて、
求めていたのは…こ、これだ….
となり、速攻取り入れました。
良いと思ったところは積極的に真似する
Google Colab なら、データの取得・加工・可視化までを完結可能 Google Colab の利点を列挙しておく
SQL のコード、データ抽出や可視化のロジックなどが Python で記述可能かつ、Google Colab で完結 matplotlib で可視化できるので、見やすく美しい図を作れる そしてそのコードは他のデータ分析でも再利用可能 pandas dataframe で Google BQ からデータを取得するので、Standard SQL だけでは難しい計算も pandas、 numpy や scipy などを使ってデータ加工が簡単にできるのも、便利 Google Sheets 同様、簡単に社内で共有できる Markdown も Google Colab 内で書けるので、凝った文章などもいれてレポートも書ける マジックコマンドで、Google BQ の結果を dataframe として保存1したり、...</p></div><footer class=entry-footer><span title='2022-05-10 22:00:23 +0900 +0900'>5月 10, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 社内でデータ分析結果を可視化・共有する際に Google Colab が便利" href=https://shunyaueta.com/posts/2022-05-10-2200/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2022-04-08/images/1.png alt="Spacy による NERの結果を Google Colab 上で可視化"></figure><header class=entry-header><h2>Google Colab で Spacy による NER の結果を表示するには、jupyter 引数を True にする必要がある</h2></header><div class=entry-content><p>自然言語処理のフレームワークの Spacy を使って、Google Colab 上で NER の可視化を行う際に
1 2 3 4 import spacy spacy.displacy.render(doc, style='ent') と実行しても
1 &lt;div class="entities" style="line-height: 2.5; direction: ltr">&lt;/br>\n&lt;mark class="entity" style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">\n 2022年\n &lt;span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">Date&lt;/span>\n&lt;/mark>\n、\n&lt;mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">\n 日本人\n &lt;span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0....</p></div><footer class=entry-footer><span title='2022-04-08 21:04:13 +0900 +0900'>4月 8, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Google Colab で Spacy による NER の結果を表示するには、jupyter 引数を True にする必要がある" href=https://shunyaueta.com/posts/2022-04-08/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Python で DeepL API Free を利用してテキストファイルを翻訳する</h2></header><div class=entry-content><p>機械翻訳サービスの DeepL はアプリだけでなく API 提供も行っている。 今回は DeepL が公開している free API を利用して、テキストファイルを英日翻訳して、翻訳結果をテキストファイルとして保存する方法について説明する。
無料 API は1か月あたり500,000文字の上限ありの制限があるが、Pro version と変わらない品質の翻訳を行うことができる。 個人利用する分にはこの文字数制限は特に大きな問題にはならないと思われる。
https://www.deepl.com/ja/pro#developer
まずアカウントを作成して、DeepL API Free のAPI_KEYを入手する。 その後、以下のスクリプトを実行すれば、翻訳元のファイル名にJA_という接頭辞がついたファイルが保存される。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import requests # NOTE: put API KEY API_KEY:str = '' # NOTE: put target file path target_file:str = "" with open(target_file) as f: txt = f....</p></div><footer class=entry-footer><span title='2022-01-05 23:02:06 +0900 +0900'>1月 5, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Python で DeepL API Free を利用してテキストファイルを翻訳する" href=https://shunyaueta.com/posts/2022-01-05/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2021-12-28/images/1.png alt="DataFrame 内部にURLから参照して画像表示"></figure><header class=entry-header><h2>Jupyter Notebook で画像をダウンロードすることなく、URLから参照してPandas DataFrame内部に表示させる</h2></header><div class=entry-content><p>データ分析などをしていると、画像はダウンロードせずに特定の CDN (GCP なら GCS, AWS なら S3 など)で提供されている画像を参照して、 Jupyter Notebook 上で良い感じに表示させたいときがありませんか?
例えば、画像と説明文がペアになっているデータを画像自体はダウンロードせずに Jupyter 上で画像と説明文を DataFrame として表示させたいときが多々ある。 元の画像自体は CDN に格納されていて、画像をダウンロードする必要はなく参照するだけのときにはすごく便利。 毎度画像を CDN からダウンロードするのも無駄なので、画像を加工せずに Jupyter 上で表示するだけなら、この方法がベストですね。
url からとってきた画像を jupyter に表示する でも同じような課題に取り組んでいるが、今回紹介する方法なら余計なパッケージを入れずに最小構成で Jupyter 上で表示できるのが利点。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import pandas as pd from IPython.display import HTML # NOTE: https://www.irasutoya.com/2021/01/onepiece.html から画像を参照 onepiece = { "モンキー・D・ルフィ" : "https://1.bp.blogspot.com/-uxIsaN0S5lQ/X-FcrvAAInI/AAAAAAABdD4/6uw_qNUh9dQrG0aUzIExybt84yTEmXOPwCNcBGAsYHQ/s200/onepiece01_luffy.png", "ロロノア・ゾロ" : "https://1....</p></div><footer class=entry-footer><span title='2021-12-28 23:04:19 +0900 +0900'>12月 28, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Jupyter Notebook で画像をダウンロードすることなく、URLから参照してPandas DataFrame内部に表示させる" href=https://shunyaueta.com/posts/2021-12-28/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2021-07-09/images/1.png alt="Streamlit app screen shot"></figure><header class=entry-header><h2>How to get the uploaded file path and processing its file in Streamlit</h2></header><div class=entry-content><p>Motivation Streamlit is a powerful tools to quickliy build the demo application. If we use Streamlit file upload feature via WebBrowser then we need to its file path to process the uploaded file. So I will introduce how to get uploaed file path in Streamlit.
Example We buid the PDF File upload feature in Streamlit and its PDF file convert to image. We use Belval/pdf2image which is a populer PDF converting tool....</p></div><footer class=entry-footer><span title='2021-07-09 22:40:37 +0900 +0900'>7月 9, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to How to get the uploaded file path and processing its file in  Streamlit" href=https://shunyaueta.com/posts/2021-07-09/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2021-07-08/images/1.png alt="Streamlit スクリーンショット"></figure><header class=entry-header><h2>Streamlit でアップロードしたファイルのパスを取得して、特定の処理をする</h2></header><div class=entry-content><p>モチベーション Streamlit は Python code のみで簡単かつ高速に Web アプリを作成できる強力なパッケージ。 Streamplit で作られた Web アプリ経由でファイルをアップロードして、そのファイルを処理したい際の具体的な実現方法がなかったので備忘録がてら残しておく。
PDF ファイルをアップロードして、画像に変換する Web アプリ 具体的に例を交えつつ説明する。 Streamlit を使って、PDF ファイルをアップロードしてアップロードされた PDF ファイルを画像化するアプリを作成する。 今回は、Belval/pdf2image という PDF パッケージを使用する。 このパッケージは処理したい PDF のファイルパスを要求するインターフェースなので今回の実例に沿っていてわかりやすい。 ローカルマシンは MacOS を想定しており、pdf2image はpoppler の事前インストールが必須。
完成形のスクリーンショット GitHub でもコードを公開しておきました。
hurutoriya/streamlist-file-uploader-example
デモ動画はこちら
Demo Movie in Youtube
Makefile Makefile は依存パッケージを事前インストールするために採用
1 2 3 4 5 install: brew install poppler poetry install run: poetry run streamlit run streamlit_pdf_uploader/main.py Poetry for package management 環境構築は poetry を使っています。...</p></div><footer class=entry-footer><span title='2021-07-08 22:40:37 +0900 +0900'>7月 8, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Streamlit でアップロードしたファイルのパスを取得して、特定の処理をする" href=https://shunyaueta.com/posts/2021-07-08/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Poetry からsetup.py を自動生成する</h2></header><div class=entry-content><p>現状の Poetry では、pyproject.toml を基にした setup.py の直接的な自動生成をサポートしていない。
Support generation of poetry manged setup.py file #761
え？なんで setup.py が必要なんですか? poetry build で生成される source と wheels で事足りるんじゃないですかというツッコミがあると思います。
PyPI や Jflog などでホストせずに、GitHub のリポジトリでパッケージを管理したり、特定のサブディレクトリをパッケージとして扱う際には、未だ setup.py での依存関係の記述が必要です。
Poetry による実現方法 poetry build コマンドと Makefile を組み合わせることで、pyproject.toml に対応した setup.py の自動生成ができるのでそれを採用します。 コマンドはGitHub のissues でのコメントを参考にしました。 1
1 2 3 4 5 6 7 8 9 10 11 12 # package name PACKAGE = lib .PHONY: build-package build-package: ## Generate setup.py by poetry command for shared package poetry build # source ....</p></div><footer class=entry-footer><span title='2021-05-23 23:42:28 +0900 +0900'>5月 23, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Poetry からsetup.py を自動生成する" href=https://shunyaueta.com/posts/2021-05-23/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>KyTeaをPythonで扱えるMykyteaを使うために必要なこと</h2></header><div class=entry-content><p>テキスト解析機 KyTea KyTeaを業務で使う機会があり、Python wrapper である Mykytea を使ってみたのですが、poetry や pip で Mykytea をインストールするだけでは、
Library not loaded: /usr/local/lib/libkytea.0.dylib in version = "0.1.5"
上記のエラーが出力され、KyTea を使うことができませんでした。 Mykytea のリポジトリに issue 1 を立てて、@chezou さんにお聞きしてみたところ、
Good point. Mykytea wheel assumes that kytea is installed under /usr/local/lib, while your kytea exists another place. This should be Mykytea issue and there are two options we can avoid it like:
Use delocate, like Linux’s audit-wheel https://realpython.com/python-wheels/ Install from source using wheel instead....</p></div><footer class=entry-footer><span title='2021-05-19 22:04:41 +0900 +0900'>5月 19, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to KyTeaをPythonで扱えるMykyteaを使うために必要なこと" href=https://shunyaueta.com/posts/2021-05-19/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>pipenv でローカルパッケージが正常にインストールされないときの対処法</h2></header><div class=entry-content><p>TL; DR; pip install pipenv==2018.11.26 をすれば直った!!!!! 実行環境 1 2 $pipenv --version pipenv, version 2020.11.15 直面した問題 1 2 3 4 5 ./app/ ├── model │ └── setup.py └── serving └── Pipfile のような構成で、modelというローカルパッケージを作成しており、serving 直下の Pipfile は、model を読み込んで setup.py に記述されている依存パッケージもインストールするようにしたい。
serving ディレクトリで、以下のコマンドを入力すればローカルパッケージが pipenv によりインストールされるはずだが
1 pipenv install --editable ../model 依存関係をすべて記述するはずの Pipenv.lock には、modelのパスのみが記述され、ローカルパッケージが要求する依存パッケージが記述されていない。
原因を探してみたところ、
Installing a local package with pipenv install ‘-e .’ doesn’t save dependencies #1024
同じ GitHub issue を発見しダメ元で pipenv を以下のコマンドでダウングレードして見たところ
1 pip install pipenv==2018....</p></div><footer class=entry-footer><span title='2021-03-13 21:43:44 +0900 +0900'>3月 13, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to pipenv でローカルパッケージが正常にインストールされないときの対処法" href=https://shunyaueta.com/posts/2021-03-13/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>pip 実行時に sys.stderr.write(f"ERROR: {exc} ") とエラーが出てpipを実行できないときの対処方法</h2></header><div class=entry-content><p>新しいマシンでpipをセットアップして実行しようとすると
1 2 3 4 5 6 7 8 9 10 11 12 13 14 >> pip Traceback (most recent call last): File "/usr/local/bin/pip", line 11, in &lt;module> load_entry_point('pip==21.0', 'console_scripts', 'pip')() File "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py", line 489, in load_entry_point return get_distribution(dist).load_entry_point(group, name) File "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py", line 2843, in load_entry_point return ep.load() File "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py", line 2434, in load return self.resolve() File "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py", line 2440, in resolve module = __import__(self.module_name, fromlist=['__name__'], level=0) File "/Library/Python/2.7/site-packages/pip-21.0-py2.7.egg/pip/_internal/cli/main.py", line 60 sys....</p></div><footer class=entry-footer><span title='2021-02-08 23:29:12 +0900 +0900'>2月 8, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label='post link to pip 実行時に sys.stderr.write(f"ERROR: {exc} ") とエラーが出てpipを実行できないときの対処方法' href=https://shunyaueta.com/posts/2021-02-08/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>PythonでApache beam 入門</h2></header><div class=entry-content><p>TensorFlowの勉強をしていたら、Apache beam を前処理に採用していたケースがあり、興味を持ったので深堀りしてみます。
興味が湧いたモチベーションとしては、
データ量が増加しても前処理部分を難なくスケールできそう(前処理部分をスケールさせて高速に実験を回したい、並列化などはすべて良い感じにbeamに任せれそう バッチとストリーミングの両者に対応可能なので、柔軟な機械学習の推論サービスが提供できるのでは? (GCPの参考資料 Data preprocessing for machine learning: options and recommendations) Apache beam を触りつつ分散データ処理を学びたい https://github.com/jhuangtw/xg2xg#services
を見てみるとGoogle 内部のFlume という並列データパイプライン技術がApache beam として公開されているみたいです。
Apache beam について端的に説明すると
Apache beam は3つの考えを基礎にしています。
Unified ストリーミング、バッチの両者のケースに一つのプログラミングモデルで対応可能な統一性 Portable 実行パイプラインが複数の実行環境で実行可能な可搬性 Extensible 新しいSDK、IO Connectorsや変換ライブラリなどをを書いて共有することができる拡張性 Java, Python, Go やScalaを使ってBeam Model を作成して任意のruntime で実行する流れです。
自分はPythonが一眼手軽に書けるのでこの記事ではPythonで紹介していきます。
Version 2.14.0 からPython 3がサポートされたのは非常にありがたいですね。それまではPython 2のみをサポートしており、その影響で技術選定時に採用しづらかったのでは無いのでしょうか?
Spotify が作成しているApach BeamとDataflowのScala APIであるscioが開発されており、そちらも気になっています。
https://github.com/spotify/scio https://engineering.atspotify.com/2017/10/16/big-data-processing-at-spotify-the-road-to-scio-part-1/ では、まずは実際に動かしながら学んでみようということで
https://beam.apache.org/get-started/try-apache-beam/
を参考にApache Beam をPython SDKで試してみます
COLABで実行を試せるので便利ですね
ですが、Python2で実行されるように設定されているのでPython3で実行してみました。
実行したcolab のコードを見ていきます。
環境準備 apache-beam のinstallとGCSからApache beamで処理を行うテキストファイルをダウンロードします。...</p></div><footer class=entry-footer><span title='2020-12-26 00:41:30 +0900 +0900'>12月 26, 2020</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to PythonでApache beam 入門" href=https://shunyaueta.com/posts/2020-12-26/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>pandas を使って特定のディレクトリのCSVファイルをすべて連結して一つのCSVファイルを作成</h2></header><div class=entry-content><p>目的 複数の同じフォーマットの CSV ファイルが特定のディレクトリに配置されており、その CSV ファイル群を一つの CSV ファイルに連結したい
今回は、Python の Pandas と pathlib を使って上記の目的を実現します。
実行環境 1 2 3 4 5 6 7 In [1]: import pandas as pd In [2]: pd.__version__ Out[2]: '1.1.2 In [3]: import sys ...: print(sys.version) 3.8.2 (default, Jul 19 2020, 07:23:27) [Clang 11.0.3 (clang-1103.0.32.62)] 目的となる csv ファイルは tmp ディレクトリに以下のような形式で配置されているとする
1 2 3 4 tmp ├── 1.csv ├── 2.csv └── 3.csv 各ファイルはこのような形式で保存されています。
1 2 3 4 id name created 1 John 2020/09/10 2 bob 2020/09/10 3 taro 2020/09/11 以下の Python スクリプトを実行...</p></div><footer class=entry-footer><span title='2020-09-09 23:49:37 +0900 +0900'>9月 9, 2020</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to pandas を使って特定のディレクトリのCSVファイルをすべて連結して一つのCSVファイルを作成" href=https://shunyaueta.com/posts/2020-09-09/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Python の内包表記とジェネレータ式のメモリ使用量比較</h2></header><div class=entry-content><p>リストを構築する際に Python ではリスト内包表記とジェネレータ式の２種類が存在する。 今回、リスト構築時にメモリ使用量にどれだけ差異が発生するのか調査をしてみた。 メモリ使用量の調査には、memory_profilerというパッケージを使用した。
まず、２つのリストのデカルト積のタプルを表示するプログラムでの比較
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from memory_profiler import profile @profile def main(): """ Comparision List comprehension VS generator memory usage """ colors = "colors" * 1000 sizes = "S" * 100 for shirts in ((color, size) for color in colors for size in sizes): print(shirts) [print((color, size)) for color in colors for size in sizes] if __name__ == "__main__": main() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Filename: src/listcomp_vs_generator....</p></div><footer class=entry-footer><span title='2020-08-23 21:28:10 +0900 +0900'>8月 23, 2020</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Python の内包表記とジェネレータ式のメモリ使用量比較" href=https://shunyaueta.com/posts/2020-08-23/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>AOJの「ITP I」40問をPythonで解いた</h2></header><div class=entry-content><p>はじめに コーディングの腕をもっと磨きたいなと思ったので、以下の記事を参考に始めてみた
https://qiita.com/e869120/items/f1c6f98364d1443148b3 全部で 44 問ありますが、最後の 4 問は競プロとはあまり関係ないので、ITP1_1-A から ITP1_10-D までの 40 問を解くことをお勧めします。
まずは最初におすすめされた、AOJ の ITP1_1-A から ITP1_10-D までの 40 問を解いてみた 無料でこのサービスが提供されてるの素晴らしい 標準入力、出力の整形が少し手間取ったけど、あとは愚直に解いていった
http://judge.u-aizu.ac.jp/onlinejudge/ 感想としては、
やってみたら、意外と楽しい。特に自分で諦めずに試行錯誤して、オンラインで一発で AC もらえるとめちゃくちゃ嬉しい テストケースに通る、すなわち正しい、それが書けたら達成感がある 何かしらのお題に沿って、コードを書くという動機ができるので、書くことに慣れたい場合も有用そう toggl で時間計測しながら、やって見直してみたら 15h46m 費やしていた。大体 1 問 25m くらい
次の目標、
AtCoder で水色を目指す!!! データ構造周りや、アルゴリズム周りはまだまだ弱いのでそこらへんを抑えていきたい 当面は、以下の２つに投資していきます
機械学習だけに縛られない、SWE としてスキル底上げ 機械学習関係の確固たる基礎知識と実装力 以下に自分が書いた回答例を放流しておきます。
Rule 15 分試行錯誤しても、緒がわからない場合は諦める わからなかったとき、もっと上手な書き方は以下を参考にしました https://qiita.com/cmtennis1042/items/5f1e7f071081176e857f ITP1_1_A: Hello World 1 print('Hello world') ITP1_1_B: X Cubic 1 2 x = input() print(x ** 3) ITP1_1_C: Rectangle 1 2 a, b = map(int, input()....</p></div><footer class=entry-footer><span title='2020-08-04 03:38:58 +0900 +0900'>8月 4, 2020</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to AOJの「ITP I」40問をPythonで解いた" href=https://shunyaueta.com/posts/2020-08-04/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>How to write the UnitTest with stdin at Pytest</h2></header><div class=entry-content><p>If you want to write UnitTest when using stdin in Python. Pytest provide setattr function in monkeypatch
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from io import StringIO import sys def divide(): input = sys.stdin.readline return list(input()) def gather(): input = sys.stdin.readline return sum(list(map(int, input().split()))) def test_divide(monkeypatch): monkeypatch.setattr('sys.stdin', StringIO('abc')) assert divide() == ['a', 'b', 'c'] def test_gather(monkeypatch): monkeypatch....</p></div><footer class=entry-footer><span title='2020-07-25 03:18:14 +0900 +0900'>7月 25, 2020</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to How to write the UnitTest with stdin at Pytest" href=https://shunyaueta.com/posts/2020-07-25/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>自走プログラマーを読み終えた</h2></header><div class=entry-content><p>自走プログラマーを読み終えた。
読み始めたきっかけとして、自分は機械学習エンジニアとして現在働いているが、できることの幅を広げるために最近はソフトウェアエンジニアとしてのスキルをもっと伸ばしたいと考えている。
自走プログラマーは、Python を使ったアプリケーション開発のアンチパターンとベストプラクティスを例示して学ぶことができる書籍で、今回の自分の状況にすごくフィットしていて楽しく学習することができた。
Python 独特のはまりどころは、Kindle: The Hitchhiker’s Guide to Python, The Hitchhiker’s Guide to Python でも数多く参照されていて、こっちも後から読んでおきたいなと思いました。
次は、ちゃんとした Pythonista になれるように、Fluent Python を読みます。@ynqa さん、以前この本を教えて下さり、ありがとうございました。
長らく積ん読になっていますが、毎日読み進めていきます。
20 歳頃の寝る間を惜しんで、ウェブアプリを開発していたときのワクワク感が徐々に蘇ってきた気がしています。
ある程度書けるようになってきたら、なにかアプリとか作って公開したいなと思っています！</p></div><footer class=entry-footer><span title='2020-05-10 17:13:34 +0900 +0900'>5月 10, 2020</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 自走プログラマーを読み終えた" href=https://shunyaueta.com/posts/2020-05-10/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://shunyaueta.com/tags/python/page/2/>次へ&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2023 <a href=https://shunyaueta.com/>hurutoriya</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>