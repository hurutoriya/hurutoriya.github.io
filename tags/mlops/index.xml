<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MLOps on Software Engineer as Data Scientist</title><link>https://shunyaueta.com/tags/mlops/</link><description>Recent content in MLOps on Software Engineer as Data Scientist</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sat, 25 Apr 2020 01:35:20 +0900</lastBuildDate><atom:link href="https://shunyaueta.com/tags/mlops/index.xml" rel="self" type="application/rss+xml"/><item><title>The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction</title><link>https://shunyaueta.com/posts/2020-04-25/</link><pubDate>Sat, 25 Apr 2020 01:35:20 +0900</pubDate><guid>https://shunyaueta.com/posts/2020-04-25/</guid><description>[抄訳] What’s your ML test score? A rubric for ML production systemsで紹介した論文の続編があったので読んだ
The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction 大規模なオフラインの機械学習実験は注目されているが、反対にオンラインでの信頼性がある機械学習システムの開発は難しく、技術的負債が溜まりやすい
図1 では、左側が伝統的なシステムのテストとモニタリング、右側は、機械学習システムのテストとモニタリングである 機械学習システムのテストとモニタリングが複雑になる要因として、コードだけではなく、動的に決定されるデータの質とモデルの多種多様な設定に依存するからである
TL; DR; 機械学習システムの信頼性を評価する28の実行可能なテスト項目とスコアリング方法を提案する。 各項目は前回抄訳した論文 が基となっている
Google 内部の調査では、調査対象の 80%のチームが機械学習システムへのテストを行っていなかった。(エンジニアリングに長けている Google 内部でさえも十分に行われてはいない)
TEST FOR DATA AND FEATURES Data 1: 期待する特徴量は全てスキーマで管理され、読み込み可能か? How: このスキーマは学習データの統計値を計算することに有用である。これらを可視化することで事前のバイアスを検知したり、Fasets1などのツールを用いた統計値の可視化もとても有用である Data 2: 全ての特徴量は有用か? 全ての特徴量は、追加すればするほどコストになる。独立した全ての特料料は有用だろうか? How: 特徴量削除や目的変数と特徴量の相関などを見てみよう Data 3: 特徴量のコストは高くないか? How: 推論速度や RAM の使用率だけを見るのではなく、その特徴量が依存先や不安定性なども考慮する Data 4: 特徴量は要件に準拠しているか?</description></item><item><title>[抄訳] What’s your ML test score? A rubric for ML production systems</title><link>https://shunyaueta.com/posts/2020-04-19/</link><pubDate>Sun, 19 Apr 2020 22:18:10 +0900</pubDate><guid>https://shunyaueta.com/posts/2020-04-19/</guid><description>NIPS206にて開催された Reliable Machine Learning in the Wild - NIPS 2016 Workshop (2016) という、現実世界でどうやって信頼性の高い機械学習に取り組んでいくかについてのワークショップがある
ここで Google から発表された What’s your ML test score? A rubric for ML production systems がとても面白く、身になるものが多かったのでメモがてら抄訳を残しておく
PDF Slide 発表動画もワークショップページにて公開されています。 概略 現実世界のプロダクションシステムで機械学習を使う際に、機械学習の研究実験と異なる、小さなもしくは小さくない問題がある テストとモニタリングはプロダクションレディの機械学習システムにとって必要不可欠 しかし、どれくらいのテストとモニタリングをすれば十分と言えるのだろうか? この論文では、それらの問題を解決する ML Test Score という基準を提案する Introduciton Google 内で積み重ねたベストプラクティスをもとに、実行可能なテスト、そしてその機械学習システムがどのていどプロダクションレディなのかを示すスコアシステムを提案
このスコアは、機械学習を始めたばかるのチームからエキスパートがあつまるチームまで幅広く適用可能
注意: 一般的なSofware Engineering のベストプラクティスは含んでいない
そのかわり、学習とサービングのためのUnit Test Coverage の計算方法など機械学習に必要不可欠な点を抑えている
ML Test Score の計算方法 各テストの加点基準 1pt: 手動で実行し、その結果を文章として共有済 2pt: CIに組み込まれ、自動的に反復実行済 最終的なML Score は以下の基準となる 0pt : プロダクション向けというよりも研究プロジェクト 1-2pt : テストが少しはされているが、プロダクションではもっと深刻な罠がある可能性あり 3-4pt : 最初のプロダクションレディへの第一歩。しかし、さらなる投資が必要 5-6pt : 適切なテストがされているが、もっと自動化してみよう 7-10pt : 自動化されたテストとモニタリングが整備されている。重要なシステムでも適切なレベルに達している 12+pt : 卓越した自動化されたテストとモニタリング 前提条件 システムアーキテクチャの前提として、生データから特徴量を抽出し、学習システムに流し込まれる。そして推論のためにサービングされ、その機能は顧客に影響を与える。また、ソースリポジトリやCIを通したテスト、実験のバージョン管理なども可能 特徴量とデータセット 各特徴量の分布が期待した値になっているか?</description></item><item><title>Google, Facebookが提供する機械学習基盤まとめ</title><link>https://shunyaueta.com/posts/2018-04-09/</link><pubDate>Mon, 09 Apr 2018 13:55:44 +0000</pubDate><guid>https://shunyaueta.com/posts/2018-04-09/</guid><description>Google, Facebook の機械学習基盤情報をまとめました
TFX
社内の勉強会で Google, Facebook が提供する機械学習基盤に関する論文を紹介したので、その資料を公開します#### 経緯
機械学習をサービスとして提供開始すると、継続的な学習やプロダクション環境での機械学習の提供はモデル構築以外にもいろいろと考える問題が多くなります ¹
[1] Hidden technical debt in Machine learning systems NIPS’15
要するに機械学習をサービスとして届けるには、実はめちゃんこ大変なんだよという話なんですが、みんな同じ問題にぶち当たります。
そのためプロダクションレディなレベルで機械学習を提供できるプラットフォームを各社が提案しておりその中でも Google, Facebook の事例を提供します。
TL; DR; FBLearner: MLaaS の事例として最初に読むべき論文、MLaaS をどのような戦略で提供しているかを抽象的にまなべるため、鳥瞰図として読みましょう TFX は逆に機械学習基盤が必要とする技術スタックや要件などを詳細に説明しており、教科書的な立ち位置です。社内で機械学習基盤を内製したい場合に詳しく読み込むこと必須 両社とも MLaaS をスケーラブルに提供する環境ができており、サービスのコアテクノロジーになっている Google : Tensorflow Expand TFX: A tensor flow-based production-scale machine learning platform
Facebook : FBLeaner Applied machine learning at facebook a datacenter infrastructure perspective HPCA18
ONNX を見ていると TensorFlow VS. ONNX 陣営が出来つつありどちらのプラットフォームに乗るかの戦略が大事だなと思います。
Google はモバイル上の ML から超大規模分散学習まで TF シリーズで提供、実際のサービング環境も TFServing というスタックを提供しはじめていて個人的に TensorFlow が何歩も先にいっているという所感</description></item></channel></rss>