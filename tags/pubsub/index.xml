<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>pubsub on Shunya Ueta</title><link>https://shunyaueta.com/tags/pubsub/</link><description>Recent content in pubsub on Shunya Ueta</description><generator>Hugo -- gohugo.io</generator><language>ja</language><lastBuildDate>Fri, 05 Nov 2021 23:05:22 +0900</lastBuildDate><atom:link href="https://shunyaueta.com/tags/pubsub/index.xml" rel="self" type="application/rss+xml"/><item><title>Google Cloud Pub/Sub に公開された結果をDataflow template を使えばめちゃくちゃ簡単に確認できる</title><link>https://shunyaueta.com/posts/2021-11-05/</link><pubDate>Fri, 05 Nov 2021 23:05:22 +0900</pubDate><guid>https://shunyaueta.com/posts/2021-11-05/</guid><description>PubSubに出力された結果を確認するのって、なかなか手間がかかりませんか?
最近同僚に簡単な確認方法を教えてもらい、感動したのでそれを記事にしました。
確認方法 GCP のPub/Sub ページに移動する 確認したい Pub/Sub topic をクリックする ページ下部にある CREATE SUBSCRIPTION ボタンを押すと選択肢で、Create subscription, Export to BigQuery, Export to Cloud Storageがあり、 Export to Cloud Storageを選択する。 BigQuery、 Google Cloud Storage への吐き出しを行い際に、自動的に subscription が生成される。 Export to Cloud Storage を選択すると、Text 形式か Abro 形式での出力にするかを選択できる。基本的には簡単に確認できる Text 形式を選ぶと良さげ。 選択後、下記のような設定画面が出てくるので情報を埋めていく。基本的には、どこの Google Cloud Storage に出力するかを埋めれば完了。 10mほどすると Streaming job のDataflowの起動が完了して、一定期間ごとにPub/Sub のtopic に公開されたデータがテキスト形式で出力され始めます。 出力されたGCSの結果を眺めるには、 gsutil コマンドなどを使うのが簡単です。自分はgsutil cat の結果をコピーしてVS Codeで確認しています。 Cloud Dataflow のテンプレート機能については、端的に説明すると、GUIでパラメータを設定するだけで、Dataflowによるデータ処理が簡単に実行できるようになる機能です。
詳しくは、 GCPUGでのCloud Dataflow がテンプレートにより気軽に使えるサーバーレスのサービスに進化した話 の記事がわかりやすいのでごらんください。</description></item></channel></rss>