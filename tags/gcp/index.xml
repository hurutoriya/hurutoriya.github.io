<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>gcp on Shunya Ueta</title><link>https://shunyaueta.com/tags/gcp/</link><description>Recent content in gcp on Shunya Ueta</description><generator>Hugo -- gohugo.io</generator><language>ja</language><lastBuildDate>Tue, 07 Sep 2021 12:22:16 +0900</lastBuildDate><atom:link href="https://shunyaueta.com/tags/gcp/index.xml" rel="self" type="application/rss+xml"/><item><title>gcloud commands で PubSub に jsonファイルをメッセージとして公開 (Pusblish) する</title><link>https://shunyaueta.com/posts/2021-09-07/</link><pubDate>Tue, 07 Sep 2021 12:22:16 +0900</pubDate><guid>https://shunyaueta.com/posts/2021-09-07/</guid><description>gcloud commands で PubSub に jsonファイルをメッセージとして公開 (Pusblish) する
jq コマンドが必要になるが、一番簡単に実現できるのは
1 $ gcloud pubsub topics publish ${PUBSUB_TOPIC_NAME} --message &amp;#34;$(cat {FILE_NAME} | jq -c)&amp;#34; jq コマンドの -c オプションは compact-output を意味している。デフォルトだと pretty-prints になってしまう。 それを避けるために-cオプションを使用している。
ref Publishing messages to topics Read a txt file JSON data to publish the messages in Cloud Pub Sub</description></item><item><title>GKE 上にて Pythonで logger.info() を行うとCloud logging では stderr に保存され、すべてエラーになる問題への対処法</title><link>https://shunyaueta.com/posts/2021-03-03/</link><pubDate>Wed, 03 Mar 2021 00:29:03 +0900</pubDate><guid>https://shunyaueta.com/posts/2021-03-03/</guid><description>Python のアプリケーションで、Cloud logger にログを出力したいときに
標準のPython logging モジュールを利用して、ログを出力する Python Cloud Logging package を使用する 上記の２つの方法があります。
不必要にパッケージを増やしたくはないので、1の標準モジュールでCloud Logger へ出力できないか試してみました。
標準のPython logging モジュールを試す 標準のlogging モジュールでログを出力したいときに
1 2 3 4 5 6 import logging logger = logging.getLogger(__name__) def hoge(): logger.info(&amp;#39;logging Start 2021&amp;#39;) と、logging.info() を仕込んで、Cloud logger にログを出力してみると、logger.info() で出しているはずなのに、Cloud logger 上ではすべてエラーとして扱われてしまっています。
原因を特定するために、logger のログを見てみると logger.info() がすべて stderr標準エラーストリームへ出力されてしまっています。
1 2 3 4 5 6 7 8 9 10 11 12 { &amp;#34;textPayload&amp;#34;: &amp;#34;2021-02-20 21:26:51,012 - root:predict:36 - INFO: logging Start\n&amp;#34;, .</description></item><item><title>pandas.read_gbq を使わずに、Google BigQueryから高速にData ETL</title><link>https://shunyaueta.com/posts/2019-10-03/</link><pubDate>Thu, 03 Oct 2019 23:52:54 +0900</pubDate><guid>https://shunyaueta.com/posts/2019-10-03/</guid><description>pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter 上でさっと動き、Google Big Query が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter と Google BQ を連携させたいときはいつも使っています
問題点 そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが半端ない 解決方法として、Google Big Query で巨大なデータをダウンロードする方法について書く
実は Google の公式ドキュメントでも推奨されています
https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas 方法は以下の２つ。
google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー 1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 1 2 pip install --upgrade google-cloud-bigquery[bqstorage,pandas] magic command を実行</description></item><item><title>How to connect the Google Compute Engine via Visual Studio Code</title><link>https://shunyaueta.com/posts/2019-09-24/</link><pubDate>Tue, 24 Sep 2019 17:35:05 +0900</pubDate><guid>https://shunyaueta.com/posts/2019-09-24/</guid><description>1. Generate SSH config file using gcloud command line gcloud compute config-ssh https://cloud.google.com/sdk/gcloud/reference/compute/config-ssh
You cant get ssh config for your Google Compute Engine project!
Notice: you need choose target GCP project before run below command.
gcloud config set project &amp;lt;your-project-id&amp;gt; 2. Install Remote SSH extention in Visual Studio Code. https://code.visualstudio.com/blogs/2019/07/25/remote-ssh 3. Press ⇧⌘P &amp;amp; Select target connection in Visual Studio Code! Finaly you can connect in Visual Studio Code.</description></item></channel></rss>