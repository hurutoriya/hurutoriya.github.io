<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>airflow on Shunya Ueta</title><link>https://shunyaueta.com/tags/airflow/</link><description>Recent content in airflow on Shunya Ueta</description><generator>Hugo -- gohugo.io</generator><language>ja</language><lastBuildDate>Mon, 04 Oct 2021 22:23:24 +0900</lastBuildDate><atom:link href="https://shunyaueta.com/tags/airflow/index.xml" rel="self" type="application/rss+xml"/><item><title>CloudComposer のDAGをCircleCIで更新する</title><link>https://shunyaueta.com/posts/2021-10-04/</link><pubDate>Mon, 04 Oct 2021 22:23:24 +0900</pubDate><guid>https://shunyaueta.com/posts/2021-10-04/</guid><description>Cloud Composer(Airflow) のDAGをGitHub リポジトリで管理して、CIによりリポジトリで管理しているDAGをPull RequestがマージされるとCloud ComposerのDAGへ同期する方法について説明する。
DAGは、ルートディレクトリ直下の dags/ というディレクトリに格納されている状態を前提とする。
以下の２つのコマンドラインツールを利用して実現できる。
Service Account の認証のために gcloud DAGの同期のために gsutil CircleCI によるワークフローの記述例は以下のとおり
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 version:2.1jobs:rsync-dags:working_directory:~/workspacedocker:- image:gcr.io/google.com/cloudsdktool/cloud-sdk:alpineenvironment:GOOGLE_APPLICATION_CREDENTIALS:/gcp-service-key.jsonsteps:- checkout- run:name:SyncDAGfoldertoGCS&amp;#39;sDAGfoldercommand:| echo &amp;#34;${CLOUD_COMPOSER_CREDENTIALS_JSON}&amp;#34; &amp;gt; ${GOOGLE_APPLICATION_CREDENTIALS}gcloudauthactivate-service-account--key-file${GOOGLE_APPLICATION_CREDENTIALS}gsutil-mrsync-d-rdags\&amp;#34;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=&amp;#34;get(config.dagGcsPrefix)&amp;#34;)&amp;#34;workflows:rsync_dags:jobs:- rsync-dags Sync DAG folder to GCS's DAG folder で行っている内容を順を追って説明する。
サービスアカウントのクレデンシャルファイルをCircleCIの環境変数として保存しておき、リダイレクトによりJSONファイルとして書き出す 1 echo &amp;#34;${CLOUD_COMPOSER_CREDENTIALS_JSON}&amp;#34; &amp;gt; ${GOOGLE_APPLICATION_CREDENTIALS} 注意: セキュリティ対策としてクレデンシャルファイルは必ず環境変数として扱う。</description></item><item><title>GCPのCloud Composer のDAGを素早く・簡単にデバッグする</title><link>https://shunyaueta.com/posts/2021-09-29/</link><pubDate>Wed, 29 Sep 2021 22:20:23 +0900</pubDate><guid>https://shunyaueta.com/posts/2021-09-29/</guid><description>GCPでAirflow をマネージドサービスで使えるサービスで Cloud Composer が存在する。 BigQueryやBigTable, PubSub などGCPの各サービスをDAGとして定義してジョブを定期実行できるので非常に便利だが、その代わりDAGを実行するまで結果がわからないので、CloudComposer を一度実行するしか無いのでデバッグが困難になる傾向がある。
また、GitHubのリポジトリにDAGを保存して、CIでCloud Composerを更新するようしていると PRを都度作ってマージされないと確認できないという場合もある
ローカルでDocker で走らせれば良いのじゃないかというツッコミがあると思いますが、結局 Cloud Composer 上での動作を確かめないといけないので、今回の記事を書くことにしました。
NOTE: 自分が使用しているComposerのversionはcomposer-1.8.0-Airflow-1.10.3 です。基本的にやれることは一緒だと思います。また、dev, prodで同一のDAGが走るCloud Composer を運用しているという前提です。
アプローチは２つ
logger.info() を仕込んで、DAGのなかで何が起こっているかを理解する 1 2 3 4 5 import logging logger = logging.getLogger(__name__) logger.info() loggerをDAGを記述した Pythonファイルに仕込んで、内部で何が起こっているかを把握する。
各DAGのlogは、
GCPのCloud ComposerのページにアクセスしてAirflow webserver 列のボタンをクリックしてAirflowのWeb applicaiton にログイン 確認したいDAGをクリック DAG内のtask をクリックして表示されるモーダル内の View Logをクリックすると、loggerの情報が確認できる gstuil rsync コマンドでのGCSへのDAGの同期 gstuil rsyncコマンドを使うことで、リポジトリのDAGファイルをGCSに格納されている開発環境上のCloudComposer のDAGファイルに直接同期してPull Request マージ後のDAGの挙動を確認できる。
Cloud Composer のDAGは、自動作成されたGoogle Cloud Storage(GCS)に格納されており、GCSをCloud Composerが定期的に監視してCloud Composerを更新している。 つまり、GCS上のDAGファイルを直接更新してやるとそれがCloud Composerに反映される。 体感として2-3分に一度は監視されているので、ほぼ待ち状態がない状態で確認できて便利です。</description></item></channel></rss>