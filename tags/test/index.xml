<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Test on Software Engineer as Data Scientist</title><link>https://shunyaueta.com/tags/test/</link><description>Recent content in Test on Software Engineer as Data Scientist</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sun, 19 Apr 2020 22:18:10 +0900</lastBuildDate><atom:link href="https://shunyaueta.com/tags/test/index.xml" rel="self" type="application/rss+xml"/><item><title>[抄訳] What’s your ML test score? A rubric for ML production systems</title><link>https://shunyaueta.com/posts/2020-04-19/</link><pubDate>Sun, 19 Apr 2020 22:18:10 +0900</pubDate><guid>https://shunyaueta.com/posts/2020-04-19/</guid><description>NIPS206にて開催された Reliable Machine Learning in the Wild - NIPS 2016 Workshop (2016) という、現実世界でどうやって信頼性の高い機械学習に取り組んでいくかについてのワークショップがある
ここで Google から発表された What’s your ML test score? A rubric for ML production systems がとても面白く、身になるものが多かったのでメモがてら抄訳を残しておく
PDF Slide 発表動画もワークショップページにて公開されています。 概略 現実世界のプロダクションシステムで機械学習を使う際に、機械学習の研究実験と異なる、小さなもしくは小さくない問題がある テストとモニタリングはプロダクションレディの機械学習システムにとって必要不可欠 しかし、どれくらいのテストとモニタリングをすれば十分と言えるのだろうか? この論文では、それらの問題を解決する ML Test Score という基準を提案する Introduciton Google 内で積み重ねたベストプラクティスをもとに、実行可能なテスト、そしてその機械学習システムがどのていどプロダクションレディなのかを示すスコアシステムを提案
このスコアは、機械学習を始めたばかるのチームからエキスパートがあつまるチームまで幅広く適用可能
注意: 一般的なSofware Engineering のベストプラクティスは含んでいない
そのかわり、学習とサービングのためのUnit Test Coverage の計算方法など機械学習に必要不可欠な点を抑えている
ML Test Score の計算方法 各テストの加点基準 1pt: 手動で実行し、その結果を文章として共有済 2pt: CIに組み込まれ、自動的に反復実行済 最終的なML Score は以下の基準となる 0pt : プロダクション向けというよりも研究プロジェクト 1-2pt : テストが少しはされているが、プロダクションではもっと深刻な罠がある可能性あり 3-4pt : 最初のプロダクションレディへの第一歩。しかし、さらなる投資が必要 5-6pt : 適切なテストがされているが、もっと自動化してみよう 7-10pt : 自動化されたテストとモニタリングが整備されている。重要なシステムでも適切なレベルに達している 12+pt : 卓越した自動化されたテストとモニタリング 前提条件 システムアーキテクチャの前提として、生データから特徴量を抽出し、学習システムに流し込まれる。そして推論のためにサービングされ、その機能は顧客に影響を与える。また、ソースリポジトリやCIを通したテスト、実験のバージョン管理なども可能 特徴量とデータセット 各特徴量の分布が期待した値になっているか?</description></item></channel></rss>