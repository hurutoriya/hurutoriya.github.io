<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>pandas on hurutoriya</title><link>https://shunyaueta.com/tags/pandas/</link><description>Recent content in pandas on hurutoriya</description><generator>Hugo -- gohugo.io</generator><language>ja</language><copyright>© Shunya Ueta</copyright><lastBuildDate>Wed, 09 Sep 2020 23:49:37 +0900</lastBuildDate><atom:link href="https://shunyaueta.com/tags/pandas/index.xml" rel="self" type="application/rss+xml"/><item><title>pandas を使って特定のディレクトリのCSVファイルをすべて連結して一つのCSVファイルを作成</title><link>https://shunyaueta.com/posts/2020-09-09/</link><pubDate>Wed, 09 Sep 2020 23:49:37 +0900</pubDate><guid>https://shunyaueta.com/posts/2020-09-09/</guid><description>目的 複数の同じフォーマットのCSVファイルが特定のディレクトリに配置されており、そのCSVファイル群を一つのCSVファイルに連結したい
今回は、PythonのPandas とpathlibを使って上記の目的を実現します。
実行環境 In [1]: import pandas as pd In [2]: pd.__version__ Out[2]: &amp;#39;1.1.2 In [3]: import sys ...: print(sys.version) 3.8.2 (default, Jul 19 2020, 07:23:27) [Clang 11.0.3 (clang-1103.0.32.62)] 目的となるcsvファイルは tmp ディレクトリに以下のような形式で配置されているとする
tmp ├── 1.csv ├── 2.csv └── 3.csv 各ファイルはこのような形式で保存されています。
id name created 1 John 2020/09/10 2 bob 2020/09/10 3 taro 2020/09/11 以下のPythonスクリプトを実行
import pathlib import pandas as pd def contcat_csv(f_path:str): # pathlibのitedir()で対象とするディレクトリのCSVファイル一覧をジェネレーターとして取得 csvs = [pd.read_csv(str(path)) for path in pathlib.</description></item><item><title>pandas.read_gbq を使わずに、Google BigQueryから高速にData ETL</title><link>https://shunyaueta.com/posts/2019-10-03/</link><pubDate>Thu, 03 Oct 2019 23:52:54 +0900</pubDate><guid>https://shunyaueta.com/posts/2019-10-03/</guid><description>pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter 上でさっと動き、Google Big Query が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter と Google BQ を連携させたいときはいつも使っています
問題点 そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが半端ない 解決方法として、Google Big Query で巨大なデータをダウンロードする方法について書く
実は Google の公式ドキュメントでも推奨されています
https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas 方法は以下の２つ。
google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー 1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 pip install --upgrade google-cloud-bigquery[bqstorage,pandas] magic command を実行</description></item></channel></rss>