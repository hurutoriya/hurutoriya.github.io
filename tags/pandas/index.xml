<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>pandas on 🦅 hurutoriya</title><link>https://shunyaueta.com/tags/pandas/</link><description>Recent content in pandas on 🦅 hurutoriya</description><image><url>https://shunyaueta.com/ogp.jpg</url><link>https://shunyaueta.com/ogp.jpg</link></image><generator>Hugo -- gohugo.io</generator><language>ja</language><lastBuildDate>Tue, 10 May 2022 22:00:23 +0900</lastBuildDate><atom:link href="https://shunyaueta.com/tags/pandas/index.xml" rel="self" type="application/rss+xml"/><item><title>社内でデータ分析結果を可視化・共有する際に Google Colab が便利</title><link>https://shunyaueta.com/posts/2022-05-10-2200/</link><pubDate>Tue, 10 May 2022 22:00:23 +0900</pubDate><guid>https://shunyaueta.com/posts/2022-05-10-2200/</guid><description>社内でデータ分析のレポートを書く際は Google Colab がとても便利な事に気がついた。
Google Bigquery でデータを抽出、Google Sheets で可視化 従来だと、自分がやっていた方法として、
Google BQ などで分析対象結果のデータを抽出 その結果を Google Spread Sheet として保存して、Google Sheets の機能で可視化。元の SQL のコードは、別シートを作ってそこに貼り付けておく。 利点としては、一度データソースを抽出した後は、Google Sheets で二次加工するさいに便利。 また、 Produc Manager が共有後に自分が出したい数値を Google Sheets を元にさっと計算することもできる。 だが、二次加工が便利なのはいいが、大抵の可視化ってパターンが決まっているので、SQL 取得とその可視化を一気通貫でできないかなと考えていた。
なにか良い方法無いかなと思っている矢先に、別のチームの同僚が、Google Colab を使って、BQ を dataframe として保存後 matplotlib で可視化しているのを見かけて、
求めていたのは&amp;hellip;こ、これだ&amp;hellip;. となり、速攻取り入れました。
良いと思ったところは積極的に真似するスタイル
Google Colab なら、データの取得・加工・可視化までを完結可能 Google Colab の利点を列挙しておくと
SQL のコード、データ抽出や可視化のロジックなどが Python で記述可能かつ、Google Colab で完結 matplotlib で可視化できるので、見やすく美しい可視化を簡単に行える。 そしてそのコードは他のデータ分析でも再利用可能 pandas dataframe で Google BQ からデータを取得するので、Standard SQL だけでは難しい計算も pandas、 numpy や scipy などを使ってデータ加工が簡単にできるのも、便利 Google Sheets 同様、簡単に社内で共有できる Markdown も Google Colab 内で書けるので、凝った文章などもいれてレポートも書ける マジックコマンドで、Google BQ の結果を dataframe として保存1したり、</description></item><item><title>Jupyter Notebook で Pandas DataFrame 内部にURLから画像を参照することで、画像をダウンロードすることなく表示させる</title><link>https://shunyaueta.com/posts/2021-12-28/</link><pubDate>Tue, 28 Dec 2021 23:04:19 +0900</pubDate><guid>https://shunyaueta.com/posts/2021-12-28/</guid><description>データ分析などをしていると、画像はダウンロードせずに特定の CDN (GCP なら GCS, AWS なら S3 など)で提供されている画像を参照して、 Jupyter Notebook 上で良い感じに表示させたいときがありませんか?
例えば、画像と説明文がペアになっているデータを画像自体はダウンロードせずに Jupyter 上で画像と説明文を DataFrame として表示させたいときが多々ある。 元の画像自体は CDN に格納されていて、画像をダウンロードする必要はなく参照するだけのときにはすごく便利。 毎度画像を CDN からダウンロードするのも無駄なので、画像を加工せずに Jupyter 上で表示するだけなら、この方法がベストですね。
url からとってきた画像を jupyter に表示する でも同じような課題に取り組んでいるが、今回紹介する方法なら余計なパッケージを入れずに最小構成で Jupyter 上で表示できるのが利点。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import pandas as pd from IPython.display import HTML # NOTE: https://www.irasutoya.com/2021/01/onepiece.html から画像を参照 onepiece = { &amp;#34;モンキー・D・ルフィ&amp;#34; : &amp;#34;https://1.bp.blogspot.com/-uxIsaN0S5lQ/X-FcrvAAInI/AAAAAAABdD4/6uw_qNUh9dQrG0aUzIExybt84yTEmXOPwCNcBGAsYHQ/s200/onepiece01_luffy.png&amp;#34;, &amp;#34;ロロノア・ゾロ&amp;#34; : &amp;#34;https://1.</description></item><item><title>pandas を使って特定のディレクトリのCSVファイルをすべて連結して一つのCSVファイルを作成</title><link>https://shunyaueta.com/posts/2020-09-09/</link><pubDate>Wed, 09 Sep 2020 23:49:37 +0900</pubDate><guid>https://shunyaueta.com/posts/2020-09-09/</guid><description>目的 複数の同じフォーマットの CSV ファイルが特定のディレクトリに配置されており、その CSV ファイル群を一つの CSV ファイルに連結したい
今回は、Python の Pandas と pathlib を使って上記の目的を実現します。
実行環境 1 2 3 4 5 6 7 In [1]: import pandas as pd In [2]: pd.__version__ Out[2]: &amp;#39;1.1.2 In [3]: import sys ...: print(sys.version) 3.8.2 (default, Jul 19 2020, 07:23:27) [Clang 11.0.3 (clang-1103.0.32.62)] 目的となる csv ファイルは tmp ディレクトリに以下のような形式で配置されているとする
1 2 3 4 tmp ├── 1.csv ├── 2.csv └── 3.csv 各ファイルはこのような形式で保存されています。</description></item><item><title>遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む</title><link>https://shunyaueta.com/posts/2019-10-03/</link><pubDate>Thu, 03 Oct 2019 23:52:54 +0900</pubDate><guid>https://shunyaueta.com/posts/2019-10-03/</guid><description>pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。
問題点 そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い 解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。
実は Google の公式ドキュメントでも推奨されています。
https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas 方法は以下の２つ。
google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー 1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 1 pip install --upgrade google-cloud-bigquery[bqstorage,pandas] magic command を実行</description></item></channel></rss>