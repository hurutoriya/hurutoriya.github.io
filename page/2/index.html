<!doctype html><html lang=ja dir=auto><head><meta name=generator content="Hugo 0.111.3"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>hurutoriya</title><meta name=description content="Shunya Ueta's blog"><meta name=author content="Shunya Ueta"><link rel=canonical href=https://shunyaueta.com/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><link rel=icon href=https://shunyaueta.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shunyaueta.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shunyaueta.com/favicon-32x32.png><link rel=apple-touch-icon href=https://shunyaueta.com/apple-touch-icon.png><link rel=mask-icon href=https://shunyaueta.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://shunyaueta.com/index.xml><link rel=alternate type=application/json href=https://shunyaueta.com/index.json><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-JMJRQJT0Q3"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JMJRQJT0Q3")</script><meta property="og:title" content><meta property="og:description" content="Shunya Ueta's blog"><meta property="og:type" content="website"><meta property="og:url" content="https://shunyaueta.com/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content><meta name=twitter:description content="Shunya Ueta's blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"hurutoriya","url":"https://shunyaueta.com/","description":"Shunya Ueta\u0026#39;s blog","thumbnailUrl":"https://shunyaueta.com/favicon.ico","sameAs":["https://github.com/hurutoriya","https://twitter.com/hurutoriya","https://mstdn.jp/web/@hurutoriya","https://www.linkedin.com/in/hurutoriya/","https://www.buymeacoffee.com/hurutoriya","index.xml"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://shunyaueta.com/ accesskey=h title="hurutoriya (Alt + H)">hurutoriya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://shunyaueta.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://shunyaueta.com/about title=About><span>About</span></a></li><li><a href=https://searchengineeringnewsletter.substack.com/ title=Newsletter><span>Newsletter</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://shunyaueta.com/index.xml title=RSS><span>RSS</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2>Beam summit 2022 雑感</h2></header><div class=entry-content><p>毎年開催される Apache Beam の会議、Beam Summit 2022 で講演資料が公開されていたので、気になる資料を読んだ。
以下に面白かった記事の備忘録を放流しておく
Google’s investment on Beam, and internal use of Beam at Google Google 内部で現在フルタイム Beam 開発者は 25 人! (多いな) Go SDK 提供開始がめでたい 現在は Java, Python, Go の３つの言語をサポート 機械学習の推論を Beam の特性を生かしてスケーラブルに実行可能な RunInference も提供できた! TypeScript SDK も提供予定! contribution している方も募集中 https://github.com/apache/beam/tree/master/sdks/typescript Beam Playground を使えば、Beam がより効果的に学べるよ https://play.beam.apache.org/ チケット管理では Jira をやめて GitHub Issues に移行したよ(最近の Apache Project の潮流な気がする。Lucene も移行していた) Beam @TwitterEvaluation, Adoption, Migration and future. 毎日実行される data pipeline の総数 5 万 200PB 超えのボリュームをデータ処理 7 兆のイベント数 Beam の魅力 batch, streaming の両者を扱うことができる、かつモダンな実行フレームワーク ランナーの柔軟性 複数のクラウド環境で実行可能 複数のプログラミング言語で動く 優れた OSS コミュニティ RunInference: Machine Learning Inferences in Beam Apache Beam 2....</p></div><footer class=entry-footer><span title='2022-11-06 22:52:09 +0900 +0900'>11月 6, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to  Beam summit 2022 雑感" href=https://shunyaueta.com/posts/2022-11-06-2252/></a></article><article class=post-entry><header class=entry-header><h2>情報検索・検索技術 Advent Calendar 2022 を開催します</h2></header><div class=entry-content><p>2021 年に引き続き、2022 年も情報検索・検索技術 Advent Calendar を作ってみました。
情報検索・検索技術 Advent Calendar 2022 - Adventar
kivantium さんの 創作+機械学習 Advent Calendar 2022 を開催します - kivantium 活動日記 の記事がいいなと思ったので、僕も自分の Blog で告知しておきます。
2021 年にアドベントカレンダーを作成したきっかけとしては、そもそもブログ記事の執筆が自分は好き他人が書いた記事を読むのは楽しい。 アドベントカレンダーの文化はそういう自分の嗜好にぴったりなので、自分の好きな検索技術領域がまだ作られていない! 作らねば! というのがモチベーションでした。
実際のところ、検索技術に携わってはいるが、Blog 記事をあまり書かない人もアドベントカレンダー起因で記事を書くきっかけになっているじゃないかなと思っています。
現時点で 登録数 12/25人となっています。ご登録頂いた方々ありがとうございます! みんなでワイワイ投稿して盛り上げていきましょう。</p></div><footer class=entry-footer><span title='2022-11-05 11:47:43 +0900 +0900'>11月 5, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 情報検索・検索技術 Advent Calendar 2022 を開催します" href=https://shunyaueta.com/posts/2022-11-05-1147/></a></article><article class=post-entry><header class=entry-header><h2>env Studio No such file or directory というVisual Studio Code 起因のエラーへの対処方法</h2></header><div class=entry-content><p>VSCode をcodeコマンドから実行可能にするとPATHに Visual Studio Code の空白スペースが含まれてしまうことが原因でこのエラーが発生する。
具体的には VSCcode の PATH が以下のように登録されてしまっている。
1 PATH = ...PATH:/Applications/Applications/Visual Studio Code.app/Contents/Resources/app/bin 自分が遭遇したエラーは、環境変数を参照する make コマンドで
1 2 make test env: Studio: No such file or directory というエラーが出てくるようになり、make task が実行できなくなってしまった。
原因として自分の場合は、brew で VSCode をインストールしなおしたら、このエラーが出てくるようになった。
対処方法 公式ページに書いてあるとおりの方式1でパスを通せば解決する。 具体的に解説すると
VS Code を起動 コマンドパレット(Cmd+Shift+P)を開いて、shell commandと打ち込み、Shell Command: Install 'code' command in PATH を選択して実行 でこのエラーが出てこなくなる。
もしくは、ダブルクォーテーションでPATHを登録すればこの問題は回避可能
1 export PATH="\$PATH:/Applications/Visual Studio Code.app/Contents/Resources/app/bin" Visual Studio Code on macOS You can also run VS Code from the terminal by typing ‘code’ after adding it to the path:...</p></div><footer class=entry-footer><span title='2022-11-04 17:27:50 +0900 +0900'>11月 4, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to env Studio No such file or directory というVisual Studio Code 起因のエラーへの対処方法" href=https://shunyaueta.com/posts/2022-11-04-1727/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2022-10-29-2337/images/1.png alt=従来の検索結果、近似近傍探索、ハイブリッド検索の比較></figure><header class=entry-header><h2>Elasticsearch 8.4 から利用可能な従来の検索機能と近似近傍探索を組み合わせたハイブリッド検索を試す</h2></header><div class=entry-content><p>表題の通り、Elasticsearch 8.4 から待望の近似近傍探索と従来の検索を組み合わたハイブリッド検索が可能になったらしいので、試してみました。
Elascticsearch 8 で導入された近似近傍探索について Elasticsearch 公式の記事1がわかりやすく近似近傍探索について語られています。 また、日本語では@pakio さんの紹介記事2も非常にわかりやすいので、そちらも御覧ください。
嬉しいけど物足りない点 公式の資料3や@pakio さんの資料でも触れられていますが、
You can’t currently use the Query DSL to filter documents for an approximate kNN search. If you need to filter the documents, consider using exact kNN instead. Elasticsearch の Query DSL との併用不可というのが物足りない点でした。
端的に説明すると Elasticsearch 8 で利用可能になった近似近傍探索は、あくまでベクトル間のみの近似近傍探索のみできるのであって、従来の Elasticsearch の検索機能(term や filter)と近似近傍探索を組み合わせて検索できないということです。
Vespa の開発者の Jo さんも同様の点4について触れていました。
The most surprising part of the announcement is that they won’t allow combining the nearest neighbor search with standard query terms and filters....</p></div><footer class=entry-footer><span title='2022-10-29 23:37:35 +0900 +0900'>10月 29, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Elasticsearch 8.4 から利用可能な従来の検索機能と近似近傍探索を組み合わせたハイブリッド検索を試す" href=https://shunyaueta.com/posts/2022-10-29-2337/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2022-10-23-2344/images/01.gif alt=デモアプリの動画></figure><header class=entry-header><h2>Elasticsearchの近似近傍探索を使って、ドラえもんのひみつ道具検索エンジンを作ってみた</h2></header><div class=entry-content><p>Elasticsearch 8 系から使用可能になった近似近傍探索1を使って、ドラえもんのひみつ道具の自然言語検索ができる検索エンジンを作ってみた。
デモ動画のように、検索したいひみつ道具を説明する文章することで近しいひみつ道具が検索されます。
コードは GitHub に公開してあるので、興味のある方は手元で、動かして遊ぶことが出来ます。 poetry と Docker さえあれば動くようになっています。
hurutoriya/doraemon-himitsu-dogu-search: Doraemon Himitsu Dogu Japanese semantic search based on Elascticsearch ANN
システムの概要図はこんな感じ
所感 ドラえもんのひみつ道具のデータセットを今回１から作ったが、パースと前処理がめんどくさくてここが一番手間がかかった。が、工夫しないと出来なかったので、一番楽しいところでもあった。 文章の特徴抽出は、sonoisa/sentence-bert-base-ja-mean-tokens-v2 · Hugging Faceを使わせていただき、驚くほど簡単に実現できた。 実際はもっと精度を高めるには、fine tune などがいいのだろうが、システム側を作ることに注力したかったので今回は割愛 デモアプリの構築は streamlit を使って 20m くらいで作れたので、相変わらず便利すぎて愛用している。今回の検索エンジンは CLI から実行もできるが、こうやってデモアプリがあったほうがそれっぽくて気持ちいい。 インデキシング時にトーカナイザーのことなど全く考えずに特徴ベクトルだけインデキシングして、それで検索が成り立つというのは新鮮。閾値設定しなければゼロヒット問題にも直面しないので、できることの幅は広がりそう。 Elasticsearch の近似近傍探索は、今回ベクトル同士の近似近傍探索しかやっていないが、それもインデキシング、クエリ部分は公式ドキュメントを見れば事足りたので変にハマることはなかった。 クエリ部分はこれだけで書けた。
1 2 3 4 5 6 7 8 9 10 query = { "knn": { "field": "vector", "query_vector": sentence_embeddings[0], "k": 10, "num_candidates": 100, }, "fields": ["name", "description"], } result = es....</p></div><footer class=entry-footer><span title='2022-10-23 23:44:13 +0900 +0900'>10月 23, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Elasticsearchの近似近傍探索を使って、ドラえもんのひみつ道具検索エンジンを作ってみた" href=https://shunyaueta.com/posts/2022-10-23-2344/></a></article><article class=post-entry><header class=entry-header><h2>CloudComposer の Variables (環境変数)を gcloud cli で取得する</h2></header><div class=entry-content><p>Airflow 1 系で設定されている環境変数を JSON ファイルとして GUI を使って書き出す方法の続報です。
前回、Airflow CLI からでも環境変数を JSON ファイルとして出力できる1が、手元から実行しても GCP 上のインスタンスにしか保存されなかったので諦めたと書きました。 ですが、その問題を解決できたので、解決方法を公開しておきます。
Cloud Storage に格納されるデータ | Cloud Composer | Google Cloudによると、Cloud Composer インスタンス内部のディレクトリは GCS にマッピングされているらしい。
マッピング関係は以下( GCP のドキュメントをそのまま引用)
フォルダ Storage パス マッピングされたディレクトリ 説明 DAG gs://bucket-name/dags /home/airflow/gcs/dags 環境の DAG を保存します。このフォルダ内の DAG のみが環境にスケジュールされます。 プラグイン gs://bucket-name/plugins /home/airflow/gcs/plugins カスタム プラグインを保存します。カスタムのインハウス Airflow 演算子、フック、センサー、インターフェースなどです。 データ gs://bucket-name/data /home/airflow/gcs/data タスクが生成して使用するデータを保存します。このフォルダは、すべてのワーカーノードにマウントされます。 ログ gs://bucket-name/logs タスクの Airflow ログを保存します。ログは Airflow ウェブ インターフェースでも利用できます。 それを使えば、/home/airflow/gcs/data にファイルを保存すれば、CloudComposer が保有している GCS の gs://bucket-name/data にアクセスすれば、そのファイルが参照可能になる。...</p></div><footer class=entry-footer><span title='2022-10-17 16:30:58 +0900 +0900'>10月 17, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to CloudComposer の Variables (環境変数)を gcloud cli で取得する" href=https://shunyaueta.com/posts/2022-10-17-1630/></a></article><article class=post-entry><header class=entry-header><h2>Python で zip関数を使う際に、２つの配列が同じ大きさを想定する場合は 3.10 から導入された strict=True を使おう</h2></header><div class=entry-content><p>Python で２つの配列を for 文で扱いたい場合によく使うのが zip() です。
zip()を使った for 文では暗黙的に同じ大きさが要求されると思っていたが、実際には以下のように２つの配列の大きさが異なっていてもエラーが出ないことに気が付かず、困ったことがあった。
1 2 3 4 5 6 7 8 9 10 In [1]: a = [1,2,3,4] In [2]: b = [1,2,3] In [3]: for i,j in zip(a,b): ...: print(i,j) ...: 1 1 2 2 3 3 てっきり、大きい配列の要素を参照時にエラーが発生するかと思ったら、そんなことはなかった。
assert とかで事前にコケるようにしておくとか必要そう。 もしくは、両者の配列のサイズが同じことを明示的に確認するのが吉。
また蛇足だが、Stackoverflow では意図的に異なる大きさの配列を上手く循環させつつ回したい場合の対処法も書いてあり勉強になった。1
2022-10-27: 追記
@ftnext さんから以下の情報2を教えてもらいました。
小さい方を読み切ったら for を抜けるの予想と違いますよね。 3.10 から zip に strict 引数が追加されており、True を指定すれば長さが異なると ValueError を送出するようになったんです！ https://docs.python.org/ja/3/library/functions.html#zip… また長い方に合わせたいときは zip_longest が標準ライブラリの itertools にありますー...</p></div><footer class=entry-footer><span title='2022-10-17 11:56:22 +0900 +0900'>10月 17, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Python で zip関数を使う際に、２つの配列が同じ大きさを想定する場合は 3.10 から導入された strict=True を使おう" href=https://shunyaueta.com/posts/2022-10-17-1156/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2022-10-04-1549/images/1.png alt="Airflow で環境変数をJSONファイルとしてお手軽に書き出す方法"></figure><header class=entry-header><h2>Airflow 1系で設定されている環境変数を JSON ファイルとしてGUIを使って書き出す方法</h2></header><div class=entry-content><p>CloudComposer(GCP の Airflow のマネージドサービス)で運用している Airflow 1 系上で設定されている環境変数を JSON ファイルとして書き出したかったが、つまずいたのでメモを公開しておく。
Airflow の運用の理想としては、リポジトリをベースに CI 経由で CloudComposer を構築していくのがベスト。 だが、Airflow では GUI でお手軽に環境変数(Airflow では Variables という概念1)が設定でき、便利な半面、デメリットとしてリポジトリをベースにした Single Source of Truth の状態が保てなくなってしまう。
Airflow の環境変数を JSON ファイルとして書き出す方法 上部の Admin メニューから、Variablesをクリックしてページに移動 With selectedボタンをクリックすると Exportボタンがドロップダウンリスト内にでてくるので、これをクリックすれば Airflow に保存されている環境変数を JSON ファイルとして書き出せる Export できるとは初見でわからなかったのでこの UI を考えた人は罪深い。@naoさんに教えていただけて感謝! Airflow CLI からでも環境変数を JSON ファイルとして出力できるらしい2が、手元から
1 gcloud composer environments run COMPOSER_NAME --location asia-northeast1 variables -- --export env.json を実行してもローカルには保存されなかったので、実行結果は CloudComposer 内部のインスタンスに保存されている模様。
Bash と GCS のオペレーターを組み合わせれば JSON ファイルを GCS に保存はできそうだが、それもめんどくさそうではある。 直接 SSH で CloudComposer のインスタンスにつなげたほうがまだ楽そうですよね...</p></div><footer class=entry-footer><span title='2022-10-04 15:49:30 +0900 +0900'>10月 4, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Airflow 1系で設定されている環境変数を JSON ファイルとしてGUIを使って書き出す方法" href=https://shunyaueta.com/posts/2022-10-04-1549/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2022-09-01-2139/images/velox.svg alt></figure><header class=entry-header><h2>Meta が公開したデータ処理の効率化・高速化を狙うエンジン Velox が面白そう</h2></header><div class=entry-content><p>日課の RSS フィードを眺めていると、クエリエンジンやデータ処理の最適化のための高速化ライブラリが Meta が OSS として公開した1 のを知った。
Velox のリポジトリはこちら
facebookincubator/velox: A C++ vectorized database acceleration library aimed to optimizing query engines and data processing systems.
実際にリポジトリを観てみると C++で書かれており、たしかにパフォーマンスが高いのが納得。
ドキュメントやチュートリアルなどはこちらのサイトで用意されています。
Hello from Velox | Velox
Meta 社内では、Presto や Spark に適用して処理の高速化、PyTorch に活用して前処理や特徴量エンジニアリングの高速化が進められているらしいです。
技術ブログ記事 1が何をやっているか明瞭なので、かいつまんでメモを残しておきます。
SQL の分析、ストリーミング処理や機械学習のためのデータ処理など実際の処理内容は似通っているが、様々なフレームワークが使われ、独立して進化している。 この断片化によって、保守と拡張が困難になっていたが、それらを統合する形で実行可能にするのが Verox Presto, Spark などのデータ処理エンジンは、一見異なるように見えるが、レイヤー構造で考えると非常に似通っている。 Verox は一般的に実行エンジンレイヤーの代わりとなり、式評価、集約、ソート、結合などの処理を提供する。(一般的に data plane と呼ばれる) 実例と結果 Presto は Java で実行されるが、それを C++の Velox で置き換えた。Prestissimo というプロジェクト名で進んだ。(カッコいいね) Java での実行と比べると、大体 6 倍ほど高速化された Spark 上では、Gluten とよばれるプロジェクトで Velox と同じように C++での実行を試みるプロジェクトが公開されている。 PyTorch の TorchArrowを Velox 上で実行可能 最終的には、Velox で従来のデータマネジメントの部分と機械学習インフラストラクチャの部分の垣根を統一することを狙っている。...</p></div><footer class=entry-footer><span title='2022-09-01 21:39:30 +0900 +0900'>9月 1, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Meta が公開したデータ処理の効率化・高速化を狙うエンジン Velox が面白そう" href=https://shunyaueta.com/posts/2022-09-01-2139/></a></article><article class=post-entry><header class=entry-header><h2>Java の memory map を理解する</h2></header><div class=entry-content><p>Apache Lucene のインデックスの取り扱いについて勉強していたら、 Java の memory map について言及されていたが、Jave の memory map1 について日本語で分かりやすく解説されている記事がなかったので、勉強がてらまとめた。 メモリマップ自体の説明はこちらのサイトが非常にわかりやすかった2
mmap はファイルとメモリーアドレスのマッピングを行う
つまり、ファイルをメモリ上にマップ(射影)してメモリ上でファイルを扱えるようにするということですね。 Apache Lucene の使用例だと、Lucene の検索用のインデックスファイルを MMap でメモリ上にマップして扱えるようにしていそう。
参考にしたのは上記２つの記事がわかりやすい記事だった。
Java プログラムに関連するメモリは 4 部分から構成される3
Stack: メソッドが呼ばれた際に、Stack はメソッドを完了させるためのメモリ空間を提供する。この空間はパラメータやローカル変数、現在のオブジェクトへの参照などが格納されている。Frame はメモリ空間を参照し、メソッドの呼び出しをサポートする。Stack は LIFO(Last in First out)方式で動作し、呼び出し基のメソッドの Stack frame を削除するために最後の Stack frame(現在実行中のメソッド) を削除する必要がある。 Heap: Java で作成されるオブジェクトは全て Heap で作成される。 Static Area: プログラムの実行中に存在する値を格納するメモリを参照する。静的な変数を宣言した際に、この領域に存在する。 Code: 実行されるコードが格納される場所。 中でも Java の Memory mapped file は、メモリから直接ファイルにアクセスするのに役立つ Java の特殊ファイル4。 Java は、java.nio パッケージで Memory mapped file をサポートしている。 Memory mapped I/O は、ファイル システムを使用して、ユーザーから直接ファイルシステムページへの仮想メモリマッピング(virtual memory mapping)を確立する。Memory mapped file は単純に大きな配列として扱うことができ、Memory mapped file に使用されるメモリーは、Java の Heap 空間外部が利用される。...</p></div><footer class=entry-footer><span title='2022-08-22 13:00:43 +0900 +0900'>8月 22, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Java の memory map を理解する" href=https://shunyaueta.com/posts/2022-08-22-1300/></a></article><article class=post-entry><header class=entry-header><h2>Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる</h2></header><div class=entry-content><p>2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1
Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow’s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX)....</p></div><footer class=entry-footer><span title='2022-08-18 19:38:29 +0900 +0900'>8月 18, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる" href=https://shunyaueta.com/posts/2022-08-18-1938/></a></article><article class=post-entry><header class=entry-header><h2>KDD2022 で気になった研究</h2></header><div class=entry-content><p>2022/08/14 - 2022/08/18 に開催される Knowledge Discovery and Data Mining (KDD) 2022 の情報が出揃ってきたので、気になった情報をメモしておく。
自分が気になるトピックは、変わらず機械学習の実応用とその周辺領域なのでそれに偏ったリストになっている。
ADS invited speaker KDD 2022 ADS Invited Speakers
An overview of AWS AI/ML’s recent contributions to open source ML tools: Accelerating discovery and innovation
招待講演は確か毎回論文化されて ACM で公開されるので論文公開されたらぜひ読みたい。
Tutorias KDD 2022 Tutorials Schedule に Tutorial の情報がまとまっているが、タイトルだけでウェブサイトへのリンクが一切なく、読み手に不親切なので来年は、改善してほしい。去年はそんなことなかったので、なんとか来年はもとに戻って欲しい。
Graph-based Representation Learning for Web-scale Recommender Systems. Authors: Ahmed El-Kishky (Twitter)*; Michael Bronstein (Twitter); Ying Xiao (Twitter); Aria Haghighi (Twitter) Twitter が開催する Tutorial で、すごく面白そうなのだが全く情報が見つからなかった。Twitter Cortex にも情報が更新されていないので、しばらくしたら公開されていることを祈る。 New Frontiers of Scientific Text Mining: Tasks, Data, and Tools....</p></div><footer class=entry-footer><span title='2022-08-15 23:35:35 +0900 +0900'>8月 15, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to KDD2022 で気になった研究" href=https://shunyaueta.com/posts/2022-08-15-2335/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2022-08-10-1717/images/1.png alt="poetry show の実行結果"></figure><header class=entry-header><h2>poetry show でパッケージ名に (!) が付与されている意味</h2></header><div class=entry-content><p>poetry show は、poetry の設定ファイルの pyproject.tomlに記載された利用可能なパッケージ名を表示してくれる。
例えば、ターミナルで poetry install を行う前に、poetry showを行うと以下のような結果がでる。
そして、grep で上記の結果を表示させてみると
1 2 > poetry show | grep aiohttp aiohttp (!) 3.8.1 Async http client/server framework (asyncio) と パッケージ名に (!)が付与されている。
この(!)ってそもそもどんな意味なのか気になったので調べてみました。
Poetry のコードを直接読んでみると、 test_show_basic_with_not_installed_packages_non_decoratedのテストケースが今回の事例にマッチしており、わかりやすかった。 意味としては、「インストールされたパッケージに対する show コマンドを非装飾モードで結果を出力」へのテストだ。 状況としては、cachyとpendulumを poetry add して、 cachyのみを poetry install している。
1 2 3 4 5 6 7 8 9 10 poetry.package.add_dependency(Factory.create_dependency("cachy", "^0.1.0")) poetry.package.add_dependency(Factory.create_dependency("pendulum", "^2.0.0")) cachy_010 = get_package("cachy", "0.1.0") cachy_010.description = "Cachy package" pendulum_200 = get_package("pendulum", "2....</p></div><footer class=entry-footer><span title='2022-08-10 17:17:29 +0900 +0900'>8月 10, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to poetry show でパッケージ名に (!) が付与されている意味" href=https://shunyaueta.com/posts/2022-08-10-1717/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2022-08-08-1145/images/1.png alt="リーダーの作法 ささいなことをていねいに"></figure><header class=entry-header><h2>「リーダーの作法」マネジメントに限らず、エンジニアとして仕事の作法について書かれた良書</h2></header><div class=entry-content><p>リーダーの作法 ささいなことをていねいにを読み終えた。 著者は Netscape でマネージャー、Apple でディレクター、Slack でエグゼクティブを経験した Michael Lopp さんで、過去にBeing Geek や Managing Humans を書かれている。 翻訳の質も非常に高く、楽しく読めた。1
そんなにマネジメント関係を読んでいるわけではないが、HITH OUTPUT MANAGEMENT や、エンジニアのためのマネジメントキャリアパス ―テックリードから CTO までマネジメントスキル向上ガイド 同じくらい良い書籍で、学びや共感を多く感じた。
自分はマネジメントのポジションについたことはないが、仕事をしていくなかでマネジメント関係のソフトスキルや複数人でどうやってうまくリーダシップを発揮して、大きい問題を解決するかに興味があるので、良い書籍が目につくと積極的に読んで解像度をあげている。
心に響いた文章と所感 マネジメントとは、まずチームが直面している障害やメンバー間の軋轢といった情報を明らかにすることであり、さらにそうして得た情報を分析して、進むべき正しい道を見出すことである、ということでした。
マネジメントの定義として、チームの課題は他の書籍でも述べられているがメンバー間の関係性について言及しているのは確かにねという腹落ちだった。
ポジションに関わらず全ての業務で一貫して言えるのは、なにが最も ROI が高い課題かを突き止めて地道に解決していくことだな~と思える。
6 章 プロフェッショナルとしての成長を図る質問表
6 章の成長を計測するための質問表は、あらためて自分が何をやりたいのかの解像度をあげる良い質問集だったので半年や一年ごとにこの質問を更新して、定期的に見つめ直していきたい。
新しい仕事では、一刻一刻で多くを学びます。その環境に関する情報をたくさん集めているのです。チームや自分の役割、そして会社についての自分の理解を日々新たにしているのです。
新メンバーがあげるチームの違和感は、フィードバックをもらった後に一ヶ月後見直して見ると重要度が下がることがあるよねと。これは、自分も経験あるが、チームで働くうちに内部の事情を理解して、重要だけど緊急じゃないよね、実際やりたいけどコスト的に割に合わないなど課題に対しての解像度があがることで、優先順位が変わることがありました。
そういう人たちが長年にわたって私に教えてくれた重要な教訓の一つは、壁に書かれた文字より、語られるストーリーの方が重要だということです。
ここで腹落ちしたのは、あらためて人を動かすのは物語(ストーリーテリング)だということ。「なぜ」 そうなったのかってものすごく重要だなと。DesignDocs もそうですが、なぜそうなったかがわかると人間ってものすごく腹落ちして理解できるな~と 自分もドキュメントを書くときは、「なぜ」そうなったのかをきちんと書いて人を動かせる文章を書けるようになっていきたい
手厳しいフィードバックの場合は、何も見落とさないようにするためにあと 2 段階のプロセスがあります。ステップ 1：どんなに批判的なフィードバックであっても、耳を傾け、ほんの少しでも理解の糸口を探す。ほんのわずかでも？よくぞ聞いてくれました。
時には、フィードバックがあまりにも衝撃的で、理解できないこともあります。そこで、第二のステップです。ステップ 2：聞いたことを繰り返しましょう。
ここのストーリーは面白くて、図星な本質的なフィードバックを感情的に反応しそうになってしまうのをどうやって傾聴して、フィードバックを受け入れて自己を改善するかが語られていて面白かった。
リアルタイムにフィードバックし、他のプレイヤーに親切かつ教育的な方法でアドバイスを送ります。災難に直面しても冷静さを失いません。わかりやすいコミュニケーション、実証された専門知識、わかりやすく行動につながるフィードバック、そして落ち着いた性格。堅実なリーダーの性質について説明しましたが、まだ大切なことがあります。いいでしょうか、こうしたふるまいは、多くの人がやっているのをこれまで見てきました。DJ が特別なのは、常にこのようなリーダーであることです。
ゲームを通して、リーダー像がどのように振る舞うべきなのかを説明しており、非常にわかりやすかった。
リーダーシップとは、他人に見せるために選ぶ服であり、私は揺るぎない優しさを選びます。
そして、書籍の最後の文章が、まさに「リーダーの作法」という心構えであり定期的に見返したい。
全体を通して、エッセイとして語られて教訓を学べるので楽しく読めた。2 他の書籍と比較すると、リーダーとして情緒的にどう振る舞うべきかも触れている点が、個人的に読み応えがあった。 自分があらためてマネジメントのポジションにつくことがあれば読み返したくなる書籍だった。
総じて、自分が体験したトピックは強く共感を抱けて学びが多かったというのが興味深い。 ビスマルクの名言で「愚者は経験に学び、賢者は歴史に学ぶ」があるが、自分も本を読むことで歴史から学んでいけるようになりたい。 そのために最近習慣化したいので、読んだ後に読書ノートを作ってちゃんと振り返ることで定着できないか試行錯誤中。 Obsidian にノートをまとめるようになってからは、読書ノートを作るのも楽しくなったので、良いツールは習慣化形成にも役立ちますね。
プライベートで書籍の翻訳プロジェクトを勧めているのだが、このような本が母国語で読める環境はあらためて素晴らしい文化だなと強く思うようになった。 ↩︎
蛇足) 13 章のストーリーが個人的に一番笑える皮肉が書いてあって、一読の価値あり。 当たり前のことをきちんとやるって、難しいけどその作法は大事。 ↩︎</p></div><footer class=entry-footer><span title='2022-08-08 11:45:24 +0900 +0900'>8月 8, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 「リーダーの作法」マネジメントに限らず、エンジニアとして仕事の作法について書かれた良書" href=https://shunyaueta.com/posts/2022-08-08-1145/></a></article><article class=post-entry><header class=entry-header><h2>Makefile でコマンドの前に @ を付けると、コマンド自身は表示されず結果のみ表示される</h2></header><div class=entry-content><p>Makefile を眺めているとコマンドの前に@をつけているターゲットがあり、その効果を調べてみた1。日本語での記事が無かったので記事を書いた。
以下のように、コマンドの前に @をつけたコマンドとつけていないコマンド両方を実行してみる。
1 2 3 echoing-silencing: @echo "表示されない" echo "表示される" 1 2 3 4 > make echoing-silencing 表示されない echo "表示される" 表示される なので、例えばお役立ち事例として、Makefile でターゲットの実行時に、何を行うか説明をしたい場合に @を付けるとスッキリした文をターミナルに表示することができる。
https://makefiletutorial.com/
Command Echoing/Silencing Add an @ before a command to stop it from being printed
You can also run make with -s to add an @ before each line
 ↩︎</p></div><footer class=entry-footer><span title='2022-06-22 00:01:36 +0900 +0900'>6月 22, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Makefile でコマンドの前に @ を付けると、コマンド自身は表示されず結果のみ表示される" href=https://shunyaueta.com/posts/2022-06-22-0001/></a></article><article class=post-entry><header class=entry-header><h2>愛用しているツールを更新: Joplin→Obsidian & TickTick → Todoist</h2></header><div class=entry-content><p>ノートアプリを Joplin から Obsidian へ 2021-03-13 に notion から joplin に乗り換えた。 理由としては
notion を マークダウンのメモ帳としか使わない DB 機能やテンプレート機能は必要としていない
なのでロックインされずに自分で管理できる OSS の joplin のほうが良いなと思って乗り換えました。 必要な機能は全て満たしてるので満足 と Joplin に乗り換えて大きな不満は無く使い続けていたのですが、Obsidian に乗り換えました。
きっかけは ryuzeee さんの記事1 なんですが、以前から存在は知っていたObsidian をとりあえず食わず嫌いせずに触って見ようかと思い立ち、Joplinの記事を Obsidian のフォーマットにすべて変換2して使い始めてみたところ、とても心地よい使い心地で感動して乗り換えを決定した。
元の Joplin がそもそも Obsidian と同じローカルのプレーンテキストを扱うという思想なので移行がとても簡単でした。
乗り換えを決めた点として、
daily note プラグインと Calendar プラグインで日誌がめちゃくちゃ快適に書けるようになった。自分は日誌という形でログを残しつつ作業をしているのだが、このプラグインで毎日の日誌の記入がとても快適になった。 ぷーおんさんが書いてくれている記事3が、このプラグインの魅力をわかりやすく紹介されています。
またバックリンクの機能がやはり強力で、プラグインも PKM(Personal Knowledge Management) を目的に作成されたツールが多いので、それも気に入った。(ランダムノートを開いて、ノートを整備する機能はなるほど!となる便利さ)
Joplin でもバックリンクのプラグインはあり使っていたが、Obsidian のバックリンク機能は比較してとても完成度が高い。バックリンクを使った繋がりの提案や、つながりが無いノートも見つける機能があったりと書いていて楽しい。
また、自分は料理を良くするのだが、普段参照するレシピも PKM で管理することで有機的な管理ができるようになりとても楽しい。これぞ PKM という事例だなとしみじみ。
logseqというアプリも気になったのだが、アウトライナーアプリで、自分は文章として作成したいので今回は Obsidian を選んだ。
一つ困った点としては、Joplin では Dropbox 経由で Mac と Android 間でノートを同期できていたのだが、Obsidian では Dropbox を使った同期ができなくなった。 だが、スマホでメモを見たくなる用途は主にレシピを見たいときだけだった。どうしても我慢できなくなったら頑張ってデバイス間同期をしてみる。...</p></div><footer class=entry-footer><span title='2022-06-03 21:33:01 +0900 +0900'>6月 3, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 愛用しているツールを更新: Joplin→Obsidian & TickTick → Todoist" href=https://shunyaueta.com/posts/2022-06-03-2133/></a></article><article class=post-entry><header class=entry-header><h2>Label Studio を k8s にデプロイする</h2></header><div class=entry-content><p>前回 Label Studio の紹介記事1を書きましたが、自分以外にもチーム全体で Label Studio を使いたいという要望があったので Web アプリとして labelstudio をホストしました。 意外と簡単に k8s 上でホストできたので、その方法を公開しておく。
Label Studio の運用方法は、 Docker イメージが提供されているので、それを使用するのが最も簡単です。
CloudRun を使ってサーバーレスで動かす方法2もありますが、今回は k8s 上に Label Studio の Docker イメージをデプロイして、運用することになりました。
k8s のマニフェストファイルは、公式リポジトリ3を参考に作成しました。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 apiVersion: v1 kind: Service metadata: name: labelstudio namespace: development spec: ports: - name: http port: 8080 protocol: TCP selector: app: labelstudio --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: labelstudio-data-pvc namespace: development spec: accessModes: - ReadWriteOnce resources: requests: storage: 50Gi --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: labelstudio name: labelstudio namespace: development spec: replicas: 1 selector: matchLabels: app: labelstudio template: metadata: labels: app: labelstudio spec: containers: - image: heartexlabs/label-studio:v1....</p></div><footer class=entry-footer><span title='2022-06-03 20:44:01 +0900 +0900'>6月 3, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Label Studio を k8s にデプロイする" href=https://shunyaueta.com/posts/2022-06-03-2044/></a></article><article class=post-entry><header class=entry-header><h2>Re:プログラム雑談 188回：ゲスト回：MessagePassingの話とか</h2></header><div class=entry-content><p>自分が大好きだった、Message Passing はなしをふったりふられたの後日談を @karino2 さんが、Podcast で語ってくれていたので、アンサーソング。
読者としては、Message Passing という共著ブログは RSS に登録して毎回とても楽しみにしていた。 著者陣はなぜかやってよかった、大成功だね! という感想を出している方が少なく @morrita さんが
特に皆が書いてくれたものを読むのはすごく楽しかったので、うまくいった気もしています。 https://messagepassing.github.io/023-s1/04-morrita/
というポジティブな感想を残してくれており、読者としても著者陣が良い体験として経験されているとすごく嬉しい。 続編を期待しております。
以下、Podcast 内でのご意見に関するお話。記憶の中から書き出しているので正確性に欠けるかもしれません
書く習慣を身に着けていないと、書けないし、普段から書く習慣を実践したい @morrita さん
同意。自分も今年から脱 Twitter をして、Blog で記事を執筆するようにしているが、以前よりもほんの少しだけど読みやすい文章に変わった気がする。
Blog 記事の感想は欲しい。。。が今だと一方通行。Twitter での言及はいまがポピュラーだが、それはなんか違う。 どちらかというち意見を書くというよりも、意見を求めているときがある。
これも非常にわかる。自分も双方的な意見交換を Blog 上で行いたくてコメント機能を導入1したが、今の所導入してから 1 件のリアクションしかされていない…2
このコメント機能の Giscus の良いところは、
記事に絵文字リアクションを行える GitHub アカウントが必要なのでスパムをかなり弾けそう なのだが、全然使われない…
はてぶで 969 件とかなりブックマーク3された記事で、自分の Blog でもおそらく歴代最大の閲覧数を持つ記事でさえもコメントはされなかった。
いや、はてぶやってるヒトははてぶでコメントすると思うので、単純にはいえないですが…一つの指標として
はてなスターや、Medium の clap のような弱いシグナルが欲しくて導入したのだが、なかなか思い通りにいかない。
海外の有名所の Blog 記事だとコメントが殺到しているので、これも日本独特の問題なのか? それとも到達する数の規模が違うからそのぶんコメントが多くなる傾向などがあるのだろうか?
increments などの雑誌に比べて Message Passsing は、他の著者が書いた記事に返信していくような形式なので、重複した内容が少ないのが良い点 @morrita さん
自分もそう思うな~。MEssage Passing の面白さは著者陣がリレー記事のように言及しあって相互作用が発生しているのが面白い点だと思う。
意見表明だけだと偉そう...</p></div><footer class=entry-footer><span title='2022-05-12 23:37:15 +0900 +0900'>5月 12, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Re:プログラム雑談 188回：ゲスト回：MessagePassingの話とか" href=https://shunyaueta.com/posts/2022-05-12-2337/></a></article><article class=post-entry><header class=entry-header><h2>社内でデータ分析結果を可視化・共有する際に Google Colab が便利</h2></header><div class=entry-content><p>社内でデータ分析のレポートを書く際は Google Colab がとても便利な事に気がついた。
Google Bigquery でデータを抽出、Google Sheets で可視化 従来だと、自分がやっていた方法として、
Google BQ などで分析対象結果のデータを抽出 その結果を Google Spread Sheet として保存して、Google Sheets の機能で可視化。元の SQL のコードは、別シートを作ってそこに貼り付けておく。 利点としては、一度データを抽出した後は、Google Sheets で二次加工が簡単にできる点がとても便利。 また、 Google Sheet を共有後に Produc Manager が出したい数値を、Product Manager 自身が Google Sheets を元にさっと計算することもできる。
だが、二次加工が便利なのはいいが、大抵の可視化ってパターンが決まっているかつ二次加工の状況が必ず発生するわけではないので、SQL 取得とその可視化を一気通貫でできないかなと考えていた。
なにか良い方法無いかなと思っている矢先に、別のチームの同僚が、Google Colab を使って、BQ を dataframe として保存後 matplotlib で可視化しているのを見かけて、
求めていたのは…こ、これだ….
となり、速攻取り入れました。
良いと思ったところは積極的に真似する
Google Colab なら、データの取得・加工・可視化までを完結可能 Google Colab の利点を列挙しておく
SQL のコード、データ抽出や可視化のロジックなどが Python で記述可能かつ、Google Colab で完結 matplotlib で可視化できるので、見やすく美しい図を作れる そしてそのコードは他のデータ分析でも再利用可能 pandas dataframe で Google BQ からデータを取得するので、Standard SQL だけでは難しい計算も pandas、 numpy や scipy などを使ってデータ加工が簡単にできるのも、便利 Google Sheets 同様、簡単に社内で共有できる Markdown も Google Colab 内で書けるので、凝った文章などもいれてレポートも書ける マジックコマンドで、Google BQ の結果を dataframe として保存1したり、...</p></div><footer class=entry-footer><span title='2022-05-10 22:00:23 +0900 +0900'>5月 10, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 社内でデータ分析結果を可視化・共有する際に Google Colab が便利" href=https://shunyaueta.com/posts/2022-05-10-2200/></a></article><article class=post-entry><header class=entry-header><h2>2022年、はじめてのまともな確定申告</h2></header><div class=entry-content><p>2022 年、初めてまともな確定申告をやったので、所感をメモ。
実際には 2021 年に初めて確定申告を行ったのだが、ふるさと納税と医療費控除のみだったので、まとも?な確定申告は 2022 年がはじめてといってもいいだろう。
会計ソフトはマネーフォワードと迷ったが、クラウド会計ソフト freee 会計を採用した。 前評判では会計知識がある人からすると抽象化されすぎており、むず痒くなるという記事を見たのだが会計知識がない自分からすると全く問題なく使えた。 会計周りもすべてクレカ経由でしか支出していないのでひたすら振り分けて終わり。 これらは実質作業時間 15 時間程度。 振替項目が何が適切なのかの調査に一番時間が取られた。 書類提出も Andorid のアプリと連携することで free で完結して、ネット上で提出できて良い体験だった。 来年もしやることがあればものすごく早く終わりそう。 マネーフォワードの確定申告は、最初の画面で心が折れて回れ右をした。
freee が「寄附金控除に関する証明書」の xml を読み込める機能に対応しており、ポチポチで終わった。 事前の証明書を発行するための手続きが若干煩雑だったが、今までに比べると圧倒的に楽だった。 証明書の管理もしなくて良くなったのは革命では? 実作業時間は 30 分程度。 紙がもったいないので、個人的には自治体から書類は送らないで欲しい。
医療費控除は、国税庁が公開してくれているエクセルにデータを打ち込んで終わり。 実作業時間は 2 時間程度。 そのフォーマットに free が対応してくれているので読み込んで終わり。
最終的な e-Tax の提出は、スマホでカードリーダーを読み込んで連携1させれば終わりだったので非常に快適だった。 2021 年は張り切って、IC カードリーダーを買ったが結局手持ちの Mac と Safari の相性が悪くかなり粘ったが結局紙に印刷して送付したという思い出がある。
アイ・オー・データ IC カードリーダー ぴタッチ 確定申告 e-Tax
国税庁が用意した Safari の拡張機能周りが魔窟で、当時のサイトを忘れたがマニュアルにかかれていない設定をしないとマイナンバー読み込みプログラムがそもそも起動しなかった。 リベンジできてなにより。 完全に無用になった IC カードリーダーは即売却した。
総評 年々システムが改善されて自動化されていて、素晴らしい。
賛否両論あると思うが、医療費もマイナンバーの保険証化2によりデータが収集されるらしいのでぜひ浸透してほしい。 最近のニュースでは別途診察代金が請求される方式らしく3、ここに税金使わないで何に使うんだろうかと思ったが、予算が取れなかったのかなと色々と考えをめぐらした。 せっかく良い活用が期待できるのに、使う動機を無くすのはもったいない。
e-Tax、スマホがあれば IC カードリーダーは不要に - ケータイ Watch ↩︎...</p></div><footer class=entry-footer><span title='2022-05-04 22:43:11 +0900 +0900'>5月 4, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 2022年、はじめてのまともな確定申告" href=https://shunyaueta.com/posts/2022-05-04-2243/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://shunyaueta.com/>«&nbsp;前へ&nbsp;</a>
<a class=next href=https://shunyaueta.com/page/3/>次へ&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2023 <a href=https://shunyaueta.com/>hurutoriya</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>