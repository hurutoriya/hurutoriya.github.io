[{"content":"記事ストックの改善 以前は、Pocket→Obisdian→ 執筆という流れだったが、 ストックされている昔の記事を Pocket からサルベージする際に、UI, UX が非常にストレスフルかつ時間を吸い取られるのでやめた。\nPocket は以下の点で辛かった。\n リスト形式は無く、グリッドしか用意されていない 20 件?ごとに疑似ページネーションが走ってめんどくさい page が 10 件以上超えると Pocket がフリーズする  最初は、Pocket 追加時にタグnl (NewsLetter の nl)を付与してみることにしたが、Pocket でニュースレターに関係無い記事を保存する際に毎回タグを付与するかいなかという考えがめんどくさくなったので(1 日で諦めた)、Obsidian の特定ファイルに毎回手動でコピペするようにした。\n記事一件あたりの追加時間が 5 秒ほど増えるが、回り回って記事を Obisidian にまとめる時間が一番めんどくさい作業なので、これにしたおかげでストレス無く完全に Obsidian で完結してニュースレター作業に取り組めるようになった。\n購読者 100 人突破 また、 17 回目のニュースレターを一般公開して、Twitter でリンクを告知するとメール購読者が 100 人を突破した。\n最新号をメール購読者のみの先行配信するのは、賛否両論あると思うが、購読者が増える要素になっているのではなかろうか?\n本来は Blog で公開していたときと同じく、すべての記事を全公開で良いのだが、Twitter がダメになった時自体に備えてメール購読者の数も増えてほしいので購読者数が増えるようなインセンティブ設計にしてみた。\nこれから 忙しいとニュースレター書くのをおろそかになってしまったりするが、書き始めると意外と楽しいし気分転換にもなっていることが多い。\nそのため一回の配信の量にはあまりこだわらず 2 週間に 1 号程度の配信頻度を保っていきたい。 なぜなら最低でも 2 週間に 1 時間程度も記事を読む時間が確保できないのは、それはそれで心の余暇が完全に喪失しており問題だからだ。\n時間計測アプリを toggl から session へと乗り換えてみたのも個人的にはすごく効果的で、集中して短時間で書き切れるようになったと思う。\n","permalink":"https://shunyaueta.com/posts/2023-02-17-0005/","summary":"記事ストックの改善 以前は、Pocket→Obisdian→ 執筆という流れだったが、 ストックされている昔の記事を Pocket からサルベージする際に、UI, UX が非常にストレスフルかつ時間を吸い取られるのでやめた。\nPocket は以下の点で辛かった。\n リスト形式は無く、グリッドしか用意されていない 20 件?ごとに疑似ページネーションが走ってめんどくさい page が 10 件以上超えると Pocket がフリーズする  最初は、Pocket 追加時にタグnl (NewsLetter の nl)を付与してみることにしたが、Pocket でニュースレターに関係無い記事を保存する際に毎回タグを付与するかいなかという考えがめんどくさくなったので(1 日で諦めた)、Obsidian の特定ファイルに毎回手動でコピペするようにした。\n記事一件あたりの追加時間が 5 秒ほど増えるが、回り回って記事を Obisidian にまとめる時間が一番めんどくさい作業なので、これにしたおかげでストレス無く完全に Obsidian で完結してニュースレター作業に取り組めるようになった。\n購読者 100 人突破 また、 17 回目のニュースレターを一般公開して、Twitter でリンクを告知するとメール購読者が 100 人を突破した。\n最新号をメール購読者のみの先行配信するのは、賛否両論あると思うが、購読者が増える要素になっているのではなかろうか?\n本来は Blog で公開していたときと同じく、すべての記事を全公開で良いのだが、Twitter がダメになった時自体に備えてメール購読者の数も増えてほしいので購読者数が増えるようなインセンティブ設計にしてみた。\nこれから 忙しいとニュースレター書くのをおろそかになってしまったりするが、書き始めると意外と楽しいし気分転換にもなっていることが多い。\nそのため一回の配信の量にはあまりこだわらず 2 週間に 1 号程度の配信頻度を保っていきたい。 なぜなら最低でも 2 週間に 1 時間程度も記事を読む時間が確保できないのは、それはそれで心の余暇が完全に喪失しており問題だからだ。\n時間計測アプリを toggl から session へと乗り換えてみたのも個人的にはすごく効果的で、集中して短時間で書き切れるようになったと思う。","title":"ニュースレター近況"},{"content":"自分はかなり前から、タスクの時間計測のために toggl の無料枠を使わせてもらっていたが、作業計測をそもそも忘れたりしてしまい、振り返っても計測自体ができていないことが多かった。 あとはどこまで詳しく取るべきかの切り分けが出来ず、結局最近は個人プロジェクトの書籍翻訳の時間計測のためにしか利用できていなかった。\n最近集中力が散漫になっていることを課題に感じていたので、良い機会なのでこの機会にツールを乗り換えてみて、日々の生産性を向上できないか試してみる。\n色々と比較したが、 Session - Pomodoro focus timer with analytics が一番自分の求めているものに近かった。\nOSS が可能なら良かったが、単純なポモドーロタイマーならあるが、自分が求めている以下のような機能が無い。\n 非 Web アプリ 統計機能 web サイトとアプリのブロック 音楽を流す ポモドーロごとへの感想  なので、 Best Pomodoro Time Alternatives - 2023 | Product Hunt　で良さげなポモドーロタイマーアプリを探してみた。\nFlow – Pomodoro timer for Mac, iPhone, and iPadもミニマルなデザインかつ、月 1$ で、買い切り 30$と値段は非常に魅力的なのだが、session と比べると、細かい機能差があり、手に馴染みそうだったのが、session だと思ったので、こちらを使い始めてみる。\n特に session の統計画面は見返していて楽しそうである。\nApple のプラットフォームでしか使えないのは一つの懸念点だが、自分はなんだかんだ当面の間 Apple 製品を使うことになると思うので、その時が来たらまた別のツールを考えれば良い。\n今までは可能なら購読形式のツールは使いたくなかったが、このツールで自分の集中力を向上できるなら年課金したとしても安い買い物かなと思って開き直ってきた。 お金で、生産性を買う姿勢\u0026hellip;\nとりあえず、一ヶ月くらい使ってみます。\n以下のページで機能更新やカスタム方法などを公開してくれているが、こういうアップデート系統の記事って見るの楽しいですよね。過去の軌跡を追いかけることができますし。\nSession - Changelog\n一つの欠点は、非常に気に入っていた toggl と Obsidian の連携プラグインである\nmcndt/obsidian-toggl-integration: A Toggl integration plugin for the popular knowledge base application Obsidian.\nが乗り換えることにより使えなくなること。 でも、その代わりに生産性が向上するならまったくもって問題ない。\n","permalink":"https://shunyaueta.com/posts/2023-02-12-0133/","summary":"自分はかなり前から、タスクの時間計測のために toggl の無料枠を使わせてもらっていたが、作業計測をそもそも忘れたりしてしまい、振り返っても計測自体ができていないことが多かった。 あとはどこまで詳しく取るべきかの切り分けが出来ず、結局最近は個人プロジェクトの書籍翻訳の時間計測のためにしか利用できていなかった。\n最近集中力が散漫になっていることを課題に感じていたので、良い機会なのでこの機会にツールを乗り換えてみて、日々の生産性を向上できないか試してみる。\n色々と比較したが、 Session - Pomodoro focus timer with analytics が一番自分の求めているものに近かった。\nOSS が可能なら良かったが、単純なポモドーロタイマーならあるが、自分が求めている以下のような機能が無い。\n 非 Web アプリ 統計機能 web サイトとアプリのブロック 音楽を流す ポモドーロごとへの感想  なので、 Best Pomodoro Time Alternatives - 2023 | Product Hunt　で良さげなポモドーロタイマーアプリを探してみた。\nFlow – Pomodoro timer for Mac, iPhone, and iPadもミニマルなデザインかつ、月 1$ で、買い切り 30$と値段は非常に魅力的なのだが、session と比べると、細かい機能差があり、手に馴染みそうだったのが、session だと思ったので、こちらを使い始めてみる。\n特に session の統計画面は見返していて楽しそうである。\nApple のプラットフォームでしか使えないのは一つの懸念点だが、自分はなんだかんだ当面の間 Apple 製品を使うことになると思うので、その時が来たらまた別のツールを考えれば良い。\n今までは可能なら購読形式のツールは使いたくなかったが、このツールで自分の集中力を向上できるなら年課金したとしても安い買い物かなと思って開き直ってきた。 お金で、生産性を買う姿勢\u0026hellip;\nとりあえず、一ヶ月くらい使ってみます。\n以下のページで機能更新やカスタム方法などを公開してくれているが、こういうアップデート系統の記事って見るの楽しいですよね。過去の軌跡を追いかけることができますし。\nSession - Changelog\n一つの欠点は、非常に気に入っていた toggl と Obsidian の連携プラグインである","title":"時間計測アプリを toggl から Session へと乗り換えてみた"},{"content":"動機 Redis を扱っているのだが、その際にデバッグ用途で、内部で保存されているデータを redis-cli で対象の redis に対してコマンドを実行して確認する。\nだがこのままだとコマンドの結果が JSON として返されるが見にくいので見やすく整形したいのが動機。\n詳細 自分が知っている方法だと\n1  redis-cli   で、対象の redis に対してコマンドを実行できるインタラクティブモードに入る。 そして以下のコマンドを実行すると、VALUE の結果が確認できる。\n1  get KEY   この結果は JSON として出力されるが、ターミナル上では整形されず非常に見づらかった。\n解決方法 よく見ると公式ドキュメントに書かれていた1。\n以下のように redis-cli と同じ行で、redis に対して実行したいコマンドを追記して実行すれば、標準出力として表示される。\n1  redis-cli get KEY   この結果が JSON として出力され、可読性を向上させるために jq2 で整形したい場合は、以下のようなコマンドを実行すればよい\n1  redis-cli get KEY | jq .      To run a Redis command and return a standard output at the terminal, include the command to execute as separate arguments of redis-cli. https://redis.io/docs/ui/cli/\n \u0026#x21a9;\u0026#xfe0e; jq - jq is a lightweight and flexible command-line JSON processor. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2023-02-10-1740/","summary":"動機 Redis を扱っているのだが、その際にデバッグ用途で、内部で保存されているデータを redis-cli で対象の redis に対してコマンドを実行して確認する。\nだがこのままだとコマンドの結果が JSON として返されるが見にくいので見やすく整形したいのが動機。\n詳細 自分が知っている方法だと\n1  redis-cli   で、対象の redis に対してコマンドを実行できるインタラクティブモードに入る。 そして以下のコマンドを実行すると、VALUE の結果が確認できる。\n1  get KEY   この結果は JSON として出力されるが、ターミナル上では整形されず非常に見づらかった。\n解決方法 よく見ると公式ドキュメントに書かれていた1。\n以下のように redis-cli と同じ行で、redis に対して実行したいコマンドを追記して実行すれば、標準出力として表示される。\n1  redis-cli get KEY   この結果が JSON として出力され、可読性を向上させるために jq2 で整形したい場合は、以下のようなコマンドを実行すればよい\n1  redis-cli get KEY | jq .      To run a Redis command and return a standard output at the terminal, include the command to execute as separate arguments of redis-cli.","title":"redis-cli の結果を標準出力として受け取って jq でわかりやすく表示したい"},{"content":"今年は、副業を行っていなかったので、寄附金控除のみ。 時間を計測していたが、今年は 30 分で確定申告できた。\n 2020 年は紙の寄附金の証明書を集めておいて、手作業で入力後、寄附金控除アップロードした覚えがある。寄附金の証明書管理が面倒くさかった。 2021 年は寄附金控除は xml で吐き出せるようになったので、freee で xml を読み込んで提出1 。医療費控除や青色申告もあったのでまとめると 24 時間ほどを要した。  納税先とマイナポータルが連携しており、事前に設定しておけば、数クリックですべてのデータが連携されるのは非常に快適な体験だった。\ne-tax 連携のおかげで、寄附金の証明書を管理しなくて良くなったのは、良いことですね。 特にこれだけの規模の書類は全体で見ると決して小さくない数だと思うので、意味のあるペーパーレス化ですね。\ne-tax は毎年どんどん快適になるので、素晴らしいサービスだなと思っています。 マイナンバーと保険証連携による、医療費控除も数年後には実現しているのかなと思うと、更に確定申告が楽になりそうですね。 開発に携わっている方に感謝です。\n  2021 年分の確定申告 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2023-02-06-2305/","summary":"今年は、副業を行っていなかったので、寄附金控除のみ。 時間を計測していたが、今年は 30 分で確定申告できた。\n 2020 年は紙の寄附金の証明書を集めておいて、手作業で入力後、寄附金控除アップロードした覚えがある。寄附金の証明書管理が面倒くさかった。 2021 年は寄附金控除は xml で吐き出せるようになったので、freee で xml を読み込んで提出1 。医療費控除や青色申告もあったのでまとめると 24 時間ほどを要した。  納税先とマイナポータルが連携しており、事前に設定しておけば、数クリックですべてのデータが連携されるのは非常に快適な体験だった。\ne-tax 連携のおかげで、寄附金の証明書を管理しなくて良くなったのは、良いことですね。 特にこれだけの規模の書類は全体で見ると決して小さくない数だと思うので、意味のあるペーパーレス化ですね。\ne-tax は毎年どんどん快適になるので、素晴らしいサービスだなと思っています。 マイナンバーと保険証連携による、医療費控除も数年後には実現しているのかなと思うと、更に確定申告が楽になりそうですね。 開発に携わっている方に感謝です。\n  2021 年分の確定申告 \u0026#x21a9;\u0026#xfe0e;\n   ","title":"2022年分の確定申告"},{"content":"私たちは子どもに何ができるのか――非認知能力を育み、格差に挑むを読んだ。\n3 行感想  非認知能力というのは、環境によって育まれるものであり、明示的にそのスキルを伸ばそうとして伸ばせるものではない 子供の感情の揺れ動きをサポートするために、予期しないことが発生しない平穏な環境を提供するのが親の役目。予期しないことが起こったとしても、親は決して直情的な反応をせず淡々と、共感して、愛情を持って接する 愛情を持って接する介入効果で、子供の身体的成長にも好影響を及ぼした結果とかあって人類面白すぎる  短く端的にどんなことが影響があるのか、何を親は徹底すべきなのかが明示的に書かれている。(総ページ数も 120p ほどでサクッと読めます)\n特に、自分が持っていた疑問である\n「で、結局非認知能力がもてはやされいるけど、どうやって伸ばすの結局何なの?」\nのしっくりした答えがここに書かれいて、この点だけでもよかった。\n","permalink":"https://shunyaueta.com/posts/2023-01-18-1001/","summary":"私たちは子どもに何ができるのか――非認知能力を育み、格差に挑むを読んだ。\n3 行感想  非認知能力というのは、環境によって育まれるものであり、明示的にそのスキルを伸ばそうとして伸ばせるものではない 子供の感情の揺れ動きをサポートするために、予期しないことが発生しない平穏な環境を提供するのが親の役目。予期しないことが起こったとしても、親は決して直情的な反応をせず淡々と、共感して、愛情を持って接する 愛情を持って接する介入効果で、子供の身体的成長にも好影響を及ぼした結果とかあって人類面白すぎる  短く端的にどんなことが影響があるのか、何を親は徹底すべきなのかが明示的に書かれている。(総ページ数も 120p ほどでサクッと読めます)\n特に、自分が持っていた疑問である\n「で、結局非認知能力がもてはやされいるけど、どうやって伸ばすの結局何なの?」\nのしっくりした答えがここに書かれいて、この点だけでもよかった。","title":"「私たちは子どもに何ができるのか」を読んだ"},{"content":"恒例の買って愛用しているものの振り返り記事です。\n過去の記事\n 2021 年に買って愛用しているもの   #blog\n恒例の振り返りです。\n 南部鉄器製の鉄器  手持ちのテフロン加工の調理器具が剥げてきて寿命を迎えつつあったので、以前から興味があった南部鉄器製の鉄器に挑戦してみた。最初は以下の二つをセットで買った。  岩鋳 揚鍋 16cm 岩鋳フライパン 24cm   結果的に大正解で、このおかげで料理をするのが更に楽しくなった。特に揚げ鍋、揚げ物の仕上がりが顕著に変わり家族から大好評。フライパンも最高に美味しいチャーハンが作れる。その後、鉄製の調理器具が気に入って、卵焼き器も鉄製に買い替えた。  及源 玉子焼き 角玉子焼   この卵焼き器はまだ修行を終えていないので必ず焦げ付いてしまうのが、焦げ付きがなくなってきているのを見るのもまた楽しみ。利用後のメンテナンスがめんどくさいと言われているが、自分は幸運にも面倒くさく感じない性質で良かった。自分が死ぬまで短めに見ても 40 年はあるが、死ぬまでこの器具を使えるというのはいいものですね。3-4 年スパンでフライパンを使い切っては買い換えるというのも嫌だったので丁度いい。 あたりまえのぜひたくという料理エッセイが自分は非常に好きなんですが、そこの茶碗蒸しの回がめっちゃ好きで、中華鍋とせいろで茶碗蒸しを作るんですが、自分でもやりたいな~と惹かれている。だが、中華鍋とせいろが置く場所が我が家には無いので、広い家に引っ越したら挑戦したい\u0026hellip; 料理は日常の中で試行回数を重ねられるし、美味しいものが食べれるしでいいことだらけの趣味ですね~。一軒家とかに住んだら七輪とかも導入してろばた焼きもやりたい。   スマートエンジェル イージスジュニア G  生まれたての子供を乗せるためにチャイルドシートを買っていたのだが、車での外出時にどうにも起きに召していないので、買い替えてみたところ大正解だった。(妻の提案に感謝)。このおかげで車に乗った際に子供が鬼のように泣くのをやめて、寝るようになり車で遠出・旅行ができるようになった。 スマートエンジェルは西松屋のプライベートブランドなのですが、どれも品質が高くてかつお値段がマザーテレサ並みに優しく愛用しています。   スマホの車載ホルダー  以前持っていた Anker の車載ホルダーは、子供が分解するのにハマっており、どこかに消え去ったので購入し直した。Anker の車載ホルダーを利用してわかったことは、自分にはワイヤレス充電機能は必要ないし、ホルダー形式も万力タイプで挟むのではなくフックタイプが良いということ。万力タイプで挟む形式は、密着率をあげるためのゲルが剥がれてきてしまい耐用回数が少ないことがわかった。なので、その不満点を解消するために、フック式スマホ車載ホルダー を購入した。完全に蛇足ですが、この系統の商品って Amazon だと有象無象のクローン商品があって、ガワは違うけど、OEM で全部中身同じではという商品しか見当たらないので、良い商品を探し当てるの大変ですね。   Amazon ベーシック デスクマウント シングル モニターアーム ブラック  リモートワークが始まった当初は、ダンボール箱の上にディスプレイを置いて、高さを底上げしてたんですが、最近、微妙に高さが足りていないのが気になってきた。日々使うものだからもっと投資しても、良いなと思いブラックフライデーで安くなっていたので買った。首を少し傾けないと行けない状態がなくなり、デスク周りもスッキリして快適。この系統の商品も 10 年以上は余裕で耐えると思うので、もっと早めに買っておけばよかった。   OOFOS(ウーフォス)  腰痛をいかに抑えるかが長期的に快適に働くための大事な要素だと思っている。そのためスタンディング状態で気軽に移れるようにしたかったので、購入。商品の存在を知ったキッカケは同僚が買っていたことで、近くのスポーツショップで試着をしてウーフォスに決めた。リカバリーサンダルは様々なブランドが商品を出してきており、有名なメーカーだとテリックやホカオネオネなどがあるが、自分は足型・触感ともにウーフォスが一番好みだった。去年、スタンディングデスクマットを買ったが、それと併用して使っています。    Software  Habitify  習慣化アプリ。買いきりライセンスで、プレミアム版を買った。自分は、サブスクはできるだけ増やしたくない派なのでありがたい。定常的に飲んでおきたい薬(自分の場合は花粉症の舌下治療)や、習慣づけたい習慣行動などを登録している。   1Password 年間プラン  今まで 7 年ほど買い切りプランで生き残ってきたが、買いきりで購入したライセンスが古すぎて、そのライセンスだと 1Password が使えなくなったので思い切って購読形式に移管した。買い切り型は DropBox での同期形式だったが、購読形式は 1Password アカウントによる同期で、DropBox に縛られない同期ができて楽。支払いは、1Password のギフトカードを購入してそこから支払うことで割安に契約した 前述したとおり極力購読型の支出はしたくなかったのだが、1Password にはとても長い間お世話になっているので、応援の意味を込めて契約した。   MargineNote3  技術書のメモを Obsidian で取っていたのだが、取っている最中に自分が行う行動は、以下の二つに大きく分けられる。  コピペ → 構造化 → 頭の中に理解して反芻する コードを書いたり、ノートを取ったりする   １つ目の作業で、ざっと読み込んでいくのだが、PDF リーダーや epub リーダーからコピペをしているときにあれ、めっちゃめんどくさい。こんなにめんどくさいなら、だれか良いアプリとか開発しているのでは思っていた。 その後、Obsidian で PDF マーカーや注釈可能な拡張機能が無いかなとフォーラムを探してたら、MarginNote3 を愛用しているぜという投稿をキッカケに出会った。結果的になんで今まで使わなかったんだろうかというレベルで情報を構造化して読み取るのが楽になった。購読型ではなく買い切り形式だったのも非常にありがたかった。(大事なことなので、何回でも言わせてください) 今の状況を疑って、もっと便利にするにはどうしたらいいんだろうかと考える癖は本当に大事ですね。今までのコピペして、構造化してという時間がもったいなすぎる。    ","permalink":"https://shunyaueta.com/posts/2023-01-03-2213/","summary":"恒例の買って愛用しているものの振り返り記事です。\n過去の記事\n 2021 年に買って愛用しているもの   #blog\n恒例の振り返りです。\n 南部鉄器製の鉄器  手持ちのテフロン加工の調理器具が剥げてきて寿命を迎えつつあったので、以前から興味があった南部鉄器製の鉄器に挑戦してみた。最初は以下の二つをセットで買った。  岩鋳 揚鍋 16cm 岩鋳フライパン 24cm   結果的に大正解で、このおかげで料理をするのが更に楽しくなった。特に揚げ鍋、揚げ物の仕上がりが顕著に変わり家族から大好評。フライパンも最高に美味しいチャーハンが作れる。その後、鉄製の調理器具が気に入って、卵焼き器も鉄製に買い替えた。  及源 玉子焼き 角玉子焼   この卵焼き器はまだ修行を終えていないので必ず焦げ付いてしまうのが、焦げ付きがなくなってきているのを見るのもまた楽しみ。利用後のメンテナンスがめんどくさいと言われているが、自分は幸運にも面倒くさく感じない性質で良かった。自分が死ぬまで短めに見ても 40 年はあるが、死ぬまでこの器具を使えるというのはいいものですね。3-4 年スパンでフライパンを使い切っては買い換えるというのも嫌だったので丁度いい。 あたりまえのぜひたくという料理エッセイが自分は非常に好きなんですが、そこの茶碗蒸しの回がめっちゃ好きで、中華鍋とせいろで茶碗蒸しを作るんですが、自分でもやりたいな~と惹かれている。だが、中華鍋とせいろが置く場所が我が家には無いので、広い家に引っ越したら挑戦したい\u0026hellip; 料理は日常の中で試行回数を重ねられるし、美味しいものが食べれるしでいいことだらけの趣味ですね~。一軒家とかに住んだら七輪とかも導入してろばた焼きもやりたい。   スマートエンジェル イージスジュニア G  生まれたての子供を乗せるためにチャイルドシートを買っていたのだが、車での外出時にどうにも起きに召していないので、買い替えてみたところ大正解だった。(妻の提案に感謝)。このおかげで車に乗った際に子供が鬼のように泣くのをやめて、寝るようになり車で遠出・旅行ができるようになった。 スマートエンジェルは西松屋のプライベートブランドなのですが、どれも品質が高くてかつお値段がマザーテレサ並みに優しく愛用しています。   スマホの車載ホルダー  以前持っていた Anker の車載ホルダーは、子供が分解するのにハマっており、どこかに消え去ったので購入し直した。Anker の車載ホルダーを利用してわかったことは、自分にはワイヤレス充電機能は必要ないし、ホルダー形式も万力タイプで挟むのではなくフックタイプが良いということ。万力タイプで挟む形式は、密着率をあげるためのゲルが剥がれてきてしまい耐用回数が少ないことがわかった。なので、その不満点を解消するために、フック式スマホ車載ホルダー を購入した。完全に蛇足ですが、この系統の商品って Amazon だと有象無象のクローン商品があって、ガワは違うけど、OEM で全部中身同じではという商品しか見当たらないので、良い商品を探し当てるの大変ですね。   Amazon ベーシック デスクマウント シングル モニターアーム ブラック  リモートワークが始まった当初は、ダンボール箱の上にディスプレイを置いて、高さを底上げしてたんですが、最近、微妙に高さが足りていないのが気になってきた。日々使うものだからもっと投資しても、良いなと思いブラックフライデーで安くなっていたので買った。首を少し傾けないと行けない状態がなくなり、デスク周りもスッキリして快適。この系統の商品も 10 年以上は余裕で耐えると思うので、もっと早めに買っておけばよかった。   OOFOS(ウーフォス)  腰痛をいかに抑えるかが長期的に快適に働くための大事な要素だと思っている。そのためスタンディング状態で気軽に移れるようにしたかったので、購入。商品の存在を知ったキッカケは同僚が買っていたことで、近くのスポーツショップで試着をしてウーフォスに決めた。リカバリーサンダルは様々なブランドが商品を出してきており、有名なメーカーだとテリックやホカオネオネなどがあるが、自分は足型・触感ともにウーフォスが一番好みだった。去年、スタンディングデスクマットを買ったが、それと併用して使っています。    Software  Habitify  習慣化アプリ。買いきりライセンスで、プレミアム版を買った。自分は、サブスクはできるだけ増やしたくない派なのでありがたい。定常的に飲んでおきたい薬(自分の場合は花粉症の舌下治療)や、習慣づけたい習慣行動などを登録している。   1Password 年間プラン  今まで 7 年ほど買い切りプランで生き残ってきたが、買いきりで購入したライセンスが古すぎて、そのライセンスだと 1Password が使えなくなったので思い切って購読形式に移管した。買い切り型は DropBox での同期形式だったが、購読形式は 1Password アカウントによる同期で、DropBox に縛られない同期ができて楽。支払いは、1Password のギフトカードを購入してそこから支払うことで割安に契約した 前述したとおり極力購読型の支出はしたくなかったのだが、1Password にはとても長い間お世話になっているので、応援の意味を込めて契約した。   MargineNote3  技術書のメモを Obsidian で取っていたのだが、取っている最中に自分が行う行動は、以下の二つに大きく分けられる。  コピペ → 構造化 → 頭の中に理解して反芻する コードを書いたり、ノートを取ったりする   １つ目の作業で、ざっと読み込んでいくのだが、PDF リーダーや epub リーダーからコピペをしているときにあれ、めっちゃめんどくさい。こんなにめんどくさいなら、だれか良いアプリとか開発しているのでは思っていた。 その後、Obsidian で PDF マーカーや注釈可能な拡張機能が無いかなとフォーラムを探してたら、MarginNote3 を愛用しているぜという投稿をキッカケに出会った。結果的になんで今まで使わなかったんだろうかというレベルで情報を構造化して読み取るのが楽になった。購読型ではなく買い切り形式だったのも非常にありがたかった。(大事なことなので、何回でも言わせてください) 今の状況を疑って、もっと便利にするにはどうしたらいいんだろうかと考える癖は本当に大事ですね。今までのコピペして、構造化してという時間がもったいなすぎる。    ","title":"2022年に買って愛用しているもの"},{"content":"Elasticsearch を Go lang から利用できるクライアントライブラリで、非公式なパッケージではolivere/elasticがあります。 このパッケージはかなりの人気を誇っており、なんと 2012 年から開発され現状 Elasticsearch v1.0 からすべてのバージョンをサポートしています。 素晴らしい OSS ですね。\nES8 をサポートせずにメンテナンスモードに入る経緯 ES8 以降のサポートをどうするか決めあぐねている現状の経緯はこちらの issue に olivere さんから直接説明してくれています。 直接 olivere さんの意見を読んだほうが良いので、引用のみしておきます。\nAnnouncement: Future directions (updated on 2022-07-22) · Issue #1533 · olivere/elastic\n Finally a word about this repository. I\u0026rsquo;m sorry about all the people still waiting for an Elastic v8 version, waiting for their PRs to be merged and issues to be answered. I had a hard time personally over the last two years, and when I had to decide whether working on Elastic or focusing on my health is the top priority, I always decide for the latter. Now that I\u0026rsquo;ve seen the future of the Go client for Elasticsearch, I couldn\u0026rsquo;t be happier, and I\u0026rsquo;ll be the first one to switch and use the typed API client. So, unfortunately, and sad to some extend, but I have to say: Elastic v8 won\u0026rsquo;t happen. I will polish v7 if I have the time and energy, but v8 won\u0026rsquo;t happen. I\u0026rsquo;m sorry.\n 作成者の Olivere さん、お疲れさまでした。 10 年間の OSS 開発と何千時間もの時間の投資には経緯しか抱けません。\nBuy Me a Coffee] というサービスで Olivere さんを支援できるので、もしご機会のある方は支援しても良いかもしれません。1\n追記 勢いで Buy Me a Coffee で支援したら 10m くらいで Olivere さんから熱い返信が来て、テンションがぶち上がりました。\n Thank you very much! It was my way of paying back just a tiny bit for all the libraries that I\u0026rsquo;ve been using throughout all of my career. We are all truly standing on the shoulder of giants. (And everyone should help wherever possible.) Thanks you again.\n 余談 この issues がきっかけで知りましたが、Typed API や specification の存在をしりましたが、とても面白そうですね。\n TypedAPI: Integration of the first working version in the client by Anaethelion · Pull Request #508 · elastic/go-elasticsearch elastic/elasticsearch-specification: Elasticsearch full specification    僕は、この issue を見て感銘を受けたのですが、気がついたら投げ銭していました。 https://www.buymeacoffee.com/Bjd96U8fm \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-12-13-2218/","summary":"Elasticsearch を Go lang から利用できるクライアントライブラリで、非公式なパッケージではolivere/elasticがあります。 このパッケージはかなりの人気を誇っており、なんと 2012 年から開発され現状 Elasticsearch v1.0 からすべてのバージョンをサポートしています。 素晴らしい OSS ですね。\nES8 をサポートせずにメンテナンスモードに入る経緯 ES8 以降のサポートをどうするか決めあぐねている現状の経緯はこちらの issue に olivere さんから直接説明してくれています。 直接 olivere さんの意見を読んだほうが良いので、引用のみしておきます。\nAnnouncement: Future directions (updated on 2022-07-22) · Issue #1533 · olivere/elastic\n Finally a word about this repository. I\u0026rsquo;m sorry about all the people still waiting for an Elastic v8 version, waiting for their PRs to be merged and issues to be answered. I had a hard time personally over the last two years, and when I had to decide whether working on Elastic or focusing on my health is the top priority, I always decide for the latter.","title":"Elasticsearch の Go client で有名な olivere/elastic がES8 以降はサポートしないので、利用者は公式クライアントに移行しよう"},{"content":"Search Engineering Newsletter vol.04 では、Revue から自分の Blog での配信に移りました。 その時の経緯としては\n だが、自分のサイトは公開しているので、誰でも見れる。 そのためリーチする層に差が出るわけでもなかったかなと今更ながら気づいた。\n でしたが、\n 実際問題、自前の Blog でもニュースレターでも誰でも見れる事自体は変わらないが、誰に届くかは Blog ではなくニュースレターの方が幅広い読者に見てもらえるのでは?  ニュースレター専用のメディアにすることで、ニュースレター自体がどれくらい需要があるかを切り分けて追跡しやすい 1 最近は更新を告知するチャンネルの Twitter が将来的にどうなるかわからない状態2。そしてその流入量は決して少なくない。   また、自分が書いた記事を届ける手段をプラットフォームに依存しない形式で保持して、読者とつながっておくのは改めて重要性を感じた  そのため、substack でニュースレターを再開してみることにしました。\nsubstack で期待していることは、読者とのコミュニケーションが取りやすくなるのでニュースレターを通じて、ピックアップした記事の議論や感想などを交えるようになると面白そうだなと思っています。\n配信方法を都度変えて、読者の方にはお手間をおかけしますが、配信しているニュースレターを面白いと感じていただけた方は substack での購読を以下からよろしくおねがいします。 ニュースレターの配信もこれ以降は substack のみで行う予定です。\n https://searchengineeringnewsletter.substack.com/\n  過去記事もこの記事以外は Blog からは削除して、substack に移行してみます。 \u0026#x21a9;\u0026#xfe0e;\n 既存の購読者の方がいた revue に戻ろうかと思ったのですが、Briefing: Twitter Will Shut Down Newsletter Product Revue By Year End — The Information という記事をみてやめた。実際に購読しているニュースレターが revue をやめまくっている(今回の Twiter 買収で Revue が停止するという情報がでまわっているため。) いまのイーロン・マスクの動きを見ていると選択と集中の時期で、revue を維持するという未来が予想できない。 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-12-04-0003/","summary":"Search Engineering Newsletter vol.04 では、Revue から自分の Blog での配信に移りました。 その時の経緯としては\n だが、自分のサイトは公開しているので、誰でも見れる。 そのためリーチする層に差が出るわけでもなかったかなと今更ながら気づいた。\n でしたが、\n 実際問題、自前の Blog でもニュースレターでも誰でも見れる事自体は変わらないが、誰に届くかは Blog ではなくニュースレターの方が幅広い読者に見てもらえるのでは?  ニュースレター専用のメディアにすることで、ニュースレター自体がどれくらい需要があるかを切り分けて追跡しやすい 1 最近は更新を告知するチャンネルの Twitter が将来的にどうなるかわからない状態2。そしてその流入量は決して少なくない。   また、自分が書いた記事を届ける手段をプラットフォームに依存しない形式で保持して、読者とつながっておくのは改めて重要性を感じた  そのため、substack でニュースレターを再開してみることにしました。\nsubstack で期待していることは、読者とのコミュニケーションが取りやすくなるのでニュースレターを通じて、ピックアップした記事の議論や感想などを交えるようになると面白そうだなと思っています。\n配信方法を都度変えて、読者の方にはお手間をおかけしますが、配信しているニュースレターを面白いと感じていただけた方は substack での購読を以下からよろしくおねがいします。 ニュースレターの配信もこれ以降は substack のみで行う予定です。\n https://searchengineeringnewsletter.substack.com/\n  過去記事もこの記事以外は Blog からは削除して、substack に移行してみます。 \u0026#x21a9;\u0026#xfe0e;\n 既存の購読者の方がいた revue に戻ろうかと思ったのですが、Briefing: Twitter Will Shut Down Newsletter Product Revue By Year End — The Information という記事をみてやめた。実際に購読しているニュースレターが revue をやめまくっている(今回の Twiter 買収で Revue が停止するという情報がでまわっているため。) いまのイーロン・マスクの動きを見ていると選択と集中の時期で、revue を維持するという未来が予想できない。 \u0026#x21a9;\u0026#xfe0e;","title":"Search Engineering Newsletter を substack へ移行します"},{"content":"毎年開催される Apache Beam の会議、Beam Summit 2022 で講演資料が公開されていたので、気になる資料を読んだ。\n以下に面白かった記事の備忘録を放流しておく\nGoogle\u0026rsquo;s investment on Beam, and internal use of Beam at Google  Google 内部で現在フルタイム Beam 開発者は 25 人! (多いな) Go SDK 提供開始がめでたい  現在は Java, Python, Go の３つの言語をサポート   機械学習の推論を Beam の特性を生かしてスケーラブルに実行可能な RunInference も提供できた! TypeScript SDK も提供予定!  contribution している方も募集中 https://github.com/apache/beam/tree/master/sdks/typescript   Beam Playground を使えば、Beam がより効果的に学べるよ https://play.beam.apache.org/ チケット管理では Jira をやめて GitHub Issues に移行したよ(最近の Apache Project の潮流な気がする。Lucene も移行していた)  Beam @TwitterEvaluation, Adoption, Migration and future.  毎日実行される  data pipeline の総数 5 万 200PB 超えのボリュームをデータ処理 7 兆のイベント数   Beam の魅力  batch, streaming の両者を扱うことができる、かつモダンな実行フレームワーク ランナーの柔軟性 複数のクラウド環境で実行可能 複数のプログラミング言語で動く 優れた OSS コミュニティ    RunInference: Machine Learning Inferences in Beam Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる でも紹介した、 RunInference API の講演\n内容自体は、自分が書いた記事と目立って違ったことは言っていなかったが、Beam の特性で推論を効率化できるのは面白い展開だと思う。 まだサポートしている機械学習フレームワークが、 scikit-learn、Pytorch, TensorFlow だけなので、これからどんどん増やしていくらしいです。\n","permalink":"https://shunyaueta.com/posts/2022-11-06-2252/","summary":"毎年開催される Apache Beam の会議、Beam Summit 2022 で講演資料が公開されていたので、気になる資料を読んだ。\n以下に面白かった記事の備忘録を放流しておく\nGoogle\u0026rsquo;s investment on Beam, and internal use of Beam at Google  Google 内部で現在フルタイム Beam 開発者は 25 人! (多いな) Go SDK 提供開始がめでたい  現在は Java, Python, Go の３つの言語をサポート   機械学習の推論を Beam の特性を生かしてスケーラブルに実行可能な RunInference も提供できた! TypeScript SDK も提供予定!  contribution している方も募集中 https://github.com/apache/beam/tree/master/sdks/typescript   Beam Playground を使えば、Beam がより効果的に学べるよ https://play.beam.apache.org/ チケット管理では Jira をやめて GitHub Issues に移行したよ(最近の Apache Project の潮流な気がする。Lucene も移行していた)  Beam @TwitterEvaluation, Adoption, Migration and future.","title":" Beam summit 2022 雑感"},{"content":"2021 年に引き続き、2022 年も情報検索・検索技術 Advent Calendar を作ってみました。\n情報検索・検索技術 Advent Calendar 2022 - Adventar\nkivantium さんの 創作+機械学習 Advent Calendar 2022 を開催します - kivantium 活動日記 の記事がいいなと思ったので、僕も自分の Blog で告知しておきます。\n2021 年にアドベントカレンダーを作成したきっかけとしては、そもそもブログ記事の執筆が自分は好き他人が書いた記事を読むのは楽しい。 アドベントカレンダーの文化はそういう自分の嗜好にぴったりなので、自分の好きな検索技術領域がまだ作られていない! 作らねば! というのがモチベーションでした。\n実際のところ、検索技術に携わってはいるが、Blog 記事をあまり書かない人もアドベントカレンダー起因で記事を書くきっかけになっているじゃないかなと思っています。\n現時点で 登録数 12/25人となっています。ご登録頂いた方々ありがとうございます! みんなでワイワイ投稿して盛り上げていきましょう。\n","permalink":"https://shunyaueta.com/posts/2022-11-05-1147/","summary":"2021 年に引き続き、2022 年も情報検索・検索技術 Advent Calendar を作ってみました。\n情報検索・検索技術 Advent Calendar 2022 - Adventar\nkivantium さんの 創作+機械学習 Advent Calendar 2022 を開催します - kivantium 活動日記 の記事がいいなと思ったので、僕も自分の Blog で告知しておきます。\n2021 年にアドベントカレンダーを作成したきっかけとしては、そもそもブログ記事の執筆が自分は好き他人が書いた記事を読むのは楽しい。 アドベントカレンダーの文化はそういう自分の嗜好にぴったりなので、自分の好きな検索技術領域がまだ作られていない! 作らねば! というのがモチベーションでした。\n実際のところ、検索技術に携わってはいるが、Blog 記事をあまり書かない人もアドベントカレンダー起因で記事を書くきっかけになっているじゃないかなと思っています。\n現時点で 登録数 12/25人となっています。ご登録頂いた方々ありがとうございます! みんなでワイワイ投稿して盛り上げていきましょう。","title":"情報検索・検索技術 Advent Calendar 2022 を開催します"},{"content":"VSCode をcodeコマンドから実行可能にするとPATHに Visual Studio Code の空白スペースが含まれてしまうことが原因でこのエラーが発生する。\n具体的には VSCcode の PATH が以下のように登録されてしまっている。\n1  PATH = ...PATH:/Applications/Applications/Visual Studio Code.app/Contents/Resources/app/bin   自分が遭遇したエラーは、環境変数を参照する make コマンドで\n1 2  make test env: Studio: No such file or directory   というエラーが出てくるようになり、make task が実行できなくなってしまった。\n原因として自分の場合は、brew で VSCode をインストールしなおしたら、このエラーが出てくるようになった。\n対処方法 公式ページに書いてあるとおりの方式1でパスを通せば解決する。 具体的に解説すると\n VS Code を起動 コマンドパレット(Cmd+Shift+P)を開いて、shell commandと打ち込み、Shell Command: Install 'code' command in PATH を選択して実行  でこのエラーが出てこなくなる。\nもしくは、ダブルクォーテーションでPATHを登録すればこの問題は回避可能\n1  export PATH=\u0026#34;\\$PATH:/Applications/Visual Studio Code.app/Contents/Resources/app/bin\u0026#34;     Visual Studio Code on macOS You can also run VS Code from the terminal by typing \u0026lsquo;code\u0026rsquo; after adding it to the path:\n Launch VS Code. Open the Command Palette (Cmd+Shift+P) and type \u0026lsquo;shell command\u0026rsquo; to find the Shell Command: Install \u0026lsquo;code\u0026rsquo; command in PATH command.  \u0026#x21a9;\u0026#xfe0e;   ","permalink":"https://shunyaueta.com/posts/2022-11-04-1727/","summary":"VSCode をcodeコマンドから実行可能にするとPATHに Visual Studio Code の空白スペースが含まれてしまうことが原因でこのエラーが発生する。\n具体的には VSCcode の PATH が以下のように登録されてしまっている。\n1  PATH = ...PATH:/Applications/Applications/Visual Studio Code.app/Contents/Resources/app/bin   自分が遭遇したエラーは、環境変数を参照する make コマンドで\n1 2  make test env: Studio: No such file or directory   というエラーが出てくるようになり、make task が実行できなくなってしまった。\n原因として自分の場合は、brew で VSCode をインストールしなおしたら、このエラーが出てくるようになった。\n対処方法 公式ページに書いてあるとおりの方式1でパスを通せば解決する。 具体的に解説すると\n VS Code を起動 コマンドパレット(Cmd+Shift+P)を開いて、shell commandと打ち込み、Shell Command: Install 'code' command in PATH を選択して実行  でこのエラーが出てこなくなる。\nもしくは、ダブルクォーテーションでPATHを登録すればこの問題は回避可能\n1  export PATH=\u0026#34;\\$PATH:/Applications/Visual Studio Code.app/Contents/Resources/app/bin\u0026#34;     Visual Studio Code on macOS You can also run VS Code from the terminal by typing \u0026lsquo;code\u0026rsquo; after adding it to the path:","title":"env Studio No such file or directory というVisual Studio Code 起因のエラーへの対処方法"},{"content":"表題の通り、Elasticsearch 8.4 から待望の近似近傍探索と従来の検索を組み合わたハイブリッド検索が可能になったらしいので、試してみました。\nElascticsearch 8 で導入された近似近傍探索について Elasticsearch 公式の記事1がわかりやすく近似近傍探索について語られています。 また、日本語では@pakio さんの紹介記事2も非常にわかりやすいので、そちらも御覧ください。\n嬉しいけど物足りない点 公式の資料3や@pakio さんの資料でも触れられていますが、\n  You can’t currently use the Query DSL to filter documents for an approximate kNN search. If you need to filter the documents, consider using exact kNN instead.   Elasticsearch の Query DSL との併用不可というのが物足りない点でした。\n端的に説明すると Elasticsearch 8 で利用可能になった近似近傍探索は、あくまでベクトル間のみの近似近傍探索のみできるのであって、従来の Elasticsearch の検索機能(term や filter)と近似近傍探索を組み合わせて検索できないということです。\nVespa の開発者の Jo さんも同様の点4について触れていました。\n The most surprising part of the announcement is that they won\u0026rsquo;t allow combining the nearest neighbor search with standard query terms and filters. I think that will be disappointing for many who have been waiting for this feature. That concludes this thread, thoughts 👇🧵 5/5\n なぜならベクトルだけの素朴な近似近傍探索なら、正直ベクトル検索エンジンのQdrant、Milvus 、Valdでも事足ります。 検索エンジンの Elasticsearch に求められるのは、従来のフィルタリング機能と近似近傍探索を組み合わせたり、term による検索と近似近傍探索を組み合わせなど素朴な近似近傍探索では実現できない点を補える事ができれば近似近傍探索の実用可能性がグンと広がるので、ちょっと物足りないねというのが正直な感想でした。\nそれを実現するための関連するチケット5などはあったのでいつか実現してほしいなと思っていましたが、Linkedin の投稿で Elasticsearch のエンジニアの方が遂に実現できたよ!!と紹介しており6 、早速試してみたいと思います。\n2022/10/30 追記: Elasticsearch version 8.2.0 | Elasticsearch Guide [8.2] | Elastic のリリースで\n Integrate filtering support for ANN #84734 (issue: #81788)\n と書かれており、ANN でのフィルタリングサポートは Elasticsearch 8.2 からサポートされていたみたいです。 なので正確には Elasticsearch 8.4 からは検索クエリが使えるようになったということですね。\nハイブリッド検索 Elasticsearch の公式ドキュメントはこちら7\nこのドキュメントを基に、\n 従来の検索機能 近似近傍探索のみによる検索結果 上記ふたつを組み合わせた検索結果  を一画面で表示できる Web アプリを作ったので結果を比較してみたいと思います。\nデータやインデキシング自体は前回作成した、Elasticsearch の近似近傍探索を使って、ドラえもんのひみつ道具検索エンジンを作ってみたを基に今回もコードは全て公開しています。release tag はv0.2 にしています。\nhttps://github.com/hurutoriya/doraemon-himitsu-dogu-search/releases/tag/v0.2.0\n前回からの変更点を主に書いていきます。\n日本語検索の設定 Docker ファイルで日本語用の analyzer であるkuromojiを追加し\n1 2 3 4 5  FROMdocker.elastic.co/elasticsearch/elasticsearch:8.4.3# set the specific password for ElasticsearchRUN echo \u0026#34;elastic\u0026#34; | bin/elasticsearch-keystore add \u0026#34;bootstrap.password\u0026#34; -xfRUN bin/elasticsearch-plugin install analysis-kuromojiRUN bin/elasticsearch-plugin install analysis-icu  Elasticsearch の mapping は zozo さんの記事8を参考にして以下のように書いてみました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custome_ja_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;icu_normalizer\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;kuromoji_tokenizer\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;kuromoji_baseform\u0026#34;, \u0026#34;kuromoji_part_of_speech\u0026#34;, \u0026#34;ja_stop\u0026#34;, \u0026#34;kuromoji_number\u0026#34;, \u0026#34;kuromoji_stemmer\u0026#34; ] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;vector\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 768, \u0026#34;index\u0026#34;: true, \u0026#34;similarity\u0026#34;: \u0026#34;l2_norm\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;custome_ja_analyzer\u0026#34; }, \u0026#34;yomi\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;custome_ja_analyzer\u0026#34; }, \u0026#34;description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;custome_ja_analyzer\u0026#34; } } } }   kuromoji のおかげで前処理を行わなくても、日本語で解析ができるようになります。\n実際に起動してインデックスを構築した、Elasticsearch が動く Docker にリクエストを投げてみると、analyzer によって処理がされます。 検索時にはこれらの tokens が転置インデックス上で検索されます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  curl --cacert http_ca.crt -u elastic:elastic -X POST \u0026#34;https://localhost:9200/himitsu_dogu/_analyze?pretty\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39;{\u0026#34;analyzer\u0026#34;: \u0026#34;custome_ja_analyzer\u0026#34;,\u0026#34;text\u0026#34;:\u0026#34;ドラえもんのひみつ道具はどこでもドア以外にもたくさん存在する\u0026#34;}\u0026#39; { \u0026#34;tokens\u0026#34; : [ { \u0026#34;token\u0026#34; : \u0026#34;ドラえもん\u0026#34;, \u0026#34;start_offset\u0026#34; : 0, \u0026#34;end_offset\u0026#34; : 5, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 0 }, { \u0026#34;token\u0026#34; : \u0026#34;ひむ\u0026#34;, \u0026#34;start_offset\u0026#34; : 6, \u0026#34;end_offset\u0026#34; : 8, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 2 }, { \u0026#34;token\u0026#34; : \u0026#34;道具\u0026#34;, \u0026#34;start_offset\u0026#34; : 9, \u0026#34;end_offset\u0026#34; : 11, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 4 }, { \u0026#34;token\u0026#34; : \u0026#34;どこ\u0026#34;, \u0026#34;start_offset\u0026#34; : 12, \u0026#34;end_offset\u0026#34; : 14, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 6 }, { \u0026#34;token\u0026#34; : \u0026#34;ドア\u0026#34;, \u0026#34;start_offset\u0026#34; : 16, \u0026#34;end_offset\u0026#34; : 18, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 8 }, { \u0026#34;token\u0026#34; : \u0026#34;以外\u0026#34;, \u0026#34;start_offset\u0026#34; : 18, \u0026#34;end_offset\u0026#34; : 20, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 9 }, { \u0026#34;token\u0026#34; : \u0026#34;たくさん\u0026#34;, \u0026#34;start_offset\u0026#34; : 22, \u0026#34;end_offset\u0026#34; : 26, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 12 }, { \u0026#34;token\u0026#34; : \u0026#34;存在\u0026#34;, \u0026#34;start_offset\u0026#34; : 26, \u0026#34;end_offset\u0026#34; : 28, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 13 } ] }   実際に簡単な multi match query(複数フィールドでの一致検索)の結果はこちら\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  $ curl --cacert http_ca.crt -u elastic:elastic -X POST \u0026#34;https://localhost:9200/himitsu_dogu/_search?pretty\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39;{\u0026#34;size\u0026#34;: 1, \u0026#34;query\u0026#34;: {\u0026#34;multi_match\u0026#34; : {\u0026#34;query\u0026#34;:\u0026#34;どこでもドア\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;name\u0026#34;, \u0026#34;description\u0026#34; ] }}}\u0026#39; { \u0026#34;took\u0026#34; : 2, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 44, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 11.774845, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;himitsu_dogu\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;816\u0026#34;, \u0026#34;_score\u0026#34; : 11.774845, \u0026#34;_source\u0026#34; : { \u0026#34;id\u0026#34; : 816, \u0026#34;name\u0026#34; : \u0026#34;どこでもドア\u0026#34;, \u0026#34;yomi\u0026#34; : \u0026#34;どこでもどあ\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;このドアを開けるだけで、行きたいところへ、どこへでも行くことができる。\u0026#34;, \u0026#34;vector\u0026#34; : [ -0.23546676337718964, ... -0.37538981437683105 ] } } ] } }   これで、Elasticsearch で日本語検索が問題なく出来ていることが確認できたので、ハイブリッド検索についての説明に\n結果比較 比較対象 multi match query, ANN, multi match + ANN の三種類のクエリで検索結果を定性的に比較します。 Python 上ではクエリは以下のような形式で書いてみました。TOP_Kは 10 で、keywordは入力クエリです。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  multimatch_query: dict = { \u0026#34;size\u0026#34;: TOP_K, \u0026#34;query\u0026#34;: {\u0026#34;multi_match\u0026#34;: {\u0026#34;query\u0026#34;: keyword, \u0026#34;fields\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;description\u0026#34;]}}, } ann_query: dict = { \u0026#34;knn\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;vector\u0026#34;, \u0026#34;query_vector\u0026#34;: sentence_embeddings[0], \u0026#34;k\u0026#34;: TOP_K, \u0026#34;num_candidates\u0026#34;: 100, } } # NOTE: score = match score + ANN score hybrid_query: dict = { \u0026#34;size\u0026#34;: TOP_K, \u0026#34;query\u0026#34;: {\u0026#34;multi_match\u0026#34;: {\u0026#34;query\u0026#34;:keyword, \u0026#34;fields\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;description\u0026#34;]}}, \u0026#34;knn\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;vector\u0026#34;, \u0026#34;query_vector\u0026#34;: sentence_embeddings[0], \u0026#34;k\u0026#34;: TOP_K, \u0026#34;num_candidates\u0026#34;: 100, }, }    multimatch_query: ひみつ道具の名前と説明文の複数フィールドでマッチするドキュメントを検索する ann_query: ひみつ道具の名前と説明文を空白区切りで結合された文章をJapaneseSententsBERT でベクトル化し、近似近傍探索を行う hybrid_query: 上記２つのクエリのスコアを足し合わせた結果(match score + ANN score)を検索結果として返す。  公式ドキュメントでは、boostを使うことで係数を書けて調整する方法も紹介されています。    実際にデモアプリの Streamlit 上では、上記３つのクエリの検索結果が side by side で比較できるようにしてみました。 これで、各検索手法で結果がどう異なっているか一目瞭然ですね。\nStreamlit で得られた結果を表として以下にまとめました。\nクエリ: AI    multi mach スコア ひみつ道具の名前 説明 ANN スコア ひみつ道具の名前 説明 hybrid(match score + ANN score スコア ひみつ道具の名前 説明     1 5.3606 エーアイアイ 高性能（せいのう）の AI（エーアイ／人工知能）が搭載（とうさい）されているサル型ロボット。一から教えてあげると、なんでも自分で考えて行動する。 1 0.0027 なんでもデリバリーバックパック デリバリー（配達）用のひみつ道具。中は四次元になっているため、いくらでも配達品を入れることができ、ゆれてもこわれることなく運べる。大きなポケットには、目的地まで自動運転してくれ、ひとこぎで１０メートル進む自転車が入っている。運転しながら食べたり飲んだりできる装置（そうち）も付いており、雨がふれば自動でカサもさしてくれる。 1 5.3606 エーアイアイ 高性能（せいのう）の AI（エーアイ／人工知能）が搭載（とうさい）されているサル型ロボット。一から教えてあげると、なんでも自分で考えて行動する。       2 0.0027 なりきりケイドロセット 本格的なおにごっこが楽しめる未来の遊び道具。通信もできる警察（けいさつ）手帳型のバッジや『強力におい追跡鼻（ついせきばな）』、身につけると体力が三割増しになる『泥棒風呂敷（どろぼうふろしき）』、『牢屋（ろうや）シール』などが入っている。 2 0.0027 なんでもデリバリーバックパック デリバリー（配達）用のひみつ道具。中は四次元になっているため、いくらでも配達品を入れることができ、ゆれてもこわれることなく運べる。大きなポケットには、目的地まで自動運転してくれ、ひとこぎで１０メートル進む自転車が入っている。運転しながら食べたり飲んだりできる装置（そうち）も付いており、雨がふれば自動でカサもさしてくれる。       3 0.0026 地下工事マシン この機械に設計図を描いた設計紙を入れ、コンクリートボンベを乗せてバルブを開くと、その設計図通りの空間を地下に作ってくれる。 3 0.0027 なりきりケイドロセット 本格的なおにごっこが楽しめる未来の遊び道具。通信もできる警察（けいさつ）手帳型のバッジや『強力におい追跡鼻（ついせきばな）』、身につけると体力が三割増しになる『泥棒風呂敷（どろぼうふろしき）』、『牢屋（ろうや）シール』などが入っている。       4 0.0026 ミチサキステッキ どちらの道を進めばいいか迷ったときに使う道具。正しい道の方向に倒れて、道を教えてくれる。 4 0.0026 地下工事マシン この機械に設計図を描いた設計紙を入れ、コンクリートボンベを乗せてバルブを開くと、その設計図通りの空間を地下に作ってくれる。       5 0.0026 ウラオモテックス これを体に貼ると、裏でコソコソやっていたことや心の中で思っていることを、みんなの前で大っぴらにやってしまう。 5 0.0026 ミチサキステッキ どちらの道を進めばいいか迷ったときに使う道具。正しい道の方向に倒れて、道を教えてくれる。       6 0.0026 かべ景色きりかえ機 かべを通して、どこの景色でも見ることができる。実際（じっさい）にうつし出した場所とつながるので、かべから出入りもできるようになる。 6 0.0026 ウラオモテックス これを体に貼ると、裏でコソコソやっていたことや心の中で思っていることを、みんなの前で大っぴらにやってしまう。       7 0.0026 クロマキーセット グリーンバックやブルーバックを背景（はいけい）にして、セットのカメラで撮影（さつえい）すると、映（うつ）したものを実際（じっさい）の景色にはめこむことができる。 7 0.0026 かべ景色きりかえ機 かべを通して、どこの景色でも見ることができる。実際（じっさい）にうつし出した場所とつながるので、かべから出入りもできるようになる。       8 0.0026 断層ビジョン 地中や機械の内部、人間の体の中など、外から見えないところを画面に映し出し、調べることができる。映し出したところに付属の旗を立てると、実際のその場所にも旗が立ち、目印になる。 8 0.0026 クロマキーセット グリーンバックやブルーバックを背景（はいけい）にして、セットのカメラで撮影（さつえい）すると、映（うつ）したものを実際（じっさい）の景色にはめこむことができる。       9 0.0026 ゆっくり反射ぞうきん このぞうきんで鏡やガラスを拭くと過去が映る。拭けば拭くほど、映る時間が過去に戻っていく。 9 0.0026 断層ビジョン 地中や機械の内部、人間の体の中など、外から見えないところを画面に映し出し、調べることができる。映し出したところに付属の旗を立てると、実際のその場所にも旗が立ち、目印になる。       10 0.0026 とうしめがね ムシめがねのようだが、これをつかってモノを見ると、中のようすがすけて見える。 10 0.0026 ゆっくり反射ぞうきん このぞうきんで鏡やガラスを拭くと過去が映る。拭けば拭くほど、映る時間が過去に戻っていく。    ANNでは検索対象になっていなかった秘密道具である「エーアイアイ」をmulti matchクエリでは検索できており、ハイブリッド検索によってmulti matchとANNがお互いを補うあうことで、主観的な評価ですが、検索結果になっていることが分かります。\nクエリ: ガリバー    multi match スコア ひみつ道具の名前 説明 ANN スコア ひみつ道具の名前 説明 hybrid(match score + ANN score スコア ひみつ道具の名前 説明     1 6.9183 ガリバートンネル 大きな入り口から入って、小さな出口から出ると、からだが小さくなるトンネル。逆にくぐりぬけると、もとに戻る。 1 0.0029 ガリバーロープ 巨大なものをかんたんにしばりつけることができるロープ。 1 6.9211 ガリバーロープ 巨大なものをかんたんにしばりつけることができるロープ。   2 6.9183 ガリバーロープ 巨大なものをかんたんにしばりつけることができるロープ。 2 0.0027 なりきりケイドロセット 本格的なおにごっこが楽しめる未来の遊び道具。通信もできる警察（けいさつ）手帳型のバッジや『強力におい追跡鼻（ついせきばな）』、身につけると体力が三割増しになる『泥棒風呂敷（どろぼうふろしき）』、『牢屋（ろうや）シール』などが入っている。 2 6.9183 ガリバートンネル 大きな入り口から入って、小さな出口から出ると、からだが小さくなるトンネル。逆にくぐりぬけると、もとに戻る。   3 5.8304 箱庭シリーズ 湖 大自然のミニチュアシリーズのひとつ。スモールライトやガリバートンネルで小さくなれば、この中で遊ぶことができる。 3 0.0027 スーパーダンごっこふろしき 未来の子どもたちがスーパーヒーローごっこで使うオモチャのふろしき。これを身につけると、低空ながら飛ぶことができ、空気銃の弾（たま）もはね返し、ものを透（す）かして見ることもできる。 3 5.8304 箱庭シリーズ 湖 大自然のミニチュアシリーズのひとつ。スモールライトやガリバートンネルで小さくなれば、この中で遊ぶことができる。   4 4.6887 ミニハウス エネルギーのムダをはぶくために作られた、ミニハウス。乾電池一本でクーラーや冷蔵庫などの家電製品をすべて動かすことができる。ガリバートンネルで小さくなって使う。 4 0.0026 ゴキブリトレとれビッグ ゴキブリをとる道具を、人間用に大きくしたもの。入ったら最後、ねんちゃくシートにつかまってしまう。 4 4.6887 ミニハウス エネルギーのムダをはぶくために作られた、ミニハウス。乾電池一本でクーラーや冷蔵庫などの家電製品をすべて動かすことができる。ガリバートンネルで小さくなって使う。   5 4.5612 ムシムシ操縦ハンドル このハンドルを自分が乗りたい虫に向けて、真ん中のボタンを押すと、その虫の背中にハンドルがくっつき、背中に乗って操縦することができる。ただし、ガリバートンネルなどで身体を小さくしてから使う。 5 0.0026 実景ひきよせ額縁 額縁の横にあるダイヤルを回すと、海でも雪山でもジャングルでも、実際にある景色をひきよせることができる。額縁のガラスを外してくぐれば、その景色の場所に行くこともできる。 5 4.5612 ムシムシ操縦ハンドル このハンドルを自分が乗りたい虫に向けて、真ん中のボタンを押すと、その虫の背中にハンドルがくっつき、背中に乗って操縦することができる。ただし、ガリバートンネルなどで身体を小さくしてから使う。       6 0.0026 ぼんのうボウシ これを頭にかぶると、その人の煩悩（ぼんのう）が雲のようなものに映像（えいぞう）となってうかび上がり、釣鐘（つりがね）のかたちにまとまる。それを撞木（しゅもく）型のハンマーでたたくと、それらの煩悩が追い払（はら）われる。 6 0.0027 なりきりケイドロセット 本格的なおにごっこが楽しめる未来の遊び道具。通信もできる警察（けいさつ）手帳型のバッジや『強力におい追跡鼻（ついせきばな）』、身につけると体力が三割増しになる『泥棒風呂敷（どろぼうふろしき）』、『牢屋（ろうや）シール』などが入っている。       7 0.0025 ねがい七夕ロケット 専用（せんよう）の短冊（たんざく）に願いごとを書いて笹（ささ）につけ、笹ごとロケットで宇宙（うちゅう）に飛ばすと、一年間その願いごとをかなえてくれる。短冊に書いたことと反対のことがかなう「うら七夕ロケット」もある。 【うら七夕ロケット】 専用（せんよう）の短冊（たんざく）に願いごとを書いて笹（ささ）につけ、笹ごとロケットで宇宙（うちゅう）に飛ばすと、一年間、短冊に書いたことと反対のことがかなう。 7 0.0027 スーパーダンごっこふろしき 未来の子どもたちがスーパーヒーローごっこで使うオモチャのふろしき。これを身につけると、低空ながら飛ぶことができ、空気銃の弾（たま）もはね返し、ものを透（す）かして見ることもできる。       8 0.0025 フエルミラー 増やしたいものをこのカガミにうつすと、本物になって出てくる。 8 0.0026 ゴキブリトレとれビッグ ゴキブリをとる道具を、人間用に大きくしたもの。入ったら最後、ねんちゃくシートにつかまってしまう。       9 0.0025 ふわふわぐすり これを口に入れると、体の中にガスができて、空気より軽くなるため、空を歩いたり、飛んだりすることができる。 9 0.0026 実景ひきよせ額縁 額縁の横にあるダイヤルを回すと、海でも雪山でもジャングルでも、実際にある景色をひきよせることができる。額縁のガラスを外してくぐれば、その景色の場所に行くこともできる。       10 0.0025 乗りものぐつ これに両足を入れてボタンを押すと、車やジェット機、潜水艦（せんすいかん）など、いろいろな乗りものになる。 10 0.0026 ぼんのうボウシ これを頭にかぶると、その人の煩悩（ぼんのう）が雲のようなものに映像（えいぞう）となってうかび上がり、釣鐘（つりがね）のかたちにまとまる。それを撞木（しゅもく）型のハンマーでたたくと、それらの煩悩が追い払（はら）われる。    multi matchクエリでは、「ガリバー」とう単語が全てマッチし検索結果としては申し分ありません。 ANNは「ガリバー」というクエリによって、「ガリバーロープ」はヒットしていますが「ガリバートンネル」はなぜか出てきていません。 ハイブリッド検索によって、上位 5 個はmulti matchクエリによる適合率(precision)が高いひみつ道具がランク付けされ、その後 ANN の結果が混ざってきています。正直「ガリバー」という単語から想起するようなひみつ道具とはあまり感じませんが、特徴ベクトルの閾値などで上手く枝刈りさえしておけば、質は担保しつつ多様性のある結果が確保できそうです。\n実際はこんなに単純な multi match計算ではなく、BM25 で計算されたスコアを導入したりなど工夫すべき点は多々ありますが、ハイブリッド検索の可能性を感じることができました。\n特徴ベクトルのチューニングや、既存のクエリとの組み合わせるためのチューニングなどやることは増えそうですが、これは夢が広がりますね。 実際にはスコアを組み合わせたりするのもそうですが、ANN の結果にフィルタリングするほうが実務的には簡単に役立つことが多そうな気がしました。\nたとえば、特定のカテゴリ内で ANN するだけでも、 「この商品に見た目が似ている商品はこちらです」を提示する際に画像としては似ているが商品としては全く違う商品(iPhone と iPhone ケースなど)を分けるのも簡単にできちゃいますしね。\nまとめ このハイブリッド検索によって近似近傍探索のスコアと従来の検索結果のスコアをかけあわせたり、フィルタリングしたりなど真の意味で近似近傍探索が Elasticsearch で活用可能になったのではないのでしょうか？\n  Introducing approximate nearest neighbor search in Elasticsearch 8.0 | Elastic Blog \u0026#x21a9;\u0026#xfe0e;\n 8.0 からの kNN はどう変わったのか / How kNN search changed in the Elasticsearch 8.0 - Speaker Deck \u0026#x21a9;\u0026#xfe0e;\n k-nearest neighbor (kNN) search | Elasticsearch Guide [8.0] | Elastic \u0026#x21a9;\u0026#xfe0e;\n https://twitter.com/jobergum/status/1491366596665016321 Jo さんの皮肉な検索エンジン批評は自分の中で名物になっている \u0026#x21a9;\u0026#xfe0e;\n  [LUCENE-10318] Reuse HNSW graphs when merging segments? - ASF JIRA [LUCENE-10382] Allow KnnVectorQuery to operate over a subset of liveDocs - ASF JIRA [LUCENE-10592] Should we build HNSW graph on the fly during indexing - ASF JIRA  \u0026#x21a9;\u0026#xfe0e; Elasticsearch 8.4 introduces a hybrid search by Mayya Sharipova Elasticsearch 8.4 introduces a hybrid search: ability to combine results from knn search with traditional search features (queries, aggs etc) and all this under a single familiar _search API. \u0026#x21a9;\u0026#xfe0e;\n https://www.elastic.co/guide/en/elasticsearch/reference/8.4/knn-search.html#_combine_approximate_knn_with_other_features \u0026#x21a9;\u0026#xfe0e;\n Elasticsearch で日本語検索を扱うためのマッピング定義 ZOZO TECH BLOG \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-10-29-2337/","summary":"表題の通り、Elasticsearch 8.4 から待望の近似近傍探索と従来の検索を組み合わたハイブリッド検索が可能になったらしいので、試してみました。\nElascticsearch 8 で導入された近似近傍探索について Elasticsearch 公式の記事1がわかりやすく近似近傍探索について語られています。 また、日本語では@pakio さんの紹介記事2も非常にわかりやすいので、そちらも御覧ください。\n嬉しいけど物足りない点 公式の資料3や@pakio さんの資料でも触れられていますが、\n  You can’t currently use the Query DSL to filter documents for an approximate kNN search. If you need to filter the documents, consider using exact kNN instead.   Elasticsearch の Query DSL との併用不可というのが物足りない点でした。\n端的に説明すると Elasticsearch 8 で利用可能になった近似近傍探索は、あくまでベクトル間のみの近似近傍探索のみできるのであって、従来の Elasticsearch の検索機能(term や filter)と近似近傍探索を組み合わせて検索できないということです。\nVespa の開発者の Jo さんも同様の点4について触れていました。\n The most surprising part of the announcement is that they won\u0026rsquo;t allow combining the nearest neighbor search with standard query terms and filters.","title":"Elasticsearch 8.4 から利用可能な従来の検索機能と近似近傍探索を組み合わせたハイブリッド検索を試す"},{"content":"Elasticsearch 8 系から使用可能になった近似近傍探索1を使って、ドラえもんのひみつ道具の自然言語検索ができる検索エンジンを作ってみた。\nデモ動画のように、検索したいひみつ道具を説明する文章することで近しいひみつ道具が検索されます。\nコードは GitHub に公開してあるので、興味のある方は手元で、動かして遊ぶことが出来ます。 poetry と Docker さえあれば動くようになっています。\nhurutoriya/doraemon-himitsu-dogu-search: Doraemon Himitsu Dogu Japanese semantic search based on Elascticsearch ANN\nシステムの概要図はこんな感じ\n所感  ドラえもんのひみつ道具のデータセットを今回１から作ったが、パースと前処理がめんどくさくてここが一番手間がかかった。が、工夫しないと出来なかったので、一番楽しいところでもあった。 文章の特徴抽出は、sonoisa/sentence-bert-base-ja-mean-tokens-v2 · Hugging Faceを使わせていただき、驚くほど簡単に実現できた。  実際はもっと精度を高めるには、fine tune などがいいのだろうが、システム側を作ることに注力したかったので今回は割愛   デモアプリの構築は streamlit を使って 20m くらいで作れたので、相変わらず便利すぎて愛用している。今回の検索エンジンは CLI から実行もできるが、こうやってデモアプリがあったほうがそれっぽくて気持ちいい。 インデキシング時にトーカナイザーのことなど全く考えずに特徴ベクトルだけインデキシングして、それで検索が成り立つというのは新鮮。閾値設定しなければゼロヒット問題にも直面しないので、できることの幅は広がりそう。 Elasticsearch の近似近傍探索は、今回ベクトル同士の近似近傍探索しかやっていないが、それもインデキシング、クエリ部分は公式ドキュメントを見れば事足りたので変にハマることはなかった。  クエリ部分はこれだけで書けた。\n1 2 3 4 5 6 7 8 9 10  query = { \u0026#34;knn\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;vector\u0026#34;, \u0026#34;query_vector\u0026#34;: sentence_embeddings[0], \u0026#34;k\u0026#34;: 10, \u0026#34;num_candidates\u0026#34;: 100, }, \u0026#34;fields\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;description\u0026#34;], } result = es.search(index=INDEX_NAME, body=query)   ぶっちゃけて言えば、この規模で近似近傍探索のみやるなら検索エンジンを使わずとも Python 内でインメモリ探索して完結して動くと思うので Elasticsearch を使う意義を考えざるをえないが、自分が触ってみたかったのでやってみた。 そのおかげで色々学べたことが多いので、手を動かしてよかった。\n これで、近似近傍探索周りで遊べるパイプライン+アプリが出来たので時間を見つけて色々試してみたいなと思っている。 例えば、\n 近似近傍探索探索部分は、ベクターデータベースの Milvusでも実現できる内容になっているが、Elasticsearch の真価であるフィルタリングや従来の検索手法と組み合わせる2ことが出来ていないのでやってみたい。 特徴抽出のパイプラインは出来たので、もっと大規模なドキュメントを取り扱って課題にぶち当たると楽しそう Vespa やQdrant、Milvus など他のライブラリでも近似近傍探索による検索    k-nearest neighbor (kNN) search | Elasticsearch Guide [master] | Elastic \u0026#x21a9;\u0026#xfe0e;\n Elastocsearch 8.4 からは従来の検索手法と近似近傍探索を組み合わせることが可能に! Elasticsearch 8.4 introduces a hybrid search by Mayya Sharipova \u0026gt; Elasticsearch 8.4 introduces a hybrid search: ability to combine results from knn search with traditional search features (queries, aggs etc) and all this under a single familiar _search API. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-10-23-2344/","summary":"Elasticsearch 8 系から使用可能になった近似近傍探索1を使って、ドラえもんのひみつ道具の自然言語検索ができる検索エンジンを作ってみた。\nデモ動画のように、検索したいひみつ道具を説明する文章することで近しいひみつ道具が検索されます。\nコードは GitHub に公開してあるので、興味のある方は手元で、動かして遊ぶことが出来ます。 poetry と Docker さえあれば動くようになっています。\nhurutoriya/doraemon-himitsu-dogu-search: Doraemon Himitsu Dogu Japanese semantic search based on Elascticsearch ANN\nシステムの概要図はこんな感じ\n所感  ドラえもんのひみつ道具のデータセットを今回１から作ったが、パースと前処理がめんどくさくてここが一番手間がかかった。が、工夫しないと出来なかったので、一番楽しいところでもあった。 文章の特徴抽出は、sonoisa/sentence-bert-base-ja-mean-tokens-v2 · Hugging Faceを使わせていただき、驚くほど簡単に実現できた。  実際はもっと精度を高めるには、fine tune などがいいのだろうが、システム側を作ることに注力したかったので今回は割愛   デモアプリの構築は streamlit を使って 20m くらいで作れたので、相変わらず便利すぎて愛用している。今回の検索エンジンは CLI から実行もできるが、こうやってデモアプリがあったほうがそれっぽくて気持ちいい。 インデキシング時にトーカナイザーのことなど全く考えずに特徴ベクトルだけインデキシングして、それで検索が成り立つというのは新鮮。閾値設定しなければゼロヒット問題にも直面しないので、できることの幅は広がりそう。 Elasticsearch の近似近傍探索は、今回ベクトル同士の近似近傍探索しかやっていないが、それもインデキシング、クエリ部分は公式ドキュメントを見れば事足りたので変にハマることはなかった。  クエリ部分はこれだけで書けた。\n1 2 3 4 5 6 7 8 9 10  query = { \u0026#34;knn\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;vector\u0026#34;, \u0026#34;query_vector\u0026#34;: sentence_embeddings[0], \u0026#34;k\u0026#34;: 10, \u0026#34;num_candidates\u0026#34;: 100, }, \u0026#34;fields\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;description\u0026#34;], } result = es.","title":"Elasticsearchの近似近傍探索を使って、ドラえもんのひみつ道具検索エンジンを作ってみた"},{"content":"Airflow 1 系で設定されている環境変数を JSON ファイルとして GUI を使って書き出す方法の続報です。\n前回、Airflow CLI からでも環境変数を JSON ファイルとして出力できる1が、手元から実行しても GCP 上のインスタンスにしか保存されなかったので諦めたと書きました。 ですが、その問題を解決できたので、解決方法を公開しておきます。\nCloud Storage に格納されるデータ | Cloud Composer | Google Cloudによると、Cloud Composer インスタンス内部のディレクトリは GCS にマッピングされているらしい。\nマッピング関係は以下( GCP のドキュメントをそのまま引用)\n   フォルダ Storage パス マッピングされたディレクトリ 説明     DAG gs://bucket-name/dags /home/airflow/gcs/dags 環境の DAG を保存します。このフォルダ内の DAG のみが環境にスケジュールされます。   プラグイン gs://bucket-name/plugins /home/airflow/gcs/plugins カスタム プラグインを保存します。カスタムのインハウス Airflow 演算子、フック、センサー、インターフェースなどです。   データ gs://bucket-name/data /home/airflow/gcs/data タスクが生成して使用するデータを保存します。このフォルダは、すべてのワーカーノードにマウントされます。   ログ gs://bucket-name/logs タスクの Airflow ログを保存します。ログは Airflow ウェブ インターフェースでも利用できます。     それを使えば、/home/airflow/gcs/data にファイルを保存すれば、CloudComposer が保有している GCS の gs://bucket-name/data にアクセスすれば、そのファイルが参照可能になる。\n実際に以下のようなコマンドを実行したところ、gs://bucket-name/data からファイルを参照できました。 めでたい 🎉\n1 2 3 4  # GCSにファイルを保存 $ gcloud composer environments run COMPOSER_NAME --location LOCATION variables -- --export /home/airflow/gcs/data/airflow_env.json # 生成されたファイルをGCS上で確認 $ gsutil cat `gs://bucket-name/data/airflow_env.json   これで前回紹介したアプローチの欠点である GUI での操作に依存せず、CLI で完結してファイルを作成できるようになったのでミスも減りますね。 このディレクトリと GCS のマッピング機能を考えた人は、頭いいなと思いました。\nまず CloudComposer を GCP 上で提供する上で DAG の同期のためにマッピング機能は不可欠なので、最初からこの機能があるんじゃないかと考えるべきでしたね\u0026hellip; 反省\n  python - Export all Airflow variables - Stack Overflow \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-10-17-1630/","summary":"Airflow 1 系で設定されている環境変数を JSON ファイルとして GUI を使って書き出す方法の続報です。\n前回、Airflow CLI からでも環境変数を JSON ファイルとして出力できる1が、手元から実行しても GCP 上のインスタンスにしか保存されなかったので諦めたと書きました。 ですが、その問題を解決できたので、解決方法を公開しておきます。\nCloud Storage に格納されるデータ | Cloud Composer | Google Cloudによると、Cloud Composer インスタンス内部のディレクトリは GCS にマッピングされているらしい。\nマッピング関係は以下( GCP のドキュメントをそのまま引用)\n   フォルダ Storage パス マッピングされたディレクトリ 説明     DAG gs://bucket-name/dags /home/airflow/gcs/dags 環境の DAG を保存します。このフォルダ内の DAG のみが環境にスケジュールされます。   プラグイン gs://bucket-name/plugins /home/airflow/gcs/plugins カスタム プラグインを保存します。カスタムのインハウス Airflow 演算子、フック、センサー、インターフェースなどです。   データ gs://bucket-name/data /home/airflow/gcs/data タスクが生成して使用するデータを保存します。このフォルダは、すべてのワーカーノードにマウントされます。   ログ gs://bucket-name/logs タスクの Airflow ログを保存します。ログは Airflow ウェブ インターフェースでも利用できます。     それを使えば、/home/airflow/gcs/data にファイルを保存すれば、CloudComposer が保有している GCS の gs://bucket-name/data にアクセスすれば、そのファイルが参照可能になる。","title":"CloudComposer の Variables (環境変数)を gcloud cli で取得する"},{"content":"Python で２つの配列を for 文で扱いたい場合によく使うのが zip() です。\nzip()を使った for 文では暗黙的に同じ大きさが要求されると思っていたが、実際には以下のように２つの配列の大きさが異なっていてもエラーが出ないことに気が付かず、困ったことがあった。\n1 2 3 4 5 6 7 8 9 10  In [1]: a = [1,2,3,4] In [2]: b = [1,2,3] In [3]: for i,j in zip(a,b): ...: print(i,j) ...: 1 1 2 2 3 3   てっきり、大きい配列の要素を参照時にエラーが発生するかと思ったら、そんなことはなかった。\nassert とかで事前にコケるようにしておくとか必要そう。 もしくは、両者の配列のサイズが同じことを明示的に確認するのが吉。\nまた蛇足だが、Stackoverflow では意図的に異なる大きさの配列を上手く循環させつつ回したい場合の対処法も書いてあり勉強になった。1\n2022-10-27: 追記\n@ftnext さんから以下の情報2を教えてもらいました。\n 小さい方を読み切ったら for を抜けるの予想と違いますよね。 3.10 から zip に strict 引数が追加されており、True を指定すれば長さが異なると ValueError を送出するようになったんです！ https://docs.python.org/ja/3/library/functions.html#zip… また長い方に合わせたいときは zip_longest が標準ライブラリの itertools にありますー\n なるほど。\n Without the strict=True argument, any bug that results in iterables of different lengths will be silenced, possibly manifesting as a hard-to-find bug in another part of the program.\n わかる〜〜ということで試してみました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  In [1]: a = [1,2,3,4] In [2]: b = [1,2,3] In [3]: for i,j in zip(a,b,strict=True): ...: print(i,j) ...: 1 1 2 2 3 3 ------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In [3], line 1 ----\u0026gt; 1 for i,j in zip(a,b,strict=True): 2 print(i,j) ValueError: zip() argument 2 is shorter than argument 1   おぉ、これは便利ですね!\n結論 Python で zip 関数を使う場合、２つの配列に同じ大きさを想定する場合は strict=True を使うとバグの温床を潰せる\n  How to zip two differently sized lists, repeating the shorter list? - Stack Overflow \u0026#x21a9;\u0026#xfe0e;\n https://twitter.com/ftnext/status/1583483767645011968 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-10-17-1156/","summary":"Python で２つの配列を for 文で扱いたい場合によく使うのが zip() です。\nzip()を使った for 文では暗黙的に同じ大きさが要求されると思っていたが、実際には以下のように２つの配列の大きさが異なっていてもエラーが出ないことに気が付かず、困ったことがあった。\n1 2 3 4 5 6 7 8 9 10  In [1]: a = [1,2,3,4] In [2]: b = [1,2,3] In [3]: for i,j in zip(a,b): ...: print(i,j) ...: 1 1 2 2 3 3   てっきり、大きい配列の要素を参照時にエラーが発生するかと思ったら、そんなことはなかった。\nassert とかで事前にコケるようにしておくとか必要そう。 もしくは、両者の配列のサイズが同じことを明示的に確認するのが吉。\nまた蛇足だが、Stackoverflow では意図的に異なる大きさの配列を上手く循環させつつ回したい場合の対処法も書いてあり勉強になった。1\n2022-10-27: 追記\n@ftnext さんから以下の情報2を教えてもらいました。\n 小さい方を読み切ったら for を抜けるの予想と違いますよね。 3.10 から zip に strict 引数が追加されており、True を指定すれば長さが異なると ValueError を送出するようになったんです！ https://docs.","title":"Python で zip関数を使う際に、２つの配列が同じ大きさを想定する場合は 3.10 から導入された strict=True を使おう"},{"content":"CloudComposer(GCP の Airflow のマネージドサービス)で運用している Airflow 1 系上で設定されている環境変数を JSON ファイルとして書き出したかったが、つまずいたのでメモを公開しておく。\nAirflow の運用の理想としては、リポジトリをベースに CI 経由で CloudComposer を構築していくのがベスト。 だが、Airflow では GUI でお手軽に環境変数(Airflow では Variables という概念1)が設定でき、便利な半面、デメリットとしてリポジトリをベースにした Single Source of Truth の状態が保てなくなってしまう。\nAirflow の環境変数を JSON ファイルとして書き出す方法  上部の Admin メニューから、Variablesをクリックしてページに移動 With selectedボタンをクリックすると Exportボタンがドロップダウンリスト内にでてくるので、これをクリックすれば Airflow に保存されている環境変数を JSON ファイルとして書き出せる  Export できるとは初見でわからなかったのでこの UI を考えた人は罪深い。@naoさんに教えていただけて感謝!    Airflow CLI からでも環境変数を JSON ファイルとして出力できるらしい2が、手元から\n1  gcloud composer environments run COMPOSER_NAME --location asia-northeast1 variables -- --export env.json   を実行してもローカルには保存されなかったので、実行結果は CloudComposer 内部のインスタンスに保存されている模様。\nBash と GCS のオペレーターを組み合わせれば JSON ファイルを GCS に保存はできそうだが、それもめんどくさそうではある。 直接 SSH で CloudComposer のインスタンスにつなげたほうがまだ楽そうですよね\nまとめ Airflow の GUI で環境変数を設定するのは便利だけど、Single Souce of Truth ではなくなるので使い所をきちんと見極めよう!!\n  Variables — Airflow Documentation \u0026#x21a9;\u0026#xfe0e;\n python - Export all Airflow variables - Stack Overflow \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-10-04-1549/","summary":"CloudComposer(GCP の Airflow のマネージドサービス)で運用している Airflow 1 系上で設定されている環境変数を JSON ファイルとして書き出したかったが、つまずいたのでメモを公開しておく。\nAirflow の運用の理想としては、リポジトリをベースに CI 経由で CloudComposer を構築していくのがベスト。 だが、Airflow では GUI でお手軽に環境変数(Airflow では Variables という概念1)が設定でき、便利な半面、デメリットとしてリポジトリをベースにした Single Source of Truth の状態が保てなくなってしまう。\nAirflow の環境変数を JSON ファイルとして書き出す方法  上部の Admin メニューから、Variablesをクリックしてページに移動 With selectedボタンをクリックすると Exportボタンがドロップダウンリスト内にでてくるので、これをクリックすれば Airflow に保存されている環境変数を JSON ファイルとして書き出せる  Export できるとは初見でわからなかったのでこの UI を考えた人は罪深い。@naoさんに教えていただけて感謝!    Airflow CLI からでも環境変数を JSON ファイルとして出力できるらしい2が、手元から\n1  gcloud composer environments run COMPOSER_NAME --location asia-northeast1 variables -- --export env.json   を実行してもローカルには保存されなかったので、実行結果は CloudComposer 内部のインスタンスに保存されている模様。","title":"Airflow 1系で設定されている環境変数を JSON ファイルとしてGUIを使って書き出す方法"},{"content":"日課の RSS フィードを眺めていると、クエリエンジンやデータ処理の最適化のための高速化ライブラリが Meta が OSS として公開した1 のを知った。\nVelox のリポジトリはこちら\nfacebookincubator/velox: A C++ vectorized database acceleration library aimed to optimizing query engines and data processing systems.\n実際にリポジトリを観てみると C++で書かれており、たしかにパフォーマンスが高いのが納得。\nドキュメントやチュートリアルなどはこちらのサイトで用意されています。\nHello from Velox | Velox\nMeta 社内では、Presto や Spark に適用して処理の高速化、PyTorch に活用して前処理や特徴量エンジニアリングの高速化が進められているらしいです。\n技術ブログ記事 1が何をやっているか明瞭なので、かいつまんでメモを残しておきます。\n  SQL の分析、ストリーミング処理や機械学習のためのデータ処理など実際の処理内容は似通っているが、様々なフレームワークが使われ、独立して進化している。  この断片化によって、保守と拡張が困難になっていたが、それらを統合する形で実行可能にするのが Verox   Presto, Spark などのデータ処理エンジンは、一見異なるように見えるが、レイヤー構造で考えると非常に似通っている。  Verox は一般的に実行エンジンレイヤーの代わりとなり、式評価、集約、ソート、結合などの処理を提供する。(一般的に data plane と呼ばれる)   実例と結果  Presto は Java で実行されるが、それを C++の Velox で置き換えた。Prestissimo というプロジェクト名で進んだ。(カッコいいね)  Java での実行と比べると、大体 6 倍ほど高速化された   Spark 上では、Gluten とよばれるプロジェクトで Velox と同じように C++での実行を試みるプロジェクトが公開されている。 PyTorch の TorchArrowを Velox 上で実行可能    最終的には、Velox で従来のデータマネジメントの部分と機械学習インフラストラクチャの部分の垣根を統一することを狙っている。\n 詳細な内容を知りたい方は、VLDB2022 で論文として発表された2ので、そちらを読むと良さそうです。\n実際に実行して手で触ってみたいという方は、10 分で始める Verox というチュートリアル資料3があったので、そちらを読むと良さそうです。 コードを眺めると Verox がどんなことをしてくれるのかイメージが付きました。 ものすごく野心的な試みで、これからの発展が楽しみなプロジェクトでした。\n  Introducing Velox: An open source unified execution engine \u0026#x21a9;\u0026#xfe0e;\n Meta’s Unified Execution Engine at VLDB2022- Meta Research \u0026#x21a9;\u0026#xfe0e;\n Velox in 10 minutes — Velox documentation \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-09-01-2139/","summary":"日課の RSS フィードを眺めていると、クエリエンジンやデータ処理の最適化のための高速化ライブラリが Meta が OSS として公開した1 のを知った。\nVelox のリポジトリはこちら\nfacebookincubator/velox: A C++ vectorized database acceleration library aimed to optimizing query engines and data processing systems.\n実際にリポジトリを観てみると C++で書かれており、たしかにパフォーマンスが高いのが納得。\nドキュメントやチュートリアルなどはこちらのサイトで用意されています。\nHello from Velox | Velox\nMeta 社内では、Presto や Spark に適用して処理の高速化、PyTorch に活用して前処理や特徴量エンジニアリングの高速化が進められているらしいです。\n技術ブログ記事 1が何をやっているか明瞭なので、かいつまんでメモを残しておきます。\n  SQL の分析、ストリーミング処理や機械学習のためのデータ処理など実際の処理内容は似通っているが、様々なフレームワークが使われ、独立して進化している。  この断片化によって、保守と拡張が困難になっていたが、それらを統合する形で実行可能にするのが Verox   Presto, Spark などのデータ処理エンジンは、一見異なるように見えるが、レイヤー構造で考えると非常に似通っている。  Verox は一般的に実行エンジンレイヤーの代わりとなり、式評価、集約、ソート、結合などの処理を提供する。(一般的に data plane と呼ばれる)   実例と結果  Presto は Java で実行されるが、それを C++の Velox で置き換えた。Prestissimo というプロジェクト名で進んだ。(カッコいいね)  Java での実行と比べると、大体 6 倍ほど高速化された   Spark 上では、Gluten とよばれるプロジェクトで Velox と同じように C++での実行を試みるプロジェクトが公開されている。 PyTorch の TorchArrowを Velox 上で実行可能    最終的には、Velox で従来のデータマネジメントの部分と機械学習インフラストラクチャの部分の垣根を統一することを狙っている。","title":"Meta が公開したデータ処理の効率化・高速化を狙うエンジン Velox が面白そう"},{"content":"Apache Lucene のインデックスの取り扱いについて勉強していたら、 Java の memory map について言及されていたが、Jave の memory map1 について日本語で分かりやすく解説されている記事がなかったので、勉強がてらまとめた。 メモリマップ自体の説明はこちらのサイトが非常にわかりやすかった2\n mmap はファイルとメモリーアドレスのマッピングを行う\n つまり、ファイルをメモリ上にマップ(射影)してメモリ上でファイルを扱えるようにするということですね。 Apache Lucene の使用例だと、Lucene の検索用のインデックスファイルを MMap でメモリ上にマップして扱えるようにしていそう。\n参考にしたのは上記２つの記事がわかりやすい記事だった。\nJava プログラムに関連するメモリは 4 部分から構成される3\n Stack: メソッドが呼ばれた際に、Stack はメソッドを完了させるためのメモリ空間を提供する。この空間はパラメータやローカル変数、現在のオブジェクトへの参照などが格納されている。Frame はメモリ空間を参照し、メソッドの呼び出しをサポートする。Stack は LIFO(Last in First out)方式で動作し、呼び出し基のメソッドの Stack frame を削除するために最後の Stack frame(現在実行中のメソッド) を削除する必要がある。 Heap: Java で作成されるオブジェクトは全て Heap で作成される。 Static Area: プログラムの実行中に存在する値を格納するメモリを参照する。静的な変数を宣言した際に、この領域に存在する。 Code: 実行されるコードが格納される場所。  中でも Java の Memory mapped file は、メモリから直接ファイルにアクセスするのに役立つ Java の特殊ファイル4。 Java は、java.nio パッケージで Memory mapped file をサポートしている。 Memory mapped I/O は、ファイル システムを使用して、ユーザーから直接ファイルシステムページへの仮想メモリマッピング(virtual memory mapping)を確立する。Memory mapped file は単純に大きな配列として扱うことができ、Memory mapped file に使用されるメモリーは、Java の Heap 空間外部が利用される。\nindeed のブログ5で JDK が提供する mmap の欠点についても語られておりそれも面白い。\nPython メモリマップファイルを試す - Emotion Explorer の記事で\n ファイルメモリーマッピングなんて昔からあったけれど、使ったためしがない。 ずっとデータ構造で処理するようにしていたので特に使う機会がなく、ファイルでも SQL データベースばかりだったので、 ビューとか使えばいらないかな。正直、私は今後も使わないかなと思う次第です。\n と書かれていたが、たしかに Lucene のようにファイルに対する処理を行いたい場合が最も効果を発揮しそうですね。\n  mmap - Wikipedia \u0026#x21a9;\u0026#xfe0e;\n 第 68 章 ファイルとメモリーのマッピング (mmap) \u0026#x21a9;\u0026#xfe0e;\n Memory Maps / Diagrams \u0026#x21a9;\u0026#xfe0e;\n What is Memory-Mapped File in Java? - GeeksforGeeks \u0026#x21a9;\u0026#xfe0e;\n util-mmap でメモリマッピング - Indeed エンジニアリング・ブログ \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-08-22-1300/","summary":"Apache Lucene のインデックスの取り扱いについて勉強していたら、 Java の memory map について言及されていたが、Jave の memory map1 について日本語で分かりやすく解説されている記事がなかったので、勉強がてらまとめた。 メモリマップ自体の説明はこちらのサイトが非常にわかりやすかった2\n mmap はファイルとメモリーアドレスのマッピングを行う\n つまり、ファイルをメモリ上にマップ(射影)してメモリ上でファイルを扱えるようにするということですね。 Apache Lucene の使用例だと、Lucene の検索用のインデックスファイルを MMap でメモリ上にマップして扱えるようにしていそう。\n参考にしたのは上記２つの記事がわかりやすい記事だった。\nJava プログラムに関連するメモリは 4 部分から構成される3\n Stack: メソッドが呼ばれた際に、Stack はメソッドを完了させるためのメモリ空間を提供する。この空間はパラメータやローカル変数、現在のオブジェクトへの参照などが格納されている。Frame はメモリ空間を参照し、メソッドの呼び出しをサポートする。Stack は LIFO(Last in First out)方式で動作し、呼び出し基のメソッドの Stack frame を削除するために最後の Stack frame(現在実行中のメソッド) を削除する必要がある。 Heap: Java で作成されるオブジェクトは全て Heap で作成される。 Static Area: プログラムの実行中に存在する値を格納するメモリを参照する。静的な変数を宣言した際に、この領域に存在する。 Code: 実行されるコードが格納される場所。  中でも Java の Memory mapped file は、メモリから直接ファイルにアクセスするのに役立つ Java の特殊ファイル4。 Java は、java.nio パッケージで Memory mapped file をサポートしている。 Memory mapped I/O は、ファイル システムを使用して、ユーザーから直接ファイルシステムページへの仮想メモリマッピング(virtual memory mapping)を確立する。Memory mapped file は単純に大きな配列として扱うことができ、Memory mapped file に使用されるメモリーは、Java の Heap 空間外部が利用される。","title":"Java の memory map を理解する"},{"content":"2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1\n Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow\u0026rsquo;s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX).\n DataFlow は Apache Beam で記述したプログラミングモデルの実行環境だ。 主に、バッチ処理・ストリーミング処理で使われており、機械学習に欠かせないデータ処理の観点からして非常に面白いと思っているソフトウェアなので、積極的に動向を追っている。\nDataFlow ML は簡単に説明すると、PyTorch と scikit-learn が Dataflow のパイプライン内部で直接推論可能になった。 直近では Python SDK に限るが DataFlow で GPU の利用が可能になったりと、大規模データ処理や機械学習を行う際に魅力的な機能が続々とサポートされはじめた。 DataFlow ML の実態は Apache Beam 2.40.0で追加された RunInference API だ。 RunInference API の DesignDocs 2が公開されているので、それを見てみる。 機械学習システムの DesignDocs として非常に面白いので興味がある人は、見てみると面白いとおもう。\nDesignDocs: RunInference: ML Inference in Beam RunInference API が作られた背景として、TensorFlow だけが Beam 内部で、処理するデータ( Apache beam ではPCollection と呼ばれる) に対して、推論を行うための RunInference transformer (Apache Beam では PTransformと呼ばれ、PCollection を入力として処理を行う) が存在しており、実装不要だった。\n 問題点としては  Beam のレポジトリに存在せず、tensorflow/tfx-bslのリポジトリに存在している。 TFX ライブラリに特化した API になっている サンプルコードが充実しておらず、学習コストが高い    この DesignDocs の目的は、RunInference API を以下の 2 つの人気のある機械学習フレームワークで使えるように実装すること\n scikit-learn PyTorch  実装方法としては、\n 内部の最適化が行われているべき 単純かつ統合されたインターフェイスで提供されるべき 入力と出力が、機械学習エンジニアに対して、直感的な型になっているべき  scikit-learn なら numpy PyTorch なら Tensors    最終的なゴールとしては、XGBoost や JAX など他の機械学習フレームワークにも適合したり、Vertex AI など外部のサービスも使えるようにしたい。\n内部の実装方針は DesignDocs で詳細に議論されているので、そこは割愛して、まずは RunInference API のサンプルコードを動かしてみる。\nサンプルコードを通じて学ぶ Example RunInference API pipelinesにサンプルコードが公開されていたので、動かしてみる。 まずは一番簡単そうなサンプルコードである scikit-learn による MNIST 分類を動かしてみる。 最初にドキュメントをじっくり読むよりも実際にコードを見たほうが理解が深まるので実際に動かしてみるのが近道。 サンプルコードはこちらbeam/sdks/python/apache_beam/examples/inference/sklearn_mnist_classification.py 公式のサンプルコードだと、推論対象の入力データと学習済みの scikit-learn のモデルを自前で用意する必要があるのでコマンド一発でサンプルコードを動かせるコードを以下のレポジトリに公開しました。 将来的には scikit-learn だけでなく、PyTorch にも対応したい。\nhttps://github.com/hurutoriya/beam-runinferenceapi-sample\n実際に何をやっているかの解説は、日本語のコメントを添えて解説してみます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91  \u0026#34;\u0026#34;\u0026#34;このパイプラインは、RunInference APIを使って、MNISTデータの分類を行う。 このパイプラインは、int 形式でCSV形式で保存されているテキストファイルを読み込む。CSVの１つ目の列はラベル、そのほかの列はMNISTのピクセルの値を格納している。データは学習済みのモデルで推論される。このパイプラインは output ファイルに推論結果の書き込みを行い、ラベルと推論結果の比較を行うことができる。 \u0026#34;\u0026#34;\u0026#34; import argparse from typing import Iterable from typing import List from typing import Tuple import apache_beam as beam from apache_beam.ml.inference.base import KeyedModelHandler from apache_beam.ml.inference.base import PredictionResult from apache_beam.ml.inference.base import RunInference from apache_beam.ml.inference.sklearn_inference import ModelFileType from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpy from apache_beam.options.pipeline_options import PipelineOptions from apache_beam.options.pipeline_options import SetupOptions def process_input(row: str) -\u0026gt; Tuple[int, List[int: \u0026#34;\u0026#34;\u0026#34;入力データをラベルとピクセルに分けて、返す \u0026#34;\u0026#34;\u0026#34; data = row.split(\u0026#39;,\u0026#39;) label, pixels = int(data[0]), data[1:] pixels = [int(pixel) for pixel in pixels] return label, pixels class PostProcessor(beam.DoFn): \u0026#34;\u0026#34;\u0026#34;予測ラベルを得るために予測結果を処理する。CSV形式で、真値と予測ラベルを返す。 \u0026#34;\u0026#34;\u0026#34; def process(self, element: Tuple[int, PredictionResult]) -\u0026gt; Iterable[str]: label, prediction_result = element prediction = prediction_result.inference yield \u0026#39;{},{}\u0026#39;.format(label, prediction) def parse_known_args(argv): \u0026#34;\u0026#34;\u0026#34;引数を定義\u0026#34;\u0026#34;\u0026#34; parser = argparse.ArgumentParser() parser.add_argument( \u0026#39;--input_file\u0026#39;, dest=\u0026#39;input\u0026#39;, required=True, help=\u0026#39;text file with comma separated int values.\u0026#39;) parser.add_argument( \u0026#39;--output\u0026#39;, dest=\u0026#39;output\u0026#39;, required=True, help=\u0026#39;Path to save output predictions.\u0026#39;) parser.add_argument( \u0026#39;--model_path\u0026#39;, dest=\u0026#39;model_path\u0026#39;, required=True, help=\u0026#39;Path to load the Sklearn model for Inference.\u0026#39;) return parser.parse_known_args(argv) def run(argv=None, save_main_session=True): \u0026#34;\u0026#34;\u0026#34;パイプラインを定義\u0026#34;\u0026#34;\u0026#34; known_args, pipeline_args = parse_known_args(argv) pipeline_options = PipelineOptions(pipeline_args) pipeline_options.view_as(SetupOptions).save_main_session = save_main_session # この例では、RunInference トランスフォームにキーとなる入力を渡している。それによって、SklearnModelHandlerNumpy のラッパーである KeyedModelHandler を使用している。 model_loader = KeyedModelHandler( SklearnModelHandlerNumpy( model_file_type=ModelFileType.PICKLE, model_uri=known_args.model_path)) with beam.Pipeline(options=pipeline_options) as p: # 入力データを読み込む label_pixel_tuple = ( p | \u0026#34;ReadFromInput\u0026#34; \u0026gt;\u0026gt; beam.io.ReadFromText( known_args.input, skip_header_lines=1) | \u0026#34;PreProcessInputs\u0026#34; \u0026gt;\u0026gt; beam.Map(process_input)) # 推論して後処理を行う predictions = ( label_pixel_tuple | \u0026#34;RunInference\u0026#34; \u0026gt;\u0026gt; RunInference(model_loader) | \u0026#34;PostProcessOutputs\u0026#34; \u0026gt;\u0026gt; beam.ParDo(PostProcessor())) # 後処理したデータを出力してファイルとして保存する _ = predictions | \u0026#34;WriteOutput\u0026#34; \u0026gt;\u0026gt; beam.io.WriteToText( known_args.output, shard_name_template=\u0026#39;\u0026#39;, append_trailing_newlines=True) if __name__ == \u0026#39;__main__\u0026#39;: run()   RunInference API についてドキュメントから学ぶ RunInference API のドキュメント3が既に公開されているので、読み込むことで実際にどんな概念で作成されているのか理解できる。 Apache Beam の基礎的な概念や用語については、この記事を読む前に過去に書いた Apache beam Python 入門を読んでいただけると、この記事が分かりやすく読めると思います。\nなぜ RunInferenceAPI を使うのか? そもそも僕の疑問は「今まで、Beam で PyTorch や scikit-learn のモデルを使って推論は可能だったが、 RunInferenceAPI は何が嬉しいのか?」への答えがドキュメントに書いてありました。\n BatchElementsトランスフォームやSharedクラスなど既存の Apache Beam の概念に沿って、機械学習の推論処理を最適化が可能になる。またマルチモデルパイプラインなど、複雑なパイプラインの構築も比較的簡単に構築できるようになる。  BatchElements PTransform とは 多くのモデルが実装しているベクトル化推論の最適化を利用するために、モデルの予測を行う前の中間段階として、BatchElements トランスフォームの追加した。このトランスフォームは、要素をバッチ処理する。そして、バッチされた要素は、RunInference の特定のフレームワークのための変換で適用される。例えば、numpy の ndarrays の場合は numpy.stack()を、torch の Tensor 要素の場合は torch.stack()を呼び出す。\nbeam.BatchElements の設定をカスタマイズするには、ModelHandler の中で、batch_elements_kwargs 関数をオーバーライドする必要がある。例えば、min_batch_size でバッチあたりの要素数の最小値を設定し、max_batch_size でバッチあたりの要素数の最大値を設定する。 詳しいドキュメントはこちら\nShared helper class RunInference API でShared クラスを使用することにより、プロセスごとに一度モデルを読み込んだ後に各プロセス内部でその読み込んだモデルをDoFn(全てのPCollectionに適用される処理ロジックを保持する)内で共有することができる。このクラスを使えば、モデルの読み込み時間とメモリ消費を削減することができる。\n詳しいドキュメント(ソースコード)はこちら。内部の処理ロジック自体 100 行未満で書かれているので、読んで見るのもあり。\n機械学習モデルを使うためにパイプラインを構築する 以下のコードを Apache Beam のパイプラインに追加すれば、RunInference トランスフォームを使用できる。\n1 2 3 4  from apache_beam.ml.inference.base import RunInference with pipeline as p: predictions = ( p | \u0026#39;Read\u0026#39; \u0026gt;\u0026gt; beam.ReadFromSource(\u0026#39;a_source\u0026#39;) | \u0026#39;RunInference\u0026#39; \u0026gt;\u0026gt; RunInference(\u0026lt;model_handler\u0026gt;)   model_handler は、モデルのハンドラーのためのセットアップコードで、モデルのインポートができる。 以下のようなModelHandlerの例がある\n1 2 3 4  from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpy from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerPandas from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerKeyedTensor   モデル A と モデル B の推論結果を並列して行ったり、\n1 2 3 4  with pipeline as p: data = p | \u0026#39;Read\u0026#39; \u0026gt;\u0026gt; beam.ReadFromSource(\u0026#39;a_source\u0026#39;) model_a_predictions = data | RunInference(\u0026lt;model_handler_A\u0026gt;) model_b_predictions = data | RunInference(\u0026lt;model_handler_B\u0026gt;)   アンサンブルも柔軟に記述できる。\n1 2 3 4 5  with pipeline as p: data = p | \u0026#39;Read\u0026#39; \u0026gt;\u0026gt; beam.ReadFromSource(\u0026#39;a_source\u0026#39;) model_a_predictions = data | RunInference(\u0026lt;model_handler_A\u0026gt;) model_b_predictions = model_a_predictions | beam.Map(some_post_processing) | RunInference(\u0026lt;model_handler_B\u0026gt;)   また驚きなのが、Apache Beam 2.41.0 移行は Multi Language SDKによって、 Java からも RunInference API を使うことができるらしい。 これってモデル構築は Python で行って、運用は安定した Java で実行可能ということなので凄い機能ですね。\nhttps://github.com/apache/beam/blob/master/sdks/java/extensions/python/src/main/java/org/apache/beam/sdk/extensions/python/transforms/RunInference.java\nMulti Language SDK も非常に面白そうだが、まだ使ったことはないので、実際にコードをかきつつ理解を深めたい。\nまとめ Apache Beam 2.40.0 から利用可能になった RunInference API についてサンプルコードと共に何をやっているかをまとめました。\nApache Beam はとても未来を感じるソフトウェアなので、この OSS にコントリビュートできる余地があれば積極的にやっていきたい。 そのため、これからはちょっとした Beam の勉強メモなども積極的に公開されていくと思います。\n  Latest Dataflow innovations for real-time streaming and AI/ML | Google Cloud Blog \u0026#x21a9;\u0026#xfe0e;\n RunInference: ML Inference in Beam \u0026#x21a9;\u0026#xfe0e;\n Apache Beam Python Machine Learning \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-08-18-1938/","summary":"2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1\n Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow\u0026rsquo;s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX).","title":"Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる"},{"content":"2022/08/14 - 2022/08/18 に開催される Knowledge Discovery and Data Mining (KDD) 2022 の情報が出揃ってきたので、気になった情報をメモしておく。\n自分が気になるトピックは、変わらず機械学習の実応用とその周辺領域なのでそれに偏ったリストになっている。\nADS invited speaker KDD 2022 ADS Invited Speakers\nAn overview of AWS AI/ML’s recent contributions to open source ML tools: Accelerating discovery and innovation\n招待講演は確か毎回論文化されて ACM で公開されるので論文公開されたらぜひ読みたい。\nTutorias KDD 2022 Tutorials Schedule に Tutorial の情報がまとまっているが、タイトルだけでウェブサイトへのリンクが一切なく、読み手に不親切なので来年は、改善してほしい。去年はそんなことなかったので、なんとか来年はもとに戻って欲しい。\n Graph-based Representation Learning for Web-scale Recommender Systems. Authors: Ahmed El-Kishky (Twitter)*; Michael Bronstein (Twitter); Ying Xiao (Twitter); Aria Haghighi (Twitter)  Twitter が開催する Tutorial で、すごく面白そうなのだが全く情報が見つからなかった。Twitter Cortex にも情報が更新されていないので、しばらくしたら公開されていることを祈る。   New Frontiers of Scientific Text Mining: Tasks, Data, and Tools. Authors: Xuan Wang (University of Illinois at Urbana-Champaign)*; Hongwei Wang (University of Illinois Urbana-Champaign); Heng Ji (University of Illinois at Urbana-Champaign); Jiawei Han (UIUC)  こちらも情報を発見できなかった。   Modern Theoretical Tools for Designing Information Retrieval System. Authors: Da Xu (Walmart Labs)*; Chuanwei Ruan (Walmart Labs); Bo Yang (Linkedin)  arxiv でTutorial: Modern Theoretical Tools for Understanding and Designing Next-generation Information Retrieval System も公開されている。WSDM2022 チュートリアルでも同じチュートリアルが開催されていた模様。チートシート的に、情報検索の領域でどんなツール(理論)を使えばいいのかを紹介してくれているチュートリアル。   Model Monitoring in Practice: Lessons Learned and Open Challenges. Authors: Krishnaram Kenthapadi (Fiddler AI)*; Himabindu Lakkaraju (Harvard); Pradeep Natarajan (Amazon.com Inc.); Mehrnoosh Sameki (Microsoft Azure AI)  モデルの監視に関するチュートリアル。大前提としては、モデルの監視は、デプロイ後に必須の時代でどんな課題が今まで存在しており、どう実現するかをツールを交えて講義。そして最終的にはモデルの監視がガバナンス向上や責任のある AI につながっていくことについて話している。   Reward Optimising Recommendation using Deep Learning and Fast Maximum Inner Product Search. Authors: Imad Aouali (ENS Paris-Saclay); Benjamin Heymann (Criteo); Martin Bompaire (Criteo); Achraf Ait Sidi Hammou (Criteo); Sergey Ivanov (Criteo); Amine Benhalloum (Criteo); DAVID J ROHDE (CRITEO)*; Flavian Vasile (Criteo); Maxime VONO (Criteo); Otmane Sakhi (Criteo)  こちらも情報を発見できなかった   Deep Search Relevance Ranking in Practice. Authors: Linsey Pang (Salesforce)*; Wei Liu (University of Technology Sydney); Moumita Bhattacharya (neflix); Keng-hao Chang (Microsoft); Xue Li (Microsoft); xianjing liu (twitter); Stephen Guo (Walmart Labs)  検索のための深層学習によるレリバンスモデルのハンズオンチュートリアル。Jupyter notebook を通して手を動かしながら学べる。    Workshops KDD 2022 Workshopは、すべてリンクがまとめられていた。素晴らしい。 論文投稿を募集するから外的にわからないと開催できないので納得。\n Workshop on Applied Machine Learning Management  機械学習プロジェクトのマネジメントや機械学習のプロダクト開発と、超実践的な領域に焦点を当てたワークショップ これは一番面白そうなワークショップで、ぜひとも資料を全公開していただきたいが、現状なにも情報は公開されていない。   Fragile Earth 2022 - AI for Good Foundation  ソーシャルハッピーのために機械学習をどう活用するかのワークショップ。災害予測や犯罪の予防などがトピック   Workshop on Decision Intelligence and Analytics for Online Marketplaces: Jobs, Ridesharing, Retail, and Beyond  オンラインマーケットプレイスでの課題をどう解決するかに焦点をあてたワークショップ。   Document Intelligence Workshop @ KDD 2022 DI@KDD2022  今年で 3 回目の開催となるワークショップ。現実世界の書類をどのように理解(例えば OCR など)して活用するかについて焦点をあてている。 現実世界のデータって全部紙なので、それを構造化してデジタル化するってものすごく大変だけどやりきった先に競合優位性が発生し、とんでもない価値を生むと思うのでこの領域は気になっている。   First Content Understanding and Generation for E-commerce Workshop  e コマースでのコンテンツ生成に関するワークショップ。e コマースに関わっている方ならおもしろトピック満載です。   ESCI Challenge for Improving Product Search  後述する KDD Cup の成果がこのワークショップで公開される。既に論文が公開されており、読むのが非常に楽しみ   KDD online and adaptive recommender systems Workshop  リアルタイムでの推薦システム構築に関して焦点を当てたトピック。    KDD Cup Search Engineering Newsletter vol.05 で言及した Amazon が主催する KDD カップ。 自分が e コマースの検索システム開発に携わっているので、このタスクは非常に興味がある。\nAmazon Product Search https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search\nPapers 論文関係は数が多いので、見出しだけで面白そうだなと思った論文をリスト化した。 時間がある時にこのリストから選んでみたい。 全ての論文が網羅されているのページがこちら。 https://kdd.org/kdd2022/toc.html\nKDD のサイトで提供されているトラックごとのページは抜け漏れ?がある模様 https://kdd.org/kdd2022/researchPosterAssignments.html ポスターだけに絞っているからそうなっている?\n会社ごとの論文は、会社によってはまとめられているので便利。 自分は職業柄、e コマースに関するに関する課題に向き合っている Amazon の論文が好きな事が多い。\n KDD 2022 - Amazon Science KDD 2022 - Apple Machine Learning Research  Research Track Full Papers  HyperLogLogLog: Cardinality Estimation With One Log More Streaming Graph Neural Networks with Generative Replay Non-stationary A/B Tests A Generalized Backward Compatibility Metric On-Device Learning for Model Personalization with Large-Scale Cloud-Coordinated Domain Adaption  SESSION: ADS Track Papers  ItemSage: Learning Product Embeddings for Shopping Recommendations at Pinterest Graph-based Multilingual Language Model: Leveraging Product Relations for Search Relevance Amazon Shop the Look: A Visual Search System for Fashion and Home TwHIN: Embedding the Twitter Heterogeneous Information Network for Personalized Recommendation Automatic Generation of Product-Image Sequence in E-commerce Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems Automatic Controllable Product Copywriting for E-Commerce Learning Backward Compatible Embeddings  埋め込み空間の後方互換性に関する研究   Rax: Composable Learning-to-Rank Using JAX A/B Testing Intuition Busters: Common Misunderstandings in Online Controlled Experiments TaxoTrans: Taxonomy-Guided Entity Translation Persia: An Open, Hybrid System Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters  PersiaML/PERSIA: High performance distributed framework for training deep learning recommendation models based on PyTorch. PyTorch による深層学習ベースの推薦モデルを分散学習するためのフレームワーク。   User Behavior Pre-training for Online Fraud Detection Lion: A GPU-Accelerated Online Serving System for Web-Scale Recommendation at Baidu Semantic Retrieval at Walmart Looper: An End-to-End ML Platform for Product Decisions  Looper: An End-to-End ML Platform for Product Decisions - Igor Markov | Stanford MLSys #60 - YouTube Stanford の講義で動画が公開されていた、Meta(Facebook)の機械学習プラットフォーム。従来の機械学習基盤であるFBLeaner からこれに移行したのだろうか?   Amazon SageMaker Model Monitor: A System for Real-Time Insights into Deployed Machine Learning Models Human-in-the-Loop Large-Scale Predictive Maintenance of Workstations PinnerFormer: Sequence Modeling for User Representation at Pinterest Improving Relevance Modeling via Heterogeneous Behavior Graph Learning in Bing Ads Profiling Deep Learning Workloads at Scale using Amazon SageMaker Recommendation in Offline Stores: A Gamification Approach for Learning the Spatiotemporal Representation of Indoor Shopping Type Linking for Query Understanding and Semantic Search  Huawei が書いている。   Interpretable Personalized Experimentation  Meta が書いている。   Training Large-Scale News Recommenders with Pretrained Language Models in the Loop Scale Calibration of Deep Ranking Models  Google が書いている   Multilingual Taxonomic Web Page Classification for Contextual Targeting at Yahoo  ","permalink":"https://shunyaueta.com/posts/2022-08-15-2335/","summary":"2022/08/14 - 2022/08/18 に開催される Knowledge Discovery and Data Mining (KDD) 2022 の情報が出揃ってきたので、気になった情報をメモしておく。\n自分が気になるトピックは、変わらず機械学習の実応用とその周辺領域なのでそれに偏ったリストになっている。\nADS invited speaker KDD 2022 ADS Invited Speakers\nAn overview of AWS AI/ML’s recent contributions to open source ML tools: Accelerating discovery and innovation\n招待講演は確か毎回論文化されて ACM で公開されるので論文公開されたらぜひ読みたい。\nTutorias KDD 2022 Tutorials Schedule に Tutorial の情報がまとまっているが、タイトルだけでウェブサイトへのリンクが一切なく、読み手に不親切なので来年は、改善してほしい。去年はそんなことなかったので、なんとか来年はもとに戻って欲しい。\n Graph-based Representation Learning for Web-scale Recommender Systems. Authors: Ahmed El-Kishky (Twitter)*; Michael Bronstein (Twitter); Ying Xiao (Twitter); Aria Haghighi (Twitter)  Twitter が開催する Tutorial で、すごく面白そうなのだが全く情報が見つからなかった。Twitter Cortex にも情報が更新されていないので、しばらくしたら公開されていることを祈る。   New Frontiers of Scientific Text Mining: Tasks, Data, and Tools.","title":"KDD2022 で気になった研究"},{"content":"poetry show は、poetry の設定ファイルの pyproject.tomlに記載された利用可能なパッケージ名を表示してくれる。\n例えば、ターミナルで poetry install を行う前に、poetry showを行うと以下のような結果がでる。\nそして、grep で上記の結果を表示させてみると\n1 2  \u0026gt; poetry show | grep aiohttp aiohttp (!) 3.8.1 Async http client/server framework (asyncio)   と パッケージ名に (!)が付与されている。\nこの(!)ってそもそもどんな意味なのか気になったので調べてみました。\nPoetry のコードを直接読んでみると、 test_show_basic_with_not_installed_packages_non_decoratedのテストケースが今回の事例にマッチしており、わかりやすかった。 意味としては、「インストールされたパッケージに対する show コマンドを非装飾モードで結果を出力」へのテストだ。 状況としては、cachyとpendulumを poetry add して、 cachyのみを poetry install している。\n1 2 3 4 5 6 7 8 9 10  poetry.package.add_dependency(Factory.create_dependency(\u0026#34;cachy\u0026#34;, \u0026#34;^0.1.0\u0026#34;)) poetry.package.add_dependency(Factory.create_dependency(\u0026#34;pendulum\u0026#34;, \u0026#34;^2.0.0\u0026#34;)) cachy_010 = get_package(\u0026#34;cachy\u0026#34;, \u0026#34;0.1.0\u0026#34;) cachy_010.description = \u0026#34;Cachy package\u0026#34; pendulum_200 = get_package(\u0026#34;pendulum\u0026#34;, \u0026#34;2.0.0\u0026#34;) pendulum_200.description = \u0026#34;Pendulum package\u0026#34; installed.add_package(cachy_010)   その後に、poetry show を実行した際に以下のように(!)が期待される出力となっている。\n1 2  cachy 0.1.0 Cachy package pendulum (!) 2.0.0 Pendulum package   実際に(!)を付与している部分はここ。 https://github.com/python-poetry/poetry/blob/68f4da8c0af754dd9afd8c96cf97a2b80fced33e/src/poetry/console/commands/show.py#L310-L311\n1 2 3  if not self.io.output.is_decorated(): # Non installed in non decorated mode install_marker = \u0026#34; (!)\u0026#34;   is_decorated は、出力結果を装飾するかどうかのフラグを指しており、このフラグの可否でTrueの場合は色がついて装飾されて出力される。 is_decoratedの定義はここにあった。\n自分の場合、設定が上手くいかなかったのが原因で内製パッケージを読み込んでいる部分を知らないうちに失敗して、そのパッケージが(!)がついて表示されていた。\nおそらくパイプを通すことで、poetry showの装飾フラグがFalseになっていそうではある\nまとめ  (!)は該当パッケージがインストールされていないことを示している 装飾(decorated)モードが Trueなら、ターミナルでは(!)の代わりに装飾されてパッケージ名が表示される(例えば自分のターミナルなら、未インストールのパッケージ名が赤色で表示される)   蛇足)設定をミスってインストール失敗していた際に、Python の実行画面では\n ModuleNotFoundError: No module named\n と表示されていたので、やはりログが言っていることは正しかった\u0026hellip;!\n","permalink":"https://shunyaueta.com/posts/2022-08-10-1717/","summary":"poetry show は、poetry の設定ファイルの pyproject.tomlに記載された利用可能なパッケージ名を表示してくれる。\n例えば、ターミナルで poetry install を行う前に、poetry showを行うと以下のような結果がでる。\nそして、grep で上記の結果を表示させてみると\n1 2  \u0026gt; poetry show | grep aiohttp aiohttp (!) 3.8.1 Async http client/server framework (asyncio)   と パッケージ名に (!)が付与されている。\nこの(!)ってそもそもどんな意味なのか気になったので調べてみました。\nPoetry のコードを直接読んでみると、 test_show_basic_with_not_installed_packages_non_decoratedのテストケースが今回の事例にマッチしており、わかりやすかった。 意味としては、「インストールされたパッケージに対する show コマンドを非装飾モードで結果を出力」へのテストだ。 状況としては、cachyとpendulumを poetry add して、 cachyのみを poetry install している。\n1 2 3 4 5 6 7 8 9 10  poetry.package.add_dependency(Factory.create_dependency(\u0026#34;cachy\u0026#34;, \u0026#34;^0.1.0\u0026#34;)) poetry.package.add_dependency(Factory.create_dependency(\u0026#34;pendulum\u0026#34;, \u0026#34;^2.0.0\u0026#34;)) cachy_010 = get_package(\u0026#34;cachy\u0026#34;, \u0026#34;0.1.0\u0026#34;) cachy_010.description = \u0026#34;Cachy package\u0026#34; pendulum_200 = get_package(\u0026#34;pendulum\u0026#34;, \u0026#34;2.","title":"poetry show でパッケージ名に (!) が付与されている意味"},{"content":"リーダーの作法 ささいなことをていねいにを読み終えた。 著者は Netscape でマネージャー、Apple でディレクター、Slack でエグゼクティブを経験した Michael Lopp さんで、過去にBeing Geek や Managing Humans を書かれている。 翻訳の質も非常に高く、楽しく読めた。1\nそんなにマネジメント関係を読んでいるわけではないが、HITH OUTPUT MANAGEMENT や、エンジニアのためのマネジメントキャリアパス ―テックリードから CTO までマネジメントスキル向上ガイド 同じくらい良い書籍で、学びや共感を多く感じた。\n自分はマネジメントのポジションについたことはないが、仕事をしていくなかでマネジメント関係のソフトスキルや複数人でどうやってうまくリーダシップを発揮して、大きい問題を解決するかに興味があるので、良い書籍が目につくと積極的に読んで解像度をあげている。\n心に響いた文章と所感  マネジメントとは、まずチームが直面している障害やメンバー間の軋轢といった情報を明らかにすることであり、さらにそうして得た情報を分析して、進むべき正しい道を見出すことである、ということでした。\n マネジメントの定義として、チームの課題は他の書籍でも述べられているがメンバー間の関係性について言及しているのは確かにねという腹落ちだった。\nポジションに関わらず全ての業務で一貫して言えるのは、なにが最も ROI が高い課題かを突き止めて地道に解決していくことだな~と思える。\n 6 章 プロフェッショナルとしての成長を図る質問表\n 6 章の成長を計測するための質問表は、あらためて自分が何をやりたいのかの解像度をあげる良い質問集だったので半年や一年ごとにこの質問を更新して、定期的に見つめ直していきたい。\n 新しい仕事では、一刻一刻で多くを学びます。その環境に関する情報をたくさん集めているのです。チームや自分の役割、そして会社についての自分の理解を日々新たにしているのです。\n 新メンバーがあげるチームの違和感は、フィードバックをもらった後に一ヶ月後見直して見ると重要度が下がることがあるよねと。これは、自分も経験あるが、チームで働くうちに内部の事情を理解して、重要だけど緊急じゃないよね、実際やりたいけどコスト的に割に合わないなど課題に対しての解像度があがることで、優先順位が変わることがありました。\n そういう人たちが長年にわたって私に教えてくれた重要な教訓の一つは、壁に書かれた文字より、語られるストーリーの方が重要だということです。\n ここで腹落ちしたのは、あらためて人を動かすのは物語(ストーリーテリング)だということ。「なぜ」 そうなったのかってものすごく重要だなと。DesignDocs もそうですが、なぜそうなったかがわかると人間ってものすごく腹落ちして理解できるな~と 自分もドキュメントを書くときは、「なぜ」そうなったのかをきちんと書いて人を動かせる文章を書けるようになっていきたい\n 手厳しいフィードバックの場合は、何も見落とさないようにするためにあと 2 段階のプロセスがあります。ステップ 1：どんなに批判的なフィードバックであっても、耳を傾け、ほんの少しでも理解の糸口を探す。ほんのわずかでも？よくぞ聞いてくれました。\n時には、フィードバックがあまりにも衝撃的で、理解できないこともあります。そこで、第二のステップです。ステップ 2：聞いたことを繰り返しましょう。\n ここのストーリーは面白くて、図星な本質的なフィードバックを感情的に反応しそうになってしまうのをどうやって傾聴して、フィードバックを受け入れて自己を改善するかが語られていて面白かった。\n リアルタイムにフィードバックし、他のプレイヤーに親切かつ教育的な方法でアドバイスを送ります。災難に直面しても冷静さを失いません。わかりやすいコミュニケーション、実証された専門知識、わかりやすく行動につながるフィードバック、そして落ち着いた性格。堅実なリーダーの性質について説明しましたが、まだ大切なことがあります。いいでしょうか、こうしたふるまいは、多くの人がやっているのをこれまで見てきました。DJ が特別なのは、常にこのようなリーダーであることです。\n ゲームを通して、リーダー像がどのように振る舞うべきなのかを説明しており、非常にわかりやすかった。\n リーダーシップとは、他人に見せるために選ぶ服であり、私は揺るぎない優しさを選びます。\n そして、書籍の最後の文章が、まさに「リーダーの作法」という心構えであり定期的に見返したい。\n全体を通して、エッセイとして語られて教訓を学べるので楽しく読めた。2 他の書籍と比較すると、リーダーとして情緒的にどう振る舞うべきかも触れている点が、個人的に読み応えがあった。 自分があらためてマネジメントのポジションにつくことがあれば読み返したくなる書籍だった。\n総じて、自分が体験したトピックは強く共感を抱けて学びが多かったというのが興味深い。 ビスマルクの名言で「愚者は経験に学び、賢者は歴史に学ぶ」があるが、自分も本を読むことで歴史から学んでいけるようになりたい。 そのために最近習慣化したいので、読んだ後に読書ノートを作ってちゃんと振り返ることで定着できないか試行錯誤中。 Obsidian にノートをまとめるようになってからは、読書ノートを作るのも楽しくなったので、良いツールは習慣化形成にも役立ちますね。\n  プライベートで書籍の翻訳プロジェクトを勧めているのだが、このような本が母国語で読める環境はあらためて素晴らしい文化だなと強く思うようになった。 \u0026#x21a9;\u0026#xfe0e;\n 蛇足) 13 章のストーリーが個人的に一番笑える皮肉が書いてあって、一読の価値あり。 当たり前のことをきちんとやるって、難しいけどその作法は大事。 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-08-08-1145/","summary":"リーダーの作法 ささいなことをていねいにを読み終えた。 著者は Netscape でマネージャー、Apple でディレクター、Slack でエグゼクティブを経験した Michael Lopp さんで、過去にBeing Geek や Managing Humans を書かれている。 翻訳の質も非常に高く、楽しく読めた。1\nそんなにマネジメント関係を読んでいるわけではないが、HITH OUTPUT MANAGEMENT や、エンジニアのためのマネジメントキャリアパス ―テックリードから CTO までマネジメントスキル向上ガイド 同じくらい良い書籍で、学びや共感を多く感じた。\n自分はマネジメントのポジションについたことはないが、仕事をしていくなかでマネジメント関係のソフトスキルや複数人でどうやってうまくリーダシップを発揮して、大きい問題を解決するかに興味があるので、良い書籍が目につくと積極的に読んで解像度をあげている。\n心に響いた文章と所感  マネジメントとは、まずチームが直面している障害やメンバー間の軋轢といった情報を明らかにすることであり、さらにそうして得た情報を分析して、進むべき正しい道を見出すことである、ということでした。\n マネジメントの定義として、チームの課題は他の書籍でも述べられているがメンバー間の関係性について言及しているのは確かにねという腹落ちだった。\nポジションに関わらず全ての業務で一貫して言えるのは、なにが最も ROI が高い課題かを突き止めて地道に解決していくことだな~と思える。\n 6 章 プロフェッショナルとしての成長を図る質問表\n 6 章の成長を計測するための質問表は、あらためて自分が何をやりたいのかの解像度をあげる良い質問集だったので半年や一年ごとにこの質問を更新して、定期的に見つめ直していきたい。\n 新しい仕事では、一刻一刻で多くを学びます。その環境に関する情報をたくさん集めているのです。チームや自分の役割、そして会社についての自分の理解を日々新たにしているのです。\n 新メンバーがあげるチームの違和感は、フィードバックをもらった後に一ヶ月後見直して見ると重要度が下がることがあるよねと。これは、自分も経験あるが、チームで働くうちに内部の事情を理解して、重要だけど緊急じゃないよね、実際やりたいけどコスト的に割に合わないなど課題に対しての解像度があがることで、優先順位が変わることがありました。\n そういう人たちが長年にわたって私に教えてくれた重要な教訓の一つは、壁に書かれた文字より、語られるストーリーの方が重要だということです。\n ここで腹落ちしたのは、あらためて人を動かすのは物語(ストーリーテリング)だということ。「なぜ」 そうなったのかってものすごく重要だなと。DesignDocs もそうですが、なぜそうなったかがわかると人間ってものすごく腹落ちして理解できるな~と 自分もドキュメントを書くときは、「なぜ」そうなったのかをきちんと書いて人を動かせる文章を書けるようになっていきたい\n 手厳しいフィードバックの場合は、何も見落とさないようにするためにあと 2 段階のプロセスがあります。ステップ 1：どんなに批判的なフィードバックであっても、耳を傾け、ほんの少しでも理解の糸口を探す。ほんのわずかでも？よくぞ聞いてくれました。\n時には、フィードバックがあまりにも衝撃的で、理解できないこともあります。そこで、第二のステップです。ステップ 2：聞いたことを繰り返しましょう。\n ここのストーリーは面白くて、図星な本質的なフィードバックを感情的に反応しそうになってしまうのをどうやって傾聴して、フィードバックを受け入れて自己を改善するかが語られていて面白かった。\n リアルタイムにフィードバックし、他のプレイヤーに親切かつ教育的な方法でアドバイスを送ります。災難に直面しても冷静さを失いません。わかりやすいコミュニケーション、実証された専門知識、わかりやすく行動につながるフィードバック、そして落ち着いた性格。堅実なリーダーの性質について説明しましたが、まだ大切なことがあります。いいでしょうか、こうしたふるまいは、多くの人がやっているのをこれまで見てきました。DJ が特別なのは、常にこのようなリーダーであることです。\n ゲームを通して、リーダー像がどのように振る舞うべきなのかを説明しており、非常にわかりやすかった。\n リーダーシップとは、他人に見せるために選ぶ服であり、私は揺るぎない優しさを選びます。\n そして、書籍の最後の文章が、まさに「リーダーの作法」という心構えであり定期的に見返したい。\n全体を通して、エッセイとして語られて教訓を学べるので楽しく読めた。2 他の書籍と比較すると、リーダーとして情緒的にどう振る舞うべきかも触れている点が、個人的に読み応えがあった。 自分があらためてマネジメントのポジションにつくことがあれば読み返したくなる書籍だった。","title":"「リーダーの作法」マネジメントに限らず、エンジニアとして仕事の作法について書かれた良書"},{"content":"Makefile を眺めているとコマンドの前に@をつけているターゲットがあり、その効果を調べてみた1。日本語での記事が無かったので記事を書いた。\n以下のように、コマンドの前に @をつけたコマンドとつけていないコマンド両方を実行してみる。\n1 2 3  echoing-silencing: @echo \u0026#34;表示されない\u0026#34; echo \u0026#34;表示される\u0026#34;   1 2 3 4  \u0026gt; make echoing-silencing 表示されない echo \u0026#34;表示される\u0026#34; 表示される   なので、例えばお役立ち事例として、Makefile でターゲットの実行時に、何を行うか説明をしたい場合に @を付けるとスッキリした文をターミナルに表示することができる。\n  https://makefiletutorial.com/\n Command Echoing/Silencing Add an @ before a command to stop it from being printed\nYou can also run make with -s to add an @ before each line\n \u0026#x21a9;\u0026#xfe0e;   ","permalink":"https://shunyaueta.com/posts/2022-06-22-0001/","summary":"Makefile を眺めているとコマンドの前に@をつけているターゲットがあり、その効果を調べてみた1。日本語での記事が無かったので記事を書いた。\n以下のように、コマンドの前に @をつけたコマンドとつけていないコマンド両方を実行してみる。\n1 2 3  echoing-silencing: @echo \u0026#34;表示されない\u0026#34; echo \u0026#34;表示される\u0026#34;   1 2 3 4  \u0026gt; make echoing-silencing 表示されない echo \u0026#34;表示される\u0026#34; 表示される   なので、例えばお役立ち事例として、Makefile でターゲットの実行時に、何を行うか説明をしたい場合に @を付けるとスッキリした文をターミナルに表示することができる。\n  https://makefiletutorial.com/\n Command Echoing/Silencing Add an @ before a command to stop it from being printed\nYou can also run make with -s to add an @ before each line\n \u0026#x21a9;\u0026#xfe0e;   ","title":"Makefile でコマンドの前に @ を付けると、コマンド自身は表示されず結果のみ表示される"},{"content":"ノートアプリを Joplin から Obsidian へ 2021-03-13 に notion から joplin に乗り換えた。 理由としては\n notion を マークダウンのメモ帳としか使わない DB 機能やテンプレート機能は必要としていない\nなのでロックインされずに自分で管理できる OSS の joplin のほうが良いなと思って乗り換えました。 必要な機能は全て満たしてるので満足  と Joplin に乗り換えて大きな不満は無く使い続けていたのですが、Obsidian に乗り換えました。\nきっかけは ryuzeee さんの記事1 なんですが、以前から存在は知っていたObsidian をとりあえず食わず嫌いせずに触って見ようかと思い立ち、Joplinの記事を Obsidian のフォーマットにすべて変換2して使い始めてみたところ、とても心地よい使い心地で感動して乗り換えを決定した。\n元の Joplin がそもそも Obsidian と同じローカルのプレーンテキストを扱うという思想なので移行がとても簡単でした。\n乗り換えを決めた点として、\ndaily note プラグインと Calendar プラグインで日誌がめちゃくちゃ快適に書けるようになった。自分は日誌という形でログを残しつつ作業をしているのだが、このプラグインで毎日の日誌の記入がとても快適になった。 ぷーおんさんが書いてくれている記事3が、このプラグインの魅力をわかりやすく紹介されています。\nまたバックリンクの機能がやはり強力で、プラグインも PKM(Personal Knowledge Management) を目的に作成されたツールが多いので、それも気に入った。(ランダムノートを開いて、ノートを整備する機能はなるほど!となる便利さ)\nJoplin でもバックリンクのプラグインはあり使っていたが、Obsidian のバックリンク機能は比較してとても完成度が高い。バックリンクを使った繋がりの提案や、つながりが無いノートも見つける機能があったりと書いていて楽しい。\nまた、自分は料理を良くするのだが、普段参照するレシピも PKM で管理することで有機的な管理ができるようになりとても楽しい。これぞ PKM という事例だなとしみじみ。\nlogseqというアプリも気になったのだが、アウトライナーアプリで、自分は文章として作成したいので今回は Obsidian を選んだ。\n一つ困った点としては、Joplin では Dropbox 経由で Mac と Android 間でノートを同期できていたのだが、Obsidian では Dropbox を使った同期ができなくなった。 だが、スマホでメモを見たくなる用途は主にレシピを見たいときだけだった。どうしても我慢できなくなったら頑張ってデバイス間同期をしてみる。\n今使っている Pixel 4 XL から次は iPhone に戻る予定なのでその時は iCloud で同期すれば大丈夫になる。\nロックインも結局の所、エクスポートさえできれば大丈夫だなと楽観的な考え方に落ち着きました。Joplin も Obsidian も 10 年後存在しているかは不明だが、元になるプレーンテキストさえあれば大丈夫でしょう。\nTodo 管理アプリを TickTick から Todoist に戻した。 2022-03-13 にタスク管理ツールを Todoist から TickTick へ試しに乗り換えてみたの記事を書いたが、長年愛用した Todoist から TickTick に乗り換えた話をした。 だが、期待していたカレンダーベースのタスク管理が全く自分の要求を満たさず4、テンションが下がっていたときに Todoist から「値上げをするけど、今購読してくれたら購読継続してくれる限りは値上げ前をずっと維持するよ」5とメールが送られたので、絶好のタイミングでの誘惑を断ち切れず Todoist に戻ることにした。\n離れてみてわかる Todoist の使い心地の良さでした。\n  アジャイルコーチは Obsidian をどのように使っているか \u0026#x21a9;\u0026#xfe0e;\n https://github.com/luxi78/joplin2obsidian Go で書かれた Joplin 形式のマークダウンファイルを、Obsidian 方式の Markdown に変換してくれるツール \u0026#x21a9;\u0026#xfe0e;\n Calendar プラグインで Daily notes は進化する \u0026#x21a9;\u0026#xfe0e;\n アプリとして全体の完成度が低く、強みであるカレンダー統合機能も作り込みが甘くてとても使いづらかった。例えばカレンダーのスケジュール招待を辞退しても、カレンダーにはその正体が表示され続ける。またそのカレンダーをデスクトップでも完了状態にすることはできなかった(モバイルアプリだと完了にできる) \u0026#x21a9;\u0026#xfe0e;\n 2022 Todoist pricing changes \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-06-03-2133/","summary":"ノートアプリを Joplin から Obsidian へ 2021-03-13 に notion から joplin に乗り換えた。 理由としては\n notion を マークダウンのメモ帳としか使わない DB 機能やテンプレート機能は必要としていない\nなのでロックインされずに自分で管理できる OSS の joplin のほうが良いなと思って乗り換えました。 必要な機能は全て満たしてるので満足  と Joplin に乗り換えて大きな不満は無く使い続けていたのですが、Obsidian に乗り換えました。\nきっかけは ryuzeee さんの記事1 なんですが、以前から存在は知っていたObsidian をとりあえず食わず嫌いせずに触って見ようかと思い立ち、Joplinの記事を Obsidian のフォーマットにすべて変換2して使い始めてみたところ、とても心地よい使い心地で感動して乗り換えを決定した。\n元の Joplin がそもそも Obsidian と同じローカルのプレーンテキストを扱うという思想なので移行がとても簡単でした。\n乗り換えを決めた点として、\ndaily note プラグインと Calendar プラグインで日誌がめちゃくちゃ快適に書けるようになった。自分は日誌という形でログを残しつつ作業をしているのだが、このプラグインで毎日の日誌の記入がとても快適になった。 ぷーおんさんが書いてくれている記事3が、このプラグインの魅力をわかりやすく紹介されています。\nまたバックリンクの機能がやはり強力で、プラグインも PKM(Personal Knowledge Management) を目的に作成されたツールが多いので、それも気に入った。(ランダムノートを開いて、ノートを整備する機能はなるほど!となる便利さ)\nJoplin でもバックリンクのプラグインはあり使っていたが、Obsidian のバックリンク機能は比較してとても完成度が高い。バックリンクを使った繋がりの提案や、つながりが無いノートも見つける機能があったりと書いていて楽しい。\nまた、自分は料理を良くするのだが、普段参照するレシピも PKM で管理することで有機的な管理ができるようになりとても楽しい。これぞ PKM という事例だなとしみじみ。\nlogseqというアプリも気になったのだが、アウトライナーアプリで、自分は文章として作成したいので今回は Obsidian を選んだ。\n一つ困った点としては、Joplin では Dropbox 経由で Mac と Android 間でノートを同期できていたのだが、Obsidian では Dropbox を使った同期ができなくなった。 だが、スマホでメモを見たくなる用途は主にレシピを見たいときだけだった。どうしても我慢できなくなったら頑張ってデバイス間同期をしてみる。","title":"愛用しているツールを更新: Joplin→Obsidian \u0026 TickTick → Todoist"},{"content":"前回 Label Studio の紹介記事1を書きましたが、自分以外にもチーム全体で Label Studio を使いたいという要望があったので Web アプリとして labelstudio をホストしました。 意外と簡単に k8s 上でホストできたので、その方法を公開しておく。\nLabel Studio の運用方法は、 Docker イメージが提供されているので、それを使用するのが最も簡単です。\nCloudRun を使ってサーバーレスで動かす方法2もありますが、今回は k8s 上に Label Studio の Docker イメージをデプロイして、運用することになりました。\nk8s のマニフェストファイルは、公式リポジトリ3を参考に作成しました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  apiVersion:v1kind:Servicemetadata:name:labelstudionamespace:developmentspec:ports:- name:httpport:8080protocol:TCPselector:app:labelstudio---apiVersion:v1kind:PersistentVolumeClaimmetadata:name:labelstudio-data-pvcnamespace:developmentspec:accessModes:- ReadWriteOnceresources:requests:storage:50Gi---apiVersion:apps/v1kind:Deploymentmetadata:labels:app:labelstudioname:labelstudionamespace:developmentspec:replicas:1selector:matchLabels:app:labelstudiotemplate:metadata:labels:app:labelstudiospec:containers:- image:heartexlabs/label-studio:v1.4.1imagePullPolicy:Alwaysname:labelstudioports:- containerPort:8080stdin:truetty:truevolumeMounts:- mountPath:/label-studio/dataname:labelstudio-data-volimagePullSecrets:- name:secretvolumes:- name:labelstudio-data-volpersistentVolumeClaim:claimName:labelstudio-data-pvc  あとは、以下のコマンドを打てば、Label Studio の Pod にポートフォワードされるので、\n1  \u0026gt; kubectl -n development port-forward svc/labelstudio 8080:8080   http://localhost:8080/\nにアクセスすれば、k8s 上でホストされる Label Studio にアクセスが可能になります。\n本当は、Google App Engine でホストしたかったんですが、GAE の Docker はマウント機能が使えず、永続層に DB を SQLite で保存する Labelstudio では採用することができませんでした。残念\n逆にデモアプリでお世話になる Streamlit は GAE でポンッとデプロイできるから、気軽に Web アプリとしてホストできるから良いですね。4\n  OSS のアノテーションツール Label Studio を使って、快適にアノテーションする \u0026#x21a9;\u0026#xfe0e;\n 【AI Shift Advent Calendar 2021】Label Studio の GCP 上でのデプロイ | 株式会社 AI Shift \u0026#x21a9;\u0026#xfe0e;\n Add kubernetes files to allow easy deployment by boogheta · Pull Request #1067 · heartexlabs/label-studio \u0026#x21a9;\u0026#xfe0e;\n https://discuss.streamlit.io/t/gcp-app-engine-integration/11322/3 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-06-03-2044/","summary":"前回 Label Studio の紹介記事1を書きましたが、自分以外にもチーム全体で Label Studio を使いたいという要望があったので Web アプリとして labelstudio をホストしました。 意外と簡単に k8s 上でホストできたので、その方法を公開しておく。\nLabel Studio の運用方法は、 Docker イメージが提供されているので、それを使用するのが最も簡単です。\nCloudRun を使ってサーバーレスで動かす方法2もありますが、今回は k8s 上に Label Studio の Docker イメージをデプロイして、運用することになりました。\nk8s のマニフェストファイルは、公式リポジトリ3を参考に作成しました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  apiVersion:v1kind:Servicemetadata:name:labelstudionamespace:developmentspec:ports:- name:httpport:8080protocol:TCPselector:app:labelstudio---apiVersion:v1kind:PersistentVolumeClaimmetadata:name:labelstudio-data-pvcnamespace:developmentspec:accessModes:- ReadWriteOnceresources:requests:storage:50Gi---apiVersion:apps/v1kind:Deploymentmetadata:labels:app:labelstudioname:labelstudionamespace:developmentspec:replicas:1selector:matchLabels:app:labelstudiotemplate:metadata:labels:app:labelstudiospec:containers:- image:heartexlabs/label-studio:v1.","title":"Label Studio を k8s にデプロイする"},{"content":" 自分が大好きだった、Message Passing はなしをふったりふられたの後日談を @karino2 さんが、Podcast で語ってくれていたので、アンサーソング。\n読者としては、Message Passing という共著ブログは RSS に登録して毎回とても楽しみにしていた。 著者陣はなぜかやってよかった、大成功だね! という感想を出している方が少なく @morrita さんが\n 特に皆が書いてくれたものを読むのはすごく楽しかったので、うまくいった気もしています。 https://messagepassing.github.io/023-s1/04-morrita/\n というポジティブな感想を残してくれており、読者としても著者陣が良い体験として経験されているとすごく嬉しい。 続編を期待しております。\n以下、Podcast 内でのご意見に関するお話。記憶の中から書き出しているので正確性に欠けるかもしれません\n 書く習慣を身に着けていないと、書けないし、普段から書く習慣を実践したい @morrita さん\n 同意。自分も今年から脱 Twitter をして、Blog で記事を執筆するようにしているが、以前よりもほんの少しだけど読みやすい文章に変わった気がする。\n Blog 記事の感想は欲しい。。。が今だと一方通行。Twitter での言及はいまがポピュラーだが、それはなんか違う。 どちらかというち意見を書くというよりも、意見を求めているときがある。\n これも非常にわかる。自分も双方的な意見交換を Blog 上で行いたくてコメント機能を導入1したが、今の所導入してから 1 件のリアクションしかされていない\u0026hellip;2\nこのコメント機能の Giscus の良いところは、\n 記事に絵文字リアクションを行える GitHub アカウントが必要なのでスパムをかなり弾けそう  なのだが、全然使われない\u0026hellip;\nはてぶで 969 件とかなりブックマーク3された記事で、自分の Blog でもおそらく歴代最大の閲覧数を持つ記事でさえもコメントはされなかった。\nいや、はてぶやってるヒトははてぶでコメントすると思うので、単純にはいえないですが\u0026hellip;一つの指標として\nはてなスターや、Medium の clap のような弱いシグナルが欲しくて導入したのだが、なかなか思い通りにいかない。\n海外の有名所の Blog 記事だとコメントが殺到しているので、これも日本独特の問題なのか? それとも到達する数の規模が違うからそのぶんコメントが多くなる傾向などがあるのだろうか?\n increments などの雑誌に比べて Message Passsing は、他の著者が書いた記事に返信していくような形式なので、重複した内容が少ないのが良い点 @morrita さん\n 自分もそう思うな~。MEssage Passing の面白さは著者陣がリレー記事のように言及しあって相互作用が発生しているのが面白い点だと思う。\n 意見表明だけだと偉そう\n これは身につまされる話で、意見家ではなく実践者として自分も絶えず手を動かし続けていきたいなと改めて思った。\n  GitHub discussion を使ったコメントシステム giscus を導入 \u0026#x21a9;\u0026#xfe0e;\n https://github.com/hurutoriya/hurutoriya.github.io/discussions/10 \u0026#x21a9;\u0026#xfe0e;\n https://b.hatena.ne.jp/entry/s/shunyaueta.com/posts/2022-03-08/ \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-05-12-2337/","summary":"自分が大好きだった、Message Passing はなしをふったりふられたの後日談を @karino2 さんが、Podcast で語ってくれていたので、アンサーソング。\n読者としては、Message Passing という共著ブログは RSS に登録して毎回とても楽しみにしていた。 著者陣はなぜかやってよかった、大成功だね! という感想を出している方が少なく @morrita さんが\n 特に皆が書いてくれたものを読むのはすごく楽しかったので、うまくいった気もしています。 https://messagepassing.github.io/023-s1/04-morrita/\n というポジティブな感想を残してくれており、読者としても著者陣が良い体験として経験されているとすごく嬉しい。 続編を期待しております。\n以下、Podcast 内でのご意見に関するお話。記憶の中から書き出しているので正確性に欠けるかもしれません\n 書く習慣を身に着けていないと、書けないし、普段から書く習慣を実践したい @morrita さん\n 同意。自分も今年から脱 Twitter をして、Blog で記事を執筆するようにしているが、以前よりもほんの少しだけど読みやすい文章に変わった気がする。\n Blog 記事の感想は欲しい。。。が今だと一方通行。Twitter での言及はいまがポピュラーだが、それはなんか違う。 どちらかというち意見を書くというよりも、意見を求めているときがある。\n これも非常にわかる。自分も双方的な意見交換を Blog 上で行いたくてコメント機能を導入1したが、今の所導入してから 1 件のリアクションしかされていない\u0026hellip;2\nこのコメント機能の Giscus の良いところは、\n 記事に絵文字リアクションを行える GitHub アカウントが必要なのでスパムをかなり弾けそう  なのだが、全然使われない\u0026hellip;\nはてぶで 969 件とかなりブックマーク3された記事で、自分の Blog でもおそらく歴代最大の閲覧数を持つ記事でさえもコメントはされなかった。\nいや、はてぶやってるヒトははてぶでコメントすると思うので、単純にはいえないですが\u0026hellip;一つの指標として\nはてなスターや、Medium の clap のような弱いシグナルが欲しくて導入したのだが、なかなか思い通りにいかない。\n海外の有名所の Blog 記事だとコメントが殺到しているので、これも日本独特の問題なのか? それとも到達する数の規模が違うからそのぶんコメントが多くなる傾向などがあるのだろうか?\n increments などの雑誌に比べて Message Passsing は、他の著者が書いた記事に返信していくような形式なので、重複した内容が少ないのが良い点 @morrita さん","title":"Re:プログラム雑談 188回：ゲスト回：MessagePassingの話とか"},{"content":"社内でデータ分析のレポートを書く際は Google Colab がとても便利な事に気がついた。\nGoogle Bigquery でデータを抽出、Google Sheets で可視化 従来だと、自分がやっていた方法として、\n Google BQ などで分析対象結果のデータを抽出 その結果を Google Spread Sheet として保存して、Google Sheets の機能で可視化。元の SQL のコードは、別シートを作ってそこに貼り付けておく。  利点としては、一度データを抽出した後は、Google Sheets で二次加工が簡単にできる点がとても便利。 また、 Google Sheet を共有後に Produc Manager が出したい数値を、Product Manager 自身が Google Sheets を元にさっと計算することもできる。\nだが、二次加工が便利なのはいいが、大抵の可視化ってパターンが決まっているかつ二次加工の状況が必ず発生するわけではないので、SQL 取得とその可視化を一気通貫でできないかなと考えていた。\nなにか良い方法無いかなと思っている矢先に、別のチームの同僚が、Google Colab を使って、BQ を dataframe として保存後 matplotlib で可視化しているのを見かけて、\n求めていたのは\u0026hellip;こ、これだ\u0026hellip;.\nとなり、速攻取り入れました。\n良いと思ったところは積極的に真似する\nGoogle Colab なら、データの取得・加工・可視化までを完結可能 Google Colab の利点を列挙しておく\n SQL のコード、データ抽出や可視化のロジックなどが Python で記述可能かつ、Google Colab で完結  matplotlib で可視化できるので、見やすく美しい図を作れる  そしてそのコードは他のデータ分析でも再利用可能   pandas dataframe で Google BQ からデータを取得するので、Standard SQL だけでは難しい計算も pandas、 numpy や scipy などを使ってデータ加工が簡単にできるのも、便利   Google Sheets 同様、簡単に社内で共有できる Markdown も Google Colab 内で書けるので、凝った文章などもいれてレポートも書ける  マジックコマンドで、Google BQ の結果を dataframe として保存1したり、\n1 2 3 4 5 6  # Save output in a variable `df` %%bigquery --project yourprojectid df SELECT COUNT(*) as total_rows FROM `bigquery-public-data.samples.gsod`   また、#@title TITLE を Google Colab のセルの上部に入力すると、コードの部分は非表示になりスッキリした見かけにすることができる。2\n1 2 3 4  #@title MODULE import numpy as np import pandas as np   と書くと\nが\nこうなり、右側をクリックすると\nコード部分を隠すこともできる。\nこの機能もレポートとして書く際には重宝しており、このセルで何をやっているかも docstring っぽく記述しておくことができる。\n入力フォームも簡単に作れる2ようなので、可視化やレポート作成という本懐を見誤ることがなければ、やはり Notebook は便利。\n二次加工が決まっている工程なら「Google Bigquery でデータを抽出、Google Sheets で可視化」の方法を取ればいいが、 サっと分析して可視化結果を共有したいときには、Google Colab が一択になった。\n余談 @yohei_kikuta さんが、以前 Blog で「BigQuery を使って分析する際の tips」のシリーズを書いており、他者のワークフローを拝見するのは参考になるなと思い、自分も書いてみた。3\n  Getting started with BigQuery \u0026#x21a9;\u0026#xfe0e;\n Google Colab Forms \u0026#x21a9;\u0026#xfe0e;\n BigQuery を使って分析する際の tips\n part1: 開発環境やデータ連携 part2: クエリを書く際に押さえておくとよいこ part3: 具体的なクエリの tips （このエントリ）  \u0026#x21a9;\u0026#xfe0e;   ","permalink":"https://shunyaueta.com/posts/2022-05-10-2200/","summary":"社内でデータ分析のレポートを書く際は Google Colab がとても便利な事に気がついた。\nGoogle Bigquery でデータを抽出、Google Sheets で可視化 従来だと、自分がやっていた方法として、\n Google BQ などで分析対象結果のデータを抽出 その結果を Google Spread Sheet として保存して、Google Sheets の機能で可視化。元の SQL のコードは、別シートを作ってそこに貼り付けておく。  利点としては、一度データを抽出した後は、Google Sheets で二次加工が簡単にできる点がとても便利。 また、 Google Sheet を共有後に Produc Manager が出したい数値を、Product Manager 自身が Google Sheets を元にさっと計算することもできる。\nだが、二次加工が便利なのはいいが、大抵の可視化ってパターンが決まっているかつ二次加工の状況が必ず発生するわけではないので、SQL 取得とその可視化を一気通貫でできないかなと考えていた。\nなにか良い方法無いかなと思っている矢先に、別のチームの同僚が、Google Colab を使って、BQ を dataframe として保存後 matplotlib で可視化しているのを見かけて、\n求めていたのは\u0026hellip;こ、これだ\u0026hellip;.\nとなり、速攻取り入れました。\n良いと思ったところは積極的に真似する\nGoogle Colab なら、データの取得・加工・可視化までを完結可能 Google Colab の利点を列挙しておく\n SQL のコード、データ抽出や可視化のロジックなどが Python で記述可能かつ、Google Colab で完結  matplotlib で可視化できるので、見やすく美しい図を作れる  そしてそのコードは他のデータ分析でも再利用可能   pandas dataframe で Google BQ からデータを取得するので、Standard SQL だけでは難しい計算も pandas、 numpy や scipy などを使ってデータ加工が簡単にできるのも、便利   Google Sheets 同様、簡単に社内で共有できる Markdown も Google Colab 内で書けるので、凝った文章などもいれてレポートも書ける  マジックコマンドで、Google BQ の結果を dataframe として保存1したり、","title":"社内でデータ分析結果を可視化・共有する際に Google Colab が便利"},{"content":"2022 年、初めてまともな確定申告をやったので、所感をメモ。\n実際には 2021 年に初めて確定申告を行ったのだが、ふるさと納税と医療費控除のみだったので、まとも?な確定申告は 2022 年がはじめてといってもいいだろう。\n会計ソフトはマネーフォワードと迷ったが、クラウド会計ソフト freee 会計を採用した。 前評判では会計知識がある人からすると抽象化されすぎており、むず痒くなるという記事を見たのだが会計知識がない自分からすると全く問題なく使えた。 会計周りもすべてクレカ経由でしか支出していないのでひたすら振り分けて終わり。 これらは実質作業時間 15 時間程度。 振替項目が何が適切なのかの調査に一番時間が取られた。 書類提出も Andorid のアプリと連携することで free で完結して、ネット上で提出できて良い体験だった。 来年もしやることがあればものすごく早く終わりそう。 マネーフォワードの確定申告は、最初の画面で心が折れて回れ右をした。\nfreee が「寄附金控除に関する証明書」の xml を読み込める機能に対応しており、ポチポチで終わった。 事前の証明書を発行するための手続きが若干煩雑だったが、今までに比べると圧倒的に楽だった。 証明書の管理もしなくて良くなったのは革命では? 実作業時間は 30 分程度。 紙がもったいないので、個人的には自治体から書類は送らないで欲しい。\n医療費控除は、国税庁が公開してくれているエクセルにデータを打ち込んで終わり。 実作業時間は 2 時間程度。 そのフォーマットに free が対応してくれているので読み込んで終わり。\n最終的な e-Tax の提出は、スマホでカードリーダーを読み込んで連携1させれば終わりだったので非常に快適だった。 2021 年は張り切って、IC カードリーダーを買ったが結局手持ちの Mac と Safari の相性が悪くかなり粘ったが結局紙に印刷して送付したという思い出がある。\nアイ・オー・データ IC カードリーダー ぴタッチ 確定申告 e-Tax\n国税庁が用意した Safari の拡張機能周りが魔窟で、当時のサイトを忘れたがマニュアルにかかれていない設定をしないとマイナンバー読み込みプログラムがそもそも起動しなかった。 リベンジできてなにより。 完全に無用になった IC カードリーダーは即売却した。\n総評 年々システムが改善されて自動化されていて、素晴らしい。\n賛否両論あると思うが、医療費もマイナンバーの保険証化2によりデータが収集されるらしいのでぜひ浸透してほしい。 最近のニュースでは別途診察代金が請求される方式らしく3、ここに税金使わないで何に使うんだろうかと思ったが、予算が取れなかったのかなと色々と考えをめぐらした。 せっかく良い活用が期待できるのに、使う動機を無くすのはもったいない。\n  e-Tax、スマホがあれば IC カードリーダーは不要に - ケータイ Watch \u0026#x21a9;\u0026#xfe0e;\n マイナンバーカードの保険証利用について（被保険者証利用について） \u0026#x21a9;\u0026#xfe0e;\n マイナンバーカード保険証に「新利用料金」のちぐはぐ \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-05-04-2243/","summary":"2022 年、初めてまともな確定申告をやったので、所感をメモ。\n実際には 2021 年に初めて確定申告を行ったのだが、ふるさと納税と医療費控除のみだったので、まとも?な確定申告は 2022 年がはじめてといってもいいだろう。\n会計ソフトはマネーフォワードと迷ったが、クラウド会計ソフト freee 会計を採用した。 前評判では会計知識がある人からすると抽象化されすぎており、むず痒くなるという記事を見たのだが会計知識がない自分からすると全く問題なく使えた。 会計周りもすべてクレカ経由でしか支出していないのでひたすら振り分けて終わり。 これらは実質作業時間 15 時間程度。 振替項目が何が適切なのかの調査に一番時間が取られた。 書類提出も Andorid のアプリと連携することで free で完結して、ネット上で提出できて良い体験だった。 来年もしやることがあればものすごく早く終わりそう。 マネーフォワードの確定申告は、最初の画面で心が折れて回れ右をした。\nfreee が「寄附金控除に関する証明書」の xml を読み込める機能に対応しており、ポチポチで終わった。 事前の証明書を発行するための手続きが若干煩雑だったが、今までに比べると圧倒的に楽だった。 証明書の管理もしなくて良くなったのは革命では? 実作業時間は 30 分程度。 紙がもったいないので、個人的には自治体から書類は送らないで欲しい。\n医療費控除は、国税庁が公開してくれているエクセルにデータを打ち込んで終わり。 実作業時間は 2 時間程度。 そのフォーマットに free が対応してくれているので読み込んで終わり。\n最終的な e-Tax の提出は、スマホでカードリーダーを読み込んで連携1させれば終わりだったので非常に快適だった。 2021 年は張り切って、IC カードリーダーを買ったが結局手持ちの Mac と Safari の相性が悪くかなり粘ったが結局紙に印刷して送付したという思い出がある。\nアイ・オー・データ IC カードリーダー ぴタッチ 確定申告 e-Tax\n国税庁が用意した Safari の拡張機能周りが魔窟で、当時のサイトを忘れたがマニュアルにかかれていない設定をしないとマイナンバー読み込みプログラムがそもそも起動しなかった。 リベンジできてなにより。 完全に無用になった IC カードリーダーは即売却した。\n総評 年々システムが改善されて自動化されていて、素晴らしい。\n賛否両論あると思うが、医療費もマイナンバーの保険証化2によりデータが収集されるらしいのでぜひ浸透してほしい。 最近のニュースでは別途診察代金が請求される方式らしく3、ここに税金使わないで何に使うんだろうかと思ったが、予算が取れなかったのかなと色々と考えをめぐらした。 せっかく良い活用が期待できるのに、使う動機を無くすのはもったいない。\n  e-Tax、スマホがあれば IC カードリーダーは不要に - ケータイ Watch \u0026#x21a9;\u0026#xfe0e;","title":"2022年、はじめてのまともな確定申告"},{"content":"gRPCurl 1 を使ってリクエストを送る際に、 reflection を機能を使わずに protobufs のファイルを読み込もうとすると\n1  Failed to process proto source files.: could not parse given files: ~ no such file or directory   とエラーがでてコマンドが実行できなかった。\n対処方法としては grpcurl コマンドを実行する際に、-proto フラグを利用するだけではなく、-import-path フラグを指定する必要がある2。\n-import-pathフラグの指定により、参照する protobufs の依存関係のパスを grpcurl に伝えることで上記のエラーが解消される。\n例えば、protobufs の内部で\n1  import \u0026#34;~/---.proto\u0026#34;  のように他の protobufs を import していると上記のエラーの発生原因となる。\nつまり、-import-pathを指定しないと、import 文実行時に grpcurl 内部で、参照する protobufs の root path が不明なので、パスがうまく処理されずに import 文の実行処理がコケてしまうと理解。\n  fullstorydev/grpcurl: Like cURL, but for gRPC: Command-line tool for interacting with gRPC servers \u0026#x21a9;\u0026#xfe0e;\n Proto Source Files \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-04-28-2114/","summary":"gRPCurl 1 を使ってリクエストを送る際に、 reflection を機能を使わずに protobufs のファイルを読み込もうとすると\n1  Failed to process proto source files.: could not parse given files: ~ no such file or directory   とエラーがでてコマンドが実行できなかった。\n対処方法としては grpcurl コマンドを実行する際に、-proto フラグを利用するだけではなく、-import-path フラグを指定する必要がある2。\n-import-pathフラグの指定により、参照する protobufs の依存関係のパスを grpcurl に伝えることで上記のエラーが解消される。\n例えば、protobufs の内部で\n1  import \u0026#34;~/---.proto\u0026#34;  のように他の protobufs を import していると上記のエラーの発生原因となる。\nつまり、-import-pathを指定しないと、import 文実行時に grpcurl 内部で、参照する protobufs の root path が不明なので、パスがうまく処理されずに import 文の実行処理がコケてしまうと理解。\n  fullstorydev/grpcurl: Like cURL, but for gRPC: Command-line tool for interacting with gRPC servers \u0026#x21a9;\u0026#xfe0e;","title":"gRPCurl で `Failed to process proto source files.: could not parse given files:` エラーが出たときの対処方法"},{"content":"過去に 1 歳児の時点で育児の際に役立ったものに関する記事1を書いた。 今回は、その中で重宝していたホワイトノイズマシンを Google Home mini からDreamegg ホワイトノイズマシンに置き換えたので備忘録がてらメモ。\nホワイトノイズマシンとしての Google Home mini の欠点 当初は既に持っている Google Home mini がホワイトノイズマシンとして使えることが分かり、とてもありがたがったが使っていくうちに課題点が見えてきた。\nまず一つ目が、ウェイクワードによる起動が手間がかかる。 子供が寝ているときや寝る前にホワイトノイズマシンを起動するのだが、その際に\n自分「OK Google, ホワイトノイズを流して」\nというわけだが、この音声入力が非常に煩わしい。子供が寝とんやぞ!と。 しかも一回で成功したならまだしも、子供が起きないように囁くように言うので、\nGoogle Home「すみません、聞き取れませんでした!」\nと返してくるのが、非常にイラッとする。子供が起きちゃうでしょうが!!となる。\n2 つ目が安定性の問題で、ホワイトノイズは 12 時間再生されるのだが、Google Home をホワイトノイズマシンとして使い始めて 4 ヶ月程度経過してくると、3-4 時間経過するとなぜか再生が切れていたりすることが多くなったり、エラーメッセージと共にそもそも再生できないことが多くなった。 使い方としては、子供が寝ている間の 7-12 時間再生を 4 ヶ月毎日行うという、耐久試験のような使い方をしているので不具合が出るのも仕方ない気がしている。\nまた、後述する Dreamegg に乗り換えたあとに知ったのだが、Google Home のホワイトノイズの再生機能のスペックが変更されたとのことで、なおさら乗り換えてよかったなと感じた。\n No longer 12 hours with small 2 second break every hour 2\n Dreamegg ホワイトノイズマシンの良さ そんなこんなで、Google Home の限界を感じていたときにホワイトノイズマシンを調べていたら、Dreamegg というメーカーのホワイトノイズマシンを知り、勢いで買ってみたがこれが大正解だった。 Dreamegg が提供するホワイトノイズマシンも安眠グッズシリーズで複数台あるので、自分の用途に合ったものを買うとよい。 自分は、一番安価なものと当時は 1500 円しか変わらなかったので、興味本位で一番ハイエンドのDreamegg ホワイトノイズマシン D3 Pro を購入してみた。\nホワイトノイズマシンとして作られた製品なので、とてもシンプルで使いやすい。ウェイクワードも必要なく、ボタンを押せば再生できる。\n再生時間の上限も無く、無制限に再生可能。自分の子供は比較的睡眠時間が長い方で 13 時間程度寝ることも普通にあるのだが、Google Home を使っていた頃はホワイトノイズが停止する 12 時間で起きてしまっていた。Dreamegg は再生時間の上限が無いので子供が寝たいだけ快眠してくれる。\nまた、使い始めて半年たったが単一機能の製品なので連続再生で調子が悪くなるという傾向がまったくない。シンプル・イズ・ベスト\nもし外で寝泊まりする際も気軽に持ち運びやすいので、外泊時も安心できる要素が一つ増えた。\n余談 自宅での作業時にもホワイトノイズマシンを使っているが、集中して作業ができるの棚ぼた的に嬉しい。 カフェでの適度な雑音って集中力が高まるが、あれに近い状態を作り出してくれる。\n  子供が 1 歳児を迎えるまでに、育児で役に立ったもの \u0026#x21a9;\u0026#xfe0e;\n White noise only playing for small amount of time  \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-04-26-2320/","summary":"過去に 1 歳児の時点で育児の際に役立ったものに関する記事1を書いた。 今回は、その中で重宝していたホワイトノイズマシンを Google Home mini からDreamegg ホワイトノイズマシンに置き換えたので備忘録がてらメモ。\nホワイトノイズマシンとしての Google Home mini の欠点 当初は既に持っている Google Home mini がホワイトノイズマシンとして使えることが分かり、とてもありがたがったが使っていくうちに課題点が見えてきた。\nまず一つ目が、ウェイクワードによる起動が手間がかかる。 子供が寝ているときや寝る前にホワイトノイズマシンを起動するのだが、その際に\n自分「OK Google, ホワイトノイズを流して」\nというわけだが、この音声入力が非常に煩わしい。子供が寝とんやぞ!と。 しかも一回で成功したならまだしも、子供が起きないように囁くように言うので、\nGoogle Home「すみません、聞き取れませんでした!」\nと返してくるのが、非常にイラッとする。子供が起きちゃうでしょうが!!となる。\n2 つ目が安定性の問題で、ホワイトノイズは 12 時間再生されるのだが、Google Home をホワイトノイズマシンとして使い始めて 4 ヶ月程度経過してくると、3-4 時間経過するとなぜか再生が切れていたりすることが多くなったり、エラーメッセージと共にそもそも再生できないことが多くなった。 使い方としては、子供が寝ている間の 7-12 時間再生を 4 ヶ月毎日行うという、耐久試験のような使い方をしているので不具合が出るのも仕方ない気がしている。\nまた、後述する Dreamegg に乗り換えたあとに知ったのだが、Google Home のホワイトノイズの再生機能のスペックが変更されたとのことで、なおさら乗り換えてよかったなと感じた。\n No longer 12 hours with small 2 second break every hour 2\n Dreamegg ホワイトノイズマシンの良さ そんなこんなで、Google Home の限界を感じていたときにホワイトノイズマシンを調べていたら、Dreamegg というメーカーのホワイトノイズマシンを知り、勢いで買ってみたがこれが大正解だった。 Dreamegg が提供するホワイトノイズマシンも安眠グッズシリーズで複数台あるので、自分の用途に合ったものを買うとよい。 自分は、一番安価なものと当時は 1500 円しか変わらなかったので、興味本位で一番ハイエンドのDreamegg ホワイトノイズマシン D3 Pro を購入してみた。","title":"子供の就寝時に使っているホワイトノイズマシンをGoogle Home から Dreamegg に変更"},{"content":"slug とは、URL 末尾の識別子のこと1で、hugo では自分だと記事を書く際に Makefile で以下のコマンドで作成していた。\n1 2  new: ## Make new post with date as slug \thugo new posts/$(shell date \u0026#39;+%Y-%m-%d\u0026#39;)/index.md   いろんな考えがあるが、自分は slug の作成時に頭を全く使いたくないので、以前からdate コマンドを使ってYYYY-MM-DDの形式で slug を作成している。 SEO などのために、slug にタイトルを入れるパターンもあるが、記事の作成の流れとして自分は、\n ドラフトを hugo new で作成。この時点でタイトルはまだ確定していない 書き上げていくうちにタイトルを最終決定  なので、タイトルは slug に含めない派。\nだが、従来の date '+%Y-%m-%d' の欠点として同日に複数の記事を作成すると衝突してしまうと問題があった。 今まではそれを避けるために、メモ帳に下書きを書いてあとからコマンドを打って記事を作成して対処していた。\nが、それもめんどくさいと感じたので、\ndate '+%Y-%m-%d' → date '+%Y-%m-%d-%H%M'\nにして、 date コマンドに時間と分数を末尾に追加するように slug 作成コマンドを変更した。\nこれで、同じ日に複数の記事を書けるようになった。\n  Why ‘slug’ and not ‘permalink’ \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-04-24-0026/","summary":"slug とは、URL 末尾の識別子のこと1で、hugo では自分だと記事を書く際に Makefile で以下のコマンドで作成していた。\n1 2  new: ## Make new post with date as slug \thugo new posts/$(shell date \u0026#39;+%Y-%m-%d\u0026#39;)/index.md   いろんな考えがあるが、自分は slug の作成時に頭を全く使いたくないので、以前からdate コマンドを使ってYYYY-MM-DDの形式で slug を作成している。 SEO などのために、slug にタイトルを入れるパターンもあるが、記事の作成の流れとして自分は、\n ドラフトを hugo new で作成。この時点でタイトルはまだ確定していない 書き上げていくうちにタイトルを最終決定  なので、タイトルは slug に含めない派。\nだが、従来の date '+%Y-%m-%d' の欠点として同日に複数の記事を作成すると衝突してしまうと問題があった。 今まではそれを避けるために、メモ帳に下書きを書いてあとからコマンドを打って記事を作成して対処していた。\nが、それもめんどくさいと感じたので、\ndate '+%Y-%m-%d' → date '+%Y-%m-%d-%H%M'\nにして、 date コマンドに時間と分数を末尾に追加するように slug 作成コマンドを変更した。\nこれで、同じ日に複数の記事を書けるようになった。\n  Why ‘slug’ and not ‘permalink’ \u0026#x21a9;\u0026#xfe0e;","title":"slug の作成パターンを変えて、同日に複数の記事を執筆できるようにした"},{"content":"自然言語処理のフレームワークの Spacy を使って、Google Colab 上で NER の可視化を行う際に\n1 2 3 4  import spacy spacy.displacy.render(doc, style=\u0026#39;ent\u0026#39;)   と実行しても\n1  \u0026lt;div class=\u0026#34;entities\u0026#34; style=\u0026#34;line-height: 2.5; direction: ltr\u0026#34;\u0026gt;\u0026lt;/br\u0026gt;\\n\u0026lt;mark class=\u0026#34;entity\u0026#34; style=\u0026#34;background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\u0026#34;\u0026gt;\\n 2022年\\n \u0026lt;span style=\u0026#34;font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\u0026#34;\u0026gt;Date\u0026lt;/span\u0026gt;\\n\u0026lt;/mark\u0026gt;\\n、\\n\u0026lt;mark class=\u0026#34;entity\u0026#34; style=\u0026#34;background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\u0026#34;\u0026gt;\\n 日本人\\n \u0026lt;span style=\u0026#34;font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\u0026#34;\u0026gt;Nationality\u0026lt;/span\u0026gt;\\n\u0026lt;/mark\u0026gt;\\nは震撼した。\u0026lt;/br\u0026gt;まだ\\n\u0026lt;mark class=\u0026#34;entity\u0026#34; style=\u0026#34;background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\u0026#34;\u0026gt;\\n ゾンビ\\n \u0026lt;span style=\u0026#34;font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\u0026#34;\u0026gt;Insect\u0026lt;/span\u0026gt;\\n\u0026lt;/mark\u0026gt;\\nの恐怖は   生の HTML が表示されるだけで、結果がレンダリングされなかったので、そのレンダリングするための方法を記しておく。\n対処方法としては、\nspacy.displacy.render() 関数の jupyter 引数を True にすれば良い。\nSpacy の公式ドキュメントには、1\n force Jupyter rendering if auto-detection fails.\n と書かれており、本来は Jupyter ノートブックからの出力を自動検出してくれるらしいのだが、Google Colab 上では自動検出されてない模様。\nそのため、明示的に jupyter=True にすれば解決する。 同じような質問が Stack Overflow2 でも投稿されていた。\n1 2 3 4 5 6 7 8 9  import spacy nlp = spacy.load(\u0026#39;ja_ginza_electra\u0026#39;) doc = \u0026#34;\u0026#34;\u0026#34; 2022年、日本人は震撼した。まだ恐怖は拭いきれておらず、ネコに癒やしを求めていた。ネコこそが癒やしなのである。もしくはフレンチブルドッグ。 \u0026#34;\u0026#34;\u0026#34; doc = nlp(doc) spacy.displacy.render(doc, style=\u0026#39;ent\u0026#39;, jupyter=True)   実際に使用した Colab のリンクはこちらです。\n  https://spacy.io/usage/visualizers#jupyter \u0026#x21a9;\u0026#xfe0e;\n https://stackoverflow.com/questions/58892382/displacy-from-spacy-in-google-colab \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-04-08/","summary":"自然言語処理のフレームワークの Spacy を使って、Google Colab 上で NER の可視化を行う際に\n1 2 3 4  import spacy spacy.displacy.render(doc, style=\u0026#39;ent\u0026#39;)   と実行しても\n1  \u0026lt;div class=\u0026#34;entities\u0026#34; style=\u0026#34;line-height: 2.5; direction: ltr\u0026#34;\u0026gt;\u0026lt;/br\u0026gt;\\n\u0026lt;mark class=\u0026#34;entity\u0026#34; style=\u0026#34;background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\u0026#34;\u0026gt;\\n 2022年\\n \u0026lt;span style=\u0026#34;font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\u0026#34;\u0026gt;Date\u0026lt;/span\u0026gt;\\n\u0026lt;/mark\u0026gt;\\n、\\n\u0026lt;mark class=\u0026#34;entity\u0026#34; style=\u0026#34;background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\u0026#34;\u0026gt;\\n 日本人\\n \u0026lt;span style=\u0026#34;font-size: 0.","title":"Google Colab で Spacy による NER の結果を表示するには、jupyter 引数を True にする必要がある"},{"content":"仕事でご縁ができた方と、自然言語処理(NLP)領域での国際会議で Industry track に相当する文化って何かありますかねとお聞きした話が面白かったので文章にしておく。\nこの領域を知ったきっかけとしては、CyberAgent さんの記事1を読んだ際に知ったのだが、自然言語処理領域のトップカンファレンスの ACL2, NAACL3, EMNLP4などでは Industry Track ではなく、 System Demonstration という部門が用意されているらしい。\n例えば、Industry Track で有名所だと KDD の Applied Data Science Track5 や RecSys の Industry Talk6 があります。 それらと比較して異なる点としては、\n What problem does the proposed system address?\nWhy is the system important and what is its impact?\nWhat is the novelty in the approach/technology on which this system is based?\nWho is the target audience?\nHow does the system work?\nHow does it compare with existing systems?\nHow is the system licensed?\nCall for System Demonstrations \u0026gt; https://2021.emnlp.org/call-for-papers/demos\n と上記の問いに答えられるような内容が System Demonstration に該当する。 すごくざっくりいうと、\n Industry Track(KDD, Recsys など): 現実世界での実応用をしてこういう学びがあった System Demonstration (ACL, NAACL, EMNLP など): NLP に関わる人に便利な道具を作った  と、発生した背景が違いそうですね。\nIndustry Track は、理論と実践の剥離を防ぐため。System Demonstration は、NLP 周りのソフトウェアの OSS 公開の動機づけやコミュニティの成長を狙っているのかなと。\nSystem Demonstration は良いツールを公開すれば、論文化するチャンスが発生するのは、良い仕組みですね。\n  自然言語処理分野のトップカンファレンス「EMNLP 2021」の System Demonstration Track にて論文採択 ー モバイル端末用の効率的なアノテーションツールを提案 ー \u0026#x21a9;\u0026#xfe0e;\n ACL’21: https://aclanthology.org/volumes/2021.acl-demo/ \u0026#x21a9;\u0026#xfe0e;\n NAACL’21: https://aclanthology.org/volumes/2021.naacl-demos/ \u0026#x21a9;\u0026#xfe0e;\n EMNLP’21: https://aclanthology.org/volumes/2021.emnlp-demo/ \u0026#x21a9;\u0026#xfe0e;\n Call for Applied Data Science Track Papers KDD2022 \u0026#x21a9;\u0026#xfe0e;\n Call for Industry Talk Proposals recsys 2022 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-04-04/","summary":"仕事でご縁ができた方と、自然言語処理(NLP)領域での国際会議で Industry track に相当する文化って何かありますかねとお聞きした話が面白かったので文章にしておく。\nこの領域を知ったきっかけとしては、CyberAgent さんの記事1を読んだ際に知ったのだが、自然言語処理領域のトップカンファレンスの ACL2, NAACL3, EMNLP4などでは Industry Track ではなく、 System Demonstration という部門が用意されているらしい。\n例えば、Industry Track で有名所だと KDD の Applied Data Science Track5 や RecSys の Industry Talk6 があります。 それらと比較して異なる点としては、\n What problem does the proposed system address?\nWhy is the system important and what is its impact?\nWhat is the novelty in the approach/technology on which this system is based?\nWho is the target audience?\nHow does the system work?","title":"自然言語処理トップ国際会議の System Demonstrations について"},{"content":"先日の記事で告知した1のですが、昨夜、「Human In The Loop」を題材にした勉強会を開催してきました。\n実際に Human In The Loop を扱った MLOps の論文2 を過去に書いているくらい興味のある分野なので、この領域を盛り上げていくために開催できてよかった。\nオンライン勉強会でしたので、配信動画を Youtube で公開しております。 Human In The Loop に興味のある方はぜひご覧ください。\nMachine Learning Casual Talks #13 (Online) \n 各発表について 各発表の説明は割愛して、一言感想を述べさせていただきます。\nEditors-in-the-loop なニュース記事要約システムの提案 by @upura  業務成果を国際会議のワークショップに通されたの素晴らしいですね 👏 (自分も論文を出したことがありますが)、論文を書くことが目的ではない職種で論文を出すのはそもそも体力が必要なので、提出してなおかつ採択されたのは素晴らしいの一言!\nActive Learning for Auto Annotation by @tkc79  自分たちのプロダクトで実際に能動学習の効果を検証して学びを得るというのは得難い経験ですね。尊い! 実際にやってみた上での実践的な経験を語ってくれたのが面白かったです。\nNeurIPS Data-Centric AI Workshop by @K_Ryuichirou  Data Centric AI Workshop の要約を話して頂きました。 The Godfather of MLOps である D.Sculley さん 3が 「Technical Debt in ML: A Data-Centric View」の話をされていたらしいのですが、これは見なければ\u0026hellip;! Moneyfoward 様での業務で @K-Ryuichirou 様が作られた資料なのですが、公開を快諾して頂き、Moneyfoward 様、 @K-Ryuichirou さまありがとうございました。\n勉強会の運営振り返り オフラインと比べると圧倒的に楽、楽すぎる。これに尽きる。 事前の運営コストの見積もり予想がバッチリあたって何より4。 代償としては、やはりオフライン懇親会とオンライン懇親会は似て非なるものということを実感できたが、運営者観点でいうとアリよりのアリではと感じた。\n配信のリアルタイム視聴者は最大 98 人で、勉強会の参加ページは 258 人だったので、見込みに対して 4 割程度の方がリアルタイムに配信を視聴してくれた。\nStreamyard が神ツール。これが無料とは\u0026hellip; OBS で頑張って配信しなくても、Streamyard を使えば Web アプリだけでヨイカンジの配信できるなんて感動ですね。 前回の MLCT 12 回目のイベントの配信時5には @chezou さんにマシンに火を吹いてもらって配信してもらっていたんですが、今回は @chezou さんがバンクーバーにいらっしゃるので代わりに Streamyard を使うことにしました。\n実際に使ってみると、招待リンクで登壇者を Web 上で完結して招待可能なのがとても便利。Web アプリベースなので、Zoom などであるあるの「あれうまく接続できないですね..」が無い。Chrome のカメラやマイク設定の承認で一度再入出する必要があったが、それは 2-3m でトラブルシューティングを完了したので全く問題がなかった。\nまた、開催者が Streamyard 参加者の画面を配信に投影するかどうかを調整可能なのも良い。複数人の参加者の画面共有を共存して待機することができるので、事前に登壇者の画面を共有してもらうことで、司会のスライドから登壇者の画面へ滑らかに遷移が可能なのでぐだつかない。DJ(というより VJ?) のように、画面を差し替えて進行していけるのは楽しさがあった。\n配信時の画面レイアウトが複数パターンあり、いろんな配信パターンに対応できる。資料と登壇者を移したり、登壇者だけでグリッド状に配信したりと良い感じに配信が行える。\n音楽配信も Streamyard 付随の音楽をアイスブレイクに流せる(動画も流せる)ので、無音で気まずい空気が流れるのもこれで回避できる。\nプライベートチャットで、時間のリマインドやコミュニケーションが Streamyard 内で完結できる。配信者としては Streamyard 内ですべて完結してコミュニケーションが取れるのはありがたい。登壇者へのタイムスケジュールのリマインドもここで行える。\nYoutube 配信が簡単に配信できる。認証すればすべてが Streamyard 内で完了する。Youtube のコメントが Streamyard 内でできるので、簡単にアナウンスができる。当日判明したのは Youtube 側のコメントが誰もがコメントできるように開放されていなかったので、何かしらの設定が必要なのかも知れない。\n以上を踏まえて、いたせりつくせりの機能となっており、Streamyard のウォーターマークが動画に入るとはいえ、無料で使わせていただいて良いのだろうかと心配になるほどいい感じの配信ができた。\nオンライン懇親会 今回の懇親会は、 gather.town で開催しました。当初は spatial.chat で開催予定でしたが無料枠の参加人数がソッと 50 人から 25 人に減っていた。ならば無料枠の参加人数が同じで自分にとって馴染みのあるツールである gather を採用することにしました。\n参加人数は 8-9 人程度で、一つの輪になって話す感じでリアルな懇親会での偶発性がある出会いはなかなか難しいですが、10 人以下のグループで話が発生するので、オンライン懇親会の開催自体は全然ありだなと思いました。開催コストも 0 なので、やって損はなし。\n個人的な理想としては、8-9 人の参加者だと 4 人グループ x 2 のような構成ができると嬉しいなと思いつつ、ここらへんの考えは人によって異なるので色々と模索していきたい。 Zoom や Google Meet のブレイクアウト機能が、矯正分割してワイワイ感を増せるのだが、懇親会に参加している人が必ずしも喋りたいというわけではない(グループに参加して話を聞くだけでも十分という人もいそう)と思うので、良い塩梅って難しい。\n総評 久々の勉強会の主催ですが、オフラインのコスト 1/10 くらいで実現できて感動しつつ、また機運があれば開催したいなと思っております。\n改めて、Twitter6 での突発的な募集にも関わらず手を上げてくださった登壇者の @upura さん、 @tkc79 さん、 @K_Ryuichiro さんありがとうございました。 また、実況や配信の監視などを行っていただいた共同運営者の @tetsuroito さん、ありがとうございました!\nオンライン配信ならではの楽しさとしては、オフラインだと 100 人が参加する会場を手配したり設営するだけでも大変ですが、オンラインだと 100 人閲覧というリアルタイムな参加者だけに着目するとパソコンだけで実現できるのでそれは素晴らしいことですね。\n  機械学習モデルの改善手法の一つ、 Human In The Loop について \u0026#x21a9;\u0026#xfe0e;\n MLOps の国際会議 OpML\u0026rsquo;20 に、機械学習を活用した商品監視の改善に関する論文が採択されたので登壇してきた \u0026#x21a9;\u0026#xfe0e;\n D. Sculley さんは、Hidden technical debt in machine learning systems や The ML test score: A rubric for ML production readiness and technical debt reduction の著者。機械学習エンジニアリングや MLOps の面白い論文を出しまくっているすごい人。 \u0026#x21a9;\u0026#xfe0e;\n オンライン開催前提だからこそ可能な省エネ勉強会運営 ~勉強会運営再開してみた~ \u0026#x21a9;\u0026#xfe0e;\n Machine Learning Casual Talks #12 (Online) \u0026#x21a9;\u0026#xfe0e;\n https://twitter.com/hurutoriya/status/1492481043747991555\n 私生活に少しに余裕が出てきたので、 実世界の機械学習のよもやま話をする #MLCT を 2 年ぶりに再開検討中です! https://mlct.connpass.com Human In The Loop をテーマに開催したいんですが、話せる人がどれくらいいるか未知数なので自分話せますよという方はリプライいただけると超絶嬉しいです\n \u0026#x21a9;\u0026#xfe0e;   ","permalink":"https://shunyaueta.com/posts/2022-03-31/","summary":"先日の記事で告知した1のですが、昨夜、「Human In The Loop」を題材にした勉強会を開催してきました。\n実際に Human In The Loop を扱った MLOps の論文2 を過去に書いているくらい興味のある分野なので、この領域を盛り上げていくために開催できてよかった。\nオンライン勉強会でしたので、配信動画を Youtube で公開しております。 Human In The Loop に興味のある方はぜひご覧ください。\nMachine Learning Casual Talks #13 (Online) \n 各発表について 各発表の説明は割愛して、一言感想を述べさせていただきます。\nEditors-in-the-loop なニュース記事要約システムの提案 by @upura  業務成果を国際会議のワークショップに通されたの素晴らしいですね 👏 (自分も論文を出したことがありますが)、論文を書くことが目的ではない職種で論文を出すのはそもそも体力が必要なので、提出してなおかつ採択されたのは素晴らしいの一言!\nActive Learning for Auto Annotation by @tkc79  自分たちのプロダクトで実際に能動学習の効果を検証して学びを得るというのは得難い経験ですね。尊い! 実際にやってみた上での実践的な経験を語ってくれたのが面白かったです。\nNeurIPS Data-Centric AI Workshop by @K_Ryuichirou  Data Centric AI Workshop の要約を話して頂きました。 The Godfather of MLOps である D.Sculley さん 3が 「Technical Debt in ML: A Data-Centric View」の話をされていたらしいのですが、これは見なければ\u0026hellip;!","title":"Human-in-the-Loop 🧐 🤝 🤖 を題材にした機械学習の勉強会を開催した"},{"content":"Human In The Loop は、機械学習のモデルのライフサイクルに人が介在することにより、機械学習モデルの改善を目指す手法。\nHuman In The Loop の定義 YANS2021 で公開された馬場先生の Human-in-the-Loop 機械学習 / Human-in-the-Loop Machine Learning の資料は、現状の HITL の取り組みをわかりやすく説明してくれています。\n ここでの Human In The Loop の定義が一番明瞭かなと個人的には思っており、\n Q. より良いモデルを効率的に学習するために人間をどう活用するか?\n と書かれています。\nHuman In The Loop はちょっと意味が広めになりがちな言葉でもあるなと個人的には思っており、機械学習モデルの出力を使ってアノテーションを行う能動学習(Active Learning)の事を主に意味していることが多いが、もう少し広めの機械学習モデルのライフサイクルの中に、人間によるデータのレビューを設置すること 1 でも使われたりする。 が、根本的には馬場先生の定義したリサーチクエスチョンに帰結しますね。\n日本語の書籍だと、\n 鹿島先生、小山先生、馬場先生らの ヒューマンコンピュテーションとクラウドソーシング 喜連川先生、森嶋先生らのクラウドソーシングが不可能を可能にする: 小さな力を集めて大きな力に変える科学と方法  などがデータをどうやって効率的に多数の人間の手によって取得していくかの領域を扱っている書籍。\n英語の書籍だと Human-in-the-Loop Machine Learning: Active learning and annotation for human-centered AI  がドンピシャの内容ですね。 中身は、能動学習とアノテーションの実践的知識について書かれています。\nチョット前に見かけたこの資料は\n人間参加型の AI 活用 (Human-in-the-loop)\nHuman In The Loop について浅く広くまとめられていているので、Human In The Loop の概観を知りたい人には良いかも知れない。\n最近だと ABEJA さんが\nABEJA、化学プラント領域で ABEJA Platform による Human in the Loop Machine Learning を実現　〜 三菱ガス化学と協業し AI を活用したプラント内の腐食配管の外観検査システムを開発\nのリリースを出しており、\n ② の AI が腐食箇所を特定\n の部分が能動学習として作業員に予測結果を提示して、修正した結果を再度フィードバックして再学習するのかなと。どれくらいの速度で再学習がされるのかなと気になったり。例えば一日に何回も再学習させるのかとか、そこらへんの運用フロー考えるの楽しそう\u0026amp;大変そう。\nどうすれば高品質なデータを効率的に収集できるかについては、実世界機械学習の永遠の課題だと思っている。 なぜならビジネス的な結果を出す前の段階として、根本的にモデルの性能を最低限だすためには、高品質なデータとある程度の規模のデータを収集しないとそもそもプロジェクトが始められないからです。\nここで唐突に広告ですが\u0026hellip; 答えの一つが Human In The Loop だと思うのですが、日本語圏だとあまり盛り上がって無いような気がするので、 Human In The Loop に興味のある方、2022/03/30に Humain In The Loop をテーマにしたオンライン勉強会を開催することにしました。\n登壇者募集をしてみたら、続々と名乗りを上げて頂き非常に感謝です。\nもし、興味のある方はご参加下さい\nMachine Learning Casual Talks #13 (Online) : 機械学習に関する知見をカジュアルに語る。テーマ: Human In The Loop\n  Google による機械学習の実応用をテーマにした Coursera の講義は、機械学習プロジェクトに携わるなら一度は見ておいても損はない \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-03-22/","summary":"Human In The Loop は、機械学習のモデルのライフサイクルに人が介在することにより、機械学習モデルの改善を目指す手法。\nHuman In The Loop の定義 YANS2021 で公開された馬場先生の Human-in-the-Loop 機械学習 / Human-in-the-Loop Machine Learning の資料は、現状の HITL の取り組みをわかりやすく説明してくれています。\n ここでの Human In The Loop の定義が一番明瞭かなと個人的には思っており、\n Q. より良いモデルを効率的に学習するために人間をどう活用するか?\n と書かれています。\nHuman In The Loop はちょっと意味が広めになりがちな言葉でもあるなと個人的には思っており、機械学習モデルの出力を使ってアノテーションを行う能動学習(Active Learning)の事を主に意味していることが多いが、もう少し広めの機械学習モデルのライフサイクルの中に、人間によるデータのレビューを設置すること 1 でも使われたりする。 が、根本的には馬場先生の定義したリサーチクエスチョンに帰結しますね。\n日本語の書籍だと、\n 鹿島先生、小山先生、馬場先生らの ヒューマンコンピュテーションとクラウドソーシング 喜連川先生、森嶋先生らのクラウドソーシングが不可能を可能にする: 小さな力を集めて大きな力に変える科学と方法  などがデータをどうやって効率的に多数の人間の手によって取得していくかの領域を扱っている書籍。\n英語の書籍だと Human-in-the-Loop Machine Learning: Active learning and annotation for human-centered AI  がドンピシャの内容ですね。 中身は、能動学習とアノテーションの実践的知識について書かれています。\nチョット前に見かけたこの資料は\n人間参加型の AI 活用 (Human-in-the-loop)\nHuman In The Loop について浅く広くまとめられていているので、Human In The Loop の概観を知りたい人には良いかも知れない。","title":"機械学習モデルの改善手法の一つ、 Human-in-the-Loop について"},{"content":"過去に執筆した記事1を見返していたら\nそういえば講師陣がめちゃくちゃ良いこと言ってるんだけど記事内に掲載してなかったなと思い、動画を見返すと今でも学びが多かったので、講義のスクリーンショットを見返しつつ筆をとってみた。\n今見たら、日本語版の講義 How Google does Machine Learning 日本語版も公開されているので、興味の湧いた方はぜひ受講しましょう。Certificate を発行しないなら無料で受講できると思います。 講義内容の説明は、過去記事1で行っているので気になる方は御覧ください。\n機械学習プロジェクトの努力の割当: 期待と現実  ML Surprise _https://www.coursera.org/learn/google-machine-learning/lecture/aUjhG/ml-surprise_\n Hidden Technical Debt in Machine Learning Systemsと同じ話ですね。上記の論文でよく参照される図よりも、\n Hidden Technical Debt in Machine Learning Systems から引用\n 機械学習プロジェクトを\n KPI の定義 データ収集 インフラ構築 モデルの最適化 システムインテグレーション  上の 5 項目で分けて、棒グラフの各項目が割合と順序を示しているので更にわかりやすいですね。\n機械学習のシステム面かプロジェクト面のどちらに注力しているかという話ですが、プロジェクト面まで包括して説明しているのは良いですね。\n機械学習で避けるべき上位 10 個の落とし穴  The secret sauce _https://www.coursera.org/learn/google-machine-learning/lecture/BdsV6/the-secret-sauce_\n 講師陣が Google 内部でのインタビューを行い、機械学習プロジェクトのアンチパターンのランキング Top10 を公開してくれています。\nアンチパターンの列挙ですが、肯定文と否定文が混じっているので、否定文で統一しています。\n  機械学習の知識と同じくらい、ソフトウェア開発とインフラストラクチャの知識・経験を要求される\n  まだデータを収集していなかった\n  データが既に使える状態だと仮定していた\n  NOTE: 例えばデータは収集されているが使える状態ではないとかですね。CSV ではなく、PDF でデータが保存されていたり、目当てのデータを取得するのに前処理をしないとそもそも取得できないなど\u0026hellip; 過去に主催した勉強会でも同じような事が言及されてます。 @yuzutas0 さん、あらためて登壇ありがとうございました!\nデータマネジメントなき ML は、破綻する。 #MLCT\n    Human In The Loop を維持していなかった\n NOTE:ここで言及される Human In The Loop (HITL)は、機械学習モデルのライフサイクルの中に、人間によるデータのレビューを設置することを指している。広義的な Human In The Loop ですね。Active Learning の事を Human In The Loop と定義されたりしていますが、機械学習モデルのライフサイクルに人間が介在することが一貫しています。    機械学習の機能に注力したプロダクトをリリースする\n 機械学習の機能を作成するためにはまず大量のデータが必要なので、まず基本となる機能で顧客を集めてデータを収集しないと鶏と卵の問題に陥る    違う問題に対して機械学習で最適化を行ってしまった\n 例えば、Google 検索を例に上げると、CTR(クリック率)のみに着目すると、悪意のある記事やフェイクニュースなどがどんどん検索ランキングの上位にきてしまう。    機械学習モデルによって、現実世界の指標がどれくらい改善されたか測定していなかった\n 成功したかどうかの判定ができないので、この機械学習モデルを使ったプロジェクトの継続提案を上司にすることができない    学習済みモデルを使うべきか、自分自身でモデルを 1 から作るべきか\n 自分で 1 からモデルを作らずとも GCP の Vison API など実際は提供されている機械学習の API を使えば事足りこともある。本当に機械学習システムを 1 から作るのは大変 NOTE: 個人的には、Billing cost などの問題もあるので、その時の状況次第ですよね。アクセス数が限られているなら、Vision API も適切な場合もあるし、自社で作ったほうが良いこともある。状況を適切に判断することが大事    機械学習モデルはデプロイ後に再学習が必要だったことを知らなかった\n 基本的に、リリース後に何度もモデルの再学習は必要になる。そのため再学習が簡単になるような仕組みづくりに対して投資をしましょう    標準的(当該領域で研究されている)な認識モデルを参照せずに、独自の認識モデルを作成する\n NOTE: ここで意味しているのは、研究領域で何十年も研究されているので自分独自の認識モデルを作成する必要はなくいい、それらを参照しましょうねという意味     ここからは、各講師陣の名言シリーズです。 Slack などの返信にこの画像を使うと楽しいかもしれません。\nデータは無いけど、機械学習がやりたいんです  あなたは機械学習をしたい、しかしまだデータを集めていない\u0026hellip;\n  全力で機械学習をするのをやめてください。あなたにはデータが必要です。\n まずはデータ、データが大事  データは必ず勝つ (機械学習の実応用において、データが最も重要。)\n 機械学習デザインパターンの第 1 著者の GCP の Lak さんの名言です。 そう、良いデータを集めれないと、何も始まらない、データを集めることで勝機が必ず生まれる。\n  Coursera で How Google does Machine Learning の講義を修了した \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2022-03-17/","summary":"過去に執筆した記事1を見返していたら\nそういえば講師陣がめちゃくちゃ良いこと言ってるんだけど記事内に掲載してなかったなと思い、動画を見返すと今でも学びが多かったので、講義のスクリーンショットを見返しつつ筆をとってみた。\n今見たら、日本語版の講義 How Google does Machine Learning 日本語版も公開されているので、興味の湧いた方はぜひ受講しましょう。Certificate を発行しないなら無料で受講できると思います。 講義内容の説明は、過去記事1で行っているので気になる方は御覧ください。\n機械学習プロジェクトの努力の割当: 期待と現実  ML Surprise _https://www.coursera.org/learn/google-machine-learning/lecture/aUjhG/ml-surprise_\n Hidden Technical Debt in Machine Learning Systemsと同じ話ですね。上記の論文でよく参照される図よりも、\n Hidden Technical Debt in Machine Learning Systems から引用\n 機械学習プロジェクトを\n KPI の定義 データ収集 インフラ構築 モデルの最適化 システムインテグレーション  上の 5 項目で分けて、棒グラフの各項目が割合と順序を示しているので更にわかりやすいですね。\n機械学習のシステム面かプロジェクト面のどちらに注力しているかという話ですが、プロジェクト面まで包括して説明しているのは良いですね。\n機械学習で避けるべき上位 10 個の落とし穴  The secret sauce _https://www.coursera.org/learn/google-machine-learning/lecture/BdsV6/the-secret-sauce_\n 講師陣が Google 内部でのインタビューを行い、機械学習プロジェクトのアンチパターンのランキング Top10 を公開してくれています。\nアンチパターンの列挙ですが、肯定文と否定文が混じっているので、否定文で統一しています。\n  機械学習の知識と同じくらい、ソフトウェア開発とインフラストラクチャの知識・経験を要求される\n  まだデータを収集していなかった\n  データが既に使える状態だと仮定していた","title":"Googleによる機械学習の実応用をテーマにしたCoursera の講義は、機械学習プロジェクトに携わるなら一度は見ておいても損はない"},{"content":"2022 年、タスク管理を TickTick で行うで、TickTick の存在をしり興味が湧いたので、Todoist のサブスクリプションの更新期限も近かったので実験的に乗り換えてみた。\nタスクのインポート自体は、TickTick 自体がよくできていて、Todoist で TickTick のアプリの認証をするだけですべて完了するのは楽だった。Todoist 自体には何の不満も感じずプレミアム登録を 4 年ほど継続していた。\n自分のタスク管理ツール遍歴 OmniFocus 2013.05-2016.09\n買い切り型の Mac, iPhone アプリが提供されているとても手触りの良いタスク管理アプリ。使い心地に全く不満はなかったが、Android, Linux, Web の提供がなかったので、Todoist に乗り換えた。高専 4 年から修士 1 年くらいまで使っていた記憶。今でも良い思い出があるくらいには使い心地とデザインが良かった。特に Todoist でも似たような最近追加されたが、予定されているタスクと左側の Mini calendar でどれくらいの量があるか把握できる Focus 機能がとても良かった。 この時の体験のおかげで Mac の有料アプリは質が高いなぁという印象がついた。あと買い切り型のアプリだったので今思えばそれもありがたかった。\nMac App: 4000 円 iOS App: 2000 円\nと少しお高めの値段だが、買い切りでこのクォリティだったので非常に大満足。\n今だと、Mac, iOS 以外に Web 版も提供していると聞いて、Web に手を出していることに驚いた。 そのまま Android アプリも作ってくれないだろうか\nTodoist 2016.10-2022.02\nApple 製品でしか使えない製品は嫌だと思って乗り換えたので Todoist。仕様当初は予定されるタスクの一覧機能がなかった覚えがあるが、2020 年位?から近日中のタスクという機能が提供され始めて嬉しかった。 機能改善のアップデートが頻繁にあり、マークダウンでコメントをかけるようになったり、カンバン表示、サブタスクにも対応したりしていて好印象。乗り換えた TickTick と比べると全体的に使い心地が良かった。そこらへんは後述します。\n購読型で年額 4056 円\nTickTick 2022.03-\nカレンダーをベースにタスク管理を行いたかったので、こちらの記事 をきっかけに乗り換えてみた。一ヶ月程度使ってみた感想としては、すぐさま Todoist に戻るほどではないが、色々改善してほしいところは多数という感じ。\n購読型で年額 3244 円\nTodoist と比較して TickTick の良いところ  カレンダーベースでタスク管理ができる。Todoist も Google カレンダーとして表示させることはできていたが、TickTick レベルでの統合は実現できていなかった。例: アプリのカレンダー上でドラッグアンドドロップでこの時間をこのタスクに設定など。 習慣化アプリも内包されている。いや、タスク管理じゃないやんけという話なんですが、毎日の定期的なタスクと考えれば習慣化アプリも同じ枠組みじゃなかろうか。以前はuhabitsというアプリを使って習慣化したいことを管理してたんですが、結局通知は来ているけども気が付かずつけ忘れたりすることが多かったです。アプリの出来としては、素晴らしく良かったで。また戻るかも? 今の所 TickTick 内で管理できて良いなと思っている点は、ToDo を見に行く時に毎回見に行く場所にあるので、TickTick に統合してからは忘れなくなった気がする。 複数のカレンダーを Auth 形式で認証してまとめることができる。Google カレンダーを見に行く回数が減った。  Todoist と比較して TickTick の改善してほしいところ   @motemen さんの言及している点とかなりかぶるが、\n 辞退した予定もカレンダービューに入ってくる。これはちょっと困る。\n 同意。自分的にはもっと TickTick の利点である、カレンダースケジュールベースの時間管理がやりづらいのでぜひ改善してほしい。\n  Chrome 拡張機能が貧弱。Web クリップなどが使えない。メモ的に今見ているサイトの URL を追加して ToDo 化しておきたいことがあるが、現状の TickTick の Chrome 拡張ではそれができない。\n  タスクのメモ欄で日本語入力するとバグる。task 入力でネストした構造を書くと markdown が壊れる。これはかなり困る。結局サビタスク機能で管理するようにした。\n  カレンダーの作り込みが甘い。\n カレンダーの情報は見れるが、Google Meet などカレンダーに付随した情報は出してくれないし、説明欄に書かれている URL も文字列として表示はされるが、ハイパーリンクではない。 アプリで日次タスクを見ている際にスワイプで前後の日付に移動できない。    Web, Android, Mac で提供されている体験が微妙に異なる。有料でお金を払っているのでもう少し頑張ってほしい。\n  ポモドーロ、習慣化アプリ、カレンダーなどなどいろんな機能を提供しているが、どれもさわり心地が良いかというそうではない。開発力が分散してしまい集中できていない感じがする。\n  総評 今すぐ利用をやめるほどではないが、一年後継続して使っているかはわからない。 細かいところに大きな不満はあるが、カレンダー形式でタスク管理ができるという点のみでも満足できている気がする。 値段も Todoist と比較したら安いのもまぁ値段相応な気がするので良かった。高かったら不満がもっと溜まっていたと推測。\nだが新たな気づきとして、タスク管理アプリは、高機能なものでないと受け入れられないというわけではなく、GTD 形式にやることを忘れずに管理できるなら、最悪それだけで十分ということがわかった。良いものであれば良いに越したことはないが\u0026hellip;\nまた、良かった点としては、Todoist から乗り換えてみた結果、Todoist の質の高いプロダクト開発を感じることができた。また来年戻っても良いかもしれない。\n使っていて表立った不満が出ないということはそれだけでも凄いことなんだなと改めた思った。\n単純に新しいものに乗り換えて試すのはやはり楽しい。 これに尽きる。 今まで使っていたものの良さや、新しいモノの良さが知れて楽しい。 使い慣れたものをずっと使うのも良いが、定期的に環境を意図的に変えるのは良い刺激になる。\n2019.10.16 に iPhone4 から iPhone X まで乗り換えて使っていた iPhone から Pixel に乗り換えて Android を初体験したが、とても良い体験だった。\n最近は次にノートパソコンを買うときは、MacBook ではなく Windows 製品を試してみたいなと思っている。\n","permalink":"https://shunyaueta.com/posts/2022-03-13/","summary":"2022 年、タスク管理を TickTick で行うで、TickTick の存在をしり興味が湧いたので、Todoist のサブスクリプションの更新期限も近かったので実験的に乗り換えてみた。\nタスクのインポート自体は、TickTick 自体がよくできていて、Todoist で TickTick のアプリの認証をするだけですべて完了するのは楽だった。Todoist 自体には何の不満も感じずプレミアム登録を 4 年ほど継続していた。\n自分のタスク管理ツール遍歴 OmniFocus 2013.05-2016.09\n買い切り型の Mac, iPhone アプリが提供されているとても手触りの良いタスク管理アプリ。使い心地に全く不満はなかったが、Android, Linux, Web の提供がなかったので、Todoist に乗り換えた。高専 4 年から修士 1 年くらいまで使っていた記憶。今でも良い思い出があるくらいには使い心地とデザインが良かった。特に Todoist でも似たような最近追加されたが、予定されているタスクと左側の Mini calendar でどれくらいの量があるか把握できる Focus 機能がとても良かった。 この時の体験のおかげで Mac の有料アプリは質が高いなぁという印象がついた。あと買い切り型のアプリだったので今思えばそれもありがたかった。\nMac App: 4000 円 iOS App: 2000 円\nと少しお高めの値段だが、買い切りでこのクォリティだったので非常に大満足。\n今だと、Mac, iOS 以外に Web 版も提供していると聞いて、Web に手を出していることに驚いた。 そのまま Android アプリも作ってくれないだろうか\nTodoist 2016.10-2022.02\nApple 製品でしか使えない製品は嫌だと思って乗り換えたので Todoist。仕様当初は予定されるタスクの一覧機能がなかった覚えがあるが、2020 年位?から近日中のタスクという機能が提供され始めて嬉しかった。 機能改善のアップデートが頻繁にあり、マークダウンでコメントをかけるようになったり、カンバン表示、サブタスクにも対応したりしていて好印象。乗り換えた TickTick と比べると全体的に使い心地が良かった。そこらへんは後述します。\n購読型で年額 4056 円\nTickTick 2022.","title":"タスク管理ツールを Todoist から TickTick へ試しに乗り換えてみた"},{"content":"Google の非公式ブログで、The Unofficial Google Data Science Blog というデータサイエンスをテーマにしたブログがある。\nその中で、\nPractical advice for analysis of large, complex data sets\nの記事を元にして作られた Google Developers Guides: Machine Learning Guides \u0026gt; Good Data Analysis を昨日見かけて読んでいたら素晴らしいドキュメントだったので、ここでその感動を共有したかったので筆をとったしだい。\nGood Data Analysis の概要  Technical: どのような技術を使ってデータを見るべきか Process: データへの取り組み方。どのような観点でデータを見るべきか Mindset: どのような考えでデータ分析をすすめていくべきか  の三段構成でガイドブックが書かれている。\n特にガイドブックで自分が好きな言葉は Mindset 章の\n Data analysis starts with questions, not data or a technique\nデータ分析は質問とともにのみ始まる、データや技術からデータ分析は始まらない\n と\n Be both skeptic and champion\n懐疑的であれ、そしてそのデータに一番詳しい存在(チャンピオン)であれ\n でした。\n最近課題の理解が本当に大事だなと痛感する出来事に直面した後にこのドキュメントを読んだので非常に刺さりました。\nGoogle は、同様のガイドブックシリーズで\n Rules of Machine Learning: 機械学習実践の聖書 People+AI Guidebook: 機械学習を用いた体験をどう作り込むかの知識とベストプラクティスがまとめられている  を公開してくれていて、彼らが経験して本質的な知見を文章化して公開してくれるのは非常にありがたい。\n「Rules of ML」 も今回の 「Good Data Analysis」も本質的な事柄を選びぬいているので、時間が経過しても全く色褪せない。時間劣化が全くしない情報を文章として残して、それが知識として伝搬される。これぞまさに知の高速道路。\n特にマインドセットは、学部や大学院で磨かれる知的生産力だよなぁと思う。 だが、実務でもこのスキルは問題なく磨けると思っている。逆に実社会だからこそ面白い問題に直面しているぶん、楽しい面もある\n日々精進が大事ですね。こういう感動する出来事を日々積み重ねたい\n","permalink":"https://shunyaueta.com/posts/2022-03-08/","summary":"Google の非公式ブログで、The Unofficial Google Data Science Blog というデータサイエンスをテーマにしたブログがある。\nその中で、\nPractical advice for analysis of large, complex data sets\nの記事を元にして作られた Google Developers Guides: Machine Learning Guides \u0026gt; Good Data Analysis を昨日見かけて読んでいたら素晴らしいドキュメントだったので、ここでその感動を共有したかったので筆をとったしだい。\nGood Data Analysis の概要  Technical: どのような技術を使ってデータを見るべきか Process: データへの取り組み方。どのような観点でデータを見るべきか Mindset: どのような考えでデータ分析をすすめていくべきか  の三段構成でガイドブックが書かれている。\n特にガイドブックで自分が好きな言葉は Mindset 章の\n Data analysis starts with questions, not data or a technique\nデータ分析は質問とともにのみ始まる、データや技術からデータ分析は始まらない\n と\n Be both skeptic and champion\n懐疑的であれ、そしてそのデータに一番詳しい存在(チャンピオン)であれ\n でした。\n最近課題の理解が本当に大事だなと痛感する出来事に直面した後にこのドキュメントを読んだので非常に刺さりました。","title":"Google が公開している、より良いデータ分析のためのガイドブック「Good Data Analysis」で、データ分析の要所が簡潔にまとめられていて感動した"},{"content":"現在 AB テスト中なのかもしれないが、かなり変化している。\n以前は QAC(Query Auto Completition) のみ、検索フォームにフォーカスがあたった時に表示していた記憶がある。 だが、現在は一度キーワードを検索して検索結果画面に遷移した後に、検索フォームにフォーカスすると\nの画面のように、Query Suggest や Knowledge graph の結果をキーワードフォームに表示するようになった。\n確かに、キーワードを再度検索する際に上記のコンポーネントを入れるのは試してみるのは良いのかもしれない。\n特にデスクトップの検索ならハコもかなり余っているのでまだ有効活用できる余地はありそう。\nでも、検索のユーザーインターフェースを紹介する書籍ではアンチパターンなのでどうなんだろうか。\n情報検索のためのユーザインタフェース\n 1.1: KEEPING THE INTERFACE SIMPLE https://searchuserinterfaces.com/book/sui_ch1_design.html\n  Figure 1.1: Search results listings from Infoseek in 1997 (left) and Google in 2007 (right), courtesy Jan Pedersen. 画像は上記から引用\n と思いつつも、今の Google 検索のインターフェイスも\n書籍の Google 検索と見比べてもかなりリッチになっているので、書籍が執筆された 2011 年からすると時代は変わっている感も否めない。\nスクリーンショットが取られた 2007 年から 15 年以上も経過してますしね\n@joho_hideo さんから面白いコメントがあったので追記しました。\n @joho_hideo 特別事例ですが「COVID-19」の検索結果はよく設計されていると思います。 https://twitter.com/joho_hideo/status/1499549278012055552\n 確かに、これぞまさに情報検索の真髄ですね。 ダッシュボード的な情報を表示している\n情報検索のためのユーザインタフェース も現状の状況も踏まえた改訂版が出たら面白そう？\n","permalink":"https://shunyaueta.com/posts/2022-03-04/","summary":"現在 AB テスト中なのかもしれないが、かなり変化している。\n以前は QAC(Query Auto Completition) のみ、検索フォームにフォーカスがあたった時に表示していた記憶がある。 だが、現在は一度キーワードを検索して検索結果画面に遷移した後に、検索フォームにフォーカスすると\nの画面のように、Query Suggest や Knowledge graph の結果をキーワードフォームに表示するようになった。\n確かに、キーワードを再度検索する際に上記のコンポーネントを入れるのは試してみるのは良いのかもしれない。\n特にデスクトップの検索ならハコもかなり余っているのでまだ有効活用できる余地はありそう。\nでも、検索のユーザーインターフェースを紹介する書籍ではアンチパターンなのでどうなんだろうか。\n情報検索のためのユーザインタフェース\n 1.1: KEEPING THE INTERFACE SIMPLE https://searchuserinterfaces.com/book/sui_ch1_design.html\n  Figure 1.1: Search results listings from Infoseek in 1997 (left) and Google in 2007 (right), courtesy Jan Pedersen. 画像は上記から引用\n と思いつつも、今の Google 検索のインターフェイスも\n書籍の Google 検索と見比べてもかなりリッチになっているので、書籍が執筆された 2011 年からすると時代は変わっている感も否めない。\nスクリーンショットが取られた 2007 年から 15 年以上も経過してますしね\n@joho_hideo さんから面白いコメントがあったので追記しました。\n @joho_hideo 特別事例ですが「COVID-19」の検索結果はよく設計されていると思います。 https://twitter.com/joho_hideo/status/1499549278012055552\n 確かに、これぞまさに情報検索の真髄ですね。 ダッシュボード的な情報を表示している","title":"デスクトップのGoogle 検索の検索フォームUIがかなり変化していた"},{"content":"Web 検索とデータマイニングのトップカンファレンス WSDM2022 のワークショップで The First International Workshop on INTERACTIVE AND SCALABLE INFORMATION RETRIEVAL METHODS FOR ECOMMERCE (ISIR-ecom) が先日開催された。\nテーマは e コマース上での検索において\n 検索システムのスケーラビリティ どうやって適合性(Relevancy)をシステムで改善したか システムの改善  についてをテーマにした検索エンジニアなら垂涎もののワークショップとなっている。\n同様の検索システムや実応用に注目したワークショップでは、以下のようなワークショップがある。\n SIGIR Workshop On eCommerce 2017 年から毎年開催。累計 5 回開催 International Workshop on Industrial Recommendation Systems 2020 年から開催。累計二回  歴史としては、 SIGIR ecom が長く、これだけの期間継続開催してくれているのはありがたい限り。\n機械学習系の国際会議でも手法ではなく、どう現実世界に適用したかに注目したワークショップが益々誕生しており非常に良い流れ。\nACCEPTED PAPERS は 5 本あり、\n Amazon: 2 eBay: 1 The Home depot: 2  と企業関係者による論文が 100%となっている。\nhttps://github.com/ISIR-eCom/ISIR-eCom.github.io/tree/main/papers 最後の PDF 番号が 9 なので、最低でも 9 本の投稿はあった模様。\n素晴らしいワークショップなので、来年も是非継続して開催してほしい。\nすべての採択論文が面白そうだったが、今回は Amazon Search が公開した Chen Luo らの「ROSE: Robust Caches for Amazon Product Search」を紹介する。\n私的な感想やコメントは NOTE: で始める文章で書き留めています。\n一言で説明 クエリの書き換え(誤植を修正する)と深層学習モデルの結果のキャッシュを同時に行えるキャッシュシステム ROSE を提案して、検索システムの応答速度と検索性能を改善。\nCitation  ROSE: Robust Caches for Amazon Product Search, Chen Luo, Vihan Lakshman, Anshumali Shrivastava, Tianyu Cao, Sreyashi Nag, Rahul Goutam, Hanqing Lu, Yiwei Song and Bing Yin, Proceedings of the International Workshop on Interactive and Scalable Information Retrieval methods for eCommerce (ISIR-eCom), 2022.\n 概要 Amazon Search のような商品検索エンジンはしばしばキャッシュを利用する。 キャッシュによって Amazon の製品検索での顧客体験を向上させることができる。 顧客体験だけではなく、検索システムのレイテンシも大幅に向上する。\nだが、検索トラフィックが増大しキャッシュサイズが大きくなりすぎると検索システム全体のパフォーマンスが劣化してしまう。 また、誤字やスペルミス、現実世界で見られる冗長な表現のクエリは不必要なキャッシュミスを引き起こし、キャッシュの効率性を低下させる。\n「RObuSt cachE(ROSE)」はロックアップコストはそのままにスペルミスや誤字を許容可能なキャッシュシステムを提案。 ROSE はあらゆるクエリの意図、誤字、文法ミスに対して理論的な頑健性が保証されている。 現実世界のデータセットにより ROSE を評価して、有効性と効率は検証を行った。 ROSE はすでに Amazon の検索エンジンにデプロイされており、既存のキャッシュシステムと比較しても大きくビジネス指標を改善している。\nINTRODUCTION 検索エンジンで大事な２つのパフォーマンス指標\n 顧客化からのリクエストに対するレスポンスタイム 顧客の意図に適合した高品質な検索結果  現代的な製品検索エンジンでは通常、計算不可が高い機械学習モデルが多く使われている。\nNOTE: [1, 8, 9, 13, 20, 28, 29, 31] の参考文献が、検索改善のための機械学習適用の文献リストとして有用そうだった。 [18] relevance matching models。[2] ranking models。[27] query annotation models。\nレイテンシの制限とコストの考慮により、実際の製品検索エンジンでは、計算コストが高い深層学習モデルをサービングして検索トラフィック全体を処理することは禁止されている[14]。そのため、深層学習モデルの推論結果をサービングする代わりに、より実践的な解決方法として頻繁にリクエストがくるクエリに対して深層学習モデルの結果をキャッシュしておく。\n従来のキャッシュは以下のようなトレードオフの問題がある\nキャッシュミス率とキャッシュサイズの関係: 小さいキャッシュサイズでは、高いキャッシュミス率となる。一方で製品検索エンジンの規模で頻繁に検索されるクエリに対してキャッシュを作成したとして異常にキャッシュサイズが大きくなってしまう。また、 ”Nike shoes”, “Nike shoe”, and “Nike’s shoe” のような同じ意図のクエリだが、形態素的には異なるそれらのクエリはすべて別のキャッシュとしてキャッシュされてしまう。\nまた、 Query rewrite for null and low search results in eCommerce. In eCOM@ SIGIR. [24]でも示されている通り、検索結果の品質を下げている要因のほとんどが 誤植)。 これらの検索品質が低いクエリは、高品質な検索結果を提供する頻繁に検索されるクエリと語彙的(Lexically) または意味的 (semantically)に似ている。\nよって、もし我々が低品質な検索結果を提供する誤植のクエリを、同じ意図を持つクエリへキャッシングの仕組みにより紐付ける(map する)ことができたなら、検索品質を向上させることができる。また、レイテンシも大幅に向上させ、キャッシュサイズも大幅に抑えることができる。\nFig. 1: ROSE は検索品質と検索速度を向上させる\n キャッシュによるクエリの書き換え(query rewrite)によって検索品質の向上 ROSE が大半の通信をカバーすることで、検索速度の向上。キャッシュヒットしないような少数派のクエリに対しては、深層学習モデルによって直接サービングを行う。  ３つの貢献点  運用システム: 製品検索のクエリをキャッシュするための、ROSE を提案。定数時間と定数メモリで web 検索規模のデータのインデックスを作成してルックアップを実行可能 技術的革新: local-sensitive hashing, reservoir sampling, count-based k-selection など複数の強力な乱択アルゴリズム技術を組み合わせたシステムを開発。これにより、定数時間での検索を維持しながら ROSE を大規模なクエリセットにスケールアップが可能になった。 現実世界での効果: Amazon 検索に ROSE をデプロイしており、既存手法と比べてパフォーマンス、ビジネス指標ともに改善された。  BACKGROUND AND RELATED WORK Robust Cashes 検索エンジンやデータベースなど応答速度がクリティカルなアプリケーションではキャッシュはとても有効なアプローチ。 hash-tables と Bloom fillters によって、キャッシュは完全一致によるキャッシュなどが考案されている[16]。 だが完全一致に注目しているため誤植に弱く従来の完全一致キャッシュでは、キャッシュヒット率が低くなってしまう。 逆に従来の単語のゆらぎに頑健(ロバスト)なキャッシュ手法は、キャッシュの文字列の類似距離を計算するコストが非常に高かった。\n定数時間と定数メモリで実行可能なロバストなキャッシュの仕組みである「ROSE」提案する。\nLocality-Sensitive Hasing LSH の解説資料はこちらの資料がわかりやすかった。\nLocality Sensitive Hashing\n他にも LSH の発展形として Jaccard 類似度を使った LSH である Minwise Hasing(MinHash) や Densified One Permutation Hashing (DOPH) なども紹介されていた。\nNOTE: 時間が足りないので一旦深い解説は割愛する。ここらへんは自分の知識が浅い部分なのでちゃんと理解しておきたい。\n問題の定式化 ROSE のフレームワーク解説\n クエリキーワードからロバストなキャッシュインデックス生成 オンライン検索: 入力されたクエリをキャッシュ上の別のクエリにマッピングする  ROSE: ROBUST CACHE VIA RANDOMIZED HASHING ROSE Index Generation ROSE のインデックス生成時には２つの条件\n 誤植や意味的に同じだが異なるクエリでもロバストな性能のために、キャッシュのルックアップ時に入力クエリの類似度を計算してマップする 大規模な製品検索エンジンなので、キャッシュサイズはクエリの量に応じてスケーリングすることを回避する必要がある  LSH のアプローチでは、データの規模に対して線形にハッシュテーブルのサイズが増加する課題がある[23]。 この課題は web 規模のデータを扱うと容易にメモリが爆発を引き起こす。 その課題を解決するために reservoir sampling algorithm[25] を使って解決している。\nreservoir sampling algorithm の日本語の解説記事はこちら\n大量のテキストからランダムに少数の行を抽出したい - Reservoir Sampling\nNOTE: ここ完全に理解不足なので言葉だけ理解。\nB はハッシュテーブルハッシュテーブルのバケットの数、Iq はハッシュ関数により生成されたインデックス。 Rand(0,B)は-から B までの乱数を生成する関数。\nROSE Online Retrieval クエリが入力された際に、ROSE は Count-based 𝑘-selection [15]という手法を使って、計算コストの高い 2 点間の類似度(pairwise-similarity)の計算を避けている。 ROSE のオンライン検索が定数時間で終わることは、3.5 Theoretical Analysisにて解説。\nLexical Preserving Hashing Lexical Preserving Hashing は語彙的な類似度を保持したハッシュ手法を指す。 クエリ間の類似度は Jacard 類似度を使って計算する。(つまるところ minhash?) 実際に計算する際には DOPS を使って minhash を算出する。\nROSE による誤植修正のための Lexical Preserving Hashing によるマッピング例: 「red nike shoos」 → 「red nike shoes」\nProduct Type Preserving Hashing 製品検索エンジンでは、クエリの製品タイプを理解することが関連した高品質な検索結果を提供するためにとても重要である。\n具体例: 「red nike shoes」 のクエリの製品タイプは 「SHOES」\nここで製品タイプをクエリから抽出するために、クエリのトークンに対して製品タイプの重みを計算する。製品タイプのトークンは QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction[30] という NER モデルによって抽出される。\n製品タイプトークンが存在しなかった場合の重みは 1、1 より以上の場合は製品タイプトークンが抽出できたことになる。 製品タイプトークンの重み付けは W \u0026gt;1 で与え、チューニングした結果 W=10 が最も結果が良好製品タイプトークンは W というしきい値で管理される。\nTheoretical Analysis アルゴリズムの理論解析について。 だが、知識がまったくないので、割愛することにした。\nオフラインでの実験 データセット 約 6 億の検索品質が高いクエリを Amazon 検索からサンプリングして、評価のためのキャッシュ対象にする。 ここでは、Xichuan ら A Dual Heterogeneous Graph Attention Network to Improve Long-Tail Performance for Shop Search in E-Commerce. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \u0026amp; Data Mining. 3405–3415 [19]と同じ評価の枠組みで評価を行った。\nクエリの評価データセットは頻度ごとに分割された３つのバケットを用意\n NQ(Normal Queries): 頻度が上位 33%(1 つめの 3 分位数)のクエリ HQ(Hard queries): 頻度が上位 34%-66% (1 つめの 3 分位数)のクエリ LTQ(Long-tail queries): 頻度が上位 67%-100% (3 つめの 3 分位数)のクエリ  ランダムに上記のクエリを一ヶ月以上の検索ログから選択して収集する。３つのバケットは 1000 のクエリが存在してる。ドメインエキスパートによる判断により、ROSE の仕組みにより re-map された結果が元のクエリが書き換えられても意図が同一かどうかの 2 値(relevant or irrelevant)によって評価。\n実験デザイン ２つの指標に対する仮設\n Robustness: どれぐらい ROSE は正確か? Efficnency: どれぐらい ROSE のインデキシングと検索処理は効率的か?  それらの仮設検証のために\n R-LP: 提案手法。ROSE の lecical preserving hasing。 ハッシュテーブルの数は L=3 6 かつ、ハッシュの数は $K=3$ R-PT: 提案手法。ROSE の product type preserving hasing EC: 完全一致によるキャッシュ実装 BF: ROSE の検索アルゴリズムを力まかせ探索に置き換えた手法。類似度の尺度として DP を使って編集距離を計算する。 FC: FAISS のアルゴリズムを使って ROSE のインデキシングと検索アルゴリズムを置き換えた手法。クエリの埋め込み空間は Semantic Product Search を使って用意。  評価指標としては、一般的な Precision, Recall, F1 を採用。\n各指標の具体例としては、 例えば「red nike shoes」という original intent となるクエリが存在するとする。この「red nike shoes」と original intent は同一だが、語彙的な誤りによって「red nike shoos」、「red nike shooes」などのクエリが存在する。\nこれら、「red nike shoes」を original intent とするクエリ数が 20 件あり、以下のような分布になっている。\n 「red nike shoes」: 10 件  「red nike shoos」: 5 件 「red nike shooes」: 5 件    例えば、完全一致キャッシュでは、「red nike shoes」とう文字列に対して lool up を行い、\n precision: 100% recall: 50%  となる。\nそして、ROSE-LP が 「red nike shoos」を「red nike shoes」に対して map ができたとする。(「red nike shooes」の map は失敗)\n precision: 100% recall: 75%  となる。\n上記のように誤植が含まれるクエリをどれだけ各種手法で書き換えることができたかが、評価指標となっている。\n全体を通した実験結果 検索性能 NQ, HQ に対しては ROSE-PT が最も性能が良く、LTQ に対しては ROSE-LP の性能が良かった。\nシステム性能 ROSE-LP は 60m でインデックス生成を完了。ROSE-PT は 75m。\nNOTE: 論文中だとシステム性能としても性能が良いと書いてあるが、他の手法のほうが早いのでこの書き方は実際どうなんだろう。と思いつつも、検索性能がこれだけ上がって cache index 作成も 65m だけで済んでいるなら全く問題なさそう。\n検索面では、力まかせ探索だと 65m(ms ではない)検索に時間がかかるらしいので、使い物にならなさそう。FAISS が 120ms 必要な中で ROSE は LP, PT ともに 3ms 以下という驚異的な速度。\n図 3 では、ROSE が担っている２つのコンポーネントを解説。\n (a) 誤植の問題を解決するクエリ書き換え。「nike shoos」という誤植のクエリが来た際に、「nike shoes」に書き換え (b) 深層学習モデル高速化のためのキャッシュ上での製品タイプ予測。Cash Hit した場合、ROSE によってクエリ(q)の製品タイプ(PT)を返す。キャッシュヒットしなかった場合は、深層学習モデルによってクエリに製品タイプの推論を行い、返す。  SYSTEM DEPLOYMENT IN AMAZON ROSE for Query Rewrite 顧客が誤植によるクエリクエリを入力してきた際にクエリ書き換えを行うために実際に ROSE をデプロイ。使った手法は ROSE-LP。これによって、検索結果がそもそも低品質なクエリが高品質な検索結果として提示されるようになった。\nオンラインのクエリ書き換えの結果はドメインエキスパートにり測定・評価され、良好な結果となった。 また、複数のビジネス指標も有意に改善された。\nNOTE: CTR +7%ってすごくないですか\u0026hellip;? 今までも、Amazon なのでもちろん Query Understanding に取り組んでいたと思うんだが、従来のクエリ書き換えよりも更に良くなったということだろうか?実際は ROSE によって検索の応答速度も改善されているので、変数は２つとなり難しいところではある。 また、Amazon の規模での利益 0.42% 上昇って半端ないですね。深層学習モデルのキャッシュでもあるので、特徴量の前処理改善にもなっていることに気がついた。\nROSE for Product Type Annotation 「red nike shoes」のようなクエリは、クエリから正しい製品タイプを特定することで、正しい製品を検索できたり、製品タイプによって検索結果を変更することが可能。\n500 万-1000 万の規模の高頻度なクエリに対して、クエリから特定の製品タイプへマッピングする ROSE を実装。\nROSE は受け取った tail query に対して、そのクエリをキャッシュされたいくつかのクエリにマッピングし、そのクエリと紐付いている製品タイプを tail query の製品タイプの予測結果として使用する。\n検索体験への影響を評価するため、ROSE の商品タイプ予測モデルを用いて、誤った商品タイプを持つ無関係な検索結果をフィルタリングし、このシステムを Amazon.com にデプロイし商品検索エンジンにおいて、ROSE により商品タイプを認識した場合としない場合の検索結果の不具合率(Defects Rate)を測定。\n製品タイプ不良率(Product type defect rate)とは、検索結果の上位 16 件に含まれる商品のうち、製品タイプが誤っている数として定義。\n表 2 より、ROSE による製品タイプを予測することで、欠陥率が 1.7%減少し、ユーザーエクスペリエンスが大幅に改善されたことがわかる。\n図 4 では ROSE によって予測された製品タイプのフィルターを通した結果の違いが示されている。\nNOTE: でもこの部分ではビジネス指標について触れていないということは、欠陥率しか改善しなかったのかなと邪推した。(それでも十分凄いけど)\n 社内の検索勉強会の発表順が回ってきたので、最近見つけた面白そうな論文である ROSE を読んでみた。 読んでいて非常に面白い論文で手法もさることながらビジネス指標もリフトさせているの本当にすごいし、価値がありますね。 この成果を公開してくれた Amazon に感謝\n面白い論文は、引用する論文もイケているので周辺知識を深堀りしたい際に辞書的に使えるのもありがたい。\nクエリの書き換えをキャッシュ(ROSE)に搭載して、システム的に密結合になったり、運用面で大変になったりしないのかなと思いつつも、これだけ性能が良いのならそれらを補って良い事だらけではと感じた。\n振り返ってみると最近 Amazon が公開してくれている検索ネタを今回含めて 3 連続で読んで解説していたが\n Daria Sorokina さんによる、 Amazon検索ランキングの奥深さ at MLconf SF 2016 Amazon が e コマース検索を Lucene により、どうスケールさせているか at Berlin Buzzwords 2019  それくらい面白い論文が多いのと e コマース検索だと飛び抜けて先進的な取り組みをしている印象。\n次は The First International Workshop on INTERACTIVE AND SCALABLE INFORMATION RETRIEVAL METHODS FOR ECOMMERCE (ISIR-ecom) の別の採択論文を読む。\nおそらく次に読むのは\n E-commerce Product Attribute Value Validation and Correction Based on Transformers: Le Yu, Haozheng Tian, Yun Zhu, Simon Hughes and Aleksandar Velkoski\n ","permalink":"https://shunyaueta.com/posts/2022-03-03/","summary":"Web 検索とデータマイニングのトップカンファレンス WSDM2022 のワークショップで The First International Workshop on INTERACTIVE AND SCALABLE INFORMATION RETRIEVAL METHODS FOR ECOMMERCE (ISIR-ecom) が先日開催された。\nテーマは e コマース上での検索において\n 検索システムのスケーラビリティ どうやって適合性(Relevancy)をシステムで改善したか システムの改善  についてをテーマにした検索エンジニアなら垂涎もののワークショップとなっている。\n同様の検索システムや実応用に注目したワークショップでは、以下のようなワークショップがある。\n SIGIR Workshop On eCommerce 2017 年から毎年開催。累計 5 回開催 International Workshop on Industrial Recommendation Systems 2020 年から開催。累計二回  歴史としては、 SIGIR ecom が長く、これだけの期間継続開催してくれているのはありがたい限り。\n機械学習系の国際会議でも手法ではなく、どう現実世界に適用したかに注目したワークショップが益々誕生しており非常に良い流れ。\nACCEPTED PAPERS は 5 本あり、\n Amazon: 2 eBay: 1 The Home depot: 2  と企業関係者による論文が 100%となっている。\nhttps://github.com/ISIR-eCom/ISIR-eCom.github.io/tree/main/papers 最後の PDF 番号が 9 なので、最低でも 9 本の投稿はあった模様。","title":"Amazon の製品検索で使われるロバストなキャッシュ手法の論文「ROSE: Robust Caches for Amazon Product Search」"},{"content":"WSDM は web 検索とデータマイニングのトップカンファレンスの一つです。検索好きなら見てて楽しい論文がたくさん公開されており、毎年採択された研究を楽しみに見ています。\n今回 WSDM2022 が 2022/02/21 - 2022/02/25 に開催されたので気になった発表をメモ。\n今までこういう気になったトピックなどは Joplin にメモして公開していなかったが、公開しても差し支えはないなと思ったので Blog 記事として公開していってみる。\n自分の興味関心トピックは今は基本的に検索関連と機械学習の実践事例なので、それに沿った選出になっています。\nIndustry Day https://www.wsdm-conference.org/2022/industry-day-schedule/\n Challenges in Data Production for AI with Human-in-the-Loop, Dmitry Ustalov (Toloka) Scalable Attribute Extraction at Instacart, Shih-Ting Lin (Instacart) Graph Neural Networks for the Global Economy with Microsoft DeepGraph, Jaewon Yang, Alex Samylkin, Baoxu Shi (LinkedIn, Microsoft) Near real time AI personalization for notifications at LinkedIn, Ajith Muralidharan (LinkedIn) Invited Talk: Rethink e-Commerce Search  Workshops https://www.wsdm-conference.org/2022/wsdm-workshops/\n Personalization and Recommendations in Search (PaRiS)  資料公開されていない\u0026hellip;? まだ、終わったばかりなので後から公開されるかもしれない。   Interactive and scalable information retrieval methods for e-commerce  検索のシステム面に特化したワークショップ。今公開された論文を読んでいるが、自分の好みのドンピシャでどれも最高に面白い。検索エンジニアならめっちゃテンションあがると思います。ROSE という Amazon 検索のキャッシュシステムの解説記事を書いているので書き終えたらまた公開します。    Accepted Papers https://www.wsdm-conference.org/2022/accepted-papers/\n Joint Learning of E-commerce Search and Recommendation with A Unified Graph Neural Network, Kai Zhao (Alibaba Group)*; Yukun Zheng (Alibaba inc.); Tao Zhuang (Alibaba Group); Xiang Li (Alibaba Group); Xiaoyi Zeng (Alibaba Group) External Evaluation of Ranking Models under Extreme Position-Bias, Yaron Fairstein (Technion); Elad Haramaty (Amazon); Arnon Lazerson (Amazon)*; Liane Lewin-Eytan (Amazon) On Sampling Collaborative Filtering Datasets, Noveen Sachdeva (UC San Diego)*; Carole-Jean Wu (Facebook AI Research); Julian McAuley (UCSD) Efficient Reachability Query with Extreme Labeling Filter. Zhixiang Su (Nanyang Technological University)*; Di Wang (Nanyang Technological University); Xiaofeng Zhang (Harbin Institute of Technology (Shenzhen)); Lizhen Cui (ShanDong University); Chunyan Miao (NTU) Wikipedia Reader Navigation: When Synthetic Data is Enough, Akhil Arora (EPFL)*; Martin Gerlach (Wikimedia Foundation); Tiziano Piccardi (EPFL); Alberto Garcia-Duran (EPFL); Robert West (EPFL) Differential Query Semantic Analysis: Discovery of Explicit Interpretable Knowledge from E-Com Search Logs, Sahiti Labhishetty (University of Illinois Urbana-Champaign); ChengXiang Zhai (University of Illinois at Urbana-Champaign)*; Min Xie (Instacart); Lin Gong (WalmartLabs); Rahul Sharnagat (WalmartLabs); Satya Chembolu (WalmartLabs) ANTHEM: Attentive Hyperbolic Entity Model for Product Search, Nurendra Choudhary (Virginia Tech)*; Nikhil Rao (Amazon); Sumeet Katariya (Amazon); Karthik Subbian (Amazon); Chandan K Reddy (Virginia Tech)  ","permalink":"https://shunyaueta.com/posts/2022-03-01/","summary":"WSDM は web 検索とデータマイニングのトップカンファレンスの一つです。検索好きなら見てて楽しい論文がたくさん公開されており、毎年採択された研究を楽しみに見ています。\n今回 WSDM2022 が 2022/02/21 - 2022/02/25 に開催されたので気になった発表をメモ。\n今までこういう気になったトピックなどは Joplin にメモして公開していなかったが、公開しても差し支えはないなと思ったので Blog 記事として公開していってみる。\n自分の興味関心トピックは今は基本的に検索関連と機械学習の実践事例なので、それに沿った選出になっています。\nIndustry Day https://www.wsdm-conference.org/2022/industry-day-schedule/\n Challenges in Data Production for AI with Human-in-the-Loop, Dmitry Ustalov (Toloka) Scalable Attribute Extraction at Instacart, Shih-Ting Lin (Instacart) Graph Neural Networks for the Global Economy with Microsoft DeepGraph, Jaewon Yang, Alex Samylkin, Baoxu Shi (LinkedIn, Microsoft) Near real time AI personalization for notifications at LinkedIn, Ajith Muralidharan (LinkedIn) Invited Talk: Rethink e-Commerce Search  Workshops https://www.","title":"Web 検索とデータマイニングのトップカンファレンス WSDM2022 で気になった研究"},{"content":"予定調整サービスのCalendly 皆さん使われていますか?\n僕は COVID-19 の影響でオンライン雑談を始めたのですが、カレンダーと連動したスケジュール調整とビデオチャットの準備が同時にされる点が便利すぎて感動して、ずっと愛用してます。\nチャットで Calendly の自分の予約ページへのリンクを共有するだけで、スケジュール調整がすべて終わるのでまぁ便利。\n社内だと相手のスケジュールが見えるので問題ないのですが、社外の方だとスケジュールが把握できないので、オンライン雑談は社外の人が主なので、重宝しています。\nチャットで従来は\n 「\u0026ndash;日の\u0026ndash;の時間帯はいかがでしょうか?」 「一応カレンダーを抑えておきたいので、メールアドレスを教えてもらってもよろしいでしょうか? カレンダーを招待させていただいております」  のコミュニケーションが必要だったんですが完全になくなりました。最高\n今までの Calendly 使用履歴を見ると 20 件以上使っていました。そんなにオンライン雑談をしていたのか\u0026hellip;!\nですが、スケジュール調整って自分と相手の２人だけ参加するパターン以外に自分を入れて 3 人以上参加する場合もあります。その場合は Calendly の仕様にあっていなかったので、泣く泣く従来の方法で確認をとっていました。\nですが、2021 年の年末にMeeting Polls という自分以外に複数人の参加者がいる場合の調整機能がリリースされました。\n機能説明などは、以下の公式 Blog 記事がわかりやすいです。\n日本語で簡単に説明すると\nCalendly Meeting Polls の流れ  主催者: スケジュール候補の時間帯を複数選択  ここが Calendly らしい顧客のことを考えた機能なんですが、候補として選ばれた時間帯は Calendly がスケジュールを作成して自分のカレンダー上で時間を抑えておいてくれます。そのおかげで、スケジュールが確定する前に「あれ、自分で候補にあげたスケジュールを抑えていなくて、別のスケジュールで埋まってしまった\u0026hellip;」という悲しい事態を防げます。最高   主催者: 参加予定のメンバーにリンクを送信  参加者: メンバーは複数の候補の参加可能かどうか投票を行う。その際にスケジュールの招待をするための、名前とメールアドレスを入力。   主催者: 全員が参加できるスケジュールを選択する  主催者: 候補日のスケジュールはすべて削除され、参加者が招待されたスケジュールのみカレンダー上に残る 参加者: カレンダー上でスケジュール招待される    この機能ですが、無料枠で使えるようにしてくれているので、非常に太っ腹。 誰でも使えます。\n自分の場合は、勉強会の運営時に登壇者の方々のスケジュール調整や、オンライン雑談が自分含めて 3 人以上の際に便利に使わせてもらっています。\nCalendly のプロダクト開発の姿勢は非常に大好きで、僕が Meeting Polls を使った履歴から対象にしていると思うんですが、メールで、Meeting Polls でみんなが待望していた機能を実装したよお知らせが来ていて、「これが本来のもらって嬉しい情報だよなぁ、気持ちの良いメール配信だ」と毎回感動しています。 顧客のことを考え抜いたプロダクト開発は、所作に現れますね~。\nというわけで、\n3 人以上のスケジュール調整をする際には、ベータ版だけど Calendly の Meeting poll がとても便利だからみんな使おうというお話でした。\n","permalink":"https://shunyaueta.com/posts/2022-02-25/","summary":"予定調整サービスのCalendly 皆さん使われていますか?\n僕は COVID-19 の影響でオンライン雑談を始めたのですが、カレンダーと連動したスケジュール調整とビデオチャットの準備が同時にされる点が便利すぎて感動して、ずっと愛用してます。\nチャットで Calendly の自分の予約ページへのリンクを共有するだけで、スケジュール調整がすべて終わるのでまぁ便利。\n社内だと相手のスケジュールが見えるので問題ないのですが、社外の方だとスケジュールが把握できないので、オンライン雑談は社外の人が主なので、重宝しています。\nチャットで従来は\n 「\u0026ndash;日の\u0026ndash;の時間帯はいかがでしょうか?」 「一応カレンダーを抑えておきたいので、メールアドレスを教えてもらってもよろしいでしょうか? カレンダーを招待させていただいております」  のコミュニケーションが必要だったんですが完全になくなりました。最高\n今までの Calendly 使用履歴を見ると 20 件以上使っていました。そんなにオンライン雑談をしていたのか\u0026hellip;!\nですが、スケジュール調整って自分と相手の２人だけ参加するパターン以外に自分を入れて 3 人以上参加する場合もあります。その場合は Calendly の仕様にあっていなかったので、泣く泣く従来の方法で確認をとっていました。\nですが、2021 年の年末にMeeting Polls という自分以外に複数人の参加者がいる場合の調整機能がリリースされました。\n機能説明などは、以下の公式 Blog 記事がわかりやすいです。\n日本語で簡単に説明すると\nCalendly Meeting Polls の流れ  主催者: スケジュール候補の時間帯を複数選択  ここが Calendly らしい顧客のことを考えた機能なんですが、候補として選ばれた時間帯は Calendly がスケジュールを作成して自分のカレンダー上で時間を抑えておいてくれます。そのおかげで、スケジュールが確定する前に「あれ、自分で候補にあげたスケジュールを抑えていなくて、別のスケジュールで埋まってしまった\u0026hellip;」という悲しい事態を防げます。最高   主催者: 参加予定のメンバーにリンクを送信  参加者: メンバーは複数の候補の参加可能かどうか投票を行う。その際にスケジュールの招待をするための、名前とメールアドレスを入力。   主催者: 全員が参加できるスケジュールを選択する  主催者: 候補日のスケジュールはすべて削除され、参加者が招待されたスケジュールのみカレンダー上に残る 参加者: カレンダー上でスケジュール招待される    この機能ですが、無料枠で使えるようにしてくれているので、非常に太っ腹。 誰でも使えます。\n自分の場合は、勉強会の運営時に登壇者の方々のスケジュール調整や、オンライン雑談が自分含めて 3 人以上の際に便利に使わせてもらっています。","title":"3人以上のスケジュール調整をする際には、ベータ版だけど Calendly の Meeting polls がとても便利"},{"content":"自分はMachine Learing Casual Talksという勉強会の運営を @chezou さん、 @tetsuroito さん、 @komiya_atsushi さんの運営陣に合流する形で 2018/7 に再開しました。\nもともと自分は根底として勉強会運営が好きで、つくばにいた頃から、tsukuba.rb や PRML勉強会などの勉強会運営をしていたというのもある。\n詳しい経緯は過去に記事に書いていた。 見返すとなかなかにエモい文章ですね。\nMachine Learning Casual Talks #5 を開催しました\nその後子供が産まれる直前の 2020/05 に12 回目を開催して以降、育児で時間的・精神的余裕がなくなって開催が途絶えてしまっていた。\n2021/06 に社内チャットで、\n育児で運営が途絶えてしまったんですが、皆さんどう克服しましたか?\nという質問したら、要約すると\n @lestrrat さん  燃え尽きてもいいじゃないか by @lestrrat   @sinmetal さん  志低く、無くならないようにしようぐらいの気持ちです。    と多種多様な考えを聞けて自分の中でも色々と考えが深まりました。\n当時の僕の反応を拾ってみるとこんな感じ\n 志が低いというのはとても良いですね。存続させるの大事だなぁと痛感してます:relaxed: 僕も学生でつくばにいた頃東京の勉強会は参加できないけど、資料公開してくれるのありがてぇ、そしてこの分野(機械学習エンジニア) に興味あるけどそもそも鶏卵問題で経験がないと参入できないから知見を公開してくれるの助かるなぁという思い出があったなと今思い出しました w 今は実務でバリバリ触れているからこそ初心を忘れてしまったのかもしれないので、情報発信の大事さを今一度噛み締めました\n で、 2022/02 の現在ふとリアル開催?の時に比べるとオンライン開催ってめちゃくちゃ省エネで開催できるなと気が付きました\u0026hellip;!\n開催の手間 やるべきことを簡単に洗い出してみます。\n共通部分  開催前  登壇者探す connpass 作成 Twitter 告知   当日  Twitter 実況 司会    リアル開催  数ヶ月前  会場確保(自分の場合メルカリの会場を毎回スポンサーとしてお借りしていた)。なぜならメルカリが勉強会会場として高頻度で使われるのでハコを抑えるのが毎回激戦区だった。 スポンサーしてもらうために申請   当日(会社にて)  開催ビルで準備。入場用の道具(入場用、案内用の看板設営、ポスター印刷して看板に挿入)  開始時間 1 時間前から動き出す   懇親会のデリバリー受け取り、配備 会場の音響設備、接続確認 100 個以上の椅子や机を勉強会スタイルに並び替える(これがマンパワーが必要で地味にきつい、これを運営のみんなでやっていた) 登壇者全員の接続確認 懇親会終了後撤収  ゴミなどがちゃんとゴミ箱に捨てられているかの確認と清掃 机・椅子などもきれいに全部拭いて、元の形に戻す。基本的に準備したものをすべてもとに戻していく  9 回目以降は、撤収ボランティア枠を設けて手伝ってもらっていた。確か 8 回目の時に @keisuke_umezawa さんや @nasuka さんが手伝いますよと自発的に行ってもらえてめちゃくちゃ感激した覚えがある(実際は 4-5 人に手伝ってもらいましたが全員は覚えてないです、すみません)。この場面は本当~に良い記憶として残っている。なんか運営していてよかったと思った一番の記憶かもしれない。その後毎回無償で手伝ってもらうのは申し訳ないので、抽選枠ではなく、ボランティア枠と撤収作業を手伝ってもらえると、確実に勉強会に参加できますよという仕組みを作った覚えがある。   21:30 に撤収開始で、終わるのは 22:30 くらい。帰宅は日付が変わるか変わらないかという感じ      オンライン開催  前日  配信が問題なくできているかのリハーサル   当日(自宅にて)  開催 30m 前に登壇者にビデオチャットに参加していただき、接続確認 懇親会終了後、そこはすでに自宅。例えば 23 時に終わったとしても、23 時には家にいるこれって凄い。    とオンライン開催のコストは比類できないほど低いことがわかりますね。\n自分の場合、子供が生まれることで出勤するとその日は全く育児に参加できなくなるので可能な限り出社はしたくないという考えになりました。フルリモートが始まると育児参加関係なく、通勤も苦痛だったことに気が付きましたが\u0026hellip;\nその前提で考えるとオンライン開催って子供を寝かしつけてからでも勉強会開催できるじゃん、超省エネ開催ではと気が付き、じゃあ試験的に再開するか~と現在に至ります。\n開催周期も「2 ヶ月ごとに一回は開催しなければ」などそんなことは全く特に気にせず、自分が聞きたいテーマがあったり、知り合いと喋っている時にこのテーマきになりませんかとネタになった時に開催するぐらいの「ゆるふわ」でいいのではと思っている。\nこれをエゴ駆動勉強会運営と名付けたい\u0026hellip;。自分が聞きたいからやる、自分がやりたからやる。これぞ究極の継続方法\n最近だと、MLOps 勉強会も存在していて、Machine Learning Casual Talks のみが機械学習の実応用を語る場ではなくなった感がありますが、同じトピックでも主催者によって色が別れるのが勉強会の特性だと思うので、Machine Learning Casual Talk も気にせず開催していくぜという気持ちです。 多様性バンザイ~\nというわけで勉強会の運営を再開して、\nMachine Learning Casual Talks #13 (Online)\nを 2022/03/30(水) に開催予定というお話でした。\n MLOps も 2018 年初頭と比べるとすごく成熟してきているのでは なんだかんだ自分はこのデータ活用による価値創出が非常に好きなので、機械学習の実応用・価値創出を盛り上げていきたいですね~\n@rindai さんが主催していた MLOps の先駆け、 MLOps Study も初回は 2017/12/19と隔世の感がありますねぇ\u0026hellip; 2018 年初頭と比べて、現状だと MLOps に関する書籍でも、英語書籍だけではなく、日本語書籍も数多く翻訳されてきて、とても充実してきているので前進していると感動\n英語書籍だとぱっと上げるだけでも\n Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and Mlops Building Machine Learning Pipelines  があるし、これらは日本語に翻訳もされているの素晴らしくないですか?\n 機械学習デザインパターン 入門 機械学習パイプライン ―TensorFlow で学ぶワークフローの自動化  日本語書籍だと\n 仕事ではじめる機械学習 第 2 版  第 2 版が出ている!! 2 版が出ているということはいかに売れているかがわかりますね(ニヤリ)。レビューに参加させていただきましたが、とても良い本でした。   AI エンジニアのための機械学習システムデザインパターン  澁井さんが自身の機械学習システムの経験則を書籍化したのは、自分は痺れました。    と良書揃いです。最近書籍読めていないので、良い本たくさん出ていると思いますが追いきれてないので自分が読めている本のみの紹介となります。すみません。\n","permalink":"https://shunyaueta.com/posts/2022-02-22/","summary":"自分はMachine Learing Casual Talksという勉強会の運営を @chezou さん、 @tetsuroito さん、 @komiya_atsushi さんの運営陣に合流する形で 2018/7 に再開しました。\nもともと自分は根底として勉強会運営が好きで、つくばにいた頃から、tsukuba.rb や PRML勉強会などの勉強会運営をしていたというのもある。\n詳しい経緯は過去に記事に書いていた。 見返すとなかなかにエモい文章ですね。\nMachine Learning Casual Talks #5 を開催しました\nその後子供が産まれる直前の 2020/05 に12 回目を開催して以降、育児で時間的・精神的余裕がなくなって開催が途絶えてしまっていた。\n2021/06 に社内チャットで、\n育児で運営が途絶えてしまったんですが、皆さんどう克服しましたか?\nという質問したら、要約すると\n @lestrrat さん  燃え尽きてもいいじゃないか by @lestrrat   @sinmetal さん  志低く、無くならないようにしようぐらいの気持ちです。    と多種多様な考えを聞けて自分の中でも色々と考えが深まりました。\n当時の僕の反応を拾ってみるとこんな感じ\n 志が低いというのはとても良いですね。存続させるの大事だなぁと痛感してます:relaxed: 僕も学生でつくばにいた頃東京の勉強会は参加できないけど、資料公開してくれるのありがてぇ、そしてこの分野(機械学習エンジニア) に興味あるけどそもそも鶏卵問題で経験がないと参入できないから知見を公開してくれるの助かるなぁという思い出があったなと今思い出しました w 今は実務でバリバリ触れているからこそ初心を忘れてしまったのかもしれないので、情報発信の大事さを今一度噛み締めました\n で、 2022/02 の現在ふとリアル開催?の時に比べるとオンライン開催ってめちゃくちゃ省エネで開催できるなと気が付きました\u0026hellip;!\n開催の手間 やるべきことを簡単に洗い出してみます。\n共通部分  開催前  登壇者探す connpass 作成 Twitter 告知   当日  Twitter 実況 司会    リアル開催  数ヶ月前  会場確保(自分の場合メルカリの会場を毎回スポンサーとしてお借りしていた)。なぜならメルカリが勉強会会場として高頻度で使われるのでハコを抑えるのが毎回激戦区だった。 スポンサーしてもらうために申請   当日(会社にて)  開催ビルで準備。入場用の道具(入場用、案内用の看板設営、ポスター印刷して看板に挿入)  開始時間 1 時間前から動き出す   懇親会のデリバリー受け取り、配備 会場の音響設備、接続確認 100 個以上の椅子や机を勉強会スタイルに並び替える(これがマンパワーが必要で地味にきつい、これを運営のみんなでやっていた) 登壇者全員の接続確認 懇親会終了後撤収  ゴミなどがちゃんとゴミ箱に捨てられているかの確認と清掃 机・椅子などもきれいに全部拭いて、元の形に戻す。基本的に準備したものをすべてもとに戻していく  9 回目以降は、撤収ボランティア枠を設けて手伝ってもらっていた。確か 8 回目の時に @keisuke_umezawa さんや @nasuka さんが手伝いますよと自発的に行ってもらえてめちゃくちゃ感激した覚えがある(実際は 4-5 人に手伝ってもらいましたが全員は覚えてないです、すみません)。この場面は本当~に良い記憶として残っている。なんか運営していてよかったと思った一番の記憶かもしれない。その後毎回無償で手伝ってもらうのは申し訳ないので、抽選枠ではなく、ボランティア枠と撤収作業を手伝ってもらえると、確実に勉強会に参加できますよという仕組みを作った覚えがある。   21:30 に撤収開始で、終わるのは 22:30 くらい。帰宅は日付が変わるか変わらないかという感じ      オンライン開催  前日  配信が問題なくできているかのリハーサル   当日(自宅にて)  開催 30m 前に登壇者にビデオチャットに参加していただき、接続確認 懇親会終了後、そこはすでに自宅。例えば 23 時に終わったとしても、23 時には家にいるこれって凄い。    とオンライン開催のコストは比類できないほど低いことがわかりますね。","title":"オンライン開催前提だからこそ可能な省エネ勉強会運営 ~勉強会運営再開してみた~"},{"content":"最近、技術的負債の優先順位付けとどうやって消化すべきかを考えていた時に、同僚のスーパーエンジニアの方から本質的なアドバイスを聞けたのでメモ。 自分は技術的負債タスクの消化をどうやって仕組み化して、定常的に消化していくべきかを試行錯誤していた。\nなぜなら、機能開発と比較すると優先度が低くなりがちな技術的負債タスクを定常的に消化できているチームこそ、短期と中長期の視線を兼ね揃えたバランスが取れた戦略が取れているのではと考えていたからだ。 なので消化できないのは仕組み化がうまくできていないから、どうにか解決できないかなと思っていた。\nだが、同僚がくれた言葉で目から鱗が落ちた\n その技術的負債解消が本当に必要ならやりますよね。 必要でないならやらない、他にもっと重要なタスクがあるなら、そっちを優先するのでいいんじゃないかなと。 本当に必要なタスクならやると優先順位付けするので、直近必要でないと思っている無理して消化する必要はないですよ。\n なるほど、これに尽きる。 今までは優先度が低かったとしても技術的負債タスクを消化できていることが良い文化なのではと勘違いしていた。\nそのタスクを解決したら何が嬉しいか、どんな価値を提供できるかを常に考えて、優先順位付けを行って抱えている技術的負債タスクの中でも選定して解くべき課題に集中して解くべきだと学べた。\nそれこそが価値を生み出すエンジニアだなぁ。学び\n注)もちろん場合によりけりなので、自分がいる環境での学びです。\n","permalink":"https://shunyaueta.com/posts/2022-02-01/","summary":"最近、技術的負債の優先順位付けとどうやって消化すべきかを考えていた時に、同僚のスーパーエンジニアの方から本質的なアドバイスを聞けたのでメモ。 自分は技術的負債タスクの消化をどうやって仕組み化して、定常的に消化していくべきかを試行錯誤していた。\nなぜなら、機能開発と比較すると優先度が低くなりがちな技術的負債タスクを定常的に消化できているチームこそ、短期と中長期の視線を兼ね揃えたバランスが取れた戦略が取れているのではと考えていたからだ。 なので消化できないのは仕組み化がうまくできていないから、どうにか解決できないかなと思っていた。\nだが、同僚がくれた言葉で目から鱗が落ちた\n その技術的負債解消が本当に必要ならやりますよね。 必要でないならやらない、他にもっと重要なタスクがあるなら、そっちを優先するのでいいんじゃないかなと。 本当に必要なタスクならやると優先順位付けするので、直近必要でないと思っている無理して消化する必要はないですよ。\n なるほど、これに尽きる。 今までは優先度が低かったとしても技術的負債タスクを消化できていることが良い文化なのではと勘違いしていた。\nそのタスクを解決したら何が嬉しいか、どんな価値を提供できるかを常に考えて、優先順位付けを行って抱えている技術的負債タスクの中でも選定して解くべき課題に集中して解くべきだと学べた。\nそれこそが価値を生み出すエンジニアだなぁ。学び\n注)もちろん場合によりけりなので、自分がいる環境での学びです。","title":"技術的負債は必要にかられて解消するからこそ大きな価値を生み出すのでは? というお話"},{"content":"Slack 公式のヘルプページには反映されていないが、GitHub の Slack integration で特定リポジトリの release を購読してチャンネルに通知することができる。\n利用用途としては、更新を追いかけておきたいリポジトリやリリースチャンネルに対して、リリースが作成されました、このリリースが次のデプロイ予定だよと通知させたい場合に便利。\nSlack コマンド\n1 2 3 4  # リリースを購読 /github subscribe elastic/elasticsearch releases # デフォルトで、issues, pulls, commits, release, deployment すべてが購読されてしまうので、 releases 以外購読を解除 /github unsubscribe elastic/elasticsearch issues,pulls,commits,deployments   Slack の日本語公式ドキュメントにはフィードバックを送ったので、近いうちに反映されるかもしれない\nReferences  GitHub と Slack を連携させる integrations/slack  ","permalink":"https://shunyaueta.com/posts/2022-01-27/","summary":"Slack 公式のヘルプページには反映されていないが、GitHub の Slack integration で特定リポジトリの release を購読してチャンネルに通知することができる。\n利用用途としては、更新を追いかけておきたいリポジトリやリリースチャンネルに対して、リリースが作成されました、このリリースが次のデプロイ予定だよと通知させたい場合に便利。\nSlack コマンド\n1 2 3 4  # リリースを購読 /github subscribe elastic/elasticsearch releases # デフォルトで、issues, pulls, commits, release, deployment すべてが購読されてしまうので、 releases 以外購読を解除 /github unsubscribe elastic/elasticsearch issues,pulls,commits,deployments   Slack の日本語公式ドキュメントにはフィードバックを送ったので、近いうちに反映されるかもしれない\nReferences  GitHub と Slack を連携させる integrations/slack  ","title":"Slack チャンネルに GitHub の特定リポジトリのrelease を通知する"},{"content":"TL;DR; UDF を独自実装する前に、bqutil.fnを眺めておくと車輪の再発明が回避できるかも\n背景 SQL は、特定の処理を行う際にデータの型が同一でないとエラーが発生しますが、もとのスキーマを紹介するよりももっとお手軽にカラムの型を確認したいときがありませんか?\n例えば、出力結果を見ただけでは、12345 が STRING なのか INT64 なのか判別不可能ですよね。(もし判別可能な方法知っている人いたら教えて下さい\u0026hellip;)\nGCP による OSS UDF の bqutil.fn なのでお手軽に BigQuery の結果の型を確認したい時になにか良い方法がないかなと調べていたら、OSS でbqutil.fnという UDF が GCP から提供されていた。\n例えば型の確認の場合、以下の ユーザー定義関数（UDF) はどの GCP プロジェクトから実行しても実行可能\n1  bqutil.fn.typeof()   このbqutil.fn はbigquery-utils/udfs/community/のディレクトリに格納されている UDF がbqutil という GCP プロジェクトのfn データセットに同期されているので、どの GCP プロジェクトの Google BigQuery から実行しても bqutil.fn.typeof()を実行可能にしているらしい。 頭良い\n This directory contains community contributed user-defined functions to extend BigQuery for more specialized usage patterns. Each UDF within this directory will be automatically synchronized to the bqutil project within the fn dataset for reference in queries.\n 実際に以下のクエリを Google BigQuery で実行すると\n1 2 3 4 5  SELECT bqutil.fn.typeof(\u0026#34;\u0026#34;), bqutil.fn.typeof(b\u0026#34;\u0026#34;), bqutil.fn.typeof(1.0), bqutil.fn.typeof(STRUCT()),   1 2 3 4  STRING BYTES FLOAT64 STRUCT   の結果が出力され、各カラムのデータの型を確認できる。\n便利!\nこれで置き換え可能な UDF は置き換えればメンテンスしないといけない UDF が削減されて嬉しいですね\nbqutil.fn.typeof() の UDF の実態としては bigquery-utils/udfs/community/typeof.sqlxが参照されて実行されている。\n他にも\n URL の key を抽出できるbqutil.fn.url_keys() ランダムな値を出力できるbqutil.fn.url_keys()  など、自前で正規表現で頑張って書いているけど実際は OSS の UDF として公開されているケースも多々ありそうなかゆいところに手が届く UDF が多数公開されていた。\n変わり種としては、 StatsLib: Statistical UDFsという統計的な処理を行う UDF も公開されていた。\n線形回帰や p 値の計算ができる UDF も公開されており、面白い\nReferences  bigquery type check operator? like typeof in Javascript; or workaround typeof(input ANY TYPE)  ","permalink":"https://shunyaueta.com/posts/2022-01-20/","summary":"TL;DR; UDF を独自実装する前に、bqutil.fnを眺めておくと車輪の再発明が回避できるかも\n背景 SQL は、特定の処理を行う際にデータの型が同一でないとエラーが発生しますが、もとのスキーマを紹介するよりももっとお手軽にカラムの型を確認したいときがありませんか?\n例えば、出力結果を見ただけでは、12345 が STRING なのか INT64 なのか判別不可能ですよね。(もし判別可能な方法知っている人いたら教えて下さい\u0026hellip;)\nGCP による OSS UDF の bqutil.fn なのでお手軽に BigQuery の結果の型を確認したい時になにか良い方法がないかなと調べていたら、OSS でbqutil.fnという UDF が GCP から提供されていた。\n例えば型の確認の場合、以下の ユーザー定義関数（UDF) はどの GCP プロジェクトから実行しても実行可能\n1  bqutil.fn.typeof()   このbqutil.fn はbigquery-utils/udfs/community/のディレクトリに格納されている UDF がbqutil という GCP プロジェクトのfn データセットに同期されているので、どの GCP プロジェクトの Google BigQuery から実行しても bqutil.fn.typeof()を実行可能にしているらしい。 頭良い\n This directory contains community contributed user-defined functions to extend BigQuery for more specialized usage patterns. Each UDF within this directory will be automatically synchronized to the bqutil project within the fn dataset for reference in queries.","title":"OSS の Google BigQuery UDF `bqutil.fn` を使えば UDF の独自実装を置き換えられるかもしれない"},{"content":"Blog にコメントシステムを採用したいなと思って giscus を入れてみた。\n経緯 Blog を読んだ際にコメントするまでではないけど、書いた人に対するシグナルとして、絵文字リアクションという仕組みがとても好き。 Medium だと clap 👏 だったり、はてなだとはてなスター ⭐️ ですね。また Slack や GitHub の emoji reaction も同じ発明だと思う\nまた、はてぶや Twitter 以外でも感想が書ける場所があると良いのではと思いコメントシステムを Blog に導入してみた。\n選定基準 コメントシステムの採用候補は２つ\n utterances GitHub issues をベースにしたコメントシステム giscus は GitHub Discussions をベースにしたコメントシステム  両者とも OSS で、コメントのデータを GitHub 上で保持できるのが魅力的。 DISQUSも同じ機能を提供してくれているが、自分のデータは自分で持ちたいかつデザインが激しめな印象があるので、uttrances, giscus が候補に残った。\n両者ともデザインシンプルで良い。\nだが、giscus は コメントを取り扱うなら issues ではなく Discussions のほうが適しているという動機から作られた。\ngiscus は記事に対してリアクションができたり、コメントに関する voting もあるので、迷わず giscus を採用する流れとなった。\n確かに issues よりも GitHub Discussions のほうがコメントという概念に適していると思う。\nというわけで GitHub でサインインすれば記事末尾にて、giscus を通じてコメントやリアクションができるようになったので、リアクションお待ちしております\u0026hellip;! 👇\n余談 Webmention などの仕組みも気になるのだが、ちょっと採用するまでの仕組みが重いので不採用にした\nReferences  Giscus: The New Commenting Engine for My Website  ","permalink":"https://shunyaueta.com/posts/2022-01-19/","summary":"Blog にコメントシステムを採用したいなと思って giscus を入れてみた。\n経緯 Blog を読んだ際にコメントするまでではないけど、書いた人に対するシグナルとして、絵文字リアクションという仕組みがとても好き。 Medium だと clap 👏 だったり、はてなだとはてなスター ⭐️ ですね。また Slack や GitHub の emoji reaction も同じ発明だと思う\nまた、はてぶや Twitter 以外でも感想が書ける場所があると良いのではと思いコメントシステムを Blog に導入してみた。\n選定基準 コメントシステムの採用候補は２つ\n utterances GitHub issues をベースにしたコメントシステム giscus は GitHub Discussions をベースにしたコメントシステム  両者とも OSS で、コメントのデータを GitHub 上で保持できるのが魅力的。 DISQUSも同じ機能を提供してくれているが、自分のデータは自分で持ちたいかつデザインが激しめな印象があるので、uttrances, giscus が候補に残った。\n両者ともデザインシンプルで良い。\nだが、giscus は コメントを取り扱うなら issues ではなく Discussions のほうが適しているという動機から作られた。\ngiscus は記事に対してリアクションができたり、コメントに関する voting もあるので、迷わず giscus を採用する流れとなった。\n確かに issues よりも GitHub Discussions のほうがコメントという概念に適していると思う。\nというわけで GitHub でサインインすれば記事末尾にて、giscus を通じてコメントやリアクションができるようになったので、リアクションお待ちしております\u0026hellip;!","title":"GitHub discussion を使ったコメントシステム giscus を導入"},{"content":"検索技術とその関連領域を取り扱うニュースレターを不定期配信してみることにします。\n2022 年から心機一転として、情報発信を今まで Twitter メインでやっていましたが、ニュースレターで行っていこうと思っています。 以下の Revue のリンクから登録が可能です。\nSearch Engineering Newsletter By hurutoriya\n扱うトピックとしては、検索エンジンと情報検索の周辺領域です。 経緯としては、自分の職域である情報は積極的に収集しており、どうせなら発信したいなと考えて Twiter や、機械学習エンジニア時代には Revue を使って MLOps 領域に特化してニュースレター配信を試しにやってみた(覚えているかたもいるかも?)ことがありましたが、結局 Twitter で配信すれば同じではと考えてしまい、なんだかんだ継続できませんでした。\nですが、最近は Twitter での情報取得から抜け出して、一息ついた状態で情報収集を行うようになりたいなと思うなかで、改めて Twitter の発信だけではなく、ニュースレターのように頻度は不定期だが高品質な情報が一箇所にまとまって届けることができたら面白いなと考えました。(またニュースレター形式だと Twitter では届かない層にも届くんじゃないのかなと)\n現在は自分は検索エンジニアとして働いており、検索技術と情報検索の情報を積極的に収集しています。そのなかで自分が当該領域で注目したニュースレターを不定期に配信できればなと思ってニュースレターを再開してみました。 検索領域に関連する気になった記事や自分の Blog 記事の執筆などをここで紹介していきます。\n不定期配信ですが、興味を持っていただけたらぜひ購読をお願いします。\n","permalink":"https://shunyaueta.com/posts/2022-01-16/","summary":"検索技術とその関連領域を取り扱うニュースレターを不定期配信してみることにします。\n2022 年から心機一転として、情報発信を今まで Twitter メインでやっていましたが、ニュースレターで行っていこうと思っています。 以下の Revue のリンクから登録が可能です。\nSearch Engineering Newsletter By hurutoriya\n扱うトピックとしては、検索エンジンと情報検索の周辺領域です。 経緯としては、自分の職域である情報は積極的に収集しており、どうせなら発信したいなと考えて Twiter や、機械学習エンジニア時代には Revue を使って MLOps 領域に特化してニュースレター配信を試しにやってみた(覚えているかたもいるかも?)ことがありましたが、結局 Twitter で配信すれば同じではと考えてしまい、なんだかんだ継続できませんでした。\nですが、最近は Twitter での情報取得から抜け出して、一息ついた状態で情報収集を行うようになりたいなと思うなかで、改めて Twitter の発信だけではなく、ニュースレターのように頻度は不定期だが高品質な情報が一箇所にまとまって届けることができたら面白いなと考えました。(またニュースレター形式だと Twitter では届かない層にも届くんじゃないのかなと)\n現在は自分は検索エンジニアとして働いており、検索技術と情報検索の情報を積極的に収集しています。そのなかで自分が当該領域で注目したニュースレターを不定期に配信できればなと思ってニュースレターを再開してみました。 検索領域に関連する気になった記事や自分の Blog 記事の執筆などをここで紹介していきます。\n不定期配信ですが、興味を持っていただけたらぜひ購読をお願いします。","title":"Search Engineering Newsletter vol.00"},{"content":"Google Spread Sheet による即席アノテーションの限界 データ分析で、ラベルがないデータに対して、自分でアノテーション(ラベルを付与)してデータの傾向を素早く掴みたい時がある。 例えば、文章に対してネガティブ・ポジティブなクラスを割り振ったり、画像に対して人が写り込んでいるか否かなどの簡単な分類タスクでは、お手軽に Google Spread Sheet などを使って、500 件のアノテーションはそこまで問題がなく気合でやれる。\n実際の流れとしては、GCP を採用している場合、Google BigQuery から SQL でデータを抽出してそのまま Google Sprad Sheet に出力、=image()関数で CDN から画像の URL を参照できたりなどなどかなり便利。 Spread Sheet を共有して複数人でも作業ができるのも魅力的。\nだが、文章の特定の部分を選択してタグを付けたかったり、クラス数が二桁など少し複雑なアノテーションタスクを行いたい場合 Google Spread Sheet では、アノテーションの生産性が劇的に落ちる、もしくは不可能になる。あくまで Google Spread Sheet はお手軽にラベリングを行うだけで、ラベリング専用ツールではないので当然の帰着ではある\u0026hellip;\nLabel Studio とは 今回紹介するLabel Studioは OSS データのラベリング(アノテーション)ツールは、\n 画像  画像分類 物体認識 セマンティックセグメンテーション   音声  音声分類 話者分類 感情認識 文字起こし   文章  文書分類 固有表現抽出(NER) 質疑応答 感情分析   時系列データ  時系列分類 時系列分割 イベント認識   マルチモーダル  対話処理 文字認識(OCR) ビデオ・音声の時系列分類    など多種多様なドメインに対してラベリングを行うことができるソフトウェアだ。\nまた、アカウント認証や、キーボードショートカットなどアノテーションの生産性を向上させる基本的な機能が標準で搭載されており、アノテーション効率の向上と管理が期待できる。\n同様に OSS の自然言語タスクに特化したアノテーションツールのdoccano も検討したが、Label Studio は自然言語以外にも画像やランキングなど多種多様なラベリングタスクに対応しているので、後々にアノテーションしたいデータの種類が増えたとしても Label Studio を活用できて便利そうなので、今回は Label Studio を選択した。\n機能として\n アノテーションのインターフェイスは xml のような形式で、GUI で柔軟に作成可能。画像・文章を組み合わせて表示したいときでも容易に表現が可能。 Web アプリとして完結していて、よほど込み入ったことをしたいと思わない限り、Docker か pip でインストールしてコマンド実行するだけで良い。データを準備するだけで、コードを書かなくてよい 実行環境として Cloud Run で実行可能、pip でインストールして実行するだけでも起動可能なのでポータビリティが高い。  が気に入った。\nまた、今は使わないが\n 機械学習モデルを組み込んで、アノテーションの補助が可能な点 AWS, GCP, Azure の CloudStorage と連携が可能 Webhook で連動させて、アノテーションが完了したらジョブを実行  が気になっている。\nLabel Studio の活用方法 以下の画像の URL が格納されている onepiece.csvを使って、各画像に指定のクラスを割り振るデータラベリングを Label Studio を使って実行する。\n1 2 3 4 5  image https://1.bp.blogspot.com/-uxIsaN0S5lQ/X-FcrvAAInI/AAAAAAABdD4/6uw_qNUh9dQrG0aUzIExybt84yTEmXOPwCNcBGAsYHQ/s200/onepiece01_luffy.png https://1.bp.blogspot.com/-rzRcgoXDqEg/YAOTCKoCpPI/AAAAAAABdOI/5Bl3_zhOxm07TUGzW8_83cXMOT9yy1VJwCNcBGAsYHQ/s200/onepiece02_zoro_bandana.png https://1.bp.blogspot.com/-2ut_UQv3iss/X-Fcs_0oAII/AAAAAAABdD8/jrCZTd_xK-Y6CP1KwOtT_LpEpjp-1nvxgCNcBGAsYHQ/s200/onepiece03_nami.png https://1.bp.blogspot.com/-mZpzgXC1Sxk/YAOTCAKwWTI/AAAAAAABdOM/5B4hXli0KLU5N-BySHgjVbhZscKLSE-bQCNcBGAsYHQ/s200/onepiece04_usopp_sogeking.png`   例とする画像の URL はいらすとやさんを参照させていただいております。\n今回はお手軽に Poetry で Label Studio をインストールして、ローカル環境で実行した。\n1 2 3 4 5  # Requires \u0026gt;=Python3.6, \u0026lt;3.9 poetry add label-studio # Start the server at http://localhost:8080 poetry run label-studio   実際の作業の撮影動画  Label Studio の起動 プロジェクトの作成 データのアップロード アノテーションの実行 アノテーション結果のダウンロード  までの動画を撮影した。\n動画で見ることで、Label Studio の魅力がわかるのではなかろうか。 キーボードショートカットが搭載されていて、実際のアノテーション作業がとても快適に行える点がわかるのではなかろうか。\n 以下に、実際の画面ごとの解説をしておく。\nプロジェクトの作成 poetry run label-studio で Lebel Studio を起動後、右上の Create ボタンからプロジェクトを作成する。プロジェクト = 必要なアノテーションデータの 1 単位と考えれば良い。 上の 3 つのタブ\n Project Name : プロジェクトの概要を入力 Data Import : アノテーション対象のデータをアップロード Labeling Setup : ラベリング方式の設定  アノテーションテンプレート選択 Labeling Setup では、様々なドメインのアノテーションタスクをテンプレートから選択が可能\nラベリングインターフェイス テンプレートを選択後、GUI でタグを自分で選択して編集する。 Code タグを選択すれば、直接インターフェイスを xml ライクに宣言ができる。 各種タグは\nhttps://labelstud.io/tags/\nを参考にすれば、柔軟にアノテーションが組める。Web アプリ上で完結しつつ柔軟にインターフェイスを構築できるのは感動した。\nタグの豆知識 \u0026lt;Header\u0026gt; タグで、各種データの説明文を設定可能\n1  \u0026lt;Header value=\u0026#34;説明文\u0026#34;/\u0026gt;   タグの末尾に / を入れることで閉じタグを省略できる。 以下の 2 つは等価\n1 2  \u0026lt;Header value=\u0026#34;$title\u0026#34;/\u0026gt; \u0026lt;Header value=\u0026#34;$title\u0026#34;\u0026gt;\u0026lt;/Header\u0026gt;   \u0026lt;TextArea\u0026gt; タグを使えば、判断に困った際のデータに対してコメントを残せる toName 要素で、何に対するコメントなのかを指定する。\n1  \u0026lt;TextArea name=\u0026#34;comment\u0026#34; toName=\u0026#34;text\u0026#34; placeholder=\u0026#34;Enter Comment here...\u0026#34; maxSubmissions=\u0026#34;1\u0026#34; rows=\u0026#34;3\u0026#34; editable=\u0026#34;true\u0026#34;/\u0026gt;   プロジェクト画面 プロジェクト画面では、アップロードされたデータやアノテーションの進捗具合などがひと目で把握できる。 データセットに画像の URL があれば、カラムの属性をimgに変更すれば一覧画面でその画像が表示される。\nアノテーション画面 Mac だと Command + Enter で Submit, 例だと、例えばナミはキーボードで 3 を押せば選択される。これらのキーボードショートカット機能により高速にアノテーションが可能。\nもう少し使い込んだら、また続編を書きたい。 OSS でこのクオリティのアノテーションソフトウェアが公開されるようになっているのは本当に素晴らしい。\n","permalink":"https://shunyaueta.com/posts/2022-01-09/","summary":"Google Spread Sheet による即席アノテーションの限界 データ分析で、ラベルがないデータに対して、自分でアノテーション(ラベルを付与)してデータの傾向を素早く掴みたい時がある。 例えば、文章に対してネガティブ・ポジティブなクラスを割り振ったり、画像に対して人が写り込んでいるか否かなどの簡単な分類タスクでは、お手軽に Google Spread Sheet などを使って、500 件のアノテーションはそこまで問題がなく気合でやれる。\n実際の流れとしては、GCP を採用している場合、Google BigQuery から SQL でデータを抽出してそのまま Google Sprad Sheet に出力、=image()関数で CDN から画像の URL を参照できたりなどなどかなり便利。 Spread Sheet を共有して複数人でも作業ができるのも魅力的。\nだが、文章の特定の部分を選択してタグを付けたかったり、クラス数が二桁など少し複雑なアノテーションタスクを行いたい場合 Google Spread Sheet では、アノテーションの生産性が劇的に落ちる、もしくは不可能になる。あくまで Google Spread Sheet はお手軽にラベリングを行うだけで、ラベリング専用ツールではないので当然の帰着ではある\u0026hellip;\nLabel Studio とは 今回紹介するLabel Studioは OSS データのラベリング(アノテーション)ツールは、\n 画像  画像分類 物体認識 セマンティックセグメンテーション   音声  音声分類 話者分類 感情認識 文字起こし   文章  文書分類 固有表現抽出(NER) 質疑応答 感情分析   時系列データ  時系列分類 時系列分割 イベント認識   マルチモーダル  対話処理 文字認識(OCR) ビデオ・音声の時系列分類    など多種多様なドメインに対してラベリングを行うことができるソフトウェアだ。","title":"OSSのアノテーションツール Label Studio を使って、快適にアノテーションする"},{"content":"2021 年に購入して、今も愛用しているものを記しておく。 買ったものだと、その後愛用しているか定かではないかつ良い品なのかわからないので、愛用しているもの限定です。\nCORONA(コロナ) 衣類乾燥除湿機 除湿量 18L (木造 20 畳 / 鉄筋 40 畳まで) 日本製 Amazon\n買ったきっかけは、2020 年に築浅の物件に引っ越したが新築の傾向なのか湿気が溜まりやすく、油断して、パントリーの床においていた旅行用のボストンバッグがカビてしまっていたことがあった。水取りぞうさんなどで対応していたことがあったが、知らぬ間に満水になり気づくのが遅れることが多かった。\nそのため思い切って一番除湿力が高そうなこの機種を買ったところ大満足。梅雨の時期には一晩起動しておくと朝には 4.5L のタンクが必ず満タンになり、部屋の中に漂う嫌なモワモワ感が完全に消え去って住心地が圧倒的に向上した。また副次的に良かったのは衣類乾燥機能がすごく良かった。雨で部屋干しをせざるを得ないときもこの機能を使えば全く臭わずに乾燥させることができて、一粒で二度美味しい家電だった。またフィルターも 10 年は交換しなくても良いのが良い。1 年経った今でも全く臭わない。タンク自体も特有の臭さが全くないので最高。\nAfterShokz Aeropex 骨伝導イヤホン Amazon\nリモートワークの開始に伴い、最初は 2019 年頃にオフィスでの騒音対策として購入したWH-1000XM3を使っていたのだが、長時間つけると耳が痛くなることが多かった。またイヤーインタイプのイヤホンも自分は耳の形が合わなくてフィット感がいまいちだったりモゾモゾして 1 時間のミーティングでも嫌なことが多かった。AirPods も検討したが、自分は iPhone ではなく Pixel を使っているし、気づかずに落として紛失しそうで怖かったのでなにか良いイヤホンがないか探していた。Shokz はその悩みを完全に解決してくれた。耳は防がないし、1-2h つけても全く痛くならないし、何より軽い。Shokz はいろんなシリーズがあるが、\nミドルレンジのものを買ってなんか違うなと思うのは嫌だったので思い切ってハイエンドモデルを買ったがとても満足している。もしかしたらOpenMove でも良いかもしれないが、試したことがないのでわからん。ブランド名も After・Shokz から Shokz に変わって、(OpenRun Pro)が出たのでちょっと気になっている。バッテリーが更に長持ちなのと小型化、低音が効くようになったらしい。\n Shokz いいですよね！自分も使ってます。新しく出る OpenRun 系統は 29g と、Aeropex と比べると 3g 重いようです。小型化の方がより適切かと感じました。 by @tatsuokundayo\n 2022/01/08: @tatsuokundayo さんにご指摘いただいた点を修正\nWH-1000XM3はノイズキャンセリング機能は感動するレベルだったが、オフィスに行くことがなくなった今、用済みなのでメルカリで売った。速攻で売れたので気持ちよかった。\nXiaomi Mi ハンディクリーナー ミニ Mi Vacuum Cleaner Mini コードレス ミニ掃除機 ハンディ掃除機 Amazon\nルンバを起動するまでもないけど、ちょっとゴミが気になるときに重宝している。子供が生まれて床にモノを置かざる得ない状況で気軽にルンバを起動するのがめんどい。でも床のはしっこに溜まっているホコリをとりたいな~というときにさっと強烈な吸引力で残らず吸い取ってくれるのが最高。USB-C で充電できるのも良い。\nAnker ファンなのでAnker Eufy HomeVac H11 とも迷ったが、吸引力が段違いだったので、Xiomi にした。MiBand を使っている影響で Xiomi のハードウェア製品はデザイン・品質共に信頼しているので買ったが結果的に期待を裏切らない良い買い物だった。\n両者の吸引力の比較、Anker と Xiomi では 2 倍以上の差がある。\n Anker Eufy HomeVac H11: 5500Pa Xiaomi Mi ハンディクリーナー ミニ Mi Vacuum Cleaner Mini コードレス ミニ掃除機 ハンディ掃除機: 吸引力: 13000Pa  八幡ねじ 疲労軽減マット Amazon\nステッパーも気になったり、エルゴドリブンも気になったりはしたが、\nステッパーはそもそもつかわなくなりそう?とかエルゴドリブンの 2 割程度の値段で購入できるマットがあるのにこれはどうなんだと思い、これを買ったが大正解だった。靴とかを室内で履くアメリカだとエルゴドリブンは良いのだろうか?\n","permalink":"https://shunyaueta.com/posts/2022-01-07/","summary":"2021 年に購入して、今も愛用しているものを記しておく。 買ったものだと、その後愛用しているか定かではないかつ良い品なのかわからないので、愛用しているもの限定です。\nCORONA(コロナ) 衣類乾燥除湿機 除湿量 18L (木造 20 畳 / 鉄筋 40 畳まで) 日本製 Amazon\n買ったきっかけは、2020 年に築浅の物件に引っ越したが新築の傾向なのか湿気が溜まりやすく、油断して、パントリーの床においていた旅行用のボストンバッグがカビてしまっていたことがあった。水取りぞうさんなどで対応していたことがあったが、知らぬ間に満水になり気づくのが遅れることが多かった。\nそのため思い切って一番除湿力が高そうなこの機種を買ったところ大満足。梅雨の時期には一晩起動しておくと朝には 4.5L のタンクが必ず満タンになり、部屋の中に漂う嫌なモワモワ感が完全に消え去って住心地が圧倒的に向上した。また副次的に良かったのは衣類乾燥機能がすごく良かった。雨で部屋干しをせざるを得ないときもこの機能を使えば全く臭わずに乾燥させることができて、一粒で二度美味しい家電だった。またフィルターも 10 年は交換しなくても良いのが良い。1 年経った今でも全く臭わない。タンク自体も特有の臭さが全くないので最高。\nAfterShokz Aeropex 骨伝導イヤホン Amazon\nリモートワークの開始に伴い、最初は 2019 年頃にオフィスでの騒音対策として購入したWH-1000XM3を使っていたのだが、長時間つけると耳が痛くなることが多かった。またイヤーインタイプのイヤホンも自分は耳の形が合わなくてフィット感がいまいちだったりモゾモゾして 1 時間のミーティングでも嫌なことが多かった。AirPods も検討したが、自分は iPhone ではなく Pixel を使っているし、気づかずに落として紛失しそうで怖かったのでなにか良いイヤホンがないか探していた。Shokz はその悩みを完全に解決してくれた。耳は防がないし、1-2h つけても全く痛くならないし、何より軽い。Shokz はいろんなシリーズがあるが、\nミドルレンジのものを買ってなんか違うなと思うのは嫌だったので思い切ってハイエンドモデルを買ったがとても満足している。もしかしたらOpenMove でも良いかもしれないが、試したことがないのでわからん。ブランド名も After・Shokz から Shokz に変わって、(OpenRun Pro)が出たのでちょっと気になっている。バッテリーが更に長持ちなのと小型化、低音が効くようになったらしい。\n Shokz いいですよね！自分も使ってます。新しく出る OpenRun 系統は 29g と、Aeropex と比べると 3g 重いようです。小型化の方がより適切かと感じました。 by @tatsuokundayo\n 2022/01/08: @tatsuokundayo さんにご指摘いただいた点を修正\nWH-1000XM3はノイズキャンセリング機能は感動するレベルだったが、オフィスに行くことがなくなった今、用済みなのでメルカリで売った。速攻で売れたので気持ちよかった。\nXiaomi Mi ハンディクリーナー ミニ Mi Vacuum Cleaner Mini コードレス ミニ掃除機 ハンディ掃除機 Amazon","title":"2021年に買って愛用しているもの"},{"content":"機械翻訳サービスの DeepL はアプリだけでなく API 提供も行っている。 今回は DeepL が公開している free API を利用して、テキストファイルを英日翻訳して、翻訳結果をテキストファイルとして保存する方法について説明する。\n無料 API は1か月あたり500,000文字の上限ありの制限があるが、Pro version と変わらない品質の翻訳を行うことができる。 個人利用する分にはこの文字数制限は特に大きな問題にはならないと思われる。\nhttps://www.deepl.com/ja/pro#developer\nまずアカウントを作成して、DeepL API Free のAPI_KEYを入手する。 その後、以下のスクリプトを実行すれば、翻訳元のファイル名にJA_という接頭辞がついたファイルが保存される。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  import requests # NOTE: put API KEY API_KEY:str = \u0026#39;\u0026#39; # NOTE: put target file path target_file:str = \u0026#34;\u0026#34; with open(target_file) as f: txt = f.read() params = { \u0026#34;auth_key\u0026#34;: API_KEY, \u0026#34;text\u0026#34;: txt, \u0026#34;source_lang\u0026#34;: \u0026#39;EN\u0026#39;, \u0026#34;target_lang\u0026#34;: \u0026#39;JA\u0026#39; } request = requests.post(\u0026#34;https://api-free.deepl.com/v2/translate\u0026#34;, data=params) result = request.json() with open(\u0026#34;JA_\u0026#34;+target_file, \u0026#34;w\u0026#34;) as text_file: text_file.write(result[\u0026#34;translations\u0026#34;][0][\u0026#34;text\u0026#34;])   ","permalink":"https://shunyaueta.com/posts/2022-01-05/","summary":"機械翻訳サービスの DeepL はアプリだけでなく API 提供も行っている。 今回は DeepL が公開している free API を利用して、テキストファイルを英日翻訳して、翻訳結果をテキストファイルとして保存する方法について説明する。\n無料 API は1か月あたり500,000文字の上限ありの制限があるが、Pro version と変わらない品質の翻訳を行うことができる。 個人利用する分にはこの文字数制限は特に大きな問題にはならないと思われる。\nhttps://www.deepl.com/ja/pro#developer\nまずアカウントを作成して、DeepL API Free のAPI_KEYを入手する。 その後、以下のスクリプトを実行すれば、翻訳元のファイル名にJA_という接頭辞がついたファイルが保存される。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  import requests # NOTE: put API KEY API_KEY:str = \u0026#39;\u0026#39; # NOTE: put target file path target_file:str = \u0026#34;\u0026#34; with open(target_file) as f: txt = f.","title":"Python で DeepL API Free を利用してテキストファイルを翻訳する"},{"content":"Objective 1 検索エンジニアの領域でシニアレベルのスキルを身につける\nKR 1  2022 年に国際会議・インダストリー会議のどちらでも良いので、対外発表できる成果を出す  成果を提出するのは 2023 年で OK\nKR 2  Python, Go の両方を自信を持って書けるように。  TODO: 定量的に測定して数値化する\nWrite Code Every Day とか良いのかもしれない? wakatime とかでどれくらい書いたかを算出しても面白いかもしれない。 もしくは Atcoder とか?\nObjective 2 有限な自分の時間の管理を生産的に行って、手を動かしまくる習慣を身につける\nKR 1  Toggl を積極的に使って、1 年間どれだけ生産的な時間を創出できてか可視化する  KR 2  継続して、SNS は断つ(ダラダラと見ない)  2021 年も設定していたが、2022 年も引き続き。連絡手段と周知を行うことのみに特化して使うようにする。 SNS での情報取得は辞めて、RSS や News letter などで情報を取得するようにしはじめた。今の所凄く良い。 自分も News letter を過去にやって続かなかったりしたけど、なんかやってみたいなと思うようになりつつも Blog でいいのではと思ったり\u0026hellip;\nKR 3  2021 年の 5 月から開始している翻訳プロジェクトを一段落させて出版可能な状態まで持っていく  詳細はまで伏せますが、自分が発起人の翻訳プロジェクトがあるんですが、今年の年末までには出版可能な状態まで仕上げていきたい。 友人 2 名を誘って始めたプロジェクトだが、初めての経験ということもあり学びが多い。\nKR 4 (Advance)  自前で検索エンジンを運用した個人開発のサービスを作る。  同僚の@yomo さんが、個人で検索サービスを運営しているのですが、この取組が素晴らしいなと思ったので自分も真似したい。\n@szdr さんの記事でも2021 年の目標とかキャリアなど\n 技術目標：自然言語処理 × 情報検索の技術を学び、サンドボックスでも良いから習得した技術を使って検索サービスを作る\n 書かれていたが、この点には非常に感銘を受けていて、知識だけでなくシステム運用・開発などもここで補えたらなと思っている。\nObjective 3 発音矯正と英語の基礎を見つめ直す\nKR  Elsa 英語の文法・英文構成本に取り組む  Anki もやってて楽しいのではじめてみた。\nObjective 4 これからも第一線で働き続けるために健康的な習慣をつける\nKR  RingFit か Strong で設定しているトレーニングセットを週 5 回は維持する。  自宅勤務で 2021 年 6 月頃には 78kg まで太ってしまっていたが、積極的に子供と散歩をして一日最低 6000 歩を意識して歩くのと、糖質制限を行ったら 70kg まで体重を落とすことができた。 だが、長時間デスクワークをしていると腰が少し傷んだり眠りが浅くなったりしがちだ。 学生時代に部活を週 6 でやってたころが一番健康的な身体だったとおもうので、そこまでとは行かずとも深く眠れて元気に仕事を続けられるような習慣を復刻させたい。\n 2021 年は、目標を立てはしたものの振り返りができずにグダグダだったので、以上の OKR を 3 ヶ月毎に定期的に振り返る。\n去年は機械学習エンジニアから検索エンジニアに鞍替えをして、色々と自分の現状と目指すべき場所のギャップを体感できて、良くも悪くもストレスを感じていた。 30 代に差し掛かる年なので、あらためて自分はやれると自信がつく年にしたい。\n余談だが、オンラインで友人や知り合いと雑談をするのが凄く楽しかったので、今年も続けていきたい。 子育ての関係で、自分はオフラインで交流することが難しくなったので逆にオンラインでのビデオチャットなどが気軽に誘えるようになったのはありがたい。\n年末の振り返りにはやれることはやったなと言うぞ~ (完)\n","permalink":"https://shunyaueta.com/posts/2022-01-01/","summary":"Objective 1 検索エンジニアの領域でシニアレベルのスキルを身につける\nKR 1  2022 年に国際会議・インダストリー会議のどちらでも良いので、対外発表できる成果を出す  成果を提出するのは 2023 年で OK\nKR 2  Python, Go の両方を自信を持って書けるように。  TODO: 定量的に測定して数値化する\nWrite Code Every Day とか良いのかもしれない? wakatime とかでどれくらい書いたかを算出しても面白いかもしれない。 もしくは Atcoder とか?\nObjective 2 有限な自分の時間の管理を生産的に行って、手を動かしまくる習慣を身につける\nKR 1  Toggl を積極的に使って、1 年間どれだけ生産的な時間を創出できてか可視化する  KR 2  継続して、SNS は断つ(ダラダラと見ない)  2021 年も設定していたが、2022 年も引き続き。連絡手段と周知を行うことのみに特化して使うようにする。 SNS での情報取得は辞めて、RSS や News letter などで情報を取得するようにしはじめた。今の所凄く良い。 自分も News letter を過去にやって続かなかったりしたけど、なんかやってみたいなと思うようになりつつも Blog でいいのではと思ったり\u0026hellip;\nKR 3  2021 年の 5 月から開始している翻訳プロジェクトを一段落させて出版可能な状態まで持っていく  詳細はまで伏せますが、自分が発起人の翻訳プロジェクトがあるんですが、今年の年末までには出版可能な状態まで仕上げていきたい。 友人 2 名を誘って始めたプロジェクトだが、初めての経験ということもあり学びが多い。","title":"2022 年の目標"},{"content":"うなすけさんの wakatime を利用した振り返り方が面白かったので、来年は真似したいと思い導入してみた。\nサーバーサイドエンジニアとして 2021 年に使った技術と来年の目標\nなので先週くらいから wakatime を使って、VSCode での利用統計をとってみることにした。\nhttps://wakatime.com/\nMarkdown が圧倒的に多いのは現在 Markdown で執筆活動をしてるからですね。\n仕事をほぼ納めてから導入して執筆しかまともにしてないからこうなってるな。。。 詳細は今はお話できないのですが、再来年くらいには形になっていることを祈る。\n利用した技術一覧  Language  New: Go  Backend 開発ではメインで使っている。今までは Python がほぼメインだったが、検索チームに異動したことで Go が必要不可欠になったので頑張って習得中。久々に新しい言語に触れるけど新鮮な気持ち。明確に型があると、エディタがガンガンサジェストしてくれて楽しい。間違ってるとすぐ知らせてくれる。Go も Python までのレベルまで引き上げて書けるようにしておきたい所存。   New: Java (code reading)  主に Apache Lucene と Apache Beam の code reading をしていたのがメイン。同僚からは VSCode ではなく、IntelliJ IDEA 入れたほうがめちゃくちゃ捗るよと言われつつもまだ使いこなせていない\u0026hellip;。Lucene, Solr, Elasticsearch のどれかに来年は contribute してみたい。   Python  Google BigQuery と組み合わせたデータ分析や可視化、Airflow で利用。あとは機械学習サービスの改修でも書いていた。なんだかんだ手に馴染んでいるのがやはり Python で、2022 年は一段階上のコードを書けるようになりたい。1/4 ほど読んで積ん読になってしまっている Fluent Python を読みきらないと\u0026hellip;   StandardSQL  Google BigQuery でお世話になっている。まだまだ「え、こんな便利関数あったんだ」となる。ちょっとした前処理は BQ に投げたほうが遥かに効率が良いので、BQ→Python で何をどこまでやるかはバランス感覚がやはり大事。     Software  New: Apache Beam  Java は code reading, Python は自分で入門がてら形態素解析する Beam model を書いていた。Apache Beam Go SDK が GA になったので、なにか作りたい。原著論文も勉強会で今度話したいな。ストリーミングで処理を行いたい際には、選択肢の第一候補に入るソフトウェアかつ動いている仕組みがめちゃくちゃおもしろいので、もっと深堀りして書いていきたい。   New: Apache Airflow (CloudComposer)  GCP の各サービスを組み合わせてゴニョゴニョしたいときにものすごく楽。なれるまではデバッグが辛かった。そんなにこなれたことやっていなかったとしても、ピタゴラスイッチ的なデバッグが必要になることが多いので、最初は辛かったけど、慣れたらめっちゃ便利。   New: Apache Lucene  社内の code readning 勉強会で、近似近傍探索のロジックを眺めていた。 Amazon が e コマース検索を Lucene により、どうスケールさせているか at Berlin Buzzwords 2019 の記事でも Lucene 自体の特性を表層的に理解できてスゲーッ! となれたが、コードの中身まで理解して使いこなしたい。Lucene in Action (English Edition) 2nd edition はぜひとも読みたい   New: Apache Solr  Apache Solr 入門 ―オープンソース全文検索エンジン を時間を見つけて読んでいるが、それに関連して書いたりしている。検索エンジンの体系だった日本語書籍の代表的な本だと思う。凄く良い本。Elasticsearch と比べて HTTP2 に対応していたりして、良い意味で競合の関係性。   New: Cloud Dataflow  Apache Beam のエンジン。Dataflow Prime や GPU 機能などが利用可能になったりと進化が激しいけどまだ全然使えていない。BigQuery では不可能な形態素解析やちょっと込み入った前処理をしたいときは Beam (on Dataflow)が役立つのではと思っており、興味が高まっている。手が足りていない\u0026hellip;.   Google BigQuery  Google BigQuery がないと自分の今の業務は成り立たない疑惑が\u0026hellip; 来年もよろしくおねがいします。   New: Google BigTable  特にハマりどころはなく素直に使えた。スケーラビリティが素晴らしいのでパフォーマンス面を気にすることなく使えるのは最高。   New: Elasticsearch  Query 書いてゴニョゴニョしてたりして、Solr, Lucene と同じく表層的に使っているだけなので使い込みたい。   New: Streamlit  業務で内製アプリで GUI をさっと提供したいときにめっちゃ便利。同じような目的の Plotly Dash を 2 年ほど前に使ったけど、それと比較してもライフサイクルと UI の抽象化が素晴らしい。でもやれることは限られているので、それを割り切って使うのが吉。     CI  NewGitHub Actions  Blog の CI・CD で Travis から GitHub Actions に移行したけど、GitHub 内で全て完結するというのは素晴らしい。   CircleCI  CircleCI は便利なんだけど、CI のピタゴラスイッチを回すための CLI スキルが足りないなと思う面が多々あった。     Middleware/Infrastructure  GCP Kubernetes  2020 年に比べるとお触りレベルを脱却してマニフェストを自分で書いて Pod をデプロイしたりしていた。理解が浅い点でトラップ(自業自得)に引っかかり、時間を溶かすことが多々あり反省。   terraform  開発環境と本番環境の剥離をなくして、スムーズにデプロイするにはやはり IaC って必須だなと思えた一年。初期の手探りのときは GUI ポチポチで作りたくなるが結局急がば回れだった。     Editor  VSCode   Tool  toggl  可能な限り作業時間を計測するようにしてみた。作業自体が計測されていると認識することで、時間を意識して作業ができる気がする。   Joplin (notion から移行)  Notion から OSS のメモアプリの Joplin に乗り換えた。全てを自分が管理できているというのはやはり良い体験。一日の作業メモ、やること一覧も全て joplin にログとして残すようにしてみたところ、結果的に作業が捗るようになった。VSCode の拡張で、Joplin を編集できるんだけどこれも便利。      目標振り返り 今年決めた目標の振り返り\n2021 年の目標\n Senior Software Engineer として確固たる実力を身につけることに集中  30%: このままの立ち位置は不味いなと思い検索エンジニアに慣れたのは非常に良かった。が、Software Enginner としての層の厚みを体感できたので 30%とした。へこたれず研鑽していきたい。   英語にふれることを習慣化  5%: Elsa をやったりやらなかったりで全く習慣化できていない。喋ったり書いたりしているときに、基礎ができていないことを痛感するので、知り合いから教えてもらった英語の教科書とかやはりやるべきかなと思っている。3 ヶ月とかのスパンで振り返ってないからグダグダになってしまっているので、来年はちゃんと中期的なスパンで振り替えれるようにしよう。発音矯正としては Elsa は凄く良い。自分の耳で全く sin, thin の違いがわかっていないことに絶望   SNS を断ち自分にとって後悔の無い時間を歩む  60%: Android にStay Focusedを入れて、SNS にそもそもアクセスできないようにした。このおかげで RSS, Pocket の消化がとても進んだ。来年も引き続き SNS 断ちを継続。時間が溶けてしまうので、連絡・告知手段としてのみ使うことにする。    今年はグダグダな気がしているので、来年は胸を張ってあぁ、自分頑張ったなと言える一年にしたい。\n","permalink":"https://shunyaueta.com/posts/2021-12-29/","summary":"うなすけさんの wakatime を利用した振り返り方が面白かったので、来年は真似したいと思い導入してみた。\nサーバーサイドエンジニアとして 2021 年に使った技術と来年の目標\nなので先週くらいから wakatime を使って、VSCode での利用統計をとってみることにした。\nhttps://wakatime.com/\nMarkdown が圧倒的に多いのは現在 Markdown で執筆活動をしてるからですね。\n仕事をほぼ納めてから導入して執筆しかまともにしてないからこうなってるな。。。 詳細は今はお話できないのですが、再来年くらいには形になっていることを祈る。\n利用した技術一覧  Language  New: Go  Backend 開発ではメインで使っている。今までは Python がほぼメインだったが、検索チームに異動したことで Go が必要不可欠になったので頑張って習得中。久々に新しい言語に触れるけど新鮮な気持ち。明確に型があると、エディタがガンガンサジェストしてくれて楽しい。間違ってるとすぐ知らせてくれる。Go も Python までのレベルまで引き上げて書けるようにしておきたい所存。   New: Java (code reading)  主に Apache Lucene と Apache Beam の code reading をしていたのがメイン。同僚からは VSCode ではなく、IntelliJ IDEA 入れたほうがめちゃくちゃ捗るよと言われつつもまだ使いこなせていない\u0026hellip;。Lucene, Solr, Elasticsearch のどれかに来年は contribute してみたい。   Python  Google BigQuery と組み合わせたデータ分析や可視化、Airflow で利用。あとは機械学習サービスの改修でも書いていた。なんだかんだ手に馴染んでいるのがやはり Python で、2022 年は一段階上のコードを書けるようになりたい。1/4 ほど読んで積ん読になってしまっている Fluent Python を読みきらないと\u0026hellip;   StandardSQL  Google BigQuery でお世話になっている。まだまだ「え、こんな便利関数あったんだ」となる。ちょっとした前処理は BQ に投げたほうが遥かに効率が良いので、BQ→Python で何をどこまでやるかはバランス感覚がやはり大事。     Software  New: Apache Beam  Java は code reading, Python は自分で入門がてら形態素解析する Beam model を書いていた。Apache Beam Go SDK が GA になったので、なにか作りたい。原著論文も勉強会で今度話したいな。ストリーミングで処理を行いたい際には、選択肢の第一候補に入るソフトウェアかつ動いている仕組みがめちゃくちゃおもしろいので、もっと深堀りして書いていきたい。   New: Apache Airflow (CloudComposer)  GCP の各サービスを組み合わせてゴニョゴニョしたいときにものすごく楽。なれるまではデバッグが辛かった。そんなにこなれたことやっていなかったとしても、ピタゴラスイッチ的なデバッグが必要になることが多いので、最初は辛かったけど、慣れたらめっちゃ便利。   New: Apache Lucene  社内の code readning 勉強会で、近似近傍探索のロジックを眺めていた。 Amazon が e コマース検索を Lucene により、どうスケールさせているか at Berlin Buzzwords 2019 の記事でも Lucene 自体の特性を表層的に理解できてスゲーッ!","title":"2021 年振り返り"},{"content":"データ分析などをしていると、画像はダウンロードせずに特定の CDN (GCP なら GCS, AWS なら S3 など)で提供されている画像を参照して、 Jupyter Notebook 上で良い感じに表示させたいときがありませんか?\n例えば、画像と説明文がペアになっているデータを画像自体はダウンロードせずに Jupyter 上で画像と説明文を DataFrame として表示させたいときが多々ある。 元の画像自体は CDN に格納されていて、画像をダウンロードする必要はなく参照するだけのときにはすごく便利。 毎度画像を CDN からダウンロードするのも無駄なので、画像を加工せずに Jupyter 上で表示するだけなら、この方法がベストですね。\nurl からとってきた画像を jupyter に表示する でも同じような課題に取り組んでいるが、今回紹介する方法なら余計なパッケージを入れずに最小構成で Jupyter 上で表示できるのが利点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import pandas as pd from IPython.display import HTML # NOTE: https://www.irasutoya.com/2021/01/onepiece.html から画像を参照 onepiece = { \u0026#34;モンキー・D・ルフィ\u0026#34; : \u0026#34;https://1.bp.blogspot.com/-uxIsaN0S5lQ/X-FcrvAAInI/AAAAAAABdD4/6uw_qNUh9dQrG0aUzIExybt84yTEmXOPwCNcBGAsYHQ/s200/onepiece01_luffy.png\u0026#34;, \u0026#34;ロロノア・ゾロ\u0026#34; : \u0026#34;https://1.bp.blogspot.com/-rzRcgoXDqEg/YAOTCKoCpPI/AAAAAAABdOI/5Bl3_zhOxm07TUGzW8_83cXMOT9yy1VJwCNcBGAsYHQ/s200/onepiece02_zoro_bandana.png\u0026#34;, \u0026#34;ナミ\u0026#34; : \u0026#34;https://1.bp.blogspot.com/-2ut_UQv3iss/X-Fcs_0oAII/AAAAAAABdD8/jrCZTd_xK-Y6CP1KwOtT_LpEpjp-1nvxgCNcBGAsYHQ/s200/onepiece03_nami.png\u0026#34;, \u0026#34;そげキング（ウソップ）\u0026#34; : \u0026#34;https://1.bp.blogspot.com/-mZpzgXC1Sxk/YAOTCAKwWTI/AAAAAAABdOM/5B4hXli0KLU5N-BySHgjVbhZscKLSE-bQCNcBGAsYHQ/s200/onepiece04_usopp_sogeking.png\u0026#34;, } df = pd.DataFrame({\u0026#34;Name\u0026#34;: onepiece.keys(), \u0026#34;Image\u0026#34;: onepiece.values()}) def path_to_image_html(path): return f\u0026#39;\u0026lt;img src=\u0026#34;{path}\u0026#34;/\u0026gt;\u0026#39; pd.set_option(\u0026#39;display.max_colwidth\u0026#39;, None) HTML(df.to_html(escape=False ,formatters=dict(Image=path_to_image_html)))   結果はこうなる\nやっていることとしては、\n Pandas のDataFrame.to_html()関数で、DataFrame を HTML に変換 変換時に、escape=False にすることでエスケープせずに HTML を出力 formattersでは、特定のカラムに関数を適用して変換できる。この関数は文字列を返す関数でなくてはならない。 最後に出力された HTML を IPython.display のHTMLモジュールで HTML を Jupyter 上で表示  Appendix Gist はこちら\n Colab でも公開しておきます\nColab link\nReferences  url からとってきた画像を jupyter に表示する Pandas / IPython Notebook: Include and display an Image in a dataframe  ","permalink":"https://shunyaueta.com/posts/2021-12-28/","summary":"データ分析などをしていると、画像はダウンロードせずに特定の CDN (GCP なら GCS, AWS なら S3 など)で提供されている画像を参照して、 Jupyter Notebook 上で良い感じに表示させたいときがありませんか?\n例えば、画像と説明文がペアになっているデータを画像自体はダウンロードせずに Jupyter 上で画像と説明文を DataFrame として表示させたいときが多々ある。 元の画像自体は CDN に格納されていて、画像をダウンロードする必要はなく参照するだけのときにはすごく便利。 毎度画像を CDN からダウンロードするのも無駄なので、画像を加工せずに Jupyter 上で表示するだけなら、この方法がベストですね。\nurl からとってきた画像を jupyter に表示する でも同じような課題に取り組んでいるが、今回紹介する方法なら余計なパッケージを入れずに最小構成で Jupyter 上で表示できるのが利点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import pandas as pd from IPython.display import HTML # NOTE: https://www.irasutoya.com/2021/01/onepiece.html から画像を参照 onepiece = { \u0026#34;モンキー・D・ルフィ\u0026#34; : \u0026#34;https://1.bp.blogspot.com/-uxIsaN0S5lQ/X-FcrvAAInI/AAAAAAABdD4/6uw_qNUh9dQrG0aUzIExybt84yTEmXOPwCNcBGAsYHQ/s200/onepiece01_luffy.png\u0026#34;, \u0026#34;ロロノア・ゾロ\u0026#34; : \u0026#34;https://1.","title":"Jupyter Notebook で画像をダウンロードすることなく、URLから参照してPandas DataFrame内部に表示させる"},{"content":"1 日遅れてしまいましたが、情報検索・検索技術 Advent Calendar 2021 25 日目の記事です。\nついにアドベントカレンダー最終日を迎えました! 今年はまだ検索領域のアドベントカレンダーが作られていないからということで、勢いで情報検索・検索技術 Advent Calendar 2021を作りましたが、多くの方に投稿に協力していただきありがとうございました。\n社内勉強会の発表でネタを探しており、2016 年と少し昔の情報ですが、Amazon の製品検索において、どのようにランキングを行っているかの公演動画が非常に面白かったので、勉強がてら残したメモを記事として公開します。\n今回の口頭発表は MLconf という開発者会議(非学会・非アカデミック)で発表されています。 自分が知る限り、MLconf は機械学習黎明期から高品質な発表が継続されて発信されており、非常に素晴らしいカンファレンスの一つ。 国際会議には投稿されていないが、実応用の観点からしてとても学びの多い発表がとても多いです。 機械学習の応用を考えている場合、世界の最先端事例を知ることができるので非常におすすめです。\n Referemces  Sorokina, D., \u0026amp; Cantu-Paz, E. (2016, July). Amazon search: The joy of ranking products. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval (pp. 459-460).\n  Amazon Search: The joy of ranking products in amazon science Youtube  メモ 自分の私的な意見は NOTE: で書いておきます。\nSearch Ranking Models  1Model = 1 context: 一つのモデルで一つの国の 1 カテゴリをサポート  日本の書籍カテゴリで 1 モデル、フランスの家電領域で更に一つのモデル モデルの軽量化と精度向上のためにコンテキストごとの最適化されたモデル作成を許している   現在、100 以上の機械学習モデルを利用している  GBDT ベースのモデルや、pairwise ランキングモデルなど モデルはだいたい 200 以上の tree で構成されている   150 種類以上の特徴量が利用可能だが、それぞれのモデルが利用しているのは 30 以下  映画では、days_since_release がとても重要だが、他のカテゴリではそうではない    Training Labels  学習データを顧客の行動ログから作成する 利用可能なポジティブラベルとして  クリックされたか? カートに入れられたか? 購入されたか? 消費されたか?   ネガティブなラベルとしては  無視された結果  検索結果には表示されたが、何も行動が起こされていない製品   検索結果から、そもそも表示されなかったページからランダムサンプリング  ページネーションにより表示されなかったページからサンプリングを行う(slide が消えていて見えなかったので論文から補足)。  NOTE: アルゴリズムバイアスを加速させるような気もした。表示されなかったが良い商品をどうやって救い出すかも大事な観点。        Mixing Click and Purchase Targets  何を目的変数とすべきか? クリックなのか、それとも購入か?  おそらく購入されたかどうかでしょう、なぜなら我々の最終目標は製品を売ることなので しかし、いつでも購入がゴールというわけではない\u0026hellip; 例をお見せします   Keyword: iPhone  最もクリックされたもの  iPhone7。700 ドル以上もするのでクリックはされますが購入されません   最も購入されたもの  一番購入されたものは何でしょうか? 最安価な iPhone モデルでしょうか? 正解は\u0026hellip; 8 ドルの Lightning to USB Cable です\u0026hellip;!   もし購入されたことのみを目的変数にしてしまった場合、iPhone と検索して Lightning Cable しか表示されなくなり、検索機能が壊れてしまいます。     これがクリックと購入を混在させたモデルの検索結果になります  iPhone 5S は iPhone モデルシリーズで最も安く最も購入され、その後に 最もクリックされる iPhone7 その後に Lightning Cable が表示されています これによって、異なる顧客の意図を汲み取った検索結果を提供できるようになりました    Fast Feature Evaluation  2 段階の特徴量選択    150→50: ランダムな特徴集合よりも良い結果を出した特徴を選択    50→20: 後方除去、前方選択により枝刈り     ツリーのアンサンブルにおける特徴量のスコアリング  全てのツリーは特徴量のスコアリングアルゴリズムを持っている 課題点として 2 値と連続値の特徴量は比較ができない   is_prime_benefit は Amazon Video で最も重要な特徴量である  しかし、特徴量選択では、いつもランダムな連続値の特徴量にスコアは低くなっている。何故?   アンサンブルツリーはランダムな分割を多く実行する  単一のツリーの分散は最終段階では平均化される   連続値の特徴量は更に可能な分割を提供する  よって、ランダムな分割で多く選ばれる それによりスコアリングが高くなる   それらを避けるには正規化の処理が重要になる 異なる一様分布の値から 6 つのランダムな特徴量を作成・追加を行い、ビデオ領域で実験を行った  本来なら全ては意味のない特徴量なので、全ては同一の低いスコアリングがされるべきである     しかし実験の結果、乱数の値に伴って対数的にスコアリングが変化していることがわかった 正規化された特徴量スコアでのバギングツリーを我々は利用している  上記の手法は OSS として公開しています  https://github.com/dariasor/TreeExtra     上記の手法を適用後、スコアリングが最も高くなったのは is_prime_benefit 特徴量になりました All Product Search green と検索を行うと以下の結果が出てくる  Components of the Blending Score  Fashion, Home\u0026amp;Kichen, Health カテゴリの商品が出てきます。どのように各カテゴリのスコアを計算しているのか? Query Category score  一般的なクエリ: クリック数、カートに追加したか、購入したか? 稀なクエリ: クエリの trigrams , bigrams, unigrams でのクリック数   Hunger score: 飢えのスコア(動的なスコア)  重要なカテゴリはこの Hunger Score が高くなりやすく設定 他のカテゴリが選択されるたび Hunger Score が高くなり、それがカテゴリ選出のスコアリングでは重要になる   In-category relevance score for each product  relevency が高い製品は選ばれやすくなる このスコアは異なるカテゴリ間でも比較可能なものにしなければならない   All Product Search - Blending green というキーワードに対しての Blending Score の具体例  Fashion, Kitchen, Books カテゴリの中からどのように各カテゴリの商品が選出されるか? 各カテゴリの Query Score は、Fashion, Kitchen, Books の順番に高い 初期の全ての Hunger score は 0。白 → 灰色 → 黒と Hunger score が高くなる    まずはじめに、各カテゴリの Top1 製品の F1, K1, B1 を比較して、F1 が QueryScore に基づき最もスコアが高い F1 が選出される。    Fashion が選択されたので、Kitchen と Books の Hunger Score が高くなり、Hunger Score がより高い K1 が選出    Fashion カテゴリは重要なので Hunger Score がより早く高くなりやすい。Query Score と合わせて、三番目には F2 が選出    Books カテゴリの Hunger Score がとても高くなっているので、ついに B1 が 4 番目には選出される    上記の考えで各カテゴリ間でスコアを比較して検索結果に混ぜていく    Match Set  Match set は、クエリに対して返された結果の集合 Match set は 2 つのパターンの製品を含む  例: zootopia というクエリで、70538 個の mache set が返ってきた   Textual matches   クエリに対して、Query understanding 後に Product description とマッチした製品     Behavioral matches   そのクエリで検索後に、クリック、カートに追加、購入した商品の集合 今回の場合は、zootopia で検索後にクリック、カートに追加、購入した製品を指す。  どのように非 text-match が発生するか    zootopia で検索       その後クリックはせずに、inside outで検索を行い製品をクリック       この 1-2 を同一セッションとして Behaivioral feature として扱う       3 の特徴量を取り入れることで、非 text-match の製品を match set に取り込む             Cold Start  新しい Harry Porrer の書籍が利用可能になります!  私達はこの書籍がベストセラーになることを知っていますが\u0026hellip;   Behaivioral feature がまだ存在しない  どれくらいクリックされるかは我々は知らない   古い書籍は多くの signal を持っているので、新しい書籍は下のランクに位置づけられる   求めている書籍が検索結果の下にあるのは顧客は不幸です。 Business チームもこの異常事態を伝えてきます ジョークですが、非公式の指標として、「今すぐこれを直してください」の要求の数が低ければ低いほどよいです   Day 0: 全ての behaivioral features は 0  時間特徴量がここでは助けになる(新製品にたいして Boost を行い、ランクを上にする) ここで顧客が製品に対してクリックを行い、signal が溜まっていきます   Day 1: Behaivioral features はまだ更新されていない  クリック情報を持っているが、検索エンジンにはまだ反映されていない 強靭なインフラが、新製品に対して signal の蓄積を迅速に反映させていく   Day 7: Behavioral features はゆっくりと蓄積されていくようになる。  signal が十分にたまり、新製品としての時間特徴量の boost はここでは取り扱われなくなる ここから、適用されるスコアリングの公式が固定される   殆どの人が Cold start は Day 0 に対してのみ考慮するが、Day0-7 まで考える必要がある 時間経過にともなって、スコアリングをどう行っているか  理解が浅いのでここは省略します。だれか解説してくれると嬉しい    Non relevence Sort  big keyward では match set がとても巨大になります  例: tv というキーワードに対して家電カテゴリで 102,4635 の結果が存在する。 relevance sort は適切な検索結果が返ってくるが\u0026hellip; Customer review での平均でソートを行うと  relevance がぐちゃぐちゃになってしまう。もちろんカスタマーレビューはありがたい機能ですが\u0026hellip;       改善方法  tv というキーワードに対して 500 の製品タイプが Amazon catalog で存在する {query, product type} のペアでスコアを計算する  どの product type が特定のクエリにおいて最もクリック、カートに追加、購入されたか   特定のキーワードでフィルターを行い、指定の product type の製品のみを表示するにした  keyboard→ キーボード製品 USB→ フラッシュメモリ、ネットワークデバイス、入力デバイス TV→ テレビタイプ   これによって Customer review の平均でソートを行っても relevance が保たれるようになった  tvで customer review の平均で sort を行った際に 991 個の製品のみが表示され、relevance が向上した         蛇足 この論文は、過去に読んだサーベイ論文で面白そうとメモしておいたのがきっかけで知ることができた。サーベイ論文の紹介記事はこちら\ne コマースの検索と推薦についてのサーベイ論文である \u0026lsquo;Challenges and research opportunities in eCommerce search and recommendations\u0026rsquo; を社内勉強会で発表した\n","permalink":"https://shunyaueta.com/posts/2021-12-26/","summary":"1 日遅れてしまいましたが、情報検索・検索技術 Advent Calendar 2021 25 日目の記事です。\nついにアドベントカレンダー最終日を迎えました! 今年はまだ検索領域のアドベントカレンダーが作られていないからということで、勢いで情報検索・検索技術 Advent Calendar 2021を作りましたが、多くの方に投稿に協力していただきありがとうございました。\n社内勉強会の発表でネタを探しており、2016 年と少し昔の情報ですが、Amazon の製品検索において、どのようにランキングを行っているかの公演動画が非常に面白かったので、勉強がてら残したメモを記事として公開します。\n今回の口頭発表は MLconf という開発者会議(非学会・非アカデミック)で発表されています。 自分が知る限り、MLconf は機械学習黎明期から高品質な発表が継続されて発信されており、非常に素晴らしいカンファレンスの一つ。 国際会議には投稿されていないが、実応用の観点からしてとても学びの多い発表がとても多いです。 機械学習の応用を考えている場合、世界の最先端事例を知ることができるので非常におすすめです。\n Referemces  Sorokina, D., \u0026amp; Cantu-Paz, E. (2016, July). Amazon search: The joy of ranking products. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval (pp. 459-460).\n  Amazon Search: The joy of ranking products in amazon science Youtube  メモ 自分の私的な意見は NOTE: で書いておきます。","title":"Amazon検索ランキングの奥深さ at MLconf SF 2016"},{"content":"少し前のことだが、Offers Magazine さんに、「エンジニアによる業務実績の論文化」をテーマとして寄稿した。\n機械学習エンジニアの学会での論文発表のススメ。応募から査読通過までの流れ\nメディアに寄稿するのは初めてなので良い経験になった。\n会社によっては業務成果を論文提出まで持っていくまでに難しい会社もあるかと思いますが、それに見合った対外的な成果を得れたので本当にやってよかったと思える。 USENIX は、MLOps に限らずシステムやセキュリティ、SRE 領域など、ソフトウェアエンジニア領域で幅広く投稿できるので、自分の成果を引用可能な形式で残したい人にはぜひ投稿してみたほしい。 素晴らしい仕組みを USENIX は提供していると思う。\n何より自分にとって、この分野のエキスパートのレビュアーからこの成果を対外発表するのは非常にリスペクトするとコメントをいただけたりして、良い刺激になった。\n論文公開して一年と少しが経過して、先日 Google Scholar を確認すると引用数が 1 になっていて非常に嬉しい!!\nAuto Content Moderation in C2C e-Commerce citation\n2021 年には、機械学習エンジニアから検索エンジニアになったが、この分野も論文化できそうなネタが無限にあるので時間はかかるだろうが 2023 年には論文提出ができる段階になりたい(否、なる)。\n","permalink":"https://shunyaueta.com/posts/2021-12-07/","summary":"少し前のことだが、Offers Magazine さんに、「エンジニアによる業務実績の論文化」をテーマとして寄稿した。\n機械学習エンジニアの学会での論文発表のススメ。応募から査読通過までの流れ\nメディアに寄稿するのは初めてなので良い経験になった。\n会社によっては業務成果を論文提出まで持っていくまでに難しい会社もあるかと思いますが、それに見合った対外的な成果を得れたので本当にやってよかったと思える。 USENIX は、MLOps に限らずシステムやセキュリティ、SRE 領域など、ソフトウェアエンジニア領域で幅広く投稿できるので、自分の成果を引用可能な形式で残したい人にはぜひ投稿してみたほしい。 素晴らしい仕組みを USENIX は提供していると思う。\n何より自分にとって、この分野のエキスパートのレビュアーからこの成果を対外発表するのは非常にリスペクトするとコメントをいただけたりして、良い刺激になった。\n論文公開して一年と少しが経過して、先日 Google Scholar を確認すると引用数が 1 になっていて非常に嬉しい!!\nAuto Content Moderation in C2C e-Commerce citation\n2021 年には、機械学習エンジニアから検索エンジニアになったが、この分野も論文化できそうなネタが無限にあるので時間はかかるだろうが 2023 年には論文提出ができる段階になりたい(否、なる)。","title":"Offers Magazine というメディアに、エンジニアによる業務実績の論文化について寄稿した"},{"content":"@potato4d さんや @takuti さんがBlog に Buy me a cofeeを導入していて、自分も導入したいと思いたち、導入してみた。\nやったこととして、Blog記事の末尾に、サポート機能として自分のbuy me a cofeeページへのリンクが表示されるようにした。\n導入経緯 以下のポストに強く共感と興味を持った。特に良い文章は抜粋しておく\n 情報に対して価値を感じてくれた人に還元してもらえるのは素直に嬉しい\n  ブログに Buy Me a Coffee の投げ銭導線を設置した   @shu223 さんの   とにかく、「技術の発信」でも収入が得られる時代が到来しつつある\n  技術書でご飯は食べられるのか？ #技術書典   技術で稼げるようになった今、内発的動機（興味）があっても外発的動機（お金）が伴わないと優先度が上がりにくいということは起きていて、だからこそ勉強自体でも稼げるようよう仕組み化したいと思っていて、それが去年から試行錯誤してる「技術情報発信のマネタイズ」です。 tweet\n の方針に凄く興味と共感が持てた。\n拝金主義というわけではなく、投げ銭文化自体が凄く良い文化なので自分もその波に乗ってみたさがあり、導入してみた。\nzenn やnote でも良い記事だなと思った際には、投げ銭をしたりするが、個人Blog でも簡単にその機能が作れるのは良い時代になった。\nもし自分が価値ある情報を提供することで、誰かの助けになり、その人達が還元してくれたなら凄くよいなと思った。業界全体がその流れになると面白いと思う。\n早速、この前書いた記事 を友人に送ったら、メンテナンスお疲れ様の意をこめて Coffee ☕️ を奢ってくれた。\n へんたい運用お疲れ様でした link\n  手探りで作った記憶が蘇る。開発、メンテとありがとう link\n ","permalink":"https://shunyaueta.com/posts/2021-12-04/","summary":"@potato4d さんや @takuti さんがBlog に Buy me a cofeeを導入していて、自分も導入したいと思いたち、導入してみた。\nやったこととして、Blog記事の末尾に、サポート機能として自分のbuy me a cofeeページへのリンクが表示されるようにした。\n導入経緯 以下のポストに強く共感と興味を持った。特に良い文章は抜粋しておく\n 情報に対して価値を感じてくれた人に還元してもらえるのは素直に嬉しい\n  ブログに Buy Me a Coffee の投げ銭導線を設置した   @shu223 さんの   とにかく、「技術の発信」でも収入が得られる時代が到来しつつある\n  技術書でご飯は食べられるのか？ #技術書典   技術で稼げるようになった今、内発的動機（興味）があっても外発的動機（お金）が伴わないと優先度が上がりにくいということは起きていて、だからこそ勉強自体でも稼げるようよう仕組み化したいと思っていて、それが去年から試行錯誤してる「技術情報発信のマネタイズ」です。 tweet\n の方針に凄く興味と共感が持てた。\n拝金主義というわけではなく、投げ銭文化自体が凄く良い文化なので自分もその波に乗ってみたさがあり、導入してみた。\nzenn やnote でも良い記事だなと思った際には、投げ銭をしたりするが、個人Blog でも簡単にその機能が作れるのは良い時代になった。\nもし自分が価値ある情報を提供することで、誰かの助けになり、その人達が還元してくれたなら凄くよいなと思った。業界全体がその流れになると面白いと思う。\n早速、この前書いた記事 を友人に送ったら、メンテナンスお疲れ様の意をこめて Coffee ☕️ を奢ってくれた。\n へんたい運用お疲れ様でした link\n  手探りで作った記憶が蘇る。開発、メンテとありがとう link\n ","title":"投げ銭サービスのBuy me a cofee をBlog に導入してみた"},{"content":"高専 5 年生の卒業前の春休みに友人 3 人と集まって、2013 年 4 月にリリースした個人開発の Web サービスが完全に終わりを迎えた。\n2021 年の現在から換算すると 8 年以上前の出来事であり、時の流れは凄まじい。\n先程アプリをデプロイしていた Heroku 上からも完全に削除をしたのだが、色々と昔のことを思い出したので筆を執ってみる。\n開発当時 当時仲の良かった同じクラスの友人 3 人で、春休みに友達の家に泊まりつつ 2 泊３日で、Rails 3 でロジック部分を作り上げた。 Twitter ログインや DB 定義なども本を読んだり、ネットの記事を参考にひいこら言いつつ実装していった。\nHeroku を使って、 http://kosen.herokuapp.com/ という URL でサービスを提供しており、Twitter ログインで編入体験談投稿、ログイン無しで掲示板で編入情報について交換できるような機能を提供していた。(注: サイト名のネーミングセンスが若気の至りすぎるので言及しません)\n最終的に Bootstrap で見た目を整えて、編入後の授業が本格的に始まる前の 2013/04/10 にはリリースしていた模様。\n共同開発した友人の 2013/04/28 に書かれた当時のブログ記事が残っていた。 8 年経過しても、その時の状況がブログ記事として残っているの凄い。 自分はブログのホスティングサービスをひたすら変遷して、現在の GitHub pages に落ち着いたので尊敬する。\n ついに動き出しました!! 編入体験談のまとめサービス 「HenTai ~編入体験談~」 http://kosen.herokuapp.com 午前 0:15 · 2013 年 4 月 10 日 tweet\n 当時全く Web サービスの運用方法も全く知らない自分が、インターネット上に Rails アプリを公開できたので間違いなく Heroku のおかげだと思う。GitHub repository と連携して Push すれば Rails アプリがデプロイされる体験はとても簡単だった。\nリリースまでこぎつけれたのは、友人たちと開発できたから。自分ひとりだとここまでモチベーションも上がりきらなかったと思う。\n全体で 106 件の編入体験談が投稿されており、編入体験談の情報源として現在からしてもかなりの規模の情報を集めれたのではなかろうか。\nリリース後、投稿体験談を投稿してもらうために自分たちの知り合いをつてにに声をかけていたので、その影響か自分が編入した大学の体験談が 2-3 割を占めていた気がする。\n与太話として、何度か同大学の編入してきた後輩と話す機会があり、このサービスが話題になった際に\n 「作ったの実は自分なんですよ」\n と話すと、\n 「あぁ、だからこの大学の体験談が多かったんですね~、編入する際にはめちゃくちゃ参考になりました!」\n と毎回ポジティブな感想がもらえて、あぁ作ってよかったなぁとしみじみと感動していた記憶がある。\nこの個人開発のサービスリリースのおかげで、編入後にベンチャー企業での Rails を利用した Web 開発のアルバイトを始めることができたり、Rails 繋がりでミクシィのインターンに合格するきっかけにもなった。 なので、この個人開発は自分の人生が前に進み出す大きなきっかけとなるサービスともなったのである。 また、作りたいものありきで道具の使い方を学習できたのは良かった。(だけど 6 年くらいはもう Ruby を書いていない気がする\u0026hellip;)\n目標がない状態で何かを学ぶよりも、やりたいことがあってそれをどう実現するかをベースに学べたのは今思い出しても本質的でかけがえのない経験だ。\n個人開発の行方 現在の自分だと Rails で Heroku で Deploy するぞ!!! なんてするわけがなく、Google Form で編入体験談を収集して、マネージドの Web サイト(Google site とか)で公開して終わりなので、矜持もなにもあったもんではない。 これが価値観の変容\u0026hellip; 若気の至り、ここに極まり\nその後編入して 1 年ほど時間が立つと、学生生活が忙しくなりメンテナンスする余裕もなくなってきた。 リリースして一年ほど立つと Twitter のログイン機能が不具合を起こしていて、新規投稿ができない状態だった気がする。\n2 年ほど経過すると、Rails3 をアップデートしてくださいと、Heroku からメールが来てたりしていたがそれができないので個人開発の常である。 結局 Twitter ログインのの不具合は修正できず。。。\n2020 年の終わりまではサイトが閲覧できた状態だったと思うが先日アクセスしてみるとアプリが完全にダウンしていたので、良い機会なので Heroku 上からも完全に削除した。\n一点だけ後悔している点 投稿いただいた 106 件の体験談データが消えてしまったのは申し訳ない。次回データを預かるようなサービスを作成する際には、もし Web アプリ自体が運営できなくなったとしても、投稿されたデータはバックアップしておき、貴重なデータが消えないようにしておきたい。\nあらためて、編入体験談を投稿していただいた皆様、ありがとうございました。\n","permalink":"https://shunyaueta.com/posts/2021-12-03/","summary":"高専 5 年生の卒業前の春休みに友人 3 人と集まって、2013 年 4 月にリリースした個人開発の Web サービスが完全に終わりを迎えた。\n2021 年の現在から換算すると 8 年以上前の出来事であり、時の流れは凄まじい。\n先程アプリをデプロイしていた Heroku 上からも完全に削除をしたのだが、色々と昔のことを思い出したので筆を執ってみる。\n開発当時 当時仲の良かった同じクラスの友人 3 人で、春休みに友達の家に泊まりつつ 2 泊３日で、Rails 3 でロジック部分を作り上げた。 Twitter ログインや DB 定義なども本を読んだり、ネットの記事を参考にひいこら言いつつ実装していった。\nHeroku を使って、 http://kosen.herokuapp.com/ という URL でサービスを提供しており、Twitter ログインで編入体験談投稿、ログイン無しで掲示板で編入情報について交換できるような機能を提供していた。(注: サイト名のネーミングセンスが若気の至りすぎるので言及しません)\n最終的に Bootstrap で見た目を整えて、編入後の授業が本格的に始まる前の 2013/04/10 にはリリースしていた模様。\n共同開発した友人の 2013/04/28 に書かれた当時のブログ記事が残っていた。 8 年経過しても、その時の状況がブログ記事として残っているの凄い。 自分はブログのホスティングサービスをひたすら変遷して、現在の GitHub pages に落ち着いたので尊敬する。\n ついに動き出しました!! 編入体験談のまとめサービス 「HenTai ~編入体験談~」 http://kosen.herokuapp.com 午前 0:15 · 2013 年 4 月 10 日 tweet\n 当時全く Web サービスの運用方法も全く知らない自分が、インターネット上に Rails アプリを公開できたので間違いなく Heroku のおかげだと思う。GitHub repository と連携して Push すれば Rails アプリがデプロイされる体験はとても簡単だった。","title":"2013年4月に友人とリリースした高専からの大学編入体験談投稿サービスが8年の時を経て成仏した"},{"content":"k8s で manifest file を編集して実行したら以下のようなエラーが出て実行できなかった。\n Exception ( Monitor Deploy ) Deploy failed: The Deployment “\u0026mdash;” is invalid: spec.selector: Invalid value: v1.LabelSelector{MatchLabels:map[string]string{“app”:“\u0026mdash;”}, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable\n 調べてみたところ、\n once deployed the label selectors of kubernetes service can not be updated until you decide to delete the existing deployment\n ref: MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutabl #508\nらしく、一度デプロイされたk8s service の label selector は、既存のdeployment を削除しないとアップデートは不可能らしい。\nなので、field is immutableというエラー文は正しいわけですね。\nそのdeployment を削除しても良い状態なら、以下のコマンドを実行後再度デプロイすれば、k8s service のlabel selector は実行されます。\n1  kubectl delete deploy \u0026lt;deployment-name\u0026gt;   もしくは、k8s service のlabel selectorの変更を諦めて既存のまま運用するのがもう一つの正解でしょうか。\nReferences  MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutabl #508 Invalid spec selector after upgrading helm template  ","permalink":"https://shunyaueta.com/posts/2021-12-02/","summary":"k8s で manifest file を編集して実行したら以下のようなエラーが出て実行できなかった。\n Exception ( Monitor Deploy ) Deploy failed: The Deployment “\u0026mdash;” is invalid: spec.selector: Invalid value: v1.LabelSelector{MatchLabels:map[string]string{“app”:“\u0026mdash;”}, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable\n 調べてみたところ、\n once deployed the label selectors of kubernetes service can not be updated until you decide to delete the existing deployment\n ref: MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutabl #508\nらしく、一度デプロイされたk8s service の label selector は、既存のdeployment を削除しないとアップデートは不可能らしい。\nなので、field is immutableというエラー文は正しいわけですね。\nそのdeployment を削除しても良い状態なら、以下のコマンドを実行後再度デプロイすれば、k8s service のlabel selector は実行されます。","title":"kubernetes デプロイ時に `MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable` エラーが出た際に対処方法"},{"content":"情報検索・検索技術 Advent Calendar 2021 1 日目の記事です。 早めに書き終えたので、カレンダー登録日の 2021/12/01 よりもはやめですが、記事を公開してしまいます。\nBerlin Buzzwords はドイツで毎年開催されている OSS を利用した検索、データ処理、データベースに焦点をあてたカンファレンスです。\n検索関係のシステムに携わっている場合、毎年面白い内容が目白押しなのでぜひとも見てほしい。\n今回は Berlin Buzzwords 2019 で発表された「Amazon では Lucene をどう活用して e コマース検索をスケールさせているか」の講演動画を社内勉強会で紹介するために視聴したので、そのメモを公開する。\nE-Commerce search at scale on Apache Lucene  YouTube Web page PDF  自分の所感などを切り分けるため、自分の意見は IMO ではじめた文にして、メモっています。\nOverview  クエリの p999 latency に対して非常に厳しい制限を行っている  IMO  このクエリの p999 latency 定義は、Lucene+(おそらく内製で今も開発している、response を返すための Lucene server?)が返す検索のレスポンスを指していると思われる p99.9 latency を SLA として、監視しているのはたしかにとてもシビアな基準だと感じる。     Amazon の query rate はめちゃくちゃピーキー (daily, weekly, yearly) Why Lucene?  Lucene は成熟しており、豊富な検索エンジンの機能が揃っている 情熱を持ったコミュニティが存在している Uber, Airbnb, Linkedin 全部 Lucene を使っている maxscore scoring , Weak AND, Lucene 8.0 での Codec の衝撃   Lucene design  100% Java で書かれている on-disc search with small in-memory index 巨大な index を扱えるが、小さな RAM で実行可能 高い並列性を兼ね揃えたインデキシングと検索機能 ニアリアルタイムでの検索とインデキシング機能   Amazon での Lucene 利用例  ニアリアルタイムでのセグメントレプリケーション (Solr, Elastic Search とは別のレプリケーションの仕組み)  IMO: 登壇者の Mike さんが Lucene 6.0 をベースにした Near realtime replication が可能なlucene server を公開している。 yelp が 2021/09 に OSS として、上記の lucene server をベースにした OSS を公開。解説記事も公開してくれている。Nrtsearch: Yelp’s Fast, Scalable and Cost Effective Search Engine   並行検索 → 複数のスレッドでの並行検索処理が可能 (Lucene で利用可能だが、Solr, ElasticSearch では 2019 年時点では利用不可) query time ではなく、index time で join を行いデータの結合を行っている。Lucene 6.0 で追加された BKD-tree を利用した Dimensional points 機能は、多次元空間を効率的に検索することができ、プライムデーで重宝されている。 また拡張性の高いデザインのおかげでカスタマイズも容易。 Custom term frequency などもとても便利。当初 Lucene にこの機能は存在しなかったので、我々で機能開発を行い Lucene にこの機能が搭載された。 2019 年時点では、Solr, Elastic Search を使っていない。理由としては、Concurrent faceting, multi-phase ranking などの機能は Solr, Elastic Search は当時は利用できなかった。また、現時点で Lucene ベースでのモジュールを多数開発しているのも要因。   Open Source at Amazon  パフォーマンス課題や小さなバグなどを発見して、それらを解決していった。これはコミュニティみんなが嬉しい。(Solr, Elastic Search にも還元されるのでとても健全な流れだよね) 以下の Lucene の機能は Amazon が開発を行い、貢献した  Custom term frequencies Concurrent indexing updates Concurrent faceting FST direct arc addressing  該当チケット 日本語での解説記事: Lucene で使われてる FST を実装してみた（正規表現マッチ：VM アプローチへの招待）   Off-heaps FSTs      Service architecture  Near-real-time replicaiton  Black Friday や Prime day などの爆発的にアクセスが増加するイベントなどに対応するために作成 Solr, Elastic Search で提供される document level レプリケーションでは限界がある   Service architecture  AWS で構築 ECS コンテナでインデクサー、サーチャーが稼働している Kinesis, DynamoDB からカタログが更新される ニアリアルタイムで、S3 にインデックスは保存される インデックスはソフトウェアが更新されるたびに、全て再構築される 人為的なクエリでサービスを暖機運転   Service system design   リクエストが来た際に、Collators がリクエストをさばいて、適切な view に割り振る。 Index(i001, i002, \u0026hellip;) は S3 にリアルタイムで保存され、service はその index を読み込んで検索を行って、レスポンスを返す。   Searching a segmented index  Lucene には検索インデックスが必要で、分割型のアーキテクチャとなっている merge して結果を返す   Searching a segmented index concurrently  index は統計的に商品品質によりソート済 この機能はシャードが巨大化している我々にとってレイテンシーを抑えるために非常に有用な機能   p999 latency figure  青色のグラフの挙動は、Lucene の分割型アーキテクチャによって発生しており、並行検索がどのように優位性をもっているのかを説明する。縦軸は latency 緑色のグラフは、どれくらいのサイズのセグメントがレプリカにコピーされているかを示している。縦軸は GB。  大きなセグメントがマージされたときにスパイクが発生する  通常は、小さなインデックスがマージされて大きなインデックスになることは良いことである、なぜならたくさんのファイルを開く必要がなくなるし、すべてのセグメントを探索しなくても良くなる。 だが、Amazon の場合は、並行検索を行っているので、大きなセグメントが存在すると逆にレイテンシー増加の要因となる。なぜならセグメントの数が減少すると、検索の並行性も失われるため。例えば、10 のセグメントがあった場合、10 スレッドで並行検索を行えるが、一つのセグメントになってしまった場合、1 スレッドでの検索しかできなくなる。 Mike さんは、例えば一つのセグメントに対して、複数スレッドで並行検索できるようになれば、この問題への改善が見込めると考えているらしい。     セグメントインデックスに対して、並行検索が可能になったことで、大きなセグメントを取り扱うことを避けれるようになった    Performance mesurement  Bemchmarking  Lucene nightly benchmarks  上記と同じような方法で、各種クエリのパフォーマンスを常に測定している。   perfoamance regression の検知は困難 performance だけではなく、検索性能も自動的に評価   Concurrent refresh  Lucene は、並行リフレッシュのために、インデックスサイドのアプリケーションのスレッドを借りるという問題があった  解決方法として、インデキシングが行われていないときのみ並行リフレッシュを行う機能を開発  Solution: use expert Lucene API to refresh concurrently        Gathering metrics using Lucene’s abstractions  Lucene の抽象化機能を使って、各指標を容易にモニタリング可能に   Garbate correction is too hard  IMO: ここらへんは知識が足りず理解できなかったので、後から勉強    Analysis challanges  Context sensitive analysis  plane は何を意味する? おもちゃの airplane? plane ←→ airplane の同義語 synonym 拡張をを index time のみで行う   Numbers a special  Toy for 3 year old というクエリには、 2-4 歳対象という文章は対象になる 1500ml は 1.5 litters とマッチするべき 1,100、1100、1.100 は一緒で、1/100、1:100 とは違う 標準的な tokenizer の後に上記のハンドリングをするのは難しい 句読点の取り扱いには注意   WordDelimiterGraphFiltter  Lucene docs  英語と数字の分割など細かい前処理が可能になる  e.g. \u0026ldquo;SD500\u0026rdquo; → \u0026ldquo;SD\u0026rdquo;, \u0026ldquo;500\u0026rdquo;     機械学習はこの問題は解決可能だが、検索ではこのような機能もやはりまだ必要である。  IMO: Lucene の機能を使って解決可能なら、たしかにできる限り機械学習を使いたくない気持ちは非常に共感できる。ここまでレイテンシーの制約が厳しいなら増加要因は可能な限り抑えたい\u0026hellip;      Query optimization  Indexed Queries  多くのクエリは共通のフィルターを使っている。 大元のインデックスに対して、共通で使われているフィルターを適用した結果を、インデキシングしてる? (post-processing index)。 single term に置き換えて、パフォーマンスをチューニング   Factoring queries  Boolean query FP growth algorithm   Query 最適化のおかげで、 +30% redline QPS が増加。p99 latency は 81ms から 54ms へ。  IMO: P99 のレイテンシー公開して良いだろうか\u0026hellip;??? (p99.9 は公開していなかったので気になる)   Indexing tuples  multi stage search を single stage に圧縮している IMO: ここらへんの最適化は、Lucene の検索の仕組みをもっと理解しないとどうやって実現しているかまだ深く理解できない   Lighting deals using dimensional points  当初は mike さんにより、地理検索などを目的として作られた機能。だが、地理検索には使用せず、Amazon での lighting deal に三次元データ(start time, end time, id) での三次元検索にこの dimensional points 機能を使ってる。 IMO: by @takuya-a さん - 社内で発表した際に、なぜ start-time, end-time の 2 次元ではなく、id を入れた 3 次元にした検索にしたのかという質問に対する完璧な @takuya-a さんの推測    ID も空間インデックスに含めることで、パフォーマンスを上げているのだとおもいます。ID が別フィールドだと、そっちのインデックスも検索して、空間インデックスの検索結果とマージして、って処理が後段で必要になるのですが、最初から ID も含めておくと BKD-tree の検索で全部処理できちゃうので。同じ期間で別の ID をもつセール対象商品がヒットしないので最初の段階でかなり絞り込めるようになるのだと思います\n    BKD tree の解説は @pon さんの記事がわかりやすいです  検索エンジンの数値インデックスを支える Bkd-Tree      Multi phase ranking  Ranking  Machine Leraning models Multi phase ranking Prorated early termination  IMO: スコアリングの知識が欠如しているのでメモだけ     Summary  Amazon で検索してるときには Lucene が使われているよ 100%ではないけども    Question  Q. indexing synonym を行っていると言っていたが、例えば query timing で synonym を扱えれば、検索者の文脈などを考慮した同義語を扱えるのではないのだろうか?  A. indexing synonym は主に効率性を重視した意思決定であり、基本的にトレードオフの関係となっている。Query rewrite なども行っているが、今回のトークは主に検索エンジンなので優先順的に Query Understanding 関係の話はしていません。    Reference  What Amazon gets by giving back to Apache Lucene  Lucene に詳しい同僚からは、Lucene に興味あるならこの本がおすすめと言われので読んでおきたい。 ちなみに著者は、この講演者のうちの一人である Mike McCandless さん\u0026hellip; (14 年間 Lucene の開発をしている!!)\n Lucene in Action 2nd edition  余談ですが、この人のことを同僚が、\n Lucene 界隈では神として知られていますね\n と言っていて、笑った w\n 情報検索・検索技術 Advent Calendar 2021 の次の記事は @sz_dr さんで 4 日目を担当してくれます!\n","permalink":"https://shunyaueta.com/posts/2021-11-26/","summary":"情報検索・検索技術 Advent Calendar 2021 1 日目の記事です。 早めに書き終えたので、カレンダー登録日の 2021/12/01 よりもはやめですが、記事を公開してしまいます。\nBerlin Buzzwords はドイツで毎年開催されている OSS を利用した検索、データ処理、データベースに焦点をあてたカンファレンスです。\n検索関係のシステムに携わっている場合、毎年面白い内容が目白押しなのでぜひとも見てほしい。\n今回は Berlin Buzzwords 2019 で発表された「Amazon では Lucene をどう活用して e コマース検索をスケールさせているか」の講演動画を社内勉強会で紹介するために視聴したので、そのメモを公開する。\nE-Commerce search at scale on Apache Lucene  YouTube Web page PDF  自分の所感などを切り分けるため、自分の意見は IMO ではじめた文にして、メモっています。\nOverview  クエリの p999 latency に対して非常に厳しい制限を行っている  IMO  このクエリの p999 latency 定義は、Lucene+(おそらく内製で今も開発している、response を返すための Lucene server?)が返す検索のレスポンスを指していると思われる p99.9 latency を SLA として、監視しているのはたしかにとてもシビアな基準だと感じる。     Amazon の query rate はめちゃくちゃピーキー (daily, weekly, yearly) Why Lucene?","title":"Amazonがeコマース検索を Lucene により、どうスケールさせているか at Berlin Buzzwords 2019"},{"content":"データの蓄積帰還が長くなってくると、例えば JSON 形式でログを取っているが、同じデータでもマイグレーションやロギングロジックの更新などでkey の名前が変化したりする場合がある。\nその場合取り扱いに困るのが、古い key と新しい key をどのように併合するかだ。 例えば特定の日次できれいにデータが入れ替わっているのなら、色々やりようがあるが、クライアントなどのログの場合データの変化も均一ではないので、徐々に変化していることが大半なので、日次で別々の抽出をして結合するというアプローチも難しい。\nその際に役立つのが Standard SQL 条件付き構文の COALESCE だ。\nCOALECSCE は、引数の最初の非 NULL の値を返す関数で、\n1  COALESCE(NULL, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;)   だと Bが返される。この関数を使うことで、複数カラムを一つに併合することができる。\n具体例を交えつつ実践してみる 例えば、以下のように昔のカラム名が title で、全く同じデータが新しいカラムの title_v2 に入ってきているとする。\nNOTE: json を例題に key の抽出にしたほうが実際の状況に沿いますが、カラムのみで表現したほうが説明が簡単なので今回はそちらを採用。\n用意したデータ 1 2 3 4 5 6 7 8  WITH menues AS (SELECT \u0026#34;うどん\u0026#34; as title, NULL as title_v2, \u0026#34;2021/10/06\u0026#34; as created UNION ALL SELECT \u0026#34;ラーメン\u0026#34;, NULL, \u0026#34;2021/10/07\u0026#34; UNION ALL SELECT NULL, \u0026#34;そば\u0026#34;, \u0026#34;2021/10/08\u0026#34; UNION ALL SELECT \u0026#34;カツ丼\u0026#34;, NULL, \u0026#34;2021/10/09\u0026#34; UNION ALL SELECT \u0026#34;カツ丼\u0026#34;, \u0026#34;カツ丼\u0026#34;, \u0026#34;2021/10/10\u0026#34; UNION ALL SELECT NULL, \u0026#34;カレー\u0026#34;, \u0026#34;2021/10/11\u0026#34;) SELECT * FROM menues       title title_v2 created     うどん  2021/10/06   ラーメン  2021/10/07    そば 2021/10/08   カツ丼  2021/10/09   カツ丼 カツ丼 2021/10/10    カレー 2021/10/11    2021/10/10 のデータなどは旧カラムと新カラムにダブルライトされています。\nCOALESCE で複数カラムを併合する 1 2 3 4 5 6 7 8 9 10 11  WITH menues AS (SELECT \u0026#34;うどん\u0026#34; as title, NULL as title_v2, \u0026#34;2021/10/06\u0026#34; as created UNION ALL SELECT \u0026#34;ラーメン\u0026#34;, NULL, \u0026#34;2021/10/07\u0026#34; UNION ALL SELECT NULL, \u0026#34;そば\u0026#34;, \u0026#34;2021/10/08\u0026#34; UNION ALL SELECT \u0026#34;カツ丼\u0026#34;, NULL, \u0026#34;2021/10/09\u0026#34; UNION ALL SELECT \u0026#34;カツ丼\u0026#34;, \u0026#34;カツ丼\u0026#34;, \u0026#34;2021/10/09\u0026#34; UNION ALL SELECT NULL, \u0026#34;カレー\u0026#34;, \u0026#34;2021/10/10\u0026#34;) SELECT COALESCE(title, title_v2) as title, created FROM menues   結果    title created     うどん 2021/10/06   ラーメン 2021/10/07   そば 2021/10/08   カツ丼 2021/10/09   カツ丼 2021/10/09   カレー 2021/10/10    上記の結果のようにCOALESCE を使えば２つの時間経過によって別れてしまったカラムを一つのカラムとして併合することができました。 常に便利ですね。\nReferences BigQuery Conditional expressions- COALESCE\n","permalink":"https://shunyaueta.com/posts/2021-11-06/","summary":"データの蓄積帰還が長くなってくると、例えば JSON 形式でログを取っているが、同じデータでもマイグレーションやロギングロジックの更新などでkey の名前が変化したりする場合がある。\nその場合取り扱いに困るのが、古い key と新しい key をどのように併合するかだ。 例えば特定の日次できれいにデータが入れ替わっているのなら、色々やりようがあるが、クライアントなどのログの場合データの変化も均一ではないので、徐々に変化していることが大半なので、日次で別々の抽出をして結合するというアプローチも難しい。\nその際に役立つのが Standard SQL 条件付き構文の COALESCE だ。\nCOALECSCE は、引数の最初の非 NULL の値を返す関数で、\n1  COALESCE(NULL, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;)   だと Bが返される。この関数を使うことで、複数カラムを一つに併合することができる。\n具体例を交えつつ実践してみる 例えば、以下のように昔のカラム名が title で、全く同じデータが新しいカラムの title_v2 に入ってきているとする。\nNOTE: json を例題に key の抽出にしたほうが実際の状況に沿いますが、カラムのみで表現したほうが説明が簡単なので今回はそちらを採用。\n用意したデータ 1 2 3 4 5 6 7 8  WITH menues AS (SELECT \u0026#34;うどん\u0026#34; as title, NULL as title_v2, \u0026#34;2021/10/06\u0026#34; as created UNION ALL SELECT \u0026#34;ラーメン\u0026#34;, NULL, \u0026#34;2021/10/07\u0026#34; UNION ALL SELECT NULL, \u0026#34;そば\u0026#34;, \u0026#34;2021/10/08\u0026#34; UNION ALL SELECT \u0026#34;カツ丼\u0026#34;, NULL, \u0026#34;2021/10/09\u0026#34; UNION ALL SELECT \u0026#34;カツ丼\u0026#34;, \u0026#34;カツ丼\u0026#34;, \u0026#34;2021/10/10\u0026#34; UNION ALL SELECT NULL, \u0026#34;カレー\u0026#34;, \u0026#34;2021/10/11\u0026#34;) SELECT * FROM menues       title title_v2 created     うどん  2021/10/06   ラーメン  2021/10/07    そば 2021/10/08   カツ丼  2021/10/09   カツ丼 カツ丼 2021/10/10    カレー 2021/10/11    2021/10/10 のデータなどは旧カラムと新カラムにダブルライトされています。","title":"Standard SQLのCOALESCEで、時間経過によってカラム名が変化したデータを柔軟に抽出する"},{"content":"PubSub に出力された結果を確認するのって、なかなか手間がかかりませんか?\n最近同僚に簡単な確認方法を教えてもらい、感動したのでそれを記事にしました。\n確認方法  PubSub のメッセージを出力する Google Cloud Storage bucket を同一 GCP プロジェクトで作成する。 GCP の Pub/Sub ページに移動する 確認したい Pub/Sub topic をクリックする ページ下部にある CREATE SUBSCRIPTION ボタンを押すと選択肢で、Create subscription, Export to BigQuery, Export to Cloud Storageがあり、 Export to Cloud Storageを選択する。  BigQuery、 Google Cloud Storage への吐き出しを行い際に、自動的に subscription が生成される。   Export to Cloud Storage を選択すると、Text 形式か Abro 形式での出力にするかを選択できる。基本的には簡単に確認できる Text 形式を選ぶと良さげ。 選択後、下記のような設定画面が出てくるので情報を埋めていく。基本的には、どこの Google Cloud Storage に出力するかを埋めれば完了。   10m ほどすると Streaming job の Dataflow の起動が完了して、一定期間ごとに Pub/Sub の topic に公開されたデータがテキスト形式で出力され始めます。 出力された GCS の結果を眺めるには、 gsutil コマンドなどを使うのが簡単です。自分はgsutil cat の結果をコピーして VS Code で確認しています。  Cloud Dataflow のテンプレート機能については、端的に説明すると、GUI でパラメータを設定するだけで、Dataflow によるデータ処理が簡単に実行できるようになる機能です。\n詳しくは、 GCPUG でのCloud Dataflow がテンプレートにより気軽に使えるサーバーレスのサービスに進化した話 の記事がわかりやすいのでごらんください。\n","permalink":"https://shunyaueta.com/posts/2021-11-05/","summary":"PubSub に出力された結果を確認するのって、なかなか手間がかかりませんか?\n最近同僚に簡単な確認方法を教えてもらい、感動したのでそれを記事にしました。\n確認方法  PubSub のメッセージを出力する Google Cloud Storage bucket を同一 GCP プロジェクトで作成する。 GCP の Pub/Sub ページに移動する 確認したい Pub/Sub topic をクリックする ページ下部にある CREATE SUBSCRIPTION ボタンを押すと選択肢で、Create subscription, Export to BigQuery, Export to Cloud Storageがあり、 Export to Cloud Storageを選択する。  BigQuery、 Google Cloud Storage への吐き出しを行い際に、自動的に subscription が生成される。   Export to Cloud Storage を選択すると、Text 形式か Abro 形式での出力にするかを選択できる。基本的には簡単に確認できる Text 形式を選ぶと良さげ。 選択後、下記のような設定画面が出てくるので情報を埋めていく。基本的には、どこの Google Cloud Storage に出力するかを埋めれば完了。   10m ほどすると Streaming job の Dataflow の起動が完了して、一定期間ごとに Pub/Sub の topic に公開されたデータがテキスト形式で出力され始めます。 出力された GCS の結果を眺めるには、 gsutil コマンドなどを使うのが簡単です。自分はgsutil cat の結果をコピーして VS Code で確認しています。  Cloud Dataflow のテンプレート機能については、端的に説明すると、GUI でパラメータを設定するだけで、Dataflow によるデータ処理が簡単に実行できるようになる機能です。","title":"Google Cloud Pub/Sub に公開された結果はDataflow template を使えばめちゃくちゃ簡単に確認できる"},{"content":"Airflow で作成したDAGを自動で定期実行せずに、あえて手動実行で一度だけ実行したい場合もある。\nDAGのオプションを以下のように設定する。\n schedule_interval を \u0026ldquo;@once\u0026rdquo; に設定することで、一度だけDAGが実行される is_paused_upon_creation を True に設定することで、DAGが作成時に自動的に実行されず、DAGが停止状態で作成される。 デフォルトではFalseとなっており、自動実行される。  1 2 3 4 5 6 7 8 9  from airflow import DAG with DAG( dag_id=\u0026#34;sellerscore_initial_batch\u0026#34;, # NOTE: dosen\u0026#39;t need to repeat schedule_interval=\u0026#34;@once\u0026#34;, # NOTE: we have to manually start the this DAG is_paused_upon_creation=True, ) as dag:   Reference  Airflow: schedule_interval = \u0026lsquo;@once\u0026rsquo; Docs - airflow.models.dag  ","permalink":"https://shunyaueta.com/posts/2021-10-12/","summary":"Airflow で作成したDAGを自動で定期実行せずに、あえて手動実行で一度だけ実行したい場合もある。\nDAGのオプションを以下のように設定する。\n schedule_interval を \u0026ldquo;@once\u0026rdquo; に設定することで、一度だけDAGが実行される is_paused_upon_creation を True に設定することで、DAGが作成時に自動的に実行されず、DAGが停止状態で作成される。 デフォルトではFalseとなっており、自動実行される。  1 2 3 4 5 6 7 8 9  from airflow import DAG with DAG( dag_id=\u0026#34;sellerscore_initial_batch\u0026#34;, # NOTE: dosen\u0026#39;t need to repeat schedule_interval=\u0026#34;@once\u0026#34;, # NOTE: we have to manually start the this DAG is_paused_upon_creation=True, ) as dag:   Reference  Airflow: schedule_interval = \u0026lsquo;@once\u0026rsquo; Docs - airflow.models.dag  ","title":"Airflow でDAGを任意のタイミングで一度だけ実行する方法"},{"content":"今年の 10 月から、新しく入社した同僚とともに、検索領域の論文や技術ブログを定期的に紹介する社内勉強会をはじめてみた。 定常的に開催されることが一番大事だよねという方針になったので、以下のような仕組みで、可能な限り低コストで継続できるような仕組みにした。\n 参加者は何も準備をしなくても大丈夫で、勉強会中に紹介された論文をみたり話を聞くだけで良い 発表者は凝った資料は用意するのは必須ではなく、極論論文を画面共有で見せながらしゃべるだけでも問題なし  当面の目標としては、来年の年末まで継続されているように気長に続けていきたい。\n第一回は、発起人の一人である自分がクエリ分類について発表を行った。\nQuery Understanding for Search Engines (The Information Retrieval Series, 46) の第二章を主にテーマとして取り上げて紹介した。\nメイントピックは KDDCup2005 として開催されたクエリ分類コンペの優勝者の手法について紹介を行ったので、気になる方はスライドを公開しているので御覧ください。\n このコンペの特徴として、\n データセットが生データ特有の問題として汚い そしてラベルデータの規模がとても少ない  という鬼畜仕様だった。 だがコンペ参加者はそんな状態を物と物せずにありとあらゆる手段で精度向上に努めていてそれらの手法と姿勢がとても参考になった。\nQuery Understanding の包括的な解説は 晋策さんが書かれた 検索体験を向上する Query Understanding とは がわかりやすいのでおすすめです。\n検索領域は本当に奥深い\u0026hellip;\n","permalink":"https://shunyaueta.com/posts/2021-10-09/","summary":"今年の 10 月から、新しく入社した同僚とともに、検索領域の論文や技術ブログを定期的に紹介する社内勉強会をはじめてみた。 定常的に開催されることが一番大事だよねという方針になったので、以下のような仕組みで、可能な限り低コストで継続できるような仕組みにした。\n 参加者は何も準備をしなくても大丈夫で、勉強会中に紹介された論文をみたり話を聞くだけで良い 発表者は凝った資料は用意するのは必須ではなく、極論論文を画面共有で見せながらしゃべるだけでも問題なし  当面の目標としては、来年の年末まで継続されているように気長に続けていきたい。\n第一回は、発起人の一人である自分がクエリ分類について発表を行った。\nQuery Understanding for Search Engines (The Information Retrieval Series, 46) の第二章を主にテーマとして取り上げて紹介した。\nメイントピックは KDDCup2005 として開催されたクエリ分類コンペの優勝者の手法について紹介を行ったので、気になる方はスライドを公開しているので御覧ください。\n このコンペの特徴として、\n データセットが生データ特有の問題として汚い そしてラベルデータの規模がとても少ない  という鬼畜仕様だった。 だがコンペ参加者はそんな状態を物と物せずにありとあらゆる手段で精度向上に努めていてそれらの手法と姿勢がとても参考になった。\nQuery Understanding の包括的な解説は 晋策さんが書かれた 検索体験を向上する Query Understanding とは がわかりやすいのでおすすめです。\n検索領域は本当に奥深い\u0026hellip;","title":"クエリ分類(Query Classification) について社内の勉強会で話してきた"},{"content":"最近昔書いていた技術記事の情報が古くなりすぎて不正確なこともあったので、書き直すときがあったのだが、そのときに自動的に最終更新日を記事に表記できないか探していたら、実現方法があったのでメモしておきます。\nやっていることは Last Modified Date with Hugo の記事をと完全に一緒だが、日本語での情報が無かったので備忘録がてら記録を残す。\nHugo は各ページに関する情報をFront Matter Variables という仕組みで Markdown 上に定義します。 主に YAML 形式で記述されていることが多いです。\nlastmod という変数が更新日を表す変数であり、この変数に対して更新日の情報を与えてやれば記事の最終更新日を表現することができる。\nFront Matter に lastmod: \u0026quot;2021-03-31\u0026quot; の形式で与えておけば、以下の形式で記事作成日と最終更新日を表記できる。\n1 2 3 4 5  {{ $date := .Date.Format \u0026#34;02.01.2006\u0026#34; }} {{ $lastmod := .Lastmod.Format \u0026#34;02.01.2006\u0026#34; }} \u0026lt;p\u0026gt;Published on: {{ $date }}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Edited on: {{ $lastmod }}\u0026lt;/p\u0026gt;   だが、毎回記事を編集するたびに lastmod 変数を追記するのは面倒なので自動化できるなら自動化したい。\nconfig.yaml で、以下の設定を行う。\n1  enableGitInfo:true  enableGitInfoを trueにすることで、各ページに対してGit 情報を更新日として付与してくれる。\n最後にconfig.yaml で以下の設定を行えば、 Front Matter の lastmod 変数に対して、 Front Matter で定義されているlastmod、もしその情報がなければ各ページの gitの最終コミット日を返すという設定がされる。\n1 2 3 4  frontmatter:lastmod:- lastmod- :git  参考になると思うので、当ブログでの実際の変更点はこちら\nテンプレートを上書きするように、./layout配下のページを編集した。 hugo の記事でたまに見かけるのがテーマを直接編集している人がいるが、それは筋が良くない方法です。\nなぜかというと、hugo の設計はよくできているので、./layout 配下にthemeと同じ構成でファイルを設置すればそちらを上書きする形で参照してサイトを構成してくれるので、元のテーマは何も変更をせずにアップデートが可能です。 また、拡張性と柔軟性が高まる良い仕組みで、個々人ごとにテーマの編集がしたい必要性に対してよく考え込まれているなと思った。\nReference  Last Modified Date with Hugo  ","permalink":"https://shunyaueta.com/posts/2021-10-06/","summary":"最近昔書いていた技術記事の情報が古くなりすぎて不正確なこともあったので、書き直すときがあったのだが、そのときに自動的に最終更新日を記事に表記できないか探していたら、実現方法があったのでメモしておきます。\nやっていることは Last Modified Date with Hugo の記事をと完全に一緒だが、日本語での情報が無かったので備忘録がてら記録を残す。\nHugo は各ページに関する情報をFront Matter Variables という仕組みで Markdown 上に定義します。 主に YAML 形式で記述されていることが多いです。\nlastmod という変数が更新日を表す変数であり、この変数に対して更新日の情報を与えてやれば記事の最終更新日を表現することができる。\nFront Matter に lastmod: \u0026quot;2021-03-31\u0026quot; の形式で与えておけば、以下の形式で記事作成日と最終更新日を表記できる。\n1 2 3 4 5  {{ $date := .Date.Format \u0026#34;02.01.2006\u0026#34; }} {{ $lastmod := .Lastmod.Format \u0026#34;02.01.2006\u0026#34; }} \u0026lt;p\u0026gt;Published on: {{ $date }}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Edited on: {{ $lastmod }}\u0026lt;/p\u0026gt;   だが、毎回記事を編集するたびに lastmod 変数を追記するのは面倒なので自動化できるなら自動化したい。\nconfig.yaml で、以下の設定を行う。\n1  enableGitInfo:true  enableGitInfoを trueにすることで、各ページに対してGit 情報を更新日として付与してくれる。\n最後にconfig.yaml で以下の設定を行えば、 Front Matter の lastmod 変数に対して、 Front Matter で定義されているlastmod、もしその情報がなければ各ページの gitの最終コミット日を返すという設定がされる。","title":"Hugo で記事の更新日をgitと連携して自動的に取得して表示させる"},{"content":"Cloud Composer(Airflow) の DAG を GitHub リポジトリで管理して、CI によりリポジトリで管理している DAG を Pull Request がマージされると Cloud Composer の DAG へ同期する方法について説明する。\nDAG は、ルートディレクトリ直下の dags/ というディレクトリに格納されている状態を前提とする。\n以下の２つのコマンドラインツールを利用して実現できる。\n Service Account の認証のために gcloud DAG の同期のために gsutil  CircleCI によるワークフローの記述例は以下のとおり\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  version:2.1jobs:rsync-dags:working_directory:~/workspacedocker:- image:gcr.io/google.com/cloudsdktool/cloud-sdk:alpineenvironment:GOOGLE_APPLICATION_CREDENTIALS:/gcp-service-key.jsonsteps:- checkout- run:name:SyncDAGfoldertoGCS\u0026#39;sDAGfoldercommand:| echo \u0026#34;${CLOUD_COMPOSER_CREDENTIALS_JSON}\u0026#34; \u0026gt; ${GOOGLE_APPLICATION_CREDENTIALS}gcloudauthactivate-service-account--key-file${GOOGLE_APPLICATION_CREDENTIALS}gsutil-mrsync-d-rdags\\\u0026#34;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=\u0026#34;get(config.dagGcsPrefix)\u0026#34;)\u0026#34;workflows:rsync_dags:jobs:- rsync-dags  Sync DAG folder to GCS's DAG folder で行っている内容を順を追って説明する。\n サービスアカウントのクレデンシャルファイルを CircleCI の環境変数として保存しておき、リダイレクトにより JSON ファイルとして書き出す  1  echo \u0026#34;${CLOUD_COMPOSER_CREDENTIALS_JSON}\u0026#34; \u0026gt; ${GOOGLE_APPLICATION_CREDENTIALS}   NOTE: セキュリティ対策としてクレデンシャルファイルは必ず環境変数として扱う。\n1 で生成したクレデンシャルファイルにより、サービスアカウント認証を gcloud コマンドで行う  1  gcloud auth activate-service-account --key-file ${GOOGLE_APPLICATION_CREDENTIALS}   CloudComposer の DAG が格納されている GCS のバケットを gcloud コマンドで取得して、gsutil コマンドで DAG ファイルの同期を行う  GCP の Cloud Composer の DAG を素早く・簡単にデバッグする の記事でも紹介した方法で DAG の同期を行う。\n1  gsutil -m rsync -d -r dags \u0026#34;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=\u0026#34;get(config.dagGcsPrefix)\u0026#34;)\u0026#34;   {XXX} には使用する環境の情報を置換してください。\n \u0026quot;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=\u0026quot;get(config.dagGcsPrefix)\u0026quot;)\u0026quot;  指定した GCP Project で動く Cloud Composer の DAG が格納されている GCS のパスを取得できる。   gsutil -m rsync -d -r dags  -m は並列処理 -d は元のディレクトリに存在しないファイルがコピー先にあれば削除(ミラーリング)。これにより、GCS 上で DAG を新たに作成して、デバッグしていたとしても、CI が走ればリポジトリにない DAG ファイルは削除され、リポジトリの DAG と完全に同期される。 -r はディレクトリとしてコピー 上記のオプションによりdags ディレクトリの DAG ファイルを GCS にミラーリングで同期を行う。    Reference  gsutil Top-Level Command-Line Options rsync - Synchronize content of two buckets/directories  ","permalink":"https://shunyaueta.com/posts/2021-10-04/","summary":"Cloud Composer(Airflow) の DAG を GitHub リポジトリで管理して、CI によりリポジトリで管理している DAG を Pull Request がマージされると Cloud Composer の DAG へ同期する方法について説明する。\nDAG は、ルートディレクトリ直下の dags/ というディレクトリに格納されている状態を前提とする。\n以下の２つのコマンドラインツールを利用して実現できる。\n Service Account の認証のために gcloud DAG の同期のために gsutil  CircleCI によるワークフローの記述例は以下のとおり\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  version:2.1jobs:rsync-dags:working_directory:~/workspacedocker:- image:gcr.io/google.com/cloudsdktool/cloud-sdk:alpineenvironment:GOOGLE_APPLICATION_CREDENTIALS:/gcp-service-key.jsonsteps:- checkout- run:name:SyncDAGfoldertoGCS\u0026#39;sDAGfoldercommand:| echo \u0026#34;${CLOUD_COMPOSER_CREDENTIALS_JSON}\u0026#34; \u0026gt; ${GOOGLE_APPLICATION_CREDENTIALS}gcloudauthactivate-service-account--key-file${GOOGLE_APPLICATION_CREDENTIALS}gsutil-mrsync-d-rdags\\\u0026#34;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=\u0026#34;get(config.","title":"CloudComposer のDAGをCircleCIで更新する"},{"content":"TL;DR;  Pull Requestのコミット履歴を汚さずにCircleCIのConfigmap をWeb上で編集して、CIの挙動をすぐ試せる機能がすごく便利  課題点 CircleCI の上のCIの挙動のデバッグをする際、ブランチにプッシュをせずに挙動が確認できる方法としてローカルCLIを利用する方法がある。 しかし、CIのマシン上で定義されている環境変数などは、ローカルCLIを使用しても確認することができない。\nそのためコミットでCIが失敗している際には、\n Rerun Job with SSHを利用してSSHで接続して、環境変数を確認 再度 config.yml を修正して、コミットをプッシュ だが、上手くいかないので1→2を繰り返す  を繰り返してしまい、コミット履歴が不用意に汚れてしまう。\n解決方法 この問題点を解決する方法として、CircleCI アプリ内の設定ファイル エディターを利用するのがすごく便利だった。\nこの機能は、ジョブのページの右上からアクセスできる。 Rerun ボタンの右に... ボタンがあり、そのボタンをクリックすると\n Project Settings Configuration File  の選択項目があり、Configuration File をクリックすると、config.yml のウェブエディターが起動する。 エディターでYAMLファイルを編集後、右上のSave and Runボタンをクリックすれば、PRで作成されているブランチと別のリモートブランチがCircleCIによって新たに作成されるので、もとのPRのコミット履歴を汚さずにCIの問題を修正できる。\nReference  CircleCI アプリ内の設定ファイル エディターの使用  ","permalink":"https://shunyaueta.com/posts/2021-10-01/","summary":"TL;DR;  Pull Requestのコミット履歴を汚さずにCircleCIのConfigmap をWeb上で編集して、CIの挙動をすぐ試せる機能がすごく便利  課題点 CircleCI の上のCIの挙動のデバッグをする際、ブランチにプッシュをせずに挙動が確認できる方法としてローカルCLIを利用する方法がある。 しかし、CIのマシン上で定義されている環境変数などは、ローカルCLIを使用しても確認することができない。\nそのためコミットでCIが失敗している際には、\n Rerun Job with SSHを利用してSSHで接続して、環境変数を確認 再度 config.yml を修正して、コミットをプッシュ だが、上手くいかないので1→2を繰り返す  を繰り返してしまい、コミット履歴が不用意に汚れてしまう。\n解決方法 この問題点を解決する方法として、CircleCI アプリ内の設定ファイル エディターを利用するのがすごく便利だった。\nこの機能は、ジョブのページの右上からアクセスできる。 Rerun ボタンの右に... ボタンがあり、そのボタンをクリックすると\n Project Settings Configuration File  の選択項目があり、Configuration File をクリックすると、config.yml のウェブエディターが起動する。 エディターでYAMLファイルを編集後、右上のSave and Runボタンをクリックすれば、PRで作成されているブランチと別のリモートブランチがCircleCIによって新たに作成されるので、もとのPRのコミット履歴を汚さずにCIの問題を修正できる。\nReference  CircleCI アプリ内の設定ファイル エディターの使用  ","title":"CircleCI アプリ内の設定ファイルエディターを利用して、CI上の環境変数などローカルCLIでは確認できない挙動を素早く確認して修正する"},{"content":"GCPでAirflow をマネージドサービスで使えるサービスで Cloud Composer が存在する。 BigQueryやBigTable, PubSub などGCPの各サービスをDAGとして定義してジョブを定期実行できるので非常に便利だが、その代わりDAGを実行するまで結果がわからないので、CloudComposer を一度実行するしか無いのでデバッグが困難になる傾向がある。\nまた、GitHubのリポジトリにDAGを保存して、CIでCloud Composerを更新するようしていると PRを都度作ってマージされないと確認できないという場合もある。\nローカルでDocker で走らせれば良いのじゃないかというツッコミがあると思いますが、結局 Cloud Composer 上での動作を確かめないといけないので、今回の記事を書くことにしました。\nNOTE: 自分が使用しているComposerのversionはcomposer-1.8.0-Airflow-1.10.3 です。基本的にやれることは一緒だと思います。また、dev, prodで同一のDAGが走るCloud Composer を運用しているという前提です。\nアプローチは２つ\nlogger.info() を仕込んで、DAGのなかで何が起こっているかを理解する 1 2 3 4 5  import logging logger = logging.getLogger(__name__) logger.info()   loggerをDAGを記述した Pythonファイルに仕込んで、内部で何が起こっているかを把握する。\n各DAGのlogは、\n GCPのCloud ComposerのページにアクセスしてAirflow webserver 列のボタンをクリックしてAirflowのWeb applicaiton にログイン 確認したいDAGをクリック DAG内のtask をクリックして表示されるモーダル内の View Logをクリックすると、loggerの情報が確認できる  gstuil rsync コマンドでのGCSへのDAGの同期 gstuil rsyncコマンドを使うことで、リポジトリのDAGファイルをGCSに格納されている開発環境上のCloudComposer のDAGファイルに直接同期してPull Request マージ後のDAGの挙動を確認できる。\nCloud Composer のDAGは、自動作成されたGoogle Cloud Storage(GCS)に格納されており、GCSをCloud Composerが定期的に監視してCloud Composerを更新している。 つまり、GCS上のDAGファイルを直接更新してやるとそれがCloud Composerに反映される。 体感として2-3分に一度は監視されているので、ほぼ待ち状態がない状態で確認できて便利です。\n以下のコマンドでリポジトリのDAGファイルをGCSに反映させます。\n1  gsutil -m rsync -d -r dags \u0026#34;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=\u0026#34;get(config.dagGcsPrefix)\u0026#34;)\u0026#34;   {XXX} には使用する環境の情報を置換してください。\n \u0026quot;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=\u0026quot;get(config.dagGcsPrefix)\u0026quot;)\u0026quot;  指定したGCP Project で動くCloud Composer のDAGが格納されているGCSのパスを取得できる。   gsutil -m rsync -d -r dags  -m は並列処理 -d は元のディレクトリに存在しないファイルがコピー先にあれば削除(ミラーリング)。これにより、GCS上でDAGを新たに作成して、デバッグしていたとしても、CIが走ればリポジトリにないDAGファイルは削除され、リポジトリのDAGと完全に同期される。 -r はディレクトリとしてコピー 上記のオプションによりdags ディレクトリのDAGファイルをGCSにミラーリングで同期を行う。    Composer のための正規のコマンドはあるが\u0026hellip; また、以下のように gcloud composer environments storage dags import コマンドで更新する方法もあるので、そちらを使っても大丈夫です。 実行内容自体はGCSのファイルを変更するのと変わりません。 ですが、ディレクトリを対象にしたファイルの同期には対応していないので、上記で説明したコマンドのほうが遥かに楽です。\n1 2 3 4  gcloud composer environments storage dags import \\  --environment {ENVIRONMENT_NAME} \\  --location {LOCATION} \\  --source {LOCAL_FILE_TO_UPLOAD}   NOTE: プロダクションのDAGを直接書き換えるのは危険なのでやめましょう。\nReference  Adding and Updating DAGs (workflows) gsutil Top-Level Command-Line Options rsync - Synchronize content of two buckets/directories  ","permalink":"https://shunyaueta.com/posts/2021-09-29/","summary":"GCPでAirflow をマネージドサービスで使えるサービスで Cloud Composer が存在する。 BigQueryやBigTable, PubSub などGCPの各サービスをDAGとして定義してジョブを定期実行できるので非常に便利だが、その代わりDAGを実行するまで結果がわからないので、CloudComposer を一度実行するしか無いのでデバッグが困難になる傾向がある。\nまた、GitHubのリポジトリにDAGを保存して、CIでCloud Composerを更新するようしていると PRを都度作ってマージされないと確認できないという場合もある。\nローカルでDocker で走らせれば良いのじゃないかというツッコミがあると思いますが、結局 Cloud Composer 上での動作を確かめないといけないので、今回の記事を書くことにしました。\nNOTE: 自分が使用しているComposerのversionはcomposer-1.8.0-Airflow-1.10.3 です。基本的にやれることは一緒だと思います。また、dev, prodで同一のDAGが走るCloud Composer を運用しているという前提です。\nアプローチは２つ\nlogger.info() を仕込んで、DAGのなかで何が起こっているかを理解する 1 2 3 4 5  import logging logger = logging.getLogger(__name__) logger.info()   loggerをDAGを記述した Pythonファイルに仕込んで、内部で何が起こっているかを把握する。\n各DAGのlogは、\n GCPのCloud ComposerのページにアクセスしてAirflow webserver 列のボタンをクリックしてAirflowのWeb applicaiton にログイン 確認したいDAGをクリック DAG内のtask をクリックして表示されるモーダル内の View Logをクリックすると、loggerの情報が確認できる  gstuil rsync コマンドでのGCSへのDAGの同期 gstuil rsyncコマンドを使うことで、リポジトリのDAGファイルをGCSに格納されている開発環境上のCloudComposer のDAGファイルに直接同期してPull Request マージ後のDAGの挙動を確認できる。\nCloud Composer のDAGは、自動作成されたGoogle Cloud Storage(GCS)に格納されており、GCSをCloud Composerが定期的に監視してCloud Composerを更新している。 つまり、GCS上のDAGファイルを直接更新してやるとそれがCloud Composerに反映される。 体感として2-3分に一度は監視されているので、ほぼ待ち状態がない状態で確認できて便利です。","title":"GCPのCloud Composer のDAGを素早く・簡単にデバッグする"},{"content":"先日の記事 では、pandoc の--extract-media オプションをオンにしても word 内部の画像を抽出することができなかった。\nだが、Google Drive を使うことで Word 内部の画像を抽出することができたのでここに記しておく。\n 対象の Word ファイルを Google Drive にアップロードする そのファイルを Google Docs で開く File → Download → Web Page (.html, zippted) でウェブページとして zip ファイルをダウンロードする zip ファイルを解凍後、その中にあるimages フォルダに Word 内部の画像が格納されている  ","permalink":"https://shunyaueta.com/posts/2021-09-27/","summary":"先日の記事 では、pandoc の--extract-media オプションをオンにしても word 内部の画像を抽出することができなかった。\nだが、Google Drive を使うことで Word 内部の画像を抽出することができたのでここに記しておく。\n 対象の Word ファイルを Google Drive にアップロードする そのファイルを Google Docs で開く File → Download → Web Page (.html, zippted) でウェブページとして zip ファイルをダウンロードする zip ファイルを解凍後、その中にあるimages フォルダに Word 内部の画像が格納されている  ","title":"Pandoc を使って抽出できなかったWord内部の画像をGoogle Driveを使って抽出する"},{"content":"表題の通り、Pandoc を使って、特定ディレクトリ配下にある複数の Wordファイル(*.docx) を Markdownファイル(*.md) へ一括変換したい。\n単一Wordファイルの変換コマンド 単一の変換である場合は、@tomo-makes さんのWordファイル(.docx)をMarkdownへ変換する を参考に実行すると良いと思います。\n自分は特に困らなかったので、despan の処理は省いた形にしました。 また、--extract-media をオンにして指定しても Wordファイル内の画像を上手く抜き出せなかったです。 WordファイルからMarkdownファイルへの完全変換って難しい。まさに餅をもち米に戻す行為に近い\u0026hellip;\n1  pandoc -s {input}.docx --wrap=none -t gfm -o {output}.md   複数Wordファイルの変換コマンド ワンライナーのシェルスクリプトを組んで実行する。 実行時には、変換元のWordファイルが配置されているディレクトリで実行する。\n1  for f in *.docx; do pandoc -s \u0026#34;$f\u0026#34; --wrap=none -t gfm -o \u0026#34;${f}.md\u0026#34;; done   \u0026quot;${f}.md\u0026quot; の部分を \u0026quot;../../docs/${f}.md\u0026quot; のような形で修正してやれば、所定のディレクトリへ変換されたMarkdownファイルが生成される。\nReference  How can I convert a whole directory of files from Markdown to RTF?  ","permalink":"https://shunyaueta.com/posts/2021-09-19/","summary":"表題の通り、Pandoc を使って、特定ディレクトリ配下にある複数の Wordファイル(*.docx) を Markdownファイル(*.md) へ一括変換したい。\n単一Wordファイルの変換コマンド 単一の変換である場合は、@tomo-makes さんのWordファイル(.docx)をMarkdownへ変換する を参考に実行すると良いと思います。\n自分は特に困らなかったので、despan の処理は省いた形にしました。 また、--extract-media をオンにして指定しても Wordファイル内の画像を上手く抜き出せなかったです。 WordファイルからMarkdownファイルへの完全変換って難しい。まさに餅をもち米に戻す行為に近い\u0026hellip;\n1  pandoc -s {input}.docx --wrap=none -t gfm -o {output}.md   複数Wordファイルの変換コマンド ワンライナーのシェルスクリプトを組んで実行する。 実行時には、変換元のWordファイルが配置されているディレクトリで実行する。\n1  for f in *.docx; do pandoc -s \u0026#34;$f\u0026#34; --wrap=none -t gfm -o \u0026#34;${f}.md\u0026#34;; done   \u0026quot;${f}.md\u0026quot; の部分を \u0026quot;../../docs/${f}.md\u0026quot; のような形で修正してやれば、所定のディレクトリへ変換されたMarkdownファイルが生成される。\nReference  How can I convert a whole directory of files from Markdown to RTF?  ","title":"Pandoc で特定のディレクトリ直下にある複数のWordをMarkdown形式に一括変換する"},{"content":"gcloud commands で PubSub に json ファイルをメッセージとして公開 (Pusblish) する\njq コマンドが必要になるが、一番簡単に実現できるのは\n1  $ gcloud pubsub topics publish {PUBSUB_TOPIC_NAME} --message \u0026#34;$(cat {FILE_NAME} | jq -c)\u0026#34;   jq コマンドの -c オプションは compact-output を意味している。デフォルトだと pretty-prints になってしまう。 それを避けるために-cオプションを使用している。\nref  Publishing messages to topics Read a txt file JSON data to publish the messages in Cloud Pub Sub  ","permalink":"https://shunyaueta.com/posts/2021-09-07/","summary":"gcloud commands で PubSub に json ファイルをメッセージとして公開 (Pusblish) する\njq コマンドが必要になるが、一番簡単に実現できるのは\n1  $ gcloud pubsub topics publish {PUBSUB_TOPIC_NAME} --message \u0026#34;$(cat {FILE_NAME} | jq -c)\u0026#34;   jq コマンドの -c オプションは compact-output を意味している。デフォルトだと pretty-prints になってしまう。 それを避けるために-cオプションを使用している。\nref  Publishing messages to topics Read a txt file JSON data to publish the messages in Cloud Pub Sub  ","title":"gcloud commands で PubSub に jsonファイルをメッセージとして公開 (Pusblish) する"},{"content":"evans\nevans は対象のサーバーの gRPC のリフレクション機能が起動されていれば、proto ファイルを参照せずに便利な [REPL mode](gRPC のリフレクション機能) を使用できます。\n If your server is enabling gRPC reflection, you can launch Evans with only -r (\u0026ndash;reflection) option.\n gRPC のリフレクション機能については evans 作者の ktr0731 さんが解説している記事が非常にわかりやすいです。\ngRPC リフレクションはなにをしているか？\nローカルの 5000 番のポートをリモートサーバの 5000 番ポートにフォワード (port-forward)しているとします。 例えば、kubectl だと以下のような実行コマンドになります。\nForward a local port to a port on the Pod\n1  kubectl port-forward pods/hoge-asas32s 5000:5000   そして、ポートフォワードのシェルは保持した上で、別にシェルを起動します。\nこの際に 対象となるlocalhost:5000 に対して、--host, --port オプションで指定してやれば evans の REPL モードが起動します。\nREPL 1 2 3 4 5 6 7 8 9 10 11  \u0026gt; evans -r --host localhost --port 5000 ______ | ____| | |__ __ __ __ _ _ __ ___ | __| \\ \\ / / / _. | | \u0026#39;_ \\  / __| | |____ \\ V / | (_| | | | | | \\__ \\  |______| \\_/ \\__,_| |_| |_| |___/ more expressive universal gRPC client   CLI CLI は cli というサブコマンドを追記するだけで起動できます。 シェル変数などを使って実行したい場合は、こちらが便利です。\n1  evans -r --host localhost --port 5000 cli list   evans で快適な gRPC ライフを楽しみましょう。\n","permalink":"https://shunyaueta.com/posts/2021-08-19/","summary":"evans\nevans は対象のサーバーの gRPC のリフレクション機能が起動されていれば、proto ファイルを参照せずに便利な [REPL mode](gRPC のリフレクション機能) を使用できます。\n If your server is enabling gRPC reflection, you can launch Evans with only -r (\u0026ndash;reflection) option.\n gRPC のリフレクション機能については evans 作者の ktr0731 さんが解説している記事が非常にわかりやすいです。\ngRPC リフレクションはなにをしているか？\nローカルの 5000 番のポートをリモートサーバの 5000 番ポートにフォワード (port-forward)しているとします。 例えば、kubectl だと以下のような実行コマンドになります。\nForward a local port to a port on the Pod\n1  kubectl port-forward pods/hoge-asas32s 5000:5000   そして、ポートフォワードのシェルは保持した上で、別にシェルを起動します。\nこの際に 対象となるlocalhost:5000 に対して、--host, --port オプションで指定してやれば evans の REPL モードが起動します。","title":"gRPC client evans で portforward 先のリモートサーバーにリクエストを行う"},{"content":"内容 システム提供において、基本的に高速であればあるほど顧客は嬉しいものだが、実際のところ高速なシステムを提供して、どの程度の価値が発生するのかが気になったので、調べてみた。\n 2021/08/14 追記  A/Bテスト実践ガイド　真のデータドリブンへ至る信用できる実験とは の書籍で同様な事例が紹介されているとのこと。情報提供ありがとうございます。     実務でA/Bテストに向き合った人間であれば必ず一度は考えたことのあるトピックについて、アメリカのテックカンパニー（Airbnb, Google, LinkedInなど）勤務の著者らが国際会議で発表された研究もちゃんと引用して見解を述べており説得力がある。 従って、現時点における最高レベルの意思決定をデータ（A/Bテスト）に基づいて行いたいと思うなら、一度は目を通しておくべきであり関係者必携だと思う。 ※個人的には”Webサービスのレイテンシーと利益の関係（５章や”多くのスピード問題”の節）”がお気に入りで、サイトのレイテンシー改善がいかに収益に貢献し得るか、つまりCodeの実行速度というエンジニアのアウトプットがダイレクトに収益に貢献できるか？をデータに基づいてきちんと測っているのが印象的で興味深かった内容でした。 Amazon review\n Three Challenges in Building Industrial-Scale Recommender Systems\u0026rdquo; - Keynote for ORSUM@RecSys\u0026rsquo;20 3rd Workshop on Online Recommender Systems and User Modeling でのkeynote session で発表された内容\n講演者は Sebastian Schelter さんという方で、アカデミックもインダストリーもどちらもバリバリにこなしている人だった。日本だとこういう経歴の人ってかなり珍しい気がするので、やはり層が厚い\nふと@hagino3000 さんのツイートが印象に残っていたので、記録のためにこちらに。1年くらい前のやり取りだけど、印象に残っていて今回この記事を書いたきっかけでもある。\n 推薦システムのレイテンシが15msと32msで差が出るかA/B Testしたって。推薦結果は同じで片方はあえて遅らせたって事だよな、はじめて聴く実験だ。15msの方がrevenueが良かったとの事。 twitter\n 公開されている動画はこちら\nThree Challenges in Building Industrial-Scale Recommender Systems\u0026rdquo; - Keynote for ORSUM@RecSys\u0026rsquo;20\n19,20枚目のスライド\n要約すると、\n既存の研究では、検索エンジン上で人工的に応答速度を遅らせた際にネガティブな影響が発生した。\nでは、逆に応答速度を早めた場合どのような影響になるのだろうか? とてもおもしろい事例があるので是非紹介したい、\nオンプレのシステムからGoogle Cloud に移行するイベントを利用した実験を行った。マイグレーション時にサービングシステムの最適化などを行い、マイグレーション後のシステム性能向上した。この最適化により、モデルやシステム構成は全く同じだが、p90 の応答速度がオンプレのシステムでは 32ms だったものが、GCPでは15ms に向上した。 これにより生じた差異を活用して、以下のA/B テストを行った。 32ms をcontroll, 15ms をtest 群に分けてA.B テストを行ったところ、商品のオーダー、収益に関する指標が２% 向上した。\nGoogle による Speed Matters 実験 Sebastian さんが上記の講演で名前を出して言及はしなかったが、言及していた実験はGoogle の Speed Matters 実験だと思う。(こんな実験をわざわざするケースが他にもあったら教えてほしい)\nその名の通り、速度は重要という実直なタイトルの実験である。\nSpped Matters\n1pのnews letter 形式でまとめられた実験結果だが非常に面白い\n実験内容としてはシンプルで、検索エンジンにリクエストを投げる際に、\n Pre-header: HTTP GET リクエストを受信後、すべてのサーバー処理を X mx の期間一時停止 Post-header: ヘッダー送信後サーバーの処理が完了する前に X mxの期間すべてのサーバー処理を一時停止 Post-ads: 広告を表示した後に X mxの期間のみ一すべてのサーバー処理を一時停止  各アプローチの応答時間の停止時間で、日次の検索実行を行う人数がどれくらい減少したかがまとめられている。\nどれも明確なインパクトが発生しているが、\nわかりやすいものを抜き出すと、Post-headerでの時間を\n 200ms 遅らせたものが 、6週間で毎日の平均検索実行者の数が -0.29% 減少 400ms 遅らせたものが 、6週間で毎日の平均検索実行者の数が -0.59% 減少  また、6週間の時間をかけて以下の結果が得られた。\nグラフでは、 6週間 Post-header の時間を、200ms, 400ms 遅らせた際に検索実行回数がどのように変化するかが示されている。\n検索エンジンの応答速度が遅くなることで、遅延される時間が長いものほど、その遅延の影響により、検索を行う顧客は、徐々に検索を行わなくなっていった。\n-0.5% と聞くとそんなに大きな数字ではなく感じるが、Google 検索の規模での -0.5% は決して小さいものではない。\n結論 速さには価値がある。それをちゃんと定量的に示した人たちがいるのは偉大。\nこういうみんなが直感的にそうだろうと感じてはいるものの、「じゃあ実際どれくらい良いの?」という問いにたいして、実験を実行した人たち、その実験が許される環境は本当に素晴らしい。\n","permalink":"https://shunyaueta.com/posts/2021-08-13/","summary":"内容 システム提供において、基本的に高速であればあるほど顧客は嬉しいものだが、実際のところ高速なシステムを提供して、どの程度の価値が発生するのかが気になったので、調べてみた。\n 2021/08/14 追記  A/Bテスト実践ガイド　真のデータドリブンへ至る信用できる実験とは の書籍で同様な事例が紹介されているとのこと。情報提供ありがとうございます。     実務でA/Bテストに向き合った人間であれば必ず一度は考えたことのあるトピックについて、アメリカのテックカンパニー（Airbnb, Google, LinkedInなど）勤務の著者らが国際会議で発表された研究もちゃんと引用して見解を述べており説得力がある。 従って、現時点における最高レベルの意思決定をデータ（A/Bテスト）に基づいて行いたいと思うなら、一度は目を通しておくべきであり関係者必携だと思う。 ※個人的には”Webサービスのレイテンシーと利益の関係（５章や”多くのスピード問題”の節）”がお気に入りで、サイトのレイテンシー改善がいかに収益に貢献し得るか、つまりCodeの実行速度というエンジニアのアウトプットがダイレクトに収益に貢献できるか？をデータに基づいてきちんと測っているのが印象的で興味深かった内容でした。 Amazon review\n Three Challenges in Building Industrial-Scale Recommender Systems\u0026rdquo; - Keynote for ORSUM@RecSys\u0026rsquo;20 3rd Workshop on Online Recommender Systems and User Modeling でのkeynote session で発表された内容\n講演者は Sebastian Schelter さんという方で、アカデミックもインダストリーもどちらもバリバリにこなしている人だった。日本だとこういう経歴の人ってかなり珍しい気がするので、やはり層が厚い\nふと@hagino3000 さんのツイートが印象に残っていたので、記録のためにこちらに。1年くらい前のやり取りだけど、印象に残っていて今回この記事を書いたきっかけでもある。\n 推薦システムのレイテンシが15msと32msで差が出るかA/B Testしたって。推薦結果は同じで片方はあえて遅らせたって事だよな、はじめて聴く実験だ。15msの方がrevenueが良かったとの事。 twitter\n 公開されている動画はこちら\nThree Challenges in Building Industrial-Scale Recommender Systems\u0026rdquo; - Keynote for ORSUM@RecSys\u0026rsquo;20\n19,20枚目のスライド\n要約すると、\n既存の研究では、検索エンジン上で人工的に応答速度を遅らせた際にネガティブな影響が発生した。","title":"システムの応答速度は本質的な価値提供であることを示す A/B テストの実例"},{"content":"去年の秋頃に子供を授かり、色々と役に立った情報や製品、サービスなどがあったので備忘録がてら残しておく。 ベビーウォール、プレイマット、服や抱っこひもなど鉄板系で買えば良いものなどは記す必要が無いのであえて書いてない。\n書籍・情報源  ★★★ 赤ちゃん寝かしつけの新常識  科学的な見地に基づいた、赤ちゃんの睡眠に関する情報を纏めた書籍。データやメタ分析、また著者の経験に基づいて、どう寝かしつけに取り組めばいいのか説明してくれている。   ★★ 小児科医のママが教える 離乳食は作らなくてもいいんです。  簡単にまとめると、離乳食をわざわざ作るのではなく販売されている商品で離乳食をカバーしたほうが栄養素、準備に係る労力もなくなってみんな幸せという書籍   ★ 厚生労働省が公開している情報  ac.jp, go.jp の資料の信憑性は高いので、優先して閲覧するようにしている。   ★ 赤ちゃんとママ・パパのための情報 by 花王  花王がお医者さんと連携して作成している F\u0026amp;Q サイト。    モノ  ★★★ 食洗機  哺乳瓶を電子レンジで殺菌するものもあるが、こちらのほうが食器も合わせて洗えるのでめちゃくちゃ楽。変にやけどする心配もない。哺乳瓶を 8 本程度買っておいて、食洗機で毎日 4 本程度洗っていく運用が非常に楽だった。   ★★★ 防水敷き布団カバー  子供がよる咳がひどくて飲んだミルクを吐いてしまっている場合があるのだが、その際に布団をすべて洗う必要がなくなるので非常に助かる。自分は西松屋で買ったけど、Amazon でも打っているので紹介   ★★★ iHerb で購入する栄養満点ベビーフード  Gerber のライスシリアル など、日本の離乳食と比較して味付けが素材そのままの味で、必須栄養素が添加されている Gerber を愛用して買っている。離乳食にミルクと混ぜて会えると簡単に離乳食が作れるので非常に便利。小児科医のママが教える 離乳食は作らなくてもいいんです。で Gerber の存在を知れた。   ★★★ スワドルアップ  2-5 ヶ月ごろまで愛用していた。抱っこしている状態から、ベッドに置くと一種で起きて泣き出す確率が 9 割から 1 割程度に減る神の道具。途中で起き出すこともかなり少なくなった。   ★★★ ニトリの引っ張るだけで取り込めるハンガー  わざわざベビーハンガーなどを買う必要は無い。引っ張って回収できるのが便利。Amazon でも同様のものが売られている   ★★★ Google Home Mini  ホワイトノイズを再生するのに使っている。「OK Google ホワイトノイズを流して」で 11 時間程度連続再生してくれるので子供の就寝時間中ほぼカバーできる。 赤ちゃん寝かしつけの新常識 で紹介されていたので導入してみたが、寝ている部屋の近くで音を立ててしまったり、一緒の部屋で寝ていて自分のベッドの軋む音で泣かなくなったので非常にありがたい。Alexa とかでも同等の機能はあるんじゃないのでしょうか。昼寝のときもちょっとした音で起きなくなるのでありがたし  2022-04-26: 追記: 子供の就寝時に使っているホワイトノイズマシンを Google Home から Dreamegg に変更     ★★ Xiaomi Mi スマートバンド  子供が深夜に起きた際にスマホで時計を確認すると光が強すぎるので、適切な光量で確認できる。また、料理しているときにタイマーとしても秀逸です。アラームも振動で起床できるので他の人を起こさずに起床できる点も秀逸。何よりも安いので気軽に買えるのが良い   ★★ クリップライト  赤ちゃん寝かしつけの新常識でも紹介されている、レッドライトを買うと高いので、クリップライトを買って、百均で買った赤色の透明の下敷きを当ててレッドライトを即席で作成したがなんの問題もなく使えている。実際に深夜に起きて作業をしていると赤色のライトだと眩しいという感覚が非常に和らいでいる気がする   ★ オムツ替え防水シート  おむつ交換時にいきなりおしっこが発射されることもあるので、防水シートを買った。購入後いざという時何度も助かったので便利    サービス・アプリ  ★★★ ぴよログ  睡眠やミルク、排泄の回数などを夫婦間で共有して管理できる。細かな使い勝手が洗練されていて感動するレベル。できた 🚩 という項目があり、これをこまめに付けておくと見返すときにニヤつきながら成長を振り替えれる。   ★★★ メルカリ  元値が数千円台の子供服やおもちゃを 300 円、送料込みで買える。数十着は買ったのではなかろうか。ここでしか見つからないようなかわいい服も多くて、見てての楽しい   ★★ ベビーカレンダー  このアプリは仕組みが面白い。こどもの生後の日数に合わせて、生後 N 日だと~ができるようになりますとか、こういうことをこころがけましょうと日めくり的に毎日記事が見れて、書籍を一度にまとめて読むよりも今必要なことを適宜教えてくれる感じで助かった。   ★★ ジモティー  4 万円相当のベビーベッドを無料でいただけた。車で 2 時間程度の場所だったが、旅行がてら受け取りに行った。感謝。   ★ とりあえず登録しておくといいサービス  楽天ママ割 Amazon ファミリー   ★ Amazon、ベビー用品クーポン  おむつやおしりふきなど時々割引クーポンで、ドラッグストアより安くなるときがあり、そのときに購入している。通常時はドラッグストアのほうがやすいので使い分けている おむつカテゴリ おしりふき    まとめ 可能な限り効率化できるような、仕組みづくりに投資したほうが幸せになれる。あと体力が一番大事、健康が最強!!\n","permalink":"https://shunyaueta.com/posts/2021-07-23/","summary":"去年の秋頃に子供を授かり、色々と役に立った情報や製品、サービスなどがあったので備忘録がてら残しておく。 ベビーウォール、プレイマット、服や抱っこひもなど鉄板系で買えば良いものなどは記す必要が無いのであえて書いてない。\n書籍・情報源  ★★★ 赤ちゃん寝かしつけの新常識  科学的な見地に基づいた、赤ちゃんの睡眠に関する情報を纏めた書籍。データやメタ分析、また著者の経験に基づいて、どう寝かしつけに取り組めばいいのか説明してくれている。   ★★ 小児科医のママが教える 離乳食は作らなくてもいいんです。  簡単にまとめると、離乳食をわざわざ作るのではなく販売されている商品で離乳食をカバーしたほうが栄養素、準備に係る労力もなくなってみんな幸せという書籍   ★ 厚生労働省が公開している情報  ac.jp, go.jp の資料の信憑性は高いので、優先して閲覧するようにしている。   ★ 赤ちゃんとママ・パパのための情報 by 花王  花王がお医者さんと連携して作成している F\u0026amp;Q サイト。    モノ  ★★★ 食洗機  哺乳瓶を電子レンジで殺菌するものもあるが、こちらのほうが食器も合わせて洗えるのでめちゃくちゃ楽。変にやけどする心配もない。哺乳瓶を 8 本程度買っておいて、食洗機で毎日 4 本程度洗っていく運用が非常に楽だった。   ★★★ 防水敷き布団カバー  子供がよる咳がひどくて飲んだミルクを吐いてしまっている場合があるのだが、その際に布団をすべて洗う必要がなくなるので非常に助かる。自分は西松屋で買ったけど、Amazon でも打っているので紹介   ★★★ iHerb で購入する栄養満点ベビーフード  Gerber のライスシリアル など、日本の離乳食と比較して味付けが素材そのままの味で、必須栄養素が添加されている Gerber を愛用して買っている。離乳食にミルクと混ぜて会えると簡単に離乳食が作れるので非常に便利。小児科医のママが教える 離乳食は作らなくてもいいんです。で Gerber の存在を知れた。   ★★★ スワドルアップ  2-5 ヶ月ごろまで愛用していた。抱っこしている状態から、ベッドに置くと一種で起きて泣き出す確率が 9 割から 1 割程度に減る神の道具。途中で起き出すこともかなり少なくなった。   ★★★ ニトリの引っ張るだけで取り込めるハンガー  わざわざベビーハンガーなどを買う必要は無い。引っ張って回収できるのが便利。Amazon でも同様のものが売られている   ★★★ Google Home Mini  ホワイトノイズを再生するのに使っている。「OK Google ホワイトノイズを流して」で 11 時間程度連続再生してくれるので子供の就寝時間中ほぼカバーできる。 赤ちゃん寝かしつけの新常識 で紹介されていたので導入してみたが、寝ている部屋の近くで音を立ててしまったり、一緒の部屋で寝ていて自分のベッドの軋む音で泣かなくなったので非常にありがたい。Alexa とかでも同等の機能はあるんじゃないのでしょうか。昼寝のときもちょっとした音で起きなくなるのでありがたし  2022-04-26: 追記: 子供の就寝時に使っているホワイトノイズマシンを Google Home から Dreamegg に変更     ★★ Xiaomi Mi スマートバンド  子供が深夜に起きた際にスマホで時計を確認すると光が強すぎるので、適切な光量で確認できる。また、料理しているときにタイマーとしても秀逸です。アラームも振動で起床できるので他の人を起こさずに起床できる点も秀逸。何よりも安いので気軽に買えるのが良い   ★★ クリップライト  赤ちゃん寝かしつけの新常識でも紹介されている、レッドライトを買うと高いので、クリップライトを買って、百均で買った赤色の透明の下敷きを当ててレッドライトを即席で作成したがなんの問題もなく使えている。実際に深夜に起きて作業をしていると赤色のライトだと眩しいという感覚が非常に和らいでいる気がする   ★ オムツ替え防水シート  おむつ交換時にいきなりおしっこが発射されることもあるので、防水シートを買った。購入後いざという時何度も助かったので便利    サービス・アプリ  ★★★ ぴよログ  睡眠やミルク、排泄の回数などを夫婦間で共有して管理できる。細かな使い勝手が洗練されていて感動するレベル。できた 🚩 という項目があり、これをこまめに付けておくと見返すときにニヤつきながら成長を振り替えれる。   ★★★ メルカリ  元値が数千円台の子供服やおもちゃを 300 円、送料込みで買える。数十着は買ったのではなかろうか。ここでしか見つからないようなかわいい服も多くて、見てての楽しい   ★★ ベビーカレンダー  このアプリは仕組みが面白い。こどもの生後の日数に合わせて、生後 N 日だと~ができるようになりますとか、こういうことをこころがけましょうと日めくり的に毎日記事が見れて、書籍を一度にまとめて読むよりも今必要なことを適宜教えてくれる感じで助かった。   ★★ ジモティー  4 万円相当のベビーベッドを無料でいただけた。車で 2 時間程度の場所だったが、旅行がてら受け取りに行った。感謝。   ★ とりあえず登録しておくといいサービス  楽天ママ割 Amazon ファミリー   ★ Amazon、ベビー用品クーポン  おむつやおしりふきなど時々割引クーポンで、ドラッグストアより安くなるときがあり、そのときに購入している。通常時はドラッグストアのほうがやすいので使い分けている おむつカテゴリ おしりふき    まとめ 可能な限り効率化できるような、仕組みづくりに投資したほうが幸せになれる。あと体力が一番大事、健康が最強!","title":"子供が1歳児を迎えるまでに、育児で役に立ったもの"},{"content":"最近、Java を業務で触っている。 門外漢の自分からすると Maven のお作法が分からなかったので、備忘録がてら残しておく。\nmvn archetype:generate コマンドのオプションの意味 mvn archetype:generate コマンドを使えば任意のテンプレートに沿ったプロジェクトを一発で作成することができる。\n具体例として、Apache Beam でプロジェクト管理ツールである Maven を使って、mvn archetype:generate コマンドを用いて、プロジェクト作成を行う場合、公式サイトでは以下のように指定されている\n1 2 3 4 5 6 7 8 9  $ mvn archetype:generate \\ -DarchetypeGroupId=org.apache.beam \\ -DarchetypeArtifactId=beam-sdks-java-maven-archetypes-examples \\ -DarchetypeVersion=2.31.0 \\ -DgroupId=org.example \\ -DartifactId=word-count-beam \\ -Dversion=\u0026#34;0.1\u0026#34; \\ -Dpackage=org.apache.beam.examples \\ -DinteractiveMode=false      オプション名 意味     archetypeGroupId archetypeの groupId つまり、テンプレートを提供している作成元の識別子   archetypeArtifactId archetype のテンプレート。個々では beam-sdk に対応したプロジェクトテンプレートを作成している。   archetypeVersion テンプレートのバージョン   groupId Java のパッケージ名のルールに則ったすべてのプロジェクトで唯一に識別可能な識別子。今回は org.example が使われており、実際は識別子として機能指定なさそうな名前ではある。(実際昔のプロジェクトでは重複可能な単語が使われていることが多いが、その場合は marven に登録する際に名前が衝突して登録ができないとのこと   artifactId 任意の名前が使用可能であり、jar ファイルのバージョン抜きの名前を指定する。プロジェクトのパッケージ名と考えたら良さそう。これが作成されるロートディレクトリのフォルダ名   version プロジェクトのバージョン情報   package クラスやインターフェースの名前空間を指す。基本的に groupid と同一だが、groupid を接頭辞にして、独自に付け足すこともある。   interactiveMode ウィザード形式の生成をするかしないか    Reference  Guide to naming conventions on groupId, artifactId, and version maven プロジェクトの作成 archtypeArtifactId を指定する package 宣言 | java-code  ","permalink":"https://shunyaueta.com/posts/2021-07-18/","summary":"最近、Java を業務で触っている。 門外漢の自分からすると Maven のお作法が分からなかったので、備忘録がてら残しておく。\nmvn archetype:generate コマンドのオプションの意味 mvn archetype:generate コマンドを使えば任意のテンプレートに沿ったプロジェクトを一発で作成することができる。\n具体例として、Apache Beam でプロジェクト管理ツールである Maven を使って、mvn archetype:generate コマンドを用いて、プロジェクト作成を行う場合、公式サイトでは以下のように指定されている\n1 2 3 4 5 6 7 8 9  $ mvn archetype:generate \\ -DarchetypeGroupId=org.apache.beam \\ -DarchetypeArtifactId=beam-sdks-java-maven-archetypes-examples \\ -DarchetypeVersion=2.31.0 \\ -DgroupId=org.example \\ -DartifactId=word-count-beam \\ -Dversion=\u0026#34;0.1\u0026#34; \\ -Dpackage=org.apache.beam.examples \\ -DinteractiveMode=false      オプション名 意味     archetypeGroupId archetypeの groupId つまり、テンプレートを提供している作成元の識別子   archetypeArtifactId archetype のテンプレート。個々では beam-sdk に対応したプロジェクトテンプレートを作成している。   archetypeVersion テンプレートのバージョン   groupId Java のパッケージ名のルールに則ったすべてのプロジェクトで唯一に識別可能な識別子。今回は org.","title":"mvn archetype:generate でJavaのプロジェクト雛形を作成する際のオプションの解説"},{"content":"SIGIR eCom を探索していたら発見したサーベイ論文の \u0026ldquo;Challenges and research opportunities in eCommerce search and recommendations\u0026quot;が面白かったので、社内の勉強会で発表してきた。\n和訳すると、「e コマースの検索と推薦における挑戦と研究トピック」で、e コマースにおける検索と推薦の課題が明瞭に書かれていて非常に面白い論文でした。 自分もまだ検索エンジニアとして日が浅いので、手持ちのパターンを増やせるように日々勉強していますが、この論文のおかげでかなり解像度が上がった。\n 個人的に面白かったのは、\n そもそも、顧客が商品を検索するというタスクの奥深さと面白さが知れる Query Understanding は、非構造なクエリを構造化されたクエリに変換するのが究極的な目標 Learn to Rank(LtR)の実践的な課題点として、LtR 適用時に、Native Ranker とのギャップが発生して非連続な検索結果を返してしまうことがある 実際のクエリから、購入される商品はクエリと商品が関連性が高いとは限らないのでモデルを学習させる際には要注意  Amazon での実例として  クエリ「ダイヤモンドリング」に対して LtR を適用すると、実際のクエリとそれに紐づくランキングシグナルから学習すると、「ダイヤモンドリング」というクエリで、「ジルコニウムリング」が大量に購買されていたので LtR では、「ダイヤモンドリング」というクエリに対して、「ジルコニウムリング」を表示するようになってしまった   これは、学習データを全く見ないで適用するとそうなりそうだけど、広範囲に影響を及ぼす LtR の QA は非常に骨が折れそう Ref: Amazon Search: The Joy of Ranking Products    スライド作成元の Markdown ファイルはhurutoriya/deckはこちらです。 スライド内のリンクに簡単にアクセスできます。\ne コマースでの検索に改善したいけど何したらいいかわからんという人は、とりあえずこれ見れば OK という論文だったので読めてよかった\n余談 Matching \u0026amp; Ranking の章までを解説したけど、それでも 45m 喋りっぱなしで最後のほうがかなり駆け足になってしまった。 また、英語での発表になったけど、やはり熟れたわかりやすい発表レベルに達するには、まだまだだなぁ感じた。精進せねば\n今回スライド作成に Marp を使いましたが、VS Code 上でスラスラとかけつつ読みやすくテンションの上がるデザインに簡単にできて感動しました。これからも愛用したいなと思います。\n年末くらいに、検索エンジニアとして 9 ヶ月経過するので、役になった学習リソースなどをまとめたい\n","permalink":"https://shunyaueta.com/posts/2021-07-10/","summary":"SIGIR eCom を探索していたら発見したサーベイ論文の \u0026ldquo;Challenges and research opportunities in eCommerce search and recommendations\u0026quot;が面白かったので、社内の勉強会で発表してきた。\n和訳すると、「e コマースの検索と推薦における挑戦と研究トピック」で、e コマースにおける検索と推薦の課題が明瞭に書かれていて非常に面白い論文でした。 自分もまだ検索エンジニアとして日が浅いので、手持ちのパターンを増やせるように日々勉強していますが、この論文のおかげでかなり解像度が上がった。\n 個人的に面白かったのは、\n そもそも、顧客が商品を検索するというタスクの奥深さと面白さが知れる Query Understanding は、非構造なクエリを構造化されたクエリに変換するのが究極的な目標 Learn to Rank(LtR)の実践的な課題点として、LtR 適用時に、Native Ranker とのギャップが発生して非連続な検索結果を返してしまうことがある 実際のクエリから、購入される商品はクエリと商品が関連性が高いとは限らないのでモデルを学習させる際には要注意  Amazon での実例として  クエリ「ダイヤモンドリング」に対して LtR を適用すると、実際のクエリとそれに紐づくランキングシグナルから学習すると、「ダイヤモンドリング」というクエリで、「ジルコニウムリング」が大量に購買されていたので LtR では、「ダイヤモンドリング」というクエリに対して、「ジルコニウムリング」を表示するようになってしまった   これは、学習データを全く見ないで適用するとそうなりそうだけど、広範囲に影響を及ぼす LtR の QA は非常に骨が折れそう Ref: Amazon Search: The Joy of Ranking Products    スライド作成元の Markdown ファイルはhurutoriya/deckはこちらです。 スライド内のリンクに簡単にアクセスできます。\ne コマースでの検索に改善したいけど何したらいいかわからんという人は、とりあえずこれ見れば OK という論文だったので読めてよかった\n余談 Matching \u0026amp; Ranking の章までを解説したけど、それでも 45m 喋りっぱなしで最後のほうがかなり駆け足になってしまった。 また、英語での発表になったけど、やはり熟れたわかりやすい発表レベルに達するには、まだまだだなぁ感じた。精進せねば","title":"eコマースの検索と推薦についてのサーベイ論文である 'Challenges and research opportunities in eCommerce search and recommendations' を社内勉強会で発表した"},{"content":"Motivation Streamlit is a powerful tools to quickliy build the demo application. If we use Streamlit file upload feature via WebBrowser then we need to its file path to process the uploaded file. So I will introduce how to get uploaed file path in Streamlit.\nExample We buid the PDF File upload feature in Streamlit and its PDF file convert to image. We use Belval/pdf2image which is a populer PDF converting tool. It needs to file path to apply the module feature. we assume local machine is the MacOS then we need to install the poppler to use pdf2image,\nDemo app screenshot and open sourced code We also publised a code example at hurutoriya/streamlist-file-uploader-example\nDemo Movie in Youtube\nMakefile It worked task runner to install the dependency and run the app.\n1 2 3 4 5  install: brew install poppler poetry install run: poetry run streamlit run streamlit_pdf_uploader/main.py   Poetry for package management 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  [tool.poetry] name = \u0026#34;streamlit-pdf-uploader\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;\u0026#34; authors = [\u0026#34;\u0026#34;] [tool.poetry.dependencies] python = \u0026#34;^3.8\u0026#34; streamlit = \u0026#34;^0.84.0\u0026#34; watchdog = \u0026#34;^2.1.3\u0026#34; pdf2image = \u0026#34;^1.16.0\u0026#34; [tool.poetry.dev-dependencies] pytest = \u0026#34;^5.2\u0026#34; [build-system] requires = [\u0026#34;poetry-core\u0026gt;=1.0.0\u0026#34;] build-backend = \u0026#34;poetry.core.masonry.api\u0026#34;   Streamlit Python file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  import base64 import tempfile import streamlit as st from pdf2image import convert_from_path from pathlib import Path def show_pdf(file_path:str): \u0026#34;\u0026#34;\u0026#34;Show the PDF in Streamlit That returns as html component Parameters ---------- file_path : [str] Uploaded PDF file path \u0026#34;\u0026#34;\u0026#34; with open(file_path, \u0026#34;rb\u0026#34;) as f: base64_pdf = base64.b64encode(f.read()).decode(\u0026#34;utf-8\u0026#34;) pdf_display = f\u0026#39;\u0026lt;embed src=\u0026#34;data:application/pdf;base64,{base64_pdf}\u0026#34; width=\u0026#34;100%\u0026#34; height=\u0026#34;1000\u0026#34; type=\u0026#34;application/pdf\u0026#34;\u0026gt;\u0026#39; st.markdown(pdf_display, unsafe_allow_html=True) def main(): \u0026#34;\u0026#34;\u0026#34;Streamlit application \u0026#34;\u0026#34;\u0026#34; st.title(\u0026#34;PDF file uplodaer\u0026#34;) uploaded_file = st.file_uploader(\u0026#34;Choose your .pdf file\u0026#34;, type=\u0026#34;pdf\u0026#34;) if uploaded_file is not None: # Make temp file path from uploaded file with tempfile.NamedTemporaryFile(delete=False) as tmp_file: st.markdown(\u0026#34;## Original PDF file\u0026#34;) fp = Path(tmp_file.name) fp.write_bytes(uploaded_file.getvalue()) st.write(show_pdf(tmp_file.name)) imgs = convert_from_path(tmp_file.name) st.markdown(f\u0026#34;Converted images from PDF\u0026#34;) st.image(imgs) if __name__ == \u0026#34;__main__\u0026#34;: main()   Conclusion Streamlit is a powerful tools to quickliy build the demo application. You leand about how to get the uploade file path from Streamlit in this post.\nReference  Showing PDF file on StreamlitSharing that’s in github directory Problem in reading a “db” object using file uploader  ","permalink":"https://shunyaueta.com/posts/2021-07-09/","summary":"Motivation Streamlit is a powerful tools to quickliy build the demo application. If we use Streamlit file upload feature via WebBrowser then we need to its file path to process the uploaded file. So I will introduce how to get uploaed file path in Streamlit.\nExample We buid the PDF File upload feature in Streamlit and its PDF file convert to image. We use Belval/pdf2image which is a populer PDF converting tool.","title":"How to get the uploaded file path and processing its file in  Streamlit"},{"content":"モチベーション Streamlit は Python code のみで簡単かつ高速に Web アプリを作成できる強力なパッケージ。 Streamplit で作られた Web アプリ経由でファイルをアップロードして、そのファイルを処理したい際の具体的な実現方法がなかったので備忘録がてら残しておく。\nPDF ファイルをアップロードして、画像に変換する Web アプリ 具体的に例を交えつつ説明する。 Streamlit を使って、PDF ファイルをアップロードしてアップロードされた PDF ファイルを画像化するアプリを作成する。 今回は、Belval/pdf2image という PDF パッケージを使用する。\u0008 このパッケージは処理したい PDF のファイルパスを要求するインターフェースなので今回の実例に沿っていてわかりやすい。 ローカルマシンは MacOS を想定しており、pdf2image はpoppler の事前インストールが必須。\n完成形のスクリーンショット GitHub でもコードを公開しておきました。\nhurutoriya/streamlist-file-uploader-example\nデモ動画はこちら\nDemo Movie in Youtube\nMakefile Makefile は依存パッケージを事前インストールするために採用\n1 2 3 4 5  install: brew install poppler poetry install run: poetry run streamlit run streamlit_pdf_uploader/main.py   Poetry for package management 環境構築は poetry を使っています。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  [tool.poetry] name = \u0026#34;streamlit-pdf-uploader\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;\u0026#34; authors = [\u0026#34;\u0026#34;] [tool.poetry.dependencies] python = \u0026#34;^3.8\u0026#34; streamlit = \u0026#34;^0.84.0\u0026#34; watchdog = \u0026#34;^2.1.3\u0026#34; pdf2image = \u0026#34;^1.16.0\u0026#34; [tool.poetry.dev-dependencies] pytest = \u0026#34;^5.2\u0026#34; [build-system] requires = [\u0026#34;poetry-core\u0026gt;=1.0.0\u0026#34;] build-backend = \u0026#34;poetry.core.masonry.api\u0026#34;   Streamlit Python file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  import base64 import tempfile import streamlit as st from pdf2image import convert_from_path from pathlib import Path def show_pdf(file_path:str): \u0026#34;\u0026#34;\u0026#34;Show the PDF in Streamlit That returns as html component Parameters ---------- file_path : [str] Uploaded PDF file path \u0026#34;\u0026#34;\u0026#34; with open(file_path, \u0026#34;rb\u0026#34;) as f: base64_pdf = base64.b64encode(f.read()).decode(\u0026#34;utf-8\u0026#34;) pdf_display = f\u0026#39;\u0026lt;embed src=\u0026#34;data:application/pdf;base64,{base64_pdf}\u0026#34; width=\u0026#34;100%\u0026#34; height=\u0026#34;1000\u0026#34; type=\u0026#34;application/pdf\u0026#34;\u0026gt;\u0026#39; st.markdown(pdf_display, unsafe_allow_html=True) def main(): \u0026#34;\u0026#34;\u0026#34;Streamlit application \u0026#34;\u0026#34;\u0026#34; st.title(\u0026#34;PDF file uplodaer\u0026#34;) uploaded_file = st.file_uploader(\u0026#34;Choose your .pdf file\u0026#34;, type=\u0026#34;pdf\u0026#34;) if uploaded_file is not None: # Make temp file path from uploaded file with tempfile.NamedTemporaryFile(delete=False) as tmp_file: st.markdown(\u0026#34;## Original PDF file\u0026#34;) fp = Path(tmp_file.name) fp.write_bytes(uploaded_file.getvalue()) st.write(show_pdf(tmp_file.name)) imgs = convert_from_path(tmp_file.name) st.markdown(f\u0026#34;Converted images from PDF\u0026#34;) st.image(imgs) if __name__ == \u0026#34;__main__\u0026#34;: main()   Conclusion これで、Streamlit 上でファイルをアップロードして、そのアップロードされたファイルに対する処理ができます。 画像や音声など色々応用先があるので便利そう。 動作が遅いという欠点はあれど、これだけ簡単に GUI を Python のみで構築できるのは驚き。 Protly Dash も 2 年ほど前に使ったことがあるけど、UI のライフサイクルや HTML を結構意識しないとかけなかったので辛かった記憶があるので、なおさら感動している。\nReference  Showing PDF file on StreamlitSharing that’s in github directory Problem in reading a “db” object using file uploader  ","permalink":"https://shunyaueta.com/posts/2021-07-08/","summary":"モチベーション Streamlit は Python code のみで簡単かつ高速に Web アプリを作成できる強力なパッケージ。 Streamplit で作られた Web アプリ経由でファイルをアップロードして、そのファイルを処理したい際の具体的な実現方法がなかったので備忘録がてら残しておく。\nPDF ファイルをアップロードして、画像に変換する Web アプリ 具体的に例を交えつつ説明する。 Streamlit を使って、PDF ファイルをアップロードしてアップロードされた PDF ファイルを画像化するアプリを作成する。 今回は、Belval/pdf2image という PDF パッケージを使用する。\u0008 このパッケージは処理したい PDF のファイルパスを要求するインターフェースなので今回の実例に沿っていてわかりやすい。 ローカルマシンは MacOS を想定しており、pdf2image はpoppler の事前インストールが必須。\n完成形のスクリーンショット GitHub でもコードを公開しておきました。\nhurutoriya/streamlist-file-uploader-example\nデモ動画はこちら\nDemo Movie in Youtube\nMakefile Makefile は依存パッケージを事前インストールするために採用\n1 2 3 4 5  install: brew install poppler poetry install run: poetry run streamlit run streamlit_pdf_uploader/main.py   Poetry for package management 環境構築は poetry を使っています。","title":"Streamlit でアップロードしたファイルのパスを取得して、特定の処理をする"},{"content":"先日、同僚に「機械学習プロジェクトに興味があるんだけど、おすすめの資料があったら教えてほしい」と言われたので、Blog 記事に現時点でのおすすめの資料としてまとめておいたら、数年後見返したら面白そうだと思ったので記事として公開しておく。\nおすすめの資料 プロジェクトマネジメントや考え方、思想  How Google does Machine Learning  これは機械学習を実応用する人たちにはぜひ見てほしいビデオ講義。前半が、機械学習プロジェクトの計画や、優先順位、よくあるアンチパターンについて GCP で機械学習について多く関わってきたエンジニアが解説してくれていて、非常に勉強になる。 感想記事   リーン・スタートアップ　ムダのない起業プロセスでイノベーションを生みだす  顧客が求めるものを作ろう。機械学習にこだわったらまずだめなので\u0026hellip; (詳しくは後述の Rules of ML を呼んでみよう。) 関連する良いフレームワークとして @nishio さんの機械学習キャンバス もおすすめです。   Make something people want. by Paul Graham  人によって意見が別れるところではありますが、機械学習エンジニアとして、これがなぜ機械学習で必要なのかの「なぜ」を説明できないとたいてい上手く行かない経験がある。つまるところ、必要とされるものを見つけ出して作っていこうぜということですね   Netflix がカスタマーを誰よりも理解するためのデータ分析プロセス、コンシューマー・サイエンスの紹介  カスタマーオブセッションの考え方を、常に心のなかに秘めつつ世の中を良くするプロダクトを作りたい    MLOps, 機械学習エンジニアリング  Rules of Machine Learning  全員これを毎日読もう。聖書   仕事ではじめる機械学習 第 2 版  MLCT 創始者の @chezou さんが筆頭に書き上げた実践的な機械学習本。日本人で機械学習をやりたいならまずこれを買うべし。   AI アルゴリズムマーケティング 自動化のための機械学習/経済モデル、ベス トプラクティス、アーキテクチャ  邦訳だとべらぼうに怪しい感じになってしまっているが、内容はとんでもなく素晴らしい。マーケティングのために機械学習を適用することが多いと思うが、かなり網羅的に適用例を解説してくれている。原著の英語は無料なので、中身が気になる人はそちらをおすすめする。無料公開偉大すぎる   MLOps: 機械学習における継続的デリバリーと自動化のパイプライン  GCP による MLOps の解説。人によって、MLOps の定義って差異がありますが、自分はここで語られている ML システム構築のすべてのステップで自動化とモニタリングを推進できます こそが、 MLOps の骨子だなと思っています。クラウドサービスは、開発に関係する知識をパターン化して、資料を公開してくれるのでありがたいですね。   Google Cloud で機械学習を実装するためのベスト プラクティス  この資料なんかは、GCP で機械学習を実践したい場合にはまず見ておけば困ることはなさそうですね   各クラウドサービスの MLOps の white paper  AWS, Azure は普段使わないので深く言及しませんが、同様の資料は公開されたりしています。  Practitioner Guide to MLOps by GCP MLOps: Continuous Delivery for Machine Learning on AWS Azure Best practices for MLOps - DevOps for machine learning.      Machine Learning Engineering for Production (MLOps) も気になっているが、手を出せていないのでまた受講が完了したら感想を書きたい。Andrew 先生の講義はハズレはないので非常に期待している。\n論文  eugeneyan/applied-ml  @euganeyan さんがまとめてくれている、機械学習の応用例をまとめてくれているリポジトリ。事例を探したいならまずここを見れば間違いない   The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction  機械学習システムの完成度を点数化できて、どの部分を改善していけばいいか明瞭にしてくれます。一度プロダクションにリリース後に、チームでスコアを計算してみて、次に何を改善するのか議論すると楽しいです。 日本語での抄訳    情報収集源 RSS (+Twitter)  Amazon Science Google AI TensorFlow blog  なんかの RSS を登録しています。なんだかんだ RSS 便利なので最近回帰している。Twitter でしか得られない情報もあったりはしますが、特定のリストを使って監視してます。Nuzzle を愛用していたが、なくなってしまったで復活してほしい。\nConference tutorial, workshop Academic  KDD Applied Data Science Track Papers  世界最高峰の機械学習の実応用例の論文が集まる KDD の ADS track。毎年論文が公開されるのが楽しみ。RecSys や SIGIR など、自分の専門分野に合わせた論文を呼んでおくと楽しいです。神嶌先生の ML, DM, and AI Conference Mapを参考にどんな国際会議があるかを調べてみるといいかもしれません。   ECNLP 2 , SIGIR eCo  近年だと E Commerce など実世界への適用を主題にしたワークショップがアカデミックな会議でも開催されていて、非常にいい流れだと思う。    少し前だと、Reliable Machine Learning in the Wild, Workshop on ML Systems, Reliable Machine Learning in the Wild のワークショップも面白かった。\nIndustrial  MLSys  機械学習の実応用をテーマにして査読付き学会   OpML  機械学習の実運用をテーマにした査読付き学会。MLSys よりも実践論がテーマになっている。2021 年は開催されないみたいで悲しい。2021 年は SRECON21 でOpML Track が開催されていた。来年は OpML が単独カンファレンスとして復活してほしい。   MLconf  海外の機械学習実践の第一人者の人たちが自社事例を惜しげもなく公開してくれているいいカンファレンス。   TWIMLcon  TWIML podcast を主催している Sam さんが開催している、機械学習事例のカンファレンス。有料だが、Sam さんのおそろべきネットワークにより、有料にする価値は間違いなくあるカンファレンスとなっている    Podcast  twiml  ゲストがかなり豪華で、理論から実践まで幅広くトピックを取り扱っている。M.J. 先生や Alex smola 先生がゲストとして着てるのは驚くしかない。。   MLOps.community  こちらは、実践的な麺をメインにした Podcast。D Sculley さんがゲストとしてきていた、The Godfather Of MLOps は神回なのでおすすめ。    読んだ記事は はてぶでまとめているので、興味のある方はフォローすると役立つかもしれません。Twitter のほうにもはてぶと連携して放流しているので、そちらのフォローも是非。\n","permalink":"https://shunyaueta.com/posts/2021-05-29/","summary":"先日、同僚に「機械学習プロジェクトに興味があるんだけど、おすすめの資料があったら教えてほしい」と言われたので、Blog 記事に現時点でのおすすめの資料としてまとめておいたら、数年後見返したら面白そうだと思ったので記事として公開しておく。\nおすすめの資料 プロジェクトマネジメントや考え方、思想  How Google does Machine Learning  これは機械学習を実応用する人たちにはぜひ見てほしいビデオ講義。前半が、機械学習プロジェクトの計画や、優先順位、よくあるアンチパターンについて GCP で機械学習について多く関わってきたエンジニアが解説してくれていて、非常に勉強になる。 感想記事   リーン・スタートアップ　ムダのない起業プロセスでイノベーションを生みだす  顧客が求めるものを作ろう。機械学習にこだわったらまずだめなので\u0026hellip; (詳しくは後述の Rules of ML を呼んでみよう。) 関連する良いフレームワークとして @nishio さんの機械学習キャンバス もおすすめです。   Make something people want. by Paul Graham  人によって意見が別れるところではありますが、機械学習エンジニアとして、これがなぜ機械学習で必要なのかの「なぜ」を説明できないとたいてい上手く行かない経験がある。つまるところ、必要とされるものを見つけ出して作っていこうぜということですね   Netflix がカスタマーを誰よりも理解するためのデータ分析プロセス、コンシューマー・サイエンスの紹介  カスタマーオブセッションの考え方を、常に心のなかに秘めつつ世の中を良くするプロダクトを作りたい    MLOps, 機械学習エンジニアリング  Rules of Machine Learning  全員これを毎日読もう。聖書   仕事ではじめる機械学習 第 2 版  MLCT 創始者の @chezou さんが筆頭に書き上げた実践的な機械学習本。日本人で機械学習をやりたいならまずこれを買うべし。   AI アルゴリズムマーケティング 自動化のための機械学習/経済モデル、ベス トプラクティス、アーキテクチャ  邦訳だとべらぼうに怪しい感じになってしまっているが、内容はとんでもなく素晴らしい。マーケティングのために機械学習を適用することが多いと思うが、かなり網羅的に適用例を解説してくれている。原著の英語は無料なので、中身が気になる人はそちらをおすすめする。無料公開偉大すぎる   MLOps: 機械学習における継続的デリバリーと自動化のパイプライン  GCP による MLOps の解説。人によって、MLOps の定義って差異がありますが、自分はここで語られている ML システム構築のすべてのステップで自動化とモニタリングを推進できます こそが、 MLOps の骨子だなと思っています。クラウドサービスは、開発に関係する知識をパターン化して、資料を公開してくれるのでありがたいですね。   Google Cloud で機械学習を実装するためのベスト プラクティス  この資料なんかは、GCP で機械学習を実践したい場合にはまず見ておけば困ることはなさそうですね   各クラウドサービスの MLOps の white paper  AWS, Azure は普段使わないので深く言及しませんが、同様の資料は公開されたりしています。  Practitioner Guide to MLOps by GCP MLOps: Continuous Delivery for Machine Learning on AWS Azure Best practices for MLOps - DevOps for machine learning.","title":"2021年05月時点で自分が実践しているMLOpsの情報収集方法"},{"content":"現状の Poetry では、pyproject.toml を基にした setup.py の直接的な自動生成をサポートしていない。\nSupport generation of poetry manged setup.py file #761\nえ？なんで setup.py が必要なんですか? poetry build で生成される source と wheels で事足りるんじゃないですかというツッコミがあると思います。\nPyPI や Jflog などでホストせずに、GitHub のリポジトリでパッケージを管理したり、特定のサブディレクトリをパッケージとして扱う際には、未だ setup.py での依存関係の記述が必要です。\nPoetry による実現方法 poetry build コマンドと Makefile を組み合わせることで、pyproject.toml に対応した setup.py の自動生成ができるのでそれを採用します。 コマンドはGitHub のissues でのコメントを参考にしました。 1\n1 2 3 4 5 6 7 8 9 10 11 12  # package name PACKAGE = lib .PHONY: build-package build-package: ## Generate setup.py by poetry command for shared package  poetry build # source ./dist で解凍 tar zxvf dist/$(PACKAGE).tar.gz -C ./dist # setup.py を手元にコピー cp dist/$(PACKAGE)/setup.py setup.py # poetry build で生成されたファイルをすべて削除 rm -rf dist     https://github.com/python-poetry/poetry/issues/761#issuecomment-689491920 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2021-05-23/","summary":"現状の Poetry では、pyproject.toml を基にした setup.py の直接的な自動生成をサポートしていない。\nSupport generation of poetry manged setup.py file #761\nえ？なんで setup.py が必要なんですか? poetry build で生成される source と wheels で事足りるんじゃないですかというツッコミがあると思います。\nPyPI や Jflog などでホストせずに、GitHub のリポジトリでパッケージを管理したり、特定のサブディレクトリをパッケージとして扱う際には、未だ setup.py での依存関係の記述が必要です。\nPoetry による実現方法 poetry build コマンドと Makefile を組み合わせることで、pyproject.toml に対応した setup.py の自動生成ができるのでそれを採用します。 コマンドはGitHub のissues でのコメントを参考にしました。 1\n1 2 3 4 5 6 7 8 9 10 11 12  # package name PACKAGE = lib .PHONY: build-package build-package: ## Generate setup.py by poetry command for shared package  poetry build # source .","title":"Poetry からsetup.py を自動生成する"},{"content":"テキスト解析機 KyTea KyTeaを業務で使う機会があり、Python wrapper である Mykytea を使ってみたのですが、poetry や pip で Mykytea をインストールするだけでは、\nLibrary not loaded: /usr/local/lib/libkytea.0.dylib in version = \u0026quot;0.1.5\u0026quot;\n上記のエラーが出力され、KyTea を使うことができませんでした。 Mykytea のリポジトリに issue 1 を立てて、@chezou さんにお聞きしてみたところ、\n Good point. Mykytea wheel assumes that kytea is installed under /usr/local/lib, while your kytea exists another place. This should be Mykytea issue and there are two options we can avoid it like:\n Use delocate, like Linux\u0026rsquo;s audit-wheel https://realpython.com/python-wheels/ Install from source using wheel instead.   とのことで、/usr/local/lib にKyTea がインストールされていることを前提としているので、KyTea自身をソースコードからビルドすることをおすすめするとご助言をいただくことができました。\nC++,C に疎いので知らなかったのですが、dylibという仕組みで共通ライブラリとして扱えるんですね。\nMakefile によるkyteaのビルドと、quick check の自動化 自分はMakefile を好んで使うのですが、以下のようなタスクを作成することで、KyTeaのビルドと、quick check をワンライナーで行えるようにしました。\n1 2 3 4 5 6 7 8 9 10 11 12  .PHONY: keytea-install keytea-install: ## install and build kytea \tcurl -SsL -o kytea.tar.gz http://www.phontron.com/kytea/download/kytea-0.4.7.tar.gz mkdir -p /tmp/kytea tar -xzvf kytea.tar.gz -C /tmp/kytea --strip-components 1 cd /tmp/kytea \u0026amp;\u0026amp; ./configure \u0026amp;\u0026amp; make -j4 \u0026amp;\u0026amp; make install rm kytea.tar.gz .PHONY: kytea-test kytea-test: ## check kytea lib installed by C++ source code build \techo \u0026#34;山田太郎はコーヒーを買って飲んだ。\u0026#34; | kytea poetry run python -c \u0026#34;import Mykytea\u0026#34;   使用した pyproject.toml は以下です。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  [tool.poetry] name = \u0026#34;kytea-test\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;\u0026#34; authors = [\u0026#34;] [tool.poetry.dependencies] python = \u0026#34;^3.7\u0026#34; kytea = \u0026#34;~=0.1.3\u0026#34; [tool.poetry.dev-dependencies] pytest = \u0026#34;^5.2\u0026#34; [build-system] requires = [\u0026#34;poetry-core\u0026gt;=1.0.0\u0026#34;] build-backend = \u0026#34;poetry.core.masonry.api\u0026#34;   機械学習タスクでは外部のデータに依存することが多いですが、Makefile でその検査を自動化することができるので愛用しています。\n  Library not loaded: /usr/local/lib/libkytea.0.dylib in version = \u0026ldquo;0.1.5\u0026rdquo; #17 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2021-05-19/","summary":"テキスト解析機 KyTea KyTeaを業務で使う機会があり、Python wrapper である Mykytea を使ってみたのですが、poetry や pip で Mykytea をインストールするだけでは、\nLibrary not loaded: /usr/local/lib/libkytea.0.dylib in version = \u0026quot;0.1.5\u0026quot;\n上記のエラーが出力され、KyTea を使うことができませんでした。 Mykytea のリポジトリに issue 1 を立てて、@chezou さんにお聞きしてみたところ、\n Good point. Mykytea wheel assumes that kytea is installed under /usr/local/lib, while your kytea exists another place. This should be Mykytea issue and there are two options we can avoid it like:\n Use delocate, like Linux\u0026rsquo;s audit-wheel https://realpython.com/python-wheels/ Install from source using wheel instead.","title":"KyTeaをPythonで扱えるMykyteaを使うために必要なこと"},{"content":"@rilmayer_jp さんのツイート をきっかけに、検索チームの成熟度モデルの存在を知りました。ありがとうございます!\nEric Pugh さんが、検索エンジンに関する会議で公演した内容で、検索チームがどのように成熟していくかをモデル化しており、それが面白かったので備忘録として残しておく\n更新  2021/05/13 : 原著者のEric Pugh さんから、抄訳のご快諾いただけました。ありがとうございます  翻訳元資料  Search Relevance Organizational Maturity Model slide Haystack LIVE! 2020 Search Relevance Organizational Maturity Model  検索エンジンのレベル 検索エンジンへの要求をどれだけ満たしているかをピラミッド構造でわかりやすく説明している\n検索チームの成熟度モデル 7 項目の検索チームの評価項目を考え、3 段階で評価を行う\n    ビジネス 顧客の要求の理解 検索技術 実験駆動 UX コンテンツ強化 データ保有     発展 ステークホルダーがリアルタイム KPI を使用している データ解析から質的なデータを得ている カスタムプラグインを作成している A/B テスト、オフラインテストをサポートしている 革新的な発見性を提供している(chatbot, 等) NLP やデータサイエンティストの専任チームが取り組んでいる 多種多様な、複雑かつ大規模なデータを扱っている   実践 不定期にレポートを行っている いくつかのユーザーテスト、基礎的な分析を行っている 関連性のための複雑な設定、プラグインの使用をしている 実験は適用可能だが、A/B テストなどはできない 発見しやすくするための UI を提供している 分類学や概念体型の適用をしている データの複雑度の監視している   基礎 ビジネスインパクトが測定されていない クエリログは存在しない、またはユーザーテストを行っていない 技術スタックを適度に調整している 検索のテストは手作業で行い、デプロイは低頻度 1 ページに 10 個の検索結果がある 僅かな取り組み(シノニムなど) とても単純なデータモデル    感想 ひと目で\n 今の自分達がどの位置にいて 何を目指せばいいか  わかる可視化は示唆に富むのでありがたい\n","permalink":"https://shunyaueta.com/posts/2021-05-12/","summary":"@rilmayer_jp さんのツイート をきっかけに、検索チームの成熟度モデルの存在を知りました。ありがとうございます!\nEric Pugh さんが、検索エンジンに関する会議で公演した内容で、検索チームがどのように成熟していくかをモデル化しており、それが面白かったので備忘録として残しておく\n更新  2021/05/13 : 原著者のEric Pugh さんから、抄訳のご快諾いただけました。ありがとうございます  翻訳元資料  Search Relevance Organizational Maturity Model slide Haystack LIVE! 2020 Search Relevance Organizational Maturity Model  検索エンジンのレベル 検索エンジンへの要求をどれだけ満たしているかをピラミッド構造でわかりやすく説明している\n検索チームの成熟度モデル 7 項目の検索チームの評価項目を考え、3 段階で評価を行う\n    ビジネス 顧客の要求の理解 検索技術 実験駆動 UX コンテンツ強化 データ保有     発展 ステークホルダーがリアルタイム KPI を使用している データ解析から質的なデータを得ている カスタムプラグインを作成している A/B テスト、オフラインテストをサポートしている 革新的な発見性を提供している(chatbot, 等) NLP やデータサイエンティストの専任チームが取り組んでいる 多種多様な、複雑かつ大規模なデータを扱っている   実践 不定期にレポートを行っている いくつかのユーザーテスト、基礎的な分析を行っている 関連性のための複雑な設定、プラグインの使用をしている 実験は適用可能だが、A/B テストなどはできない 発見しやすくするための UI を提供している 分類学や概念体型の適用をしている データの複雑度の監視している   基礎 ビジネスインパクトが測定されていない クエリログは存在しない、またはユーザーテストを行っていない 技術スタックを適度に調整している 検索のテストは手作業で行い、デプロイは低頻度 1 ページに 10 個の検索結果がある 僅かな取り組み(シノニムなど) とても単純なデータモデル    感想 ひと目で","title":"[抄訳] 検索エンジンの達成度と検索チームの成熟度モデル"},{"content":"データ処理のタスクをこなしていると、Python で SQL に変数を挿入し柔軟に SQL クエリを構築したくなる。 例えば、\n 中間テーブルを作るために Airflow などで定期的なジョブを実行し、SQL の createdの時間を当日のものに変更する training, dev, test でデータを分割する際に、createdの条件を変更して 3 パターンのデータを取得する  などが考えられる。\n変数を SQL に組み込んで実行したい際には、kayak/pypikaのような SQL builder もあるが、個人的に可読性が悪くなったり、SQL クエリの作成のためだけに余計なパッケージをいれたくない。そのためパッケージを入れずにシンプルに完結する方法をここでは紹介する。\n編集履歴  2021/05/12: twitter で docstring ではなく string literal ですよという指摘をいただき修正 ref 2021/05/12: twitter での意見を反映  1. 単なる文字列として SQL クエリを構築 1 2 3  def get_guery(num: int, category: str): sql=f\u0026#34;SELECT field1, field2, field3, field4 FROM TABLE WHERE condition1={num} AND condition2={category}\u0026#34; return sql    f-string で文字列に変数を挿入して、SQL クエリを構築  だが、\n SQL が長くなると PEP8 に準拠せず、E501 line too longに抵触する 視認性が低く、SQL クエリの実行内容を理解しづらい  2. 複数行文字列として SQL クエリを構築 1 2 3 4 5 6 7 8  def get_guery(num: int, category: str): sql = f\u0026#34;\u0026#34;\u0026#34; SELECT field1, field2, field3, field4 FROM table WHERE condition1={num} AND condition2={category} \u0026#34;\u0026#34;\u0026#34; return sql    1 番目と比較すると、複数行を扱えるstring literal-longstringを採用することで、SQL クエリが複数行になることでで見やすい (@cocu_tan さん、ご指摘ありがとうございます!)  だが\n SQL 構文に関する lint や フォーマッターを活用できない SQL へのシンタックスハイライトが無いので、視認性が低い  3. SQL ファイルを文字列として読み込み、 .format()で変数を挿入する SQL ファイルが ./sql/test.sql に設置されている状態とする。\n1 2 3 4 5 6 7 8 9 10  SELECT field1, field2, field3, field4 FROM TABLE WHERE condition1={num} AND condition2={category}   Python 側での実装例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import os def get_guery(num: int, category: str, filename: str)-\u0026gt;str: \u0026#34;\u0026#34;\u0026#34; SQLファイルに変数を挿入してクエリを作成する Args: num (int): 年齢 category (str): 好きな果物 filename (str): SQLファイルの名前 Returns: str: SQLクエリ \u0026#34;\u0026#34;\u0026#34; with open(os.path.join(\u0026#34;./sql\u0026#34;, filename), \u0026#34;r\u0026#34;) as f: return f.read().format(num=num, category=category) print(get_guery(num=18, category=\u0026#34;apple\u0026#34;, filename=\u0026#34;test.sql\u0026#34;))   実行結果\n1 2 3 4 5 6 7 8 9 10  SELECT field1, field2, field3, field4 FROM TABLE WHERE condition1=18 AND condition2=apple    SQL クエリをSQLファイルで管理できるので、フォーマッターやシンタックスハイライトの恩恵を活用できる。 ファイルオブジェクトに対して .read()メソッドを使うことで文字列として扱うことができる。そのおかげで、SQL ファイルに{var_1}, {var_2} のような形式で、Python 上の変数を SQL の挿入して柔軟にクエリを構築可能になる。  SQL ファイルの中に Python の .format() 記法を事前に埋め込むことが今回のコツですね。\n余談 SQL ファイル内に\n1 2 3 4 5 6 7 8 9 10 11  -- {}の中にPythonから変数が挿入される SELECT field1, field2, field3, field4 FROM TABLE WHERE condition1={num} AND condition2={category}   1 行目のようなコメントを入れてしまうと、SQL ファイルの中ではコメントとして評価される部分も Pythn の文字列内では.format()の挿入対象となる。その影響で.format()で挿入されるべき変数の数が 3 つと評価され、実際の引数は 2 つしか与えておらずエラーが発生する。 SQL ファイルのコメントであろうと、{}と記述しないように気をつけておきましょう。\n追記 Twitter でみんなの管理方法をお聞きできたので、記しておく。 ありがたい。\n@shuhei_fujiwara さん https://twitter.com/shuhei_fujiwara/status/1387815866436243458\n BigQuery の話だけど僕は\n ほとんどの場合 =\u0026gt; parameterized query 本当に短くて自明なクエリ =\u0026gt; 1 or 2 どうにもならんとき =\u0026gt; 3 https://cloud.google.com/bigquery/docs/parameterized-queries 3 はわかるんだけど、Python コードで読み込む前提よりは SQL ファイル単体で成立している方が好きなので可能なら避けてます   単体で成立したい気持ち、たしかに分かる~\n@satoshihirose さん https://twitter.com/satoshihirose/status/1387776005943840772\n Airflow のみの話にはなってしまいますが、sql はファイルとして管理して基本 jinja template 対応のオペレーター使ってましたね https://qiita.com/munaita_/items/6bdcfb10f36c8c6b4753\n Airflow のときに jinja 使えるんですね。今度から使います!\n@reto_nayuta さん https://twitter.com/reto_nayuta/status/1387801584512364544\n 私も（Airflow ではないですが）jinja 使うことが多いですね\n @SassaHero さん https://twitter.com/SassaHero/status/1387878168481075200\n 3 を、from string import Template で変数置き換えを実現してます！\n from string import Template 知らなかった\nRef  Python SQL query string formatting  ","permalink":"https://shunyaueta.com/posts/2021-04-29/","summary":"データ処理のタスクをこなしていると、Python で SQL に変数を挿入し柔軟に SQL クエリを構築したくなる。 例えば、\n 中間テーブルを作るために Airflow などで定期的なジョブを実行し、SQL の createdの時間を当日のものに変更する training, dev, test でデータを分割する際に、createdの条件を変更して 3 パターンのデータを取得する  などが考えられる。\n変数を SQL に組み込んで実行したい際には、kayak/pypikaのような SQL builder もあるが、個人的に可読性が悪くなったり、SQL クエリの作成のためだけに余計なパッケージをいれたくない。そのためパッケージを入れずにシンプルに完結する方法をここでは紹介する。\n編集履歴  2021/05/12: twitter で docstring ではなく string literal ですよという指摘をいただき修正 ref 2021/05/12: twitter での意見を反映  1. 単なる文字列として SQL クエリを構築 1 2 3  def get_guery(num: int, category: str): sql=f\u0026#34;SELECT field1, field2, field3, field4 FROM TABLE WHERE condition1={num} AND condition2={category}\u0026#34; return sql    f-string で文字列に変数を挿入して、SQL クエリを構築  だが、","title":"Pythonで、変数を挿入して柔軟にSQLクエリを構築する"},{"content":"2018 年 2 月に株式会社メルカリに機械学習エンジニアとして入社したが、来月から同社で検索エンジニアに転向する。\n機械学習エンジニアとして 入社してからは主に、機械学習により同社での Cusotmer Service 分野の業務効率化を推進してきた。 この３年間で、実応用を前提にした機械学習プロジェクトの1→100と０→１の両者を体験できたのは得難い経験だった。\n主に 2 つのプロジェクトを取り組んできたが、そのうちの一つである「機械学習による商品監視プロジェクト」の成果を論文として公開 できたのは、自分としても嬉しい。\nこの領域はTrust and Safaty とよばれており、少し聞き慣れない言葉かもしれないが、サービスを提供するにあたって必須となるサービスの信頼と安全性を高めることを指す。 少し前に Pinterest がこの領域でのカンファレンスを開催していて、この領域での機械学習の需要がなくなることは無いだろうなと改めて確信を得た。\n Trust \u0026amp; Safety Machine Learning Summit   なぜ検索エンジニア? キャリアで新しいことに挑戦したい時、鶏と卵問題は必ず発生すると思っている。 例えば機械学習エンジニアが募集されているが、実務経験を重視されるので未経験だとそもそもポジションに就くことができない。\nデータとエンジニアリングの重なる領域でもう一段スキルと経験を深めたいなと検討した領域が検索領域だったが、一般的な募集では検索領域での経験 N 年以上を求むというものが多く、未経験での転職の壁はなかなかに厳しい。 なら内部での異動はどうだろうと去年の年末に希望を出してみたところ、まずは実験的にチーム異動してみようという話になった。 2 月の半ばから 1 ヶ月程度検証期間を経て問題なしということで、ひとまず正式に検索チームに異動できることになった。\n身内贔屓というわけではないが、世界的に見ても同社の検索チームは領域的にもチームとしても凄くエキサイティングだと思っている。 まさに情熱プログラマーでも提唱されている、一番の下手くそでいようが実践できるので凄くワクワクしている。\n 一番の下手くそでいよう by 情熱プログラマー ソフトウェア開発者の幸せな生き方 \n 扱う技術スタックなどは変わりますが、僕の中では根本的にデータとエンジニアリングにどっぷり浸かる点では機械学習エンジニアだろうと検索エンジニアだろうと変わりはしないので、着実に経験を積んでいきたいなと思っています。\n直近の目標としては 2 年以内に、検索領域で自分の興味とマッチする査読付き国際会議\n KDD CHIIR SIGIR ecom ECNLP OpML MLSys  に採択されるような、Novel な(今までにない)成果を出したい\n","permalink":"https://shunyaueta.com/posts/2021-03-27/","summary":"2018 年 2 月に株式会社メルカリに機械学習エンジニアとして入社したが、来月から同社で検索エンジニアに転向する。\n機械学習エンジニアとして 入社してからは主に、機械学習により同社での Cusotmer Service 分野の業務効率化を推進してきた。 この３年間で、実応用を前提にした機械学習プロジェクトの1→100と０→１の両者を体験できたのは得難い経験だった。\n主に 2 つのプロジェクトを取り組んできたが、そのうちの一つである「機械学習による商品監視プロジェクト」の成果を論文として公開 できたのは、自分としても嬉しい。\nこの領域はTrust and Safaty とよばれており、少し聞き慣れない言葉かもしれないが、サービスを提供するにあたって必須となるサービスの信頼と安全性を高めることを指す。 少し前に Pinterest がこの領域でのカンファレンスを開催していて、この領域での機械学習の需要がなくなることは無いだろうなと改めて確信を得た。\n Trust \u0026amp; Safety Machine Learning Summit   なぜ検索エンジニア? キャリアで新しいことに挑戦したい時、鶏と卵問題は必ず発生すると思っている。 例えば機械学習エンジニアが募集されているが、実務経験を重視されるので未経験だとそもそもポジションに就くことができない。\nデータとエンジニアリングの重なる領域でもう一段スキルと経験を深めたいなと検討した領域が検索領域だったが、一般的な募集では検索領域での経験 N 年以上を求むというものが多く、未経験での転職の壁はなかなかに厳しい。 なら内部での異動はどうだろうと去年の年末に希望を出してみたところ、まずは実験的にチーム異動してみようという話になった。 2 月の半ばから 1 ヶ月程度検証期間を経て問題なしということで、ひとまず正式に検索チームに異動できることになった。\n身内贔屓というわけではないが、世界的に見ても同社の検索チームは領域的にもチームとしても凄くエキサイティングだと思っている。 まさに情熱プログラマーでも提唱されている、一番の下手くそでいようが実践できるので凄くワクワクしている。\n 一番の下手くそでいよう by 情熱プログラマー ソフトウェア開発者の幸せな生き方 \n 扱う技術スタックなどは変わりますが、僕の中では根本的にデータとエンジニアリングにどっぷり浸かる点では機械学習エンジニアだろうと検索エンジニアだろうと変わりはしないので、着実に経験を積んでいきたいなと思っています。\n直近の目標としては 2 年以内に、検索領域で自分の興味とマッチする査読付き国際会議\n KDD CHIIR SIGIR ecom ECNLP OpML MLSys  に採択されるような、Novel な(今までにない)成果を出したい","title":"機械学習エンジニアから検索エンジニアに転生"},{"content":"TL; DR;  pip install pipenv==2018.11.26 をすれば直った!!!!!  実行環境 1 2  $pipenv --version pipenv, version 2020.11.15   直面した問題 1 2 3 4 5  ./app/ ├── model │ └── setup.py └── serving └── Pipfile   のような構成で、modelというローカルパッケージを作成しており、serving 直下の Pipfile は、model を読み込んで setup.py に記述されている依存パッケージもインストールするようにしたい。\nserving ディレクトリで、以下のコマンドを入力すればローカルパッケージが pipenv によりインストールされるはずだが\n1  pipenv install --editable ../model   依存関係をすべて記述するはずの Pipenv.lock には、modelのパスのみが記述され、ローカルパッケージが要求する依存パッケージが記述されていない。\n原因を探してみたところ、\nInstalling a local package with pipenv install \u0026lsquo;-e .\u0026rsquo; doesn\u0026rsquo;t save dependencies #1024\n同じ GitHub issue を発見しダメ元で pipenv を以下のコマンドでダウングレードして見たところ\n1  pip install pipenv==2018.11.26   なんと\u0026hellip;. 直った。無事にローカルパッケージの依存パッケージが Pipenv.lock に記述されており、無事にローカルパッケージが serving 直下で動くようになった。\n後方互換性が無いとかそんなちゃちな問題ではない気がするのだが、この問題でかなり時間が溶けたので非常に精神的に消耗した。\n思えばこういう問題が多くて、普段は poetry を使っていたのを思い出した。\nRef  Installing dependencies of a local dependency with pipenv  ","permalink":"https://shunyaueta.com/posts/2021-03-13/","summary":"TL; DR;  pip install pipenv==2018.11.26 をすれば直った!!!!!  実行環境 1 2  $pipenv --version pipenv, version 2020.11.15   直面した問題 1 2 3 4 5  ./app/ ├── model │ └── setup.py └── serving └── Pipfile   のような構成で、modelというローカルパッケージを作成しており、serving 直下の Pipfile は、model を読み込んで setup.py に記述されている依存パッケージもインストールするようにしたい。\nserving ディレクトリで、以下のコマンドを入力すればローカルパッケージが pipenv によりインストールされるはずだが\n1  pipenv install --editable ../model   依存関係をすべて記述するはずの Pipenv.lock には、modelのパスのみが記述され、ローカルパッケージが要求する依存パッケージが記述されていない。\n原因を探してみたところ、\nInstalling a local package with pipenv install \u0026lsquo;-e .\u0026rsquo; doesn\u0026rsquo;t save dependencies #1024","title":"pipenv でローカルパッケージが正常にインストールされないときの対処法"},{"content":"ちょっと遅れましたが、最近これからどうするかについて色々と考えていて、その過程で 2021 年をどう過ごすかが決まったのでメモ。\n3 つの目標  Senior Software Engineer として確固たる実力を身につけることに集中 英語にふれることを習慣化 SNS を断ち自分にとって後悔の無い時間を歩む  Senior Software Engineer として確固たる実力を身につけることに集中 まず Senior Software Engineer と自分で胸を張って宣言できる実力を身につける。\n最近は自分のキャリアを専門性を深堀りしていくとして、Software Engineer としての道を進んでいくことを決めた。\n上記の理由として自分は Data Scientist になれるほど専門性は無いと思っている。Ph.D は持っていないし、再び博士課程に行くとしても研究は辛い。また Machine Learning Engineer や Data Scientist に特化していっても将来 30 年間働いていく中でそれらのレッドオーシャンで自分が生き抜いていけるとは思えない。 Software Engineer という職に飽きるまでは Product Manager のキャリアも考えなくていいと思った。その葛藤を同僚の人にも相談したが、輝かしい経歴(アメリカの有名大学で CS 専攻 →SWE→PM と天上人)の人でも、手を動かせなくなることに焦りを感じるよとおっしゃられていて非常に共感した。\n何よりも今の会社に入社して 3 年間働いてきた経験で身に染みたのは、Software Engineer という職業が楽しいなと思えた。もちろんデータ分析や機械学習などにも楽しさがあるが、それらを最大限レバレッジをきかせるためには Software Enginner としてのスキルが必須であり、最終的に物事を実現するためも避けて通れないと考えている。\n2020/03 からは新しいキャリアを見据えて少し方向転換をしてみることした。機械学習やデータ分析領域からは離れないが、チームが変わって扱う領域も異なるので非常に楽しみだ。弱くてニューゲームを楽しもう\n英語にふれることを習慣化 最近親しい人と雑談する際に言葉に出すようにしているんですが、国外で働くことに憧れを持っています。業務でも英語を使ったコミュニケーションはムラがあるが 4-7 割ほどになっている。今の段階はコミュニケーションはできるが、細かいミスや相手にネイティブレベルのスピードで喋られると脳みそがストップして働くのをやめてしまうのでなんとかしたい。社内の語学学習強化トレーナーの人に相談したところ、やはり毎日時間を積み重ねていくしかないと言われたので、今の気合でしゃべる英会話から自分でも自信を持てる英語の力を持っていきたいなと思っている。\nSNS を断ち自分にとって後悔の無い時間を歩む これは自戒なのですが、何気なくボーッとして SNS を見たりする時間が多く、学生の頃にあったひたむきに何かを学び積み重ねていくという習慣が消え去ってます。やばい。集中力も継続できないしで焦燥感だけが高まり行動ができていないという悪循環です。\n人生を楽しむためにも、貴重な時間を自分自身に自信を持って過ごせるように後悔のない時間を歩んでいきたい。\n  これらの目標は toggl で追跡しておいて、毎月どれくらいの時間を費やせたかを振り替えれるようにする 学び続けるということを習慣化したいので、学びの内容が可視化されるように、これからは学んだことを文章化して気軽に Blog 記事としてアウトプットしていく  ","permalink":"https://shunyaueta.com/posts/2021-03-05/","summary":"ちょっと遅れましたが、最近これからどうするかについて色々と考えていて、その過程で 2021 年をどう過ごすかが決まったのでメモ。\n3 つの目標  Senior Software Engineer として確固たる実力を身につけることに集中 英語にふれることを習慣化 SNS を断ち自分にとって後悔の無い時間を歩む  Senior Software Engineer として確固たる実力を身につけることに集中 まず Senior Software Engineer と自分で胸を張って宣言できる実力を身につける。\n最近は自分のキャリアを専門性を深堀りしていくとして、Software Engineer としての道を進んでいくことを決めた。\n上記の理由として自分は Data Scientist になれるほど専門性は無いと思っている。Ph.D は持っていないし、再び博士課程に行くとしても研究は辛い。また Machine Learning Engineer や Data Scientist に特化していっても将来 30 年間働いていく中でそれらのレッドオーシャンで自分が生き抜いていけるとは思えない。 Software Engineer という職に飽きるまでは Product Manager のキャリアも考えなくていいと思った。その葛藤を同僚の人にも相談したが、輝かしい経歴(アメリカの有名大学で CS 専攻 →SWE→PM と天上人)の人でも、手を動かせなくなることに焦りを感じるよとおっしゃられていて非常に共感した。\n何よりも今の会社に入社して 3 年間働いてきた経験で身に染みたのは、Software Engineer という職業が楽しいなと思えた。もちろんデータ分析や機械学習などにも楽しさがあるが、それらを最大限レバレッジをきかせるためには Software Enginner としてのスキルが必須であり、最終的に物事を実現するためも避けて通れないと考えている。\n2020/03 からは新しいキャリアを見据えて少し方向転換をしてみることした。機械学習やデータ分析領域からは離れないが、チームが変わって扱う領域も異なるので非常に楽しみだ。弱くてニューゲームを楽しもう\n英語にふれることを習慣化 最近親しい人と雑談する際に言葉に出すようにしているんですが、国外で働くことに憧れを持っています。業務でも英語を使ったコミュニケーションはムラがあるが 4-7 割ほどになっている。今の段階はコミュニケーションはできるが、細かいミスや相手にネイティブレベルのスピードで喋られると脳みそがストップして働くのをやめてしまうのでなんとかしたい。社内の語学学習強化トレーナーの人に相談したところ、やはり毎日時間を積み重ねていくしかないと言われたので、今の気合でしゃべる英会話から自分でも自信を持てる英語の力を持っていきたいなと思っている。\nSNS を断ち自分にとって後悔の無い時間を歩む これは自戒なのですが、何気なくボーッとして SNS を見たりする時間が多く、学生の頃にあったひたむきに何かを学び積み重ねていくという習慣が消え去ってます。やばい。集中力も継続できないしで焦燥感だけが高まり行動ができていないという悪循環です。\n人生を楽しむためにも、貴重な時間を自分自身に自信を持って過ごせるように後悔のない時間を歩んでいきたい。","title":"2021年の目標"},{"content":"Python のアプリケーションで、Cloud logger にログを出力したいときに\n 標準の Python logging モジュールを利用して、ログを出力する Python Cloud Logging package を使用する  上記の２つの方法があります。\n不必要にパッケージを増やしたくはないので、1 の標準モジュールで Cloud Logger へ出力できないか試してみました。\n標準の Python logging モジュールを試す 標準の logging モジュールでログを出力したいときに\n1 2 3 4 5 6  import logging logger = logging.getLogger(__name__) def hoge(): logger.info(\u0026#39;logging Start 2021\u0026#39;)   と、logging.info() を仕込んで、Cloud logger にログを出力してみると、logger.info() で出しているはずなのに、Cloud logger 上ではすべてエラーとして扱われてしまっています。\n原因を特定するために、logger のログを見てみると logger.info() がすべて stderr標準エラーストリームへ出力されてしまっています。\n1 2 3 4 5 6 7 8 9 10 11 12  { \u0026#34;textPayload\u0026#34;: \u0026#34;2021-02-20 21:26:51,012 - root:predict:36 - INFO: logging Start\\n\u0026#34;, ... }, \u0026#34;timestamp\u0026#34;: \u0026#34;2021-02-20T12:26:51.013213826Z\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;ERROR\u0026#34;, \u0026#34;labels\u0026#34;: { ... }, \u0026#34;logName\u0026#34;: \u0026#34;projects/.../logs/stderr\u0026#34;, \u0026#34;receiveTimestamp\u0026#34;: \u0026#34;2021-02-20T12:26:55.050911180Z\u0026#34; }   そのため、明示的に標準出力へ出力先を変更してみました。\n1 2 3 4 5 6 7 8 9  import logging import sys logger = logging.getLogger(__name__) handler = logging.StreamHandler(sys.stdout) logger.addHandler(handler) def hoge(): logger.info(\u0026#39;logging Start 2021\u0026#39;)   これで解決できているはずと、Cloud Logger を見てみると\nなぜか、stderr への出力は残ったまま、stdoutへの出力が新たに増えました。このままでも INFO レベルでログは残せているので目的は達成できていますが、stderrへの出力が残ってしまっているせいで Cloud logger 上でエラーが発生しているようになり問題です。なので、これを避けるために２つ目の方法である Python Cloud Logging package を利用してみます。 (というかパッケージをむやみに増やしたくないのだが利用せざるを得ない。)\nPython Cloud Logging package を使用する まずPython Client for Cloud Logging のチュートリアルをすすめて行きます。\nCloud logging の Python package のQuick Start にてわかりやすく導入方法が紹介されています。ここで混乱しやすいのが、GCP のLogging Client Libraries のドキュメントではまず Service Account を作って、GOOGLE_APPLICATION_CREDENTIALS を使用して Cloud Logging の認証を行ってくださいと書かれています。\nですが、\n GCP 上での VM でパッケージを使用するなら認証は必要ない 。ただし、デフォルトの SA を使いたくない場合や、GCP 外などで動かしたい場合は SA が必要  と本元のパッケージのドキュメントでは書かれており、こちらのほうが誤解が少なくていいですね。(公式ドキュメントだと、SA 作成時に Project \u0026gt; Owner を指定しろと書かれているんだけど、これって権限過多なのでは無いのだろうか??)\nちなみに GCP 上の認証過程はこちらのドキュメントが詳しく書かれているので、気になった方は御覧ください。\nAuthenticating as a service account\n上記の認証仮定をまとめると\n GOOGLE_APPLICATION_CREDENTIALS を参照する ADC (Application Default Credentials) が、コードに紐付けられている SA を使用する ADC は各サービスの SA を利用する 1-3 が使用できなかった場合、認証エラーが発生する  最終的に GKE 上で Cloud logging へログを出力するためには、 以下のコードで無事に INFO のログがstderr に吐き出されることなく、stdout のみにINFOとして出力されるようになります。\n1 2 3 4 5 6 7 8 9 10 11 12  import logging import sys # NOTE: GKE ではCotainerEngineHandler が必要 from google.cloud.logging.handlers import ContainerEngineHandler logger = logging.getLogger(__name__) # NOTE: stream で stdout を指定する logger.addHandler(ContainerEngineHandler(name=__name__, stream=sys.stdout)) # NOTE: ログが重複して出力されるので、propagate を切る logger.propagate = False logger.info(\u0026#39;Hello Cloud Logging.\u0026#39;)   Appendix  https://docs.python.org/3/howto/logging.html https://cloud.google.com/logging/docs/setup/python https://www.ai-shift.co.jp/techblog/1217 https://stackoverflow.com/questions/48078051/duplicate-log-entries-with-google-cloud-stackdriver-logging-of-python-code-on-ku  同じような質問があった    ","permalink":"https://shunyaueta.com/posts/2021-03-03/","summary":"Python のアプリケーションで、Cloud logger にログを出力したいときに\n 標準の Python logging モジュールを利用して、ログを出力する Python Cloud Logging package を使用する  上記の２つの方法があります。\n不必要にパッケージを増やしたくはないので、1 の標準モジュールで Cloud Logger へ出力できないか試してみました。\n標準の Python logging モジュールを試す 標準の logging モジュールでログを出力したいときに\n1 2 3 4 5 6  import logging logger = logging.getLogger(__name__) def hoge(): logger.info(\u0026#39;logging Start 2021\u0026#39;)   と、logging.info() を仕込んで、Cloud logger にログを出力してみると、logger.info() で出しているはずなのに、Cloud logger 上ではすべてエラーとして扱われてしまっています。\n原因を特定するために、logger のログを見てみると logger.info() がすべて stderr標準エラーストリームへ出力されてしまっています。\n1 2 3 4 5 6 7 8 9 10 11 12  { \u0026#34;textPayload\u0026#34;: \u0026#34;2021-02-20 21:26:51,012 - root:predict:36 - INFO: logging Start\\n\u0026#34;, .","title":"GKE 上にて Pythonで logger.info() を行うとCloud logging では stderr に保存され、すべてエラーになる問題への対処法"},{"content":"前提 ローカルからkubectlでポートフォワードして、GKEにリクエストを投げて確認を行っている\n発生した問題 deployment のローリングアップデート前は問題なくポートフォワードを通してリクエストが返っていた。コードに変更を加えてGKE上でも確認をしたかったので、まずローカルで確認をして問題がなかった変更が、ローリングアップデート後ポートフォワード でGKE にリクエストを投げると curl: (52) Empty reply from server と返ってくる。\nTL; DR;  ポートフォワードはrolling update が終わったら貼り直そう。なぜなら、ポートフォワードの接続先はローリングアップデート前後で変化するため。  エラーメッセージ curl でリクエストした際のメッセージ\n1  curl: (52) Empty reply from server   Port foward の出力\n1  uid : Error: No such container: xxxxx   まとめ 発生していた問題は、ローリングアップデートを行うと、ポートフォワードの接続先が変更され、その際にローリングアップデート前後でkubectl でのポートフォワードは固定されたままなのでリクエストはサーバーから返ってこないという説明するのも恥ずかしい問題でした。\n理由は単純だけど、気づくのに時間がかかってしまった。k8s の動きを理解していないからこういうので時間を溶かしてしまった。反省\nAppendix  Performing a Rolling Update  ","permalink":"https://shunyaueta.com/posts/2021-02-21/","summary":"前提 ローカルからkubectlでポートフォワードして、GKEにリクエストを投げて確認を行っている\n発生した問題 deployment のローリングアップデート前は問題なくポートフォワードを通してリクエストが返っていた。コードに変更を加えてGKE上でも確認をしたかったので、まずローカルで確認をして問題がなかった変更が、ローリングアップデート後ポートフォワード でGKE にリクエストを投げると curl: (52) Empty reply from server と返ってくる。\nTL; DR;  ポートフォワードはrolling update が終わったら貼り直そう。なぜなら、ポートフォワードの接続先はローリングアップデート前後で変化するため。  エラーメッセージ curl でリクエストした際のメッセージ\n1  curl: (52) Empty reply from server   Port foward の出力\n1  uid : Error: No such container: xxxxx   まとめ 発生していた問題は、ローリングアップデートを行うと、ポートフォワードの接続先が変更され、その際にローリングアップデート前後でkubectl でのポートフォワードは固定されたままなのでリクエストはサーバーから返ってこないという説明するのも恥ずかしい問題でした。\n理由は単純だけど、気づくのに時間がかかってしまった。k8s の動きを理解していないからこういうので時間を溶かしてしまった。反省\nAppendix  Performing a Rolling Update  ","title":"GKE でローリングアップデート後、ローカルからポートフォワードでリクエストを投げるとcurl: (52) Empty reply from server と返ってくるときの対処方法"},{"content":"group by は集計作業において根幹となる処理ですが、少し手の混んだ集計をしたいときに毎回調べていることが多かったのでここに学んだことをまとめておく\n今回やりたいことは\nA 列が α になっている行の B 列の種類を集計したい\nです。\nはじめに 実際のデータを用意したほうが、理解が深まるので擬似的なテーブルを作成する。 テーブルのデータの概略として、何日に sender (送信者) が receiver (受信者) にいくら送金(price)したかを格納しているテーブルとする。\nStandardSQL は WITH を使って簡単にモックテーブルを作れるのが良いところ。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96  #standardSQL WITH `transactions` AS ( SELECT \u0026#39;A\u0026#39; AS sender, \u0026#39;B\u0026#39; AS receiver, 600 AS price, \u0026#39;2020-01-01\u0026#39; AS day UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 1200, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 600, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;C\u0026#39;, 2000, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;D\u0026#39;, 3000, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;D\u0026#39;, 2000, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, 700, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, 300, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;B\u0026#39;, \u0026#39;D\u0026#39;, 250, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 400, \u0026#39;2020-01-02\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 1000, \u0026#39;2020-01-02\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 1200, \u0026#39;2020-01-02\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 2000, \u0026#39;2020-01-02\u0026#39; UNION ALL SELECT \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, 450, \u0026#39;2020-01-02\u0026#39; UNION ALL SELECT \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, 500, \u0026#39;2020-01-02\u0026#39; ) SELECT * FROM transactions      sender receiver price day     A B 600 2020-01-01   A B 1200 2020-01-01   A B 1800 2020-01-01   A C 2000 2020-01-01   A D 3000 2020-01-01   A D 2000 2020-01-01   B C 700 2020-01-01   B C 300 2020-01-01   B D 250 2020-01-01   A B 400 2020-01-02   A B 1000 2020-01-02   A B 1200 2020-01-02   A B 2000 2020-01-02   B C 450 2020-01-02   B C 500 2020-01-02    列と列の組み合わせの数を集計する 日次ごとに送金者が何人に送ったかを集計したい、つまり(sender, receiver)のペアを考えて、sender を固定した上で何人に送金したいかを集計したとする。 上記のデータだと\n2020-01-01 では {(A,B), (A,C), (A,D)} の三通りになる。\n愚直に思いつくのは\ngroup by を２つの列で行うパターン  day sender  をgroup by して COUNT(distinct sender)で集計するアプローチがあるが、\n1 2 3 4 5 6 7 8 9  SELECT day, sender, COUNT(distinct receiver) AS receiver_uu FROM transactions GROUP BY day, sender   結果    day sender receiver_uu     2020-01-01 A 3   2020-01-01 B 2   2020-01-02 A 1   2020-01-02 B 1    day, sender の組み合わせとなっている。 このクエリを WITH でテーブルにして、Where でフィルターをかけると\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  receiver_uu AS ( SELECT day, sender, COUNT(DISTINCT receiver) AS receiver_uu FROM transactions GROUP BY day, sender) SELECT * FROM receiver_uu WHERE sender = \u0026#39;A\u0026#39;      day sender receiver_uu     2020-01-01 A 3   2020-01-02 A 1    の結果が得られる。\nだが、dayを基準に送信者、AとBの情報を 1 行にまとめたい場合はどうすべきだろうか?\n1 行に 2 列のパターン数をまとめて集計する AとBが送り先の数を調べたいなどの目的がある際には、COUNT()を行う際に IF 文で条件を集計することで上記を達成することができる。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  SELECT day, COUNT(DISTINCT -- 以下の結果の重複排除  IF (sender=\u0026#39;A\u0026#39;, -- sender がAだったときに  receiver, -- receiver を返す  NULL) -- それいがいはNULL  ) AS from_A_receiver_uu, COUNT(DISTINCT IF (sender=\u0026#39;B\u0026#39;, receiver, NULL)) AS from_B_receiver_uu FROM transactions GROUP BY day   流れを説明すると\n day で日次を基準にした計算を行う IF(sender='A\u0026rsquo;, receiver, NULL) で、スキャン時にAが送信した場合はその同行のreciver を返し、Aでなかった場合は NULL を返す 2 の結果の重複を排除して、その行数をカウントする  上記の処理のイメージ (2020-01-01の場合)\n COUNT(Distinct IF(sender='A\u0026rsquo;,receiver, NULL)) COUNT(Distinct (B,B,B,C,D,D,NULL,NULL,NULL)) COUNT((B,C,D)) 3  となり、Aが2020-01-01に送信した人数は 3 人となる。\n結果    day from_A_receiver_uu from_B_receiver_uu     2020-01-01 3 2   2020-01-02 1 1    今回は(A,B)の組み合わせで A を固定した際のパターン数を集計したが、A の種類が大規模な場合は SELECT 文をすべて書ききるの現実ではないが、A 自体が有限で集計したい場合はよくある処理なのでメモとしてまとめた。 UU などを計算したいときは必須の集計になると思います。\nRef  Syntax to count the number of UNIQUE matching values How COUNT(DISTINCT [field]) Works in Google BigQuery  ","permalink":"https://shunyaueta.com/posts/2021-02-09/","summary":"group by は集計作業において根幹となる処理ですが、少し手の混んだ集計をしたいときに毎回調べていることが多かったのでここに学んだことをまとめておく\n今回やりたいことは\nA 列が α になっている行の B 列の種類を集計したい\nです。\nはじめに 実際のデータを用意したほうが、理解が深まるので擬似的なテーブルを作成する。 テーブルのデータの概略として、何日に sender (送信者) が receiver (受信者) にいくら送金(price)したかを格納しているテーブルとする。\nStandardSQL は WITH を使って簡単にモックテーブルを作れるのが良いところ。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96  #standardSQL WITH `transactions` AS ( SELECT \u0026#39;A\u0026#39; AS sender, \u0026#39;B\u0026#39; AS receiver, 600 AS price, \u0026#39;2020-01-01\u0026#39; AS day UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 1200, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 600, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;C\u0026#39;, 2000, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;D\u0026#39;, 3000, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;D\u0026#39;, 2000, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, 700, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, 300, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;B\u0026#39;, \u0026#39;D\u0026#39;, 250, \u0026#39;2020-01-01\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 400, \u0026#39;2020-01-02\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 1000, \u0026#39;2020-01-02\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 1200, \u0026#39;2020-01-02\u0026#39; UNION ALL SELECT \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, 2000, \u0026#39;2020-01-02\u0026#39; UNION ALL SELECT \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, 450, \u0026#39;2020-01-02\u0026#39; UNION ALL SELECT \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, 500, \u0026#39;2020-01-02\u0026#39; ) SELECT * FROM transactions      sender receiver price day     A B 600 2020-01-01   A B 1200 2020-01-01   A B 1800 2020-01-01   A C 2000 2020-01-01   A D 3000 2020-01-01   A D 2000 2020-01-01   B C 700 2020-01-01   B C 300 2020-01-01   B D 250 2020-01-01   A B 400 2020-01-02   A B 1000 2020-01-02   A B 1200 2020-01-02   A B 2000 2020-01-02   B C 450 2020-01-02   B C 500 2020-01-02    列と列の組み合わせの数を集計する 日次ごとに送金者が何人に送ったかを集計したい、つまり(sender, receiver)のペアを考えて、sender を固定した上で何人に送金したいかを集計したとする。 上記のデータだと","title":"Standard SQLで 列と列の組み合わせの数を集計したい"},{"content":"新しいマシンでpipをセットアップして実行しようとすると\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  \u0026gt;\u0026gt; pip Traceback (most recent call last): File \u0026#34;/usr/local/bin/pip\u0026#34;, line 11, in \u0026lt;module\u0026gt; load_entry_point(\u0026#39;pip==21.0\u0026#39;, \u0026#39;console_scripts\u0026#39;, \u0026#39;pip\u0026#39;)() File \u0026#34;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py\u0026#34;, line 489, in load_entry_point return get_distribution(dist).load_entry_point(group, name) File \u0026#34;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py\u0026#34;, line 2843, in load_entry_point return ep.load() File \u0026#34;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py\u0026#34;, line 2434, in load return self.resolve() File \u0026#34;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py\u0026#34;, line 2440, in resolve module = __import__(self.module_name, fromlist=[\u0026#39;__name__\u0026#39;], level=0) File \u0026#34;/Library/Python/2.7/site-packages/pip-21.0-py2.7.egg/pip/_internal/cli/main.py\u0026#34;, line 60 sys.stderr.write(f\u0026#34;ERROR: {exc}\u0026#34;)   というエラーが出た。\nデフォルトの Python の実行ランタイムが 2.x 系なのでそれに起因するエラーだった。 pip を Python2 系でおそらくインストールしており、pip21 に更新後は Python3.6 からの機能である f-string が利用され始めたことが起因。\n対処方法 既存の pip を削除した上で、\n1 2  curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py   で pip が実行できるようになりエラーが解消された\nReferences  pip install failing on python2  ","permalink":"https://shunyaueta.com/posts/2021-02-08/","summary":"新しいマシンでpipをセットアップして実行しようとすると\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  \u0026gt;\u0026gt; pip Traceback (most recent call last): File \u0026#34;/usr/local/bin/pip\u0026#34;, line 11, in \u0026lt;module\u0026gt; load_entry_point(\u0026#39;pip==21.0\u0026#39;, \u0026#39;console_scripts\u0026#39;, \u0026#39;pip\u0026#39;)() File \u0026#34;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py\u0026#34;, line 489, in load_entry_point return get_distribution(dist).load_entry_point(group, name) File \u0026#34;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py\u0026#34;, line 2843, in load_entry_point return ep.load() File \u0026#34;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py\u0026#34;, line 2434, in load return self.resolve() File \u0026#34;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/__init__.py\u0026#34;, line 2440, in resolve module = __import__(self.module_name, fromlist=[\u0026#39;__name__\u0026#39;], level=0) File \u0026#34;/Library/Python/2.7/site-packages/pip-21.0-py2.7.egg/pip/_internal/cli/main.py\u0026#34;, line 60 sys.","title":"pip 実行時に sys.stderr.write(f\"ERROR: {exc} \") とエラーが出てpipを実行できないときの対処方法"},{"content":"この記事はMLOps Advent Calendar 2020の 25 日目の記事です。(盛大に遅れました)\nKDD2019 の招待講演で Google が TFX の歴史について発表されており、TFX 信者の自分としては発表内容が以前から気になっていたが、公開はされておらずなんとかして見れないかな~と思っていましたが、TensorFlow の Blogで該当の招待講演が論文化されたことを知ったのでメモがてら抄訳として残しておく。\n 注意)この翻訳記事は原著論文の著者陣からレビューはされていません Shunya Ueta, are providing a translation and abridgment, which has not been reviewed by the authors.  Citation  Karmarkar, A., Altay, A., Zaks, A., Polyzotis, N., Ramesh, A., Mathes, B., … \u0026amp; Li, Z. (2020). Towards ML Engineering: A Brief History Of TensorFlow Extended (TFX). arXiv preprint arXiv:2010.02013. ***\n Towards ML Engineering with TensorFlow Extended (TFX) at KDD2019 Towards ML Engineering with TensorFlow Extended (TFX) ACM PDF は arxiv でも閲覧可能  https://arxiv.org/abs/2010.02013    TFX を知らない方向け TFXとは、Google が開発する TensorFlow Extended(TFX)の略称で、Google 社内で開発された TensorFlow を基にした機械学習基盤のプロジェクト。 TFX の各コンポーネントは OSS として公開されている。\n抄訳 Abstract ソフトウェアエンジニアリングは成熟しつつあるが、一方で機械学習エンジニアリングはまだ成熟しきっていない。この論文では Alphabet で成功しているエンドツーエンドの機械学習基盤である Sybil, TFX の紹介を行う。主に TFX が機械学習エンジニアリングの実現にどう役立つかについて紹介していく。そして我々は機械学習による恩恵を受けるために、まずは機械学習チームの成熟と強固なインフラストラクチャ、組織内での教育活動が同時に必要であることを主張する。また、先進的なモデル開発を行う前に、容易に相互運用可能な機械学習基盤導入を強くすすめる。\nWhere We Are Coming From Sibyl 2002-2020 2007 年からSibyl は運用開始されており、2020 年まで運用されていた。最近では開発は廃止され、TFX に移行中であり、主に非深層学習をサポートする機械学習基盤である。\nTFX と同等のミドルウェアである Data Ingestion, Data Analysis and Validation, Training (of course), Model Analysis, and Training-Serving Skew Detection を解決するコンポーネントが存在している。\n(といっても Sibyl は TFX の先祖みたいなものなので、TFX がこれらの機能を踏襲してると考えるのが正しそうですね)\nTFX (2017 - ?) 2015 年からは深層学習の台頭により、TensorFlow がリリースされた。TensorFlow の課題点としてエンドツーエンドの機械学習基盤としては提供されてはいないことである。Sibyl は逆に深層学習用途に対する柔軟性が足りていないのが欠点であり、それらを補うために TFX の開発を開始したのは 2017 年のことだった。偶然にも TFX は Sibyl 誕生から約十年後に開発されことになった。\nTFX の運用開始から 3 年、Alphabet 内で TFX は企業レベルの機械学習基盤として数千のユーザーから使われており、数百の Alphabet (GCP 含む) の機械学習プロダクトを支えている。毎日数千の TFX のパイプラインが稼働し、エクサスケールのデータとともに数万ものモデルが生み出されており、TFX が提供する推論のリクエスト数は秒間数百万超えの規模になっている。TFX により Alphabet 内の研究成果がより早くプロダクションにリリースされ、機械学習基盤の開発よりもモデル開発に集中することが可能になった。\nTensorFlow が Alphabet 内部、そして外部でも人気と衝撃を与えたのと同じく、機械学習エンジニアから TFX にも人気と衝撃が走った。外部の機械学習エンジニアからの要望を叶えるために我々は TFX を徐々に OSS で公開していった。TFX, sibyl は Google 内部の強固なインフラストラクチャから構成されており、そのおかげで OSS での公開が加速した。例えば Sibyl は MapReduce と Flume を使ったデータ処理を行っており、TFX は同じくポータビリティ性の高い Apache beam をベースにしているので OSS での公開がスムーズに進んだ。\nTFX は最終的に 2019 年の頭に OSS として公開され、オンプレスミスや GCP の Cloud AI Platform Pipelines などで活用可能になった。パートナー企業も TFX を活用し始めており、実践的な機械学習の開発速度が改善されはじめている。\n参考文献: The Winding Road to Better Machine Learning Infrastructure Through Tensorflow Extended and Kubeflow\nLessons From Our 10+ Year Journey Of ML Platform Evolution Google での機械学習基盤の進化の旅は長くそしてワクワクするものであり、ぜひとも我々が学んだことをここで共有したい。\n我々が学んだことは大きく 2 つに分類される。\nこの長い旅で\n 何が変わり 何が変わらなかったのか  だ。Sibyl と TFX の開発を通じで学んだことを共有するが、Google 外部での機械学習基盤開発にも適用可能だと我々は信じている。\nWhat Remains The Same And 何が変わらず、そしてなぜそれが残ったのか この章では Sibyl, TFX の両者の長期間の運用期間にも耐えることができた機械学習とインフラストラクチャの観点についてまず説明を行う。\nApplied ML Rules of Machine Learning という Google 内部での機械学習プロダクト開発で学んだ経験則を我々は公開している。\nRules of Machine Learning の一例\n シンプルなルールベースとヒューリスティックから開発をまずはじめ、そこからデータを生み出し学びを深めよう。基本的にはここでルールベースのサービングシステムを開発する。 次に単純な機械学習モデルに移行することで、大きな利益を実現させます。ここでやっと機械学習パイプラインが導入される。 特徴量を増やし機械学習モデルも先進的なものを適用してみる。 SOTA を達成した機械学習モデルを適用してみまる(管理コストは増大しパイプラインも複雑になるが、それに見合った価値があることが前提である。) 上記のサイクルを念頭に置いて機械学習プロダクトの開発に役立ててみよう。そして常に費用対効果(ROI)は忘れないようにするのが大事である。  我々は Rules of Machine Learning がたとえサービスドメインや稼働する基盤が異なったり、時間が経過したとしても揺るぎない価値があると信じている。特に機械学習エンジニアリングの観点において Rules of Machine Learning は我々とその読者の致命的な失敗を避ける助けになると信じており、TFX は上記の流れを極めて高速に試行するのに役立つ。\nThe Discipline Of ML Engineering Rules of Machine Learning の作成を通じ、我々はソフトウェアエンジニアリングを基にした複雑なコードとデータの処理が実行可能な頑強なシステムの理念を学んだ。ここで我々は機械学習エンジニアリングを以下のように定義する。\n 機械学習エンジニアリングの定義: ソフトウェアエンジニアリングを包含する形での実践的な機械学習の複雑性を制御するための学問  実際、機械学習エンジニアリングのすべてをまとめようとするのは、なかなか難しいが\n 我々の持っている限られた理解していることは、プラットフォームと時間を超えてうまく稼働している 過去から学び、相似のものとしてみなすことは強力なアプローチであり、ソフトウェアエンジニアリングの様々な視点は、機械学習エンジニアリングにより機械学習プログラミングをどう進化させれるかが分かる  参考書籍: Software Engineering at Google: Lessons Learned from Programming over Time\nTFX に関する基礎的な考えは、TFX: A TensorFlow-Based Production-Scale Machine Learning Platform にて公開されており、現在 ML metadata を軸にして OSS として公開されている。\nでは、これから機械学習エンジニアリングの基礎的な要素について、ソフトウェアエンジニアリングとの相似を基に説明を行う。\nData ソフトウェアの中心がコードであるように、データは機械学習の中心的な存在である。データマネジメントはプロダクション環境での機械学習適用において挑戦的な課題である。\n参考文献: Data Management Challenges in Production Machine Learning\nまずはデータに対するユニットテストを考える。ユニットテストはコードがどのように振る舞うべきかを検証する。同様に、データ形式への明示的な期待(スキーマ、不変量や分布)を設定し検証を行うことができる。\n参考文献: Data Validation for Machine Learning\nコードリポジトリとバージョンコントロールは、コード管理の軸となっている。。データ管理を行うことができるシステムはまた機械学習エンジニアリングでも同様に重要である。TFX の ExampleGen, StasticsGen, SchmaGen と Example Validator の各種コンポーネントにより継続的な機械学習パイプラインのデータ管理、分析、検証が可能になりデータをコードのように扱うために非常に役に立つ。\n参考文献: TensorFlow DataValidation , The ExampleGen TFX Pipeline Component\nModels (機械学習モデル) ソフトウェアエンジニアがコンパイルされたコードを作成するのと同じように、機械学習エンジニアは、データとコードを基に「コンパイル」された機械学習プログラムを作成する。コンパイルされたものが、一般的には機械学習モデルとして知られている。この 2 種類のプログラムの性質は大きく異なっている。ソフトウェアはプログラムを通して一貫性を保つが、機械学習モデルは一貫性を保つことが非常に難しい。またこの一貫性の検証は何かしらの要約された形式でしか検証できない。(例えばラベルデータの部分集合で十分な精度が出ているなどの形でしか機械学習モデルの一貫性の保証ができない)\nコードとデータは時間経過により変化し、モデルもそれに伴い変化する。しかし、モデルの変化はコードやデータの変化に比べて更に複雑になる。例えば、コードに対する高いテストカバレッジはコードの一部の正しさと変化に対する高い信頼性を得ることができるが、モデルに対するテストでは、分布外のデータや因果関係を理解できるデータを作成しテストすることはとても困難である。\n同じく結合テストでも同じ問題が発生し、end-to-end でのモデルの検証と理解が機械学習エンジニアリングの重要な構成要素である。\n参考文献: Slice Finder: Automated Data Slicing for Model Validation\nTFX の evaluator と InfraValidator コンポーネントはモデルの検証と理解についての機能を提供する。これらもまた機械学習エンジニアリングの大事な一要素である。\nThe Evaluator TFX Pipeline Component\nMergeable Fragments ソフトウェアエンジニアがプログラムを既存のパッケージを合わせて作成するのと同じように、機械学習エンジニアはコード、データ、解析、モデルの断片を合わせて機械学習パイプラインを構築する。ソフトウェアエンジニアリングと機械学習エンジニアリングの決定的な違いは、コードは不変だが、データは常に揮発性を持ってり、変化する点である。(基本的にデータは常に新しくなり続ける)。例えば、入力データの一部分でも傾向が変化した場合には、それに対応して新しいモデルを作成する必要がある。\nこのように作成される生成物は常に mergeable (併合可能) であることは、機械学習パイプラインにおいて重要である。例えば一つのデータセットの統計値の要約は、2 つのデータセットの和集合の統計値を簡単に要約できるように、もう一つのデータセットへ簡単に併合可能であるべきである。またモデルの観点から言えば、一つのモデルの学習を別のモデルへと簡単に転移学習できるようにしておくべきである。\nしかしこれは前章で述べたモデルのテストカバレッジと同じ課題を抱えている。新しくモデルにフラグメントをマージすると、分布外データや反実仮想の評価データの作成が必要になる可能性があり、それによってモデルの複雑性があがってしまう。\nTFX の Example Gen, Transform, Trainer, Tuner のコンポーネントはTensorFlow Hub で管理され、併合可能なパイプラインを作成することに役立ちます。\nArtifact Lineage (生成物の経路追跡) ソフトウェアエンジニアリングは方法論やツールなどが高度に発展しているが、それでも常にデバッグが必要である。それは機械学習プログラムも同様の課題があるが、ソフトウェアエンジニアリングと比べると一段と難易度が増す。なぜなら、機械学習プログラムではプログラムに付随する生成物が難易度への影響を与えている。モデルのデバッグを行う際にコードの欠陥、学習アルゴリズム、学習データ、サービングパス、サービングされるデータなどいくつものエラー原因があり、それらが原因で精度の低下につながる。ソフトウェア開発において、根本原因を探すためにスタックトレースが重要なように、機械学習パイプラインではすべての生成物の生産と消費の追跡が機械学習モデルの精度劣化の根本原因の解明に役立つ。\nTFX ではML Metadata (MLMD) を使用して、生成物を管理している。MLMD は、生成物のメタデータ管理と追跡を可能にし、機械学習パイプライン外に高い信頼性を持って生成物の管理が可能になる。\nContinuous Learning And Unlerning プロダクション環境下での機械学習パイプラインは、動的な環境下で以下の特徴がある。\n 定期的に新しいデータが到来する モデルのコードは、初期段階では頻繁に変化する インフラ面も変化する  もし上記の変化が発生した場合、パイプラインはそれらに対して追従する必要があり、変更後の環境でパイプラインを新たに実行する必要がある。パイプライン実行の監視は、デバッグと原因究明の解析にとても有用である。例えば単純な例では、モデルの失敗原因をデバッグするためには、どのデータが学習済のモデルに使われたかだけではなく、モデルコードとパッケージのバージョンの把握が必要不可欠である。\n機械学習パイプラインはそれらの変化に対して対応できる仕組みが必要である。例えば新しいデータが到来した際には、モデルの再学習が必須である。これらは推薦システムや広告関係などのように迅速に変化する環境においてごく自然に要求される。もしデータが頻繁かつ定期的に変化する環境だった場合、エンジニアが手作業でモデルの再学習を実行することが要求されているならそれは非現実的である。その代わりに TFX はパイプラインが新規のデータを検知して、モデルの再学習を行うことで継続的学習を実現し、手作業を自動化することができる。その自動化を実現するためには高度なオーケストレーションが必要である。また機械学習パイプラインは、何年間もコードとデータを取り込み、継続的に意思決定に役立つ予測を行うモデルを作成するのが一般的である。\n参考文献: Continuous Training for Production ML in the TensorFlow Extended (TFX) Platform\n次に機械学習パイプラインのために必要とされる仕組みの例では、backfilling 機能がある。例えば、モデルのパッケージやコードは更新済のものを既存のデータに適用するケースなど、エンジニアはコンポーネントが更新された状態でパイプラインを実行する必要があるかもしれない。次の backfilling が必要となる状況例として、データ関連の問題を解決するために、既存のデータの新しいバージョンに対してパイプラインを実行することが考えられる。これらの backfill は継続的学習のために直交である必要がある。実例では、エンジニアは手作業で学習ジョブを実行可能であり、モデルが生成後、そのモデルは自動的にモデルの評価と検証が実行が可能である。\nThe TFX User Guide ではまだ継続的機械学習パイプラインを公開できていないが、この公開に向けて我々は動き続けている。 参考: RFC: TFX Advanced DSL semantics #253\nInfrastructure Building On The Shoulders Of Giants 野心的な目標実現のために、強固な基盤を作る必要がある。TFX は Sibyl のシステムデザインの多くを再利用を行った。\n Sibyl のアルゴリズムとワークフローは MapReduce を基に実現されていたが、TFX は TensorFlow, Apache Beamを使って、分散学習とデータ処理のワークフローを実現した Sybyl はカラムベースのデータ管理を行っていたが、Apache arrow によりインメモリでのカラムベースのデータ管理に適合した。  また、頑強な標準化が進んでいる箇所では、依存性をあえてもたせることにより TFX とその使用者にパフォーマンスとスケーラビリティを提供することに成功した。例えば、TFX の依存先でもあるKubeflow Pipelines や Apache Airflow はそれらを使用する利益が依存先の管理に勝ると判断されたので選択されました。\nInteroperability And Positive Externalities 機械学習基盤は孤立した環境で運用されるのではなく、既存のデータ基盤からの ETL やシステムへのモデルのデプロイなどの相互運用を意識する必要がある。\n Google Ads で Sibyl が相互運用されていたのと同じく、TFX では複数のサービスからのデータの ETL、サービングが複数の開発環境やデバイスに提供している Sibyl で Google 社内技術で相互運用されていたのと同様に、TFX は Apache Beamは Apache Flink や Apache Spark のクラスターやGoogle Cloud Dataflow などのサーバーレスデータ処理を実行するために活用されている TFX はオーケストレーション機能の抽象化を MLMD を用いて実現し Apache Airflow, Apache Beam, Kubeflow Pipelines も選択肢の一つとして採用できるようにしている  上記で説明したように、運用が簡単になるような技術群を採用しており既存のデータ基盤との接続や、サービングなどが Alphabet 外でも容易に構築可能になっている。\nまた複数の技術の組み合わせは指数関数的に開発環境の設定が増加するため、マネージドサービスである Could AI Platform Pipelines も提供を開始した。\nWhat Is Different And Why ここでは、TFX を現実的な問題に適応するために変化させた部分を説明する。\nEnvironment And Device Portability Sibyl は Google での大規模クラスタである Borg へのデプロイを前提にデザインされていた。当初、Google での機械学習適用についてこれで問題なかったが、世界的に機械学習技術の発展に伴い、Google 外部かつ大規模だけではない小規模な環境へのデプロイの必要性も高まった。その結果、ポータビリティが重視された機械学習基盤が必要になってきた(が厳密な一貫性を保つことももちろん要求される)。\n Sibyl が Google のデータセンターでのみ動くが、TFX はラップトップ、ワークステーション、外部のデータセンター、クラウド上で実行することができる。特にGoogle Cloud では最適化と自動化された TFX が提供される Sibyl は CPU でのみ動くが、TFX は異なるハードウェア(CPU, GPU, TPU )で動く Sibyl で作成されたモデルはサーバー上での実行のみ可能だが、TFX はラップトップ、サーバー、TensorFlow Serving, Apache Beam、モバイルや IOT では TensorFlow Lite, ブラウザ上では TensorFlow JSによって実行可能である  小規模から大規模な問題を解くために、TFX のポータビリティ性は洗練され多種多様な環境、デバイスでの実行を可能にしている。\nだがポータビリティはコストも高くなってしまう。メンテンスを行う際に、環境固有もしくはデバイス固有の特殊な要求を満たす必要があり、環境やデバイスの数が増えるとそれらは超線形にコストが増加する。が、一般の使用者はそれらのメンテンスコストは意識せず TFX を使用することが可能です。\nModularity And Layering Sibyl は既存のプロダクトと密結合され、モノリシックに構成されており、既存の仕様に把握して、それに合わせる必要があった。対象的に TFX はモジュラー化とレイヤー化が意識されたアーキテクチャ設計になっており、チーム間で連携しつつ開発を行えるようになっている。\nTFX のレイヤーデザイン\n ML Service  Cloud AutoML Cloud Recommendations AI Cloud AI Platform Cloud Dataflow Cloud BigQuery   Pipelines  TensorFlow Extended   Binaries  TensorFlow Serving   Libraries  TensorFlow DataValidation TensorFlow Transform TensorFlow Hub TensorFlow Model Analysis TFX Basic Shared Libraries ML Metadata    TFX のレイヤーアーキテクチャは、使用者の必要性に合わせて、開発者にはライブラリから、パイプライン、エンドユーザー向けには ML Service などを提供可能にしている。\nMulti-faceted Flexibility Sibyl は開発当初は他の利用可能なツールに比べると、柔軟性があったが機械学習ワークフローの高速化の需要に答えることができなかったことも TFX の開発に繋がった。\n Sibyl は特定のデータ解析しか提供していなかったが、TFX は TensorFlow DataValidation により柔軟なデータ分析が可能になっている Sibyl は特定の mappers しか提供していなかったが、TFX は TensorFlow Transform により、カスタマイズ可能な柔軟性に長けた mappers, analyzers が使用可能になっている Sibyl は非深層学習のみサポートしていたが、TFX は Trainer Component により TensorFlow ベースのモデルを提供可能になっており、TensorFlow Hub などで転移学習なども容易に行えるようになっている Sibyl は非深層学習モデルに対する自動的な特徴量の組み合わせ(feature conjunction) のみ提供していたが、TFX では Tuner component により state of the art ベースのハイパーパラメータチューニングが利用可能になっている Sibyl は特定のモデルの解析手法しか提供していなかったが、TFX は TensorFlow Model Analysis により、Evaluator Component をベースにビルトインのメトリクスなど多様なモデル解析手法を提供している Sibyl は 固定されたパイプラインしか扱えなかったが、TFX はカスタムコンポーネントが使用可能であり、柔軟にパイプラインを設計可能である  Where We Are Going 2020 年において、TFX の開発はroadmap、 RFC、 Contribution guideline を基に進めています。我々は応用機械学習の普及のために、機械学習エンジニアリングのさらなる発展に尽力し、responsible AI (TensorFlow を用いた高い信頼性を持つ機械学習開発のための方法論)をGoogle の AI 原則論 に基づき実世界へ適用していく。\nDrive Interoperability And Standards 相互運用性を高め、我々は原則論である、社会に利益をもたらすことを意識していく。我々のミッションとして、我々は OSS による先進的な機械学習システム構築をサポートし、機械学習による生成物の標準化も行っていく。例えば具体例として上げると\n TFX standardized input TFX DSL semantics, Data model and IR 標準化された機械学習生成物とメタデータ 多様な実行環境での分散実行環境の標準化 分散かつストリーミングでのモデルの推論 モバイルやエッジでの相互運用を意識した改善 相互運用を意識した機械学習フレームワークの改善  Increase Automation 自動化こそ、高信頼性のプロダクションシステムの柱となる考えであり、TFX は自動化にとてつもなく投資を行っている。我々の原則論である”安全のためのビルドとテスト”、””\nTFX pipeline では開発が進むことで、 TFX Tuner によるモデルの自動改善や、教師あり学習のモデルの多次元でのスライスでの挙動の自動検知、Model Cards の自動生成もサポート、training-serving skew の自動検知も開始された。GCP での TFX は先進的な機能が提供されていく。\nImprove ML Understanding 機械学習を理解することは、プロダクション環境下に機械学習を適用する際に重要な側面の一つである。機械学習を理解するためにはモデル作成に付随する生成物の追跡( lineage) は最も重要事項の一つである。struct2tensor (TensorFlow 内部の構造化データのパーシングライブラリ)は構造化データにおいて、学習やサービング、解析においてより役立つ TFX の技術の一つである。\nUphold High Standards And Best Practices 高水準の標準化とベストプラクティスの共有のために、TFX チームはこれからも科学的論文の執筆・公開を継続していくことで Alphabet 外にもこのベストプラクティスが普及することを狙っていく。AutoML pipelines のベンチマークツールとしてNitroMLも再現性の鍵の一つとなる。\nImprove Tooling TFX は機械学習エンジニアリングと機械学習ライフサイクルのいくつもの段階のためのツールを提供しており、いまだこの領域は黎明期であることを願っている。\n大規模かつ重要度が高いストリーミングかつレイテンシーが要求されるデータは挑戦的な課題である。TFX では Apache Beam と Cloud Dataflow の実行によりストリーミングイベントにも対応した予測を実行可能になっている。我々はこの枠組みを TensorFlow Serving でも同等のことが行えないか計画している。\n更に、たくさんのツールが機械学習ワークフローでもまだ自動化が必要とされているものが多い。例えば、機械学習パイプライン実行時に、そのジョブの結果が現行のモデルよりもパフォーマンスが低いことを積極的に予測しジョブの実行を停止することで、大幅なリソースと実行時間の削減が期待できる。\nA Joint Journey TFX の構築は、機械学習エンジニアリングの基礎の探索でもあり、多くの人々の長年の努力の累積である。\nさあ、我々もあなたを\u0026quot;機械学習エンジニアリングに向けた\u0026rdquo; 旅に招待します。\n","permalink":"https://shunyaueta.com/posts/2021-01-17/","summary":"この記事はMLOps Advent Calendar 2020の 25 日目の記事です。(盛大に遅れました)\nKDD2019 の招待講演で Google が TFX の歴史について発表されており、TFX 信者の自分としては発表内容が以前から気になっていたが、公開はされておらずなんとかして見れないかな~と思っていましたが、TensorFlow の Blogで該当の招待講演が論文化されたことを知ったのでメモがてら抄訳として残しておく。\n 注意)この翻訳記事は原著論文の著者陣からレビューはされていません Shunya Ueta, are providing a translation and abridgment, which has not been reviewed by the authors.  Citation  Karmarkar, A., Altay, A., Zaks, A., Polyzotis, N., Ramesh, A., Mathes, B., … \u0026amp; Li, Z. (2020). Towards ML Engineering: A Brief History Of TensorFlow Extended (TFX). arXiv preprint arXiv:2010.02013. ***\n Towards ML Engineering with TensorFlow Extended (TFX) at KDD2019 Towards ML Engineering with TensorFlow Extended (TFX) ACM PDF は arxiv でも閲覧可能  https://arxiv.","title":"TFXの歴史を振り返りつつ機械学習エンジニアリングを提案する論文「Towards ML Engineering: A Brief History Of TensorFlow Extended (TFX)」"},{"content":"TensorFlowの勉強をしていたら、Apache beam を前処理に採用していたケースがあり、興味を持ったので深堀りしてみます。\n興味が湧いたモチベーションとしては、\n データ量が増加しても前処理部分を難なくスケールできそう(前処理部分をスケールさせて高速に実験を回したい、並列化などはすべて良い感じにbeamに任せれそう バッチとストリーミングの両者に対応可能なので、柔軟な機械学習の推論サービスが提供できるのでは? (GCPの参考資料 Data preprocessing for machine learning: options and recommendations) Apache beam を触りつつ分散データ処理を学びたい  https://github.com/jhuangtw/xg2xg#services\nを見てみるとGoogle 内部のFlume という並列データパイプライン技術がApache beam として公開されているみたいです。\n Apache beam について端的に説明すると\nApache beam は3つの考えを基礎にしています。\n Unified  ストリーミング、バッチの両者のケースに一つのプログラミングモデルで対応可能な統一性   Portable  実行パイプラインが複数の実行環境で実行可能な可搬性   Extensible  新しいSDK、IO Connectorsや変換ライブラリなどをを書いて共有することができる拡張性    Java, Python, Go やScalaを使ってBeam Model を作成して任意のruntime で実行する流れです。\n自分はPythonが一眼手軽に書けるのでこの記事ではPythonで紹介していきます。\nVersion 2.14.0 からPython 3がサポートされたのは非常にありがたいですね。それまではPython 2のみをサポートしており、その影響で技術選定時に採用しづらかったのでは無いのでしょうか?\nSpotify が作成しているApach BeamとDataflowのScala APIであるscioが開発されており、そちらも気になっています。\n https://github.com/spotify/scio https://engineering.atspotify.com/2017/10/16/big-data-processing-at-spotify-the-road-to-scio-part-1/  では、まずは実際に動かしながら学んでみようということで\nhttps://beam.apache.org/get-started/try-apache-beam/\nを参考にApache Beam をPython SDKで試してみます\nCOLABで実行を試せるので便利ですね\nですが、Python2で実行されるように設定されているのでPython3で実行してみました。\n実行したcolab のコードを見ていきます。\n環境準備 apache-beam のinstallとGCSからApache beamで処理を行うテキストファイルをダウンロードします。\n1 2 3 4 5 6 7 8 9 10 11 12  # shell コマンドを実行して表示する関数 def run(cmd): print(\u0026#39;\u0026gt;\u0026gt; {}\u0026#39;.format(cmd)) !{cmd} print(\u0026#39;\u0026#39;) # Install apache-beam. run(\u0026#39;pip install --quiet apache-beam\u0026#39;) # 対象ファイルの格納ディレクトリを作成後、gsutil を使って /data ディレクトリに格納 run(\u0026#39;mkdir -p data\u0026#39;) run(\u0026#39;gsutil cp gs://dataflow-samples/shakespeare/kinglear.txt data/\u0026#39;)   文字のカウント Hello World として単語のカウントを行うデータ処理をbeam で記述してみます。\nテキストファイルを読み込んで、各単語の頻度のカウンティングを行う単純なデータパイプラインを作成しています。\nパイプラインの結果はファイルシステム上で保存されるので分散環境下での大規模処理でも取り扱いに役立ちます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  import apache_beam as beam import re inputs_pattern = \u0026#39;data/*\u0026#39; outputs_prefix = \u0026#39;outputs/part\u0026#39; with beam.Pipeline() as pipeline: ( pipeline | \u0026#39;Read lines\u0026#39; \u0026gt;\u0026gt; beam.io.ReadFromText(inputs_pattern) | \u0026#39;Find words\u0026#39; \u0026gt;\u0026gt; beam.FlatMap(lambda line: re.findall(r\u0026#34;[a-zA-Z\u0026#39;]+\u0026#34;, line)) | \u0026#39;Pair words with 1\u0026#39; \u0026gt;\u0026gt; beam.Map(lambda word: (word, 1)) | \u0026#39;Group and sum\u0026#39; \u0026gt;\u0026gt; beam.CombinePerKey(sum) | \u0026#39;Format results\u0026#39; \u0026gt;\u0026gt; beam.Map(lambda word_count: str(word_count)) | \u0026#39;Write results\u0026#39; \u0026gt;\u0026gt; beam.io.WriteToText(outputs_prefix) )   はい、いきなり\nwith beam.Pipeline() as pipeline: の以降から意味がわからなくなりました。\nApache beam特有の概念を理解する必要があるので\nhttps://beam.apache.org/documentation/programming-guide/\nを参考に解説してみます。\nApache Beam Programming Guide Beasm SDKで提供されるクラス郡をここでは紹介していきます。このクラス郡を使うことでデータパイプラインを作成することができます。\nOverview まずBeamを使用するためには、まず最初にBeam SDKのクラスを使って起動プログラムを作成する必要があります。driver program は、あなたのパイプライン(入力、変形と出力のすべて)と実行環境を定義する必要があります。\nBeam SDKは大規模なデータ処理のメカニズムを単純な形で抽象化している\n Pipeline  Pipeline はデータ処理タスクの実行開始から終了までをカブセル化するクラスです。これは入力データの読み込みやデータの変形、出力データの書き込みを含む。Beam driver programs は必ずPipelineを作成します。またPipelines 作成時には、実行オプション(どの実行環境下でどのように実行するか)を必ず明記する必要があ。   PCollection  Pcollection は分散データセットを表現するクラスです。ここでのデータセットは bounded (ファイルなどの固定されたソース、つまりバッチ)とunbounded (subscriptionなど連続的にアップロードされるソース、つまりストリーム)の両者を指しています。実行するパイプラインは外部データの読み込みによって初期化されたPCollectionによって構築されます。また外部データだけでなく、インメモリのデータからPCollectionを作ることも可能です。つまり、PCollectionはPipelineの出力と入力を担当する。   PTransform  PTransform パイプラインでのはデータ処理命令を表現するクラスです。すべての PTransform は一つ以上のPCollection オブジェクトを入力として受け取り、ゼロもしくはそれ以上の数のPCollectionを出力として作成する   IOS transrforms  Beamはいくらかの入力と出力のインタフェースがあり、PTransformが読み込み、もしくは書き込みを多種多様な外部のストレージシステムに対して行う    基本的なBeamの起動プログラムは以下の手順で動く\n Pipeline オブジェクトを作成し、パイプラインの実行オプションとパイプラインランナーを設定する Pipeline データのために初期のPCollection を行う。そのためにIOs を用いて外部のストレージシステムもしくはインメモリデータから PCollectionを作成する。 PTransform を各 PCollection へ適用する。Transform は新しい出力としてPCollection を作成する。PCollection を変数、PTransformは関数として考えると、Pipelineは変数と関数からなる複雑な処理グラフとして捉えることができる。 IOs を使用し、最終的に PCollection を外部ソースに書き出す パイプラインをパイプラインランナーで実行する  あなたが Beam の起動プログラムを実行した時、パイプラインランナーはPCollection を基に作成した transformが適用される workflow graph が構築される。このグラフは適切な分散処理バックエンドで、非同期ジョブとして実行される。\nここまでで Beam の基礎的な概念を理解できたと思うので最初のサンプルコードでの各行の実行内容について解説します\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  import apache_beam as beam import re inputs_pattern = \u0026#39;data/*\u0026#39; outputs_prefix = \u0026#39;outputs/part\u0026#39; # ローカル環境でDirectRunnerを実行 with beam.Pipeline() as pipeline: # 文字の集計データをPCollection に格納 # 各要素は (word, count) のタプルであり、(str, int)の型となっている word_counts = ( # 入力のPCollection は空のパイプラインとする pipeline # テキストファイルから行を読み込む | \u0026#39;Read lines\u0026#39; \u0026gt;\u0026gt; beam.io.ReadFromText(inputs_pattern) # Element type: str - text line # 正規表現を利用して行内のすべての単語に反復処理を行う # FlatMap will yield an element for every element in an iterable. | \u0026#39;Find words\u0026#39; \u0026gt;\u0026gt; beam.FlatMap(lambda line: re.findall(r\u0026#34;[a-zA-Z\u0026#39;]+\u0026#34;, line)) # Element type: str - word # 単語が存在した場合、value が１となるkey-value のペアを作成 # すべての単語を集計していき、同一単語をグループ化する | \u0026#39;Pair words with 1\u0026#39; \u0026gt;\u0026gt; beam.Map(lambda word: (word, 1)) # Element type: (str, int) - key: word, value: 1 # sum() 関数を使ってkeyごとにグループを行う | \u0026#39;Group and sum\u0026#39; \u0026gt;\u0026gt; beam.CombinePerKey(sum) # Element type: (str, int) - key: word, value: counts ) ( # 入力となるPCollection は上記で作成された word_counts # 結果は文字列に処理することで、テキストファイルとして書き込み可能にする | \u0026#39;Format results\u0026#39; \u0026gt;\u0026gt; beam.Map(lambda word_count: str(word_count)) # Element type: str - text line # 最後に結果をファイルに書き込みます。 | \u0026#39;Write results\u0026#39; \u0026gt;\u0026gt; beam.io.WriteToText(outputs_prefix) ) #20個の結果を各ファイルから出力してみる。その際に順序は保証されない run(\u0026#39;head -n 20 {}-00000-of-*\u0026#39;.format(outputs_prefix))   実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features. \u0026gt;\u0026gt; head -n 20 outputs/part-00000-of-* (\u0026#39;KING\u0026#39;, 243) (\u0026#39;LEAR\u0026#39;, 236) (\u0026#39;DRAMATIS\u0026#39;, 1) (\u0026#39;PERSONAE\u0026#39;, 1) (\u0026#39;king\u0026#39;, 65) (\u0026#39;of\u0026#39;, 447) (\u0026#39;Britain\u0026#39;, 2) (\u0026#39;OF\u0026#39;, 15) (\u0026#39;FRANCE\u0026#39;, 10) (\u0026#39;DUKE\u0026#39;, 3) (\u0026#39;BURGUNDY\u0026#39;, 8) (\u0026#39;CORNWALL\u0026#39;, 63) (\u0026#39;ALBANY\u0026#39;, 67) (\u0026#39;EARL\u0026#39;, 2) (\u0026#39;KENT\u0026#39;, 156) (\u0026#39;GLOUCESTER\u0026#39;, 141) (\u0026#39;EDGAR\u0026#39;, 126) (\u0026#39;son\u0026#39;, 29) (\u0026#39;to\u0026#39;, 438) (\u0026#39;Gloucester\u0026#39;, 26)   どうでしたか? まだサンプルコードを動かしただけなので使いこなす自信はありませんが、Apache beam がどのような考えで設計され動かすことができるかが少しはつかめたのでは無いのでしょうか?\n次回は典型的なデータ処理をApache beamで動かしてみたいと思います。\n","permalink":"https://shunyaueta.com/posts/2020-12-26/","summary":"TensorFlowの勉強をしていたら、Apache beam を前処理に採用していたケースがあり、興味を持ったので深堀りしてみます。\n興味が湧いたモチベーションとしては、\n データ量が増加しても前処理部分を難なくスケールできそう(前処理部分をスケールさせて高速に実験を回したい、並列化などはすべて良い感じにbeamに任せれそう バッチとストリーミングの両者に対応可能なので、柔軟な機械学習の推論サービスが提供できるのでは? (GCPの参考資料 Data preprocessing for machine learning: options and recommendations) Apache beam を触りつつ分散データ処理を学びたい  https://github.com/jhuangtw/xg2xg#services\nを見てみるとGoogle 内部のFlume という並列データパイプライン技術がApache beam として公開されているみたいです。\n Apache beam について端的に説明すると\nApache beam は3つの考えを基礎にしています。\n Unified  ストリーミング、バッチの両者のケースに一つのプログラミングモデルで対応可能な統一性   Portable  実行パイプラインが複数の実行環境で実行可能な可搬性   Extensible  新しいSDK、IO Connectorsや変換ライブラリなどをを書いて共有することができる拡張性    Java, Python, Go やScalaを使ってBeam Model を作成して任意のruntime で実行する流れです。\n自分はPythonが一眼手軽に書けるのでこの記事ではPythonで紹介していきます。\nVersion 2.14.0 からPython 3がサポートされたのは非常にありがたいですね。それまではPython 2のみをサポートしており、その影響で技術選定時に採用しづらかったのでは無いのでしょうか?\nSpotify が作成しているApach BeamとDataflowのScala APIであるscioが開発されており、そちらも気になっています。\n https://github.com/spotify/scio https://engineering.","title":"PythonでApache beam 入門"},{"content":"機械学習エンジニアをしている hurutoriya が機械学習、Software Engineering 周りに関する学んだことなど最近楽しいと思っていることを発信していくPodcast です\n\n扱うトピックとしては、普段学んだ機械学習、ソフトウェアエンジニアリング、読んだ論文やデータ分析などについて扱っていきます。 もし、ご興味あれば購読お願いします。主要なPodcast Platformで購読可能です。\n just4fun.fm Google Podcasts Apple Podcasts Pocket Casts  ご感想などは Twitter ハッシュタグ #just4funfm でお待ちしております\nなぜ Podcast なのか？ 以前からPodcastは日常的に聴いていて、自分の中でもすごく好きなメディアのうちの一つです。 前からやってみようかなと思っていたんですが、周りの人たちがPodcastを始めたのを機によし自分もやるかと始めてみました\nBlogと比べてみると、\n ブログ書くよりかはサラッと配信できる、音声で配信するのも新鮮で面白い その代わり、換気扇とかの音が気になり音源環境とかはめちゃくちゃ気を使う 発表レベルの構造化したトークよりも雑談レベルでワイワイするほうがPodcastは向いてる感じはする。前者だと準備やらなんやらで公開できずに死の谷に落ちそう 喋っているうちに思考が洗練されていくので、あまり気負いすぎずに喋りたい事を発信していくのが良さそう  しばらく試験的に続けていきたいと思います。\nAnchor 凄い  AnchorでPodcastを始めると、自動的にAppole Podcast, Google Podcast, Pocket CastなどPodcastのプラットフォームに自動配信される。  Google Podcastは、配信開始と通知が来ても実際は2-3日後に視聴可能になると問い合わせたら返信がきた     Support here. Sorry for the confusion with your podcast on Google. Allow me to explain this to you. We have noted that for some users the podcast link takes a week or so to become active, so it should begin working soon! If you\u0026rsquo;re still not seeing it update in the next few days though, I suggest reaching out to Google directly to see why your podcast doesn\u0026rsquo;t seem to be appearing on their end and be sure to let them know that the link doesn\u0026rsquo;t appear to be working. Thanks so much for understanding as we continue to distribute your podcast! All the best,\n Anchor は Spotify に買収されているので、サービス生存戦略的にも安心なので今回使ってみました\nサービス開発もかなり熱心にやっているので、これからの進化が楽しみです。 URLを送るだけで、共同収録が可能な機能はゲストがZoomとか使えない場合にはすごく気軽に招待できるので便利。難点としては、スマホアプリをホストとして動くので、web appからでも収録できるようにして欲しい\nちょっと微妙な点としては、Episode description がうまくパースされずHTMLのリンクがanchor上だと死んでたり(他のGoogle Podcastなどはうまくパースされている)、音声データを扱う宿命でeditor がめちゃくちゃ重かったりする。後はスマホファーストでの開発の影響かweb appの機能が必要最低限なのは、優先度の問題で仕方ないと思いつつ改善されたら嬉しい・\nあと全然関係ないですが、GoogleがPodcastの内容を文字起こしして、エピソード検索のメタデータとして活用しているらしくて感動した。\n日本語の音声認識モデルもGoogleで問題なく使われるクオリティになれば、Podcastの中身を検索できるのも近いですね\nチラ裏 Podcast の由来は僕の好きな書籍の一つである それがぼくには楽しかったから 全世界を巻き込んだリナックス革命の真実 から引用しました。\n","permalink":"https://shunyaueta.com/posts/2020-09-27/","summary":"機械学習エンジニアをしている hurutoriya が機械学習、Software Engineering 周りに関する学んだことなど最近楽しいと思っていることを発信していくPodcast です\n\n扱うトピックとしては、普段学んだ機械学習、ソフトウェアエンジニアリング、読んだ論文やデータ分析などについて扱っていきます。 もし、ご興味あれば購読お願いします。主要なPodcast Platformで購読可能です。\n just4fun.fm Google Podcasts Apple Podcasts Pocket Casts  ご感想などは Twitter ハッシュタグ #just4funfm でお待ちしております\nなぜ Podcast なのか？ 以前からPodcastは日常的に聴いていて、自分の中でもすごく好きなメディアのうちの一つです。 前からやってみようかなと思っていたんですが、周りの人たちがPodcastを始めたのを機によし自分もやるかと始めてみました\nBlogと比べてみると、\n ブログ書くよりかはサラッと配信できる、音声で配信するのも新鮮で面白い その代わり、換気扇とかの音が気になり音源環境とかはめちゃくちゃ気を使う 発表レベルの構造化したトークよりも雑談レベルでワイワイするほうがPodcastは向いてる感じはする。前者だと準備やらなんやらで公開できずに死の谷に落ちそう 喋っているうちに思考が洗練されていくので、あまり気負いすぎずに喋りたい事を発信していくのが良さそう  しばらく試験的に続けていきたいと思います。\nAnchor 凄い  AnchorでPodcastを始めると、自動的にAppole Podcast, Google Podcast, Pocket CastなどPodcastのプラットフォームに自動配信される。  Google Podcastは、配信開始と通知が来ても実際は2-3日後に視聴可能になると問い合わせたら返信がきた     Support here. Sorry for the confusion with your podcast on Google. Allow me to explain this to you.","title":"機械学習・ソフトウェアエンジニアリングをテーマにしたPodcast just4fun.fm を始めてみた"},{"content":"機械学習エンジニアとして働き始めて2年7ヶ月が経過した。\n機械学習エンジニアというロールは会社によって期待される内容が異なってくるが、今の会社で働いてきた経験に基づき自分の中の機械学習エンジニアスキル構成論を整理してみる。\nTL; DR  人によって考える理想のスキル論は違うので他の人の持論を聞いてみたい テクいことをやりたい気持ちはあるが、地道なやるべきことがたくさんあるのがこの世界  自分の中で機械学習エンジニアにとって大事なスキル  Software Engineer 40%  機械学習サービス実装スキル   Product Manager 30%  機械学習プロジェクト自体を成功に導くスキル   Data Scientist 30%  データに基づき、意思決定し改善していくスキル    Machine Learning Engineer as Software Engineer なぜSWEの比率が一番上なのか？\nどれだけ良いモデルができたとしてもそれが活用されるシステムがなければ成果を出せないからです\n データを活用してインパクトの大きい課題を解決するための、サービスを実装して、運用して改善していく どの方法がベストか考えた上で、それを実現していく  例えば、R\u0026amp;Dスタイルでモデル開発と実装者を完全に分ける組織構造もあると思いますが、このスタイルはいろんな会社のお話を聞く限りは、組織構造がよほど洗練されていないとうまく稼働しないじゃないかなと思っている\nFull Cycle Developers at Netflixでは、システム開発のライフサイクルである\ndesign, development, test, deploy, operate, support\nを1チームが一気通貫で責任を持つスタイルをNetflixが提唱している。\n今所属している会社もMicro Serviceでの開発に注力していて、まさにFull Cycleスタイルで機械学習サービス開発を行っている。 個人的に機械学習プロジェクトとこの方式の相性の良いところは、例えば、職能ごとにモデル開発、システム開発と運用を行うメンバーを分割すると、\n モデルを作ってデプロイはしたが運用は他人任せになってしまい継続的な改善が回しづらい 役割が分離されていることで、モデルの詳細を完全に把握できないので実際のトラブル発生時に対応が困難 運用を考えてモデルがデザインされていないので運用者にしわ寄せがくる  などアンチパターンが数多く存在する\nMicro Serviceでの開発は上記の課題を解決して、作って終わりではなく自分たちでシステムデザインからサポートまで行うことで、そのサービスの継続的な改善に責任と自由を手にして開発することできる また、プロジェクトデザインの段階からシステム開発・運用を念頭に動くことができるので、やってみてうまく動かないなどの不確実性を大きく減少させる\nMachine Learning Engineer as Product Manager 機械学習プロジェクトは、POCなどで検証を行いプロジェクトが始まりますが最初の壁である\n 機械学習で解ける余地のある大きなインパクト(やる価値)のある問題  をまず自分たちのサービスで発見する必要があります。(正直コレが一番難しい)\nCourseraでHow Google does Machine Learning の講義を修了したの講義でもGoogleでの機械学習プロジェクトのマネジメントについて言及されていますが、\n そもそもデータがない Human In The Loopを導入していない(継続的なデータの自浄作用が存在しない)  などそれらの要素が欠けるだけで簡単にプロジェクトは失敗します。\n上記の講義でまず取り掛かるべき機械学習プロジェクトというのは、\n 現実世界で適用可能な機械学習というのは、自動化の技術です。 今まで人間が手作業で行っていた作業がログとして残っていて、そのデータを活用してそれらの作業を徐々に自動化していきます。\n 結論からして金言ですね、銀の弾丸など存在しない、機械学習エンジニアとして、テクいことはやりたいがその前にやるべき地道なことが無限にあるんです(切実)\nまた、システム開発要素以外にもデータ収集・活用の要素が入ってくるのでステークホルダーとの協力が必要不可欠です。 MVPとして小さな成功を高速に積み重ねていくことで、組織内での機械学習への理解・信頼が高まっていきます。\n個人的に機械学習プロジェクトと非常に相性が良いなと思っているのが、リーン・スタートアップの考え方です。\nその結果データ分析で判明した考察から、\n 既存のUIのこの部分を改善したい 今は取れていないデータを構造化した状態で保存するようにしたい  など優先度が上がりづらい提案も受け入れやすくなる土壌になっていきます。\n日に日に、データの整備やワークフローの改善なども重要ポイントなので、コンサルティングとしての能力は凄い重要だと感じる\nネット上で閲覧できる有用な機械学習プロジェクトマネジメントの日本語資料  nishio さんの機械学習キャンバス0.1 @yurfuwa さんのAI Project Management Anti Pattern ブレインパッドにおける機械学習プロジェクトの進め方  Machine Learning Engineer as Data Scientist 施策の効果検証、機械学習の知識、実験デザイン、データ分析などを活用して、プロジェクトの方向性、同改善していくべきかそれらをどう検証していくかがこの部分です。\n ちょっと疲れてきたのでこのへんで。\n機械学習エンジニアになった直後には、こんな抄訳記事も\n[抄訳] Data engineers vs. data scientists\nもあって当時は感銘を受けたが、今になって振り返るとかなり視座が変わったと思う\n経験を積む中で自分の中の軸が出来上がってきた証拠だと信じたい\nえ、仕事で機械学習をやっているのに仕事ではじめる機械学習を読んだことがない？ 神書籍なので、今すぐ買いましょう!\n","permalink":"https://shunyaueta.com/posts/2020-09-21/","summary":"機械学習エンジニアとして働き始めて2年7ヶ月が経過した。\n機械学習エンジニアというロールは会社によって期待される内容が異なってくるが、今の会社で働いてきた経験に基づき自分の中の機械学習エンジニアスキル構成論を整理してみる。\nTL; DR  人によって考える理想のスキル論は違うので他の人の持論を聞いてみたい テクいことをやりたい気持ちはあるが、地道なやるべきことがたくさんあるのがこの世界  自分の中で機械学習エンジニアにとって大事なスキル  Software Engineer 40%  機械学習サービス実装スキル   Product Manager 30%  機械学習プロジェクト自体を成功に導くスキル   Data Scientist 30%  データに基づき、意思決定し改善していくスキル    Machine Learning Engineer as Software Engineer なぜSWEの比率が一番上なのか？\nどれだけ良いモデルができたとしてもそれが活用されるシステムがなければ成果を出せないからです\n データを活用してインパクトの大きい課題を解決するための、サービスを実装して、運用して改善していく どの方法がベストか考えた上で、それを実現していく  例えば、R\u0026amp;Dスタイルでモデル開発と実装者を完全に分ける組織構造もあると思いますが、このスタイルはいろんな会社のお話を聞く限りは、組織構造がよほど洗練されていないとうまく稼働しないじゃないかなと思っている\nFull Cycle Developers at Netflixでは、システム開発のライフサイクルである\ndesign, development, test, deploy, operate, support\nを1チームが一気通貫で責任を持つスタイルをNetflixが提唱している。\n今所属している会社もMicro Serviceでの開発に注力していて、まさにFull Cycleスタイルで機械学習サービス開発を行っている。 個人的に機械学習プロジェクトとこの方式の相性の良いところは、例えば、職能ごとにモデル開発、システム開発と運用を行うメンバーを分割すると、\n モデルを作ってデプロイはしたが運用は他人任せになってしまい継続的な改善が回しづらい 役割が分離されていることで、モデルの詳細を完全に把握できないので実際のトラブル発生時に対応が困難 運用を考えてモデルがデザインされていないので運用者にしわ寄せがくる  などアンチパターンが数多く存在する\nMicro Serviceでの開発は上記の課題を解決して、作って終わりではなく自分たちでシステムデザインからサポートまで行うことで、そのサービスの継続的な改善に責任と自由を手にして開発することできる また、プロジェクトデザインの段階からシステム開発・運用を念頭に動くことができるので、やってみてうまく動かないなどの不確実性を大きく減少させる","title":"自分なりの機械学習エンジニアスキル構成論"},{"content":"GitHub の CODEOWNERS という機能を使えば、レポジトリに対する PR では設定された CODEOWNER が APPROVE を出さないとマージされないようにできます。\nこの機能を使うことで、例えばそのリポジトリのオーナーであるグループが必ず PR を確認しないとマージできないようにすることでコードのクオリティを保つ仕組みが作れます。\nTL;DR  GitHub codeowners で特定のグループを CODEOWNERS に設定したいときは、そのグループをレポジトリの /settings/accessで Maintain として追加しないと GitHub PR で自動的に reviwer に追加されない リポジトリで.github/CODEOWNERS のファイルを作成して、以下の形式で GitHub group を追加する  1  * @octo-org/codeowners-team    リポジトリの設定の/settings/accessにアクセスして、@octo-org/codeowners-team を Maintain として追加する。  *試していないのですが、Write や Triage 権限でも問題ないかもしれません。\nこの設定をしたあとに、GitHub PR を新たに作成すると、自動的に CODEOWNERS の approve がないとマージされないように設定されるはずです。\n自分自身がハマった経緯 グループ全体のアカウントが追加されているa-group/allという Github Group がすでにリポジトリのアクセス権限に Write 権限として追加されており、全員が write 権限をもっているなら codeowners としての権限も問題ないだろうと思っていたらハマりました。\nCODEOWNERS の仕組みを知ると理解できるのですが、a-group/all が指定したいグループの包含関係にあるからといって、そのように取り扱ってくれるわけではないということですね。\n","permalink":"https://shunyaueta.com/posts/2020-09-19/","summary":"GitHub の CODEOWNERS という機能を使えば、レポジトリに対する PR では設定された CODEOWNER が APPROVE を出さないとマージされないようにできます。\nこの機能を使うことで、例えばそのリポジトリのオーナーであるグループが必ず PR を確認しないとマージできないようにすることでコードのクオリティを保つ仕組みが作れます。\nTL;DR  GitHub codeowners で特定のグループを CODEOWNERS に設定したいときは、そのグループをレポジトリの /settings/accessで Maintain として追加しないと GitHub PR で自動的に reviwer に追加されない リポジトリで.github/CODEOWNERS のファイルを作成して、以下の形式で GitHub group を追加する  1  * @octo-org/codeowners-team    リポジトリの設定の/settings/accessにアクセスして、@octo-org/codeowners-team を Maintain として追加する。  *試していないのですが、Write や Triage 権限でも問題ないかもしれません。\nこの設定をしたあとに、GitHub PR を新たに作成すると、自動的に CODEOWNERS の approve がないとマージされないように設定されるはずです。\n自分自身がハマった経緯 グループ全体のアカウントが追加されているa-group/allという Github Group がすでにリポジトリのアクセス権限に Write 権限として追加されており、全員が write 権限をもっているなら codeowners としての権限も問題ないだろうと思っていたらハマりました。\nCODEOWNERS の仕組みを知ると理解できるのですが、a-group/all が指定したいグループの包含関係にあるからといって、そのように取り扱ってくれるわけではないということですね。","title":"GitHub codeowners でGithubグループを指定しても反映されない時の対処方法"},{"content":"目的 複数の同じフォーマットの CSV ファイルが特定のディレクトリに配置されており、その CSV ファイル群を一つの CSV ファイルに連結したい\n今回は、Python の Pandas と pathlib を使って上記の目的を実現します。\n実行環境 1 2 3 4 5 6 7  In [1]: import pandas as pd In [2]: pd.__version__ Out[2]: \u0026#39;1.1.2 In [3]: import sys ...: print(sys.version) 3.8.2 (default, Jul 19 2020, 07:23:27) [Clang 11.0.3 (clang-1103.0.32.62)]   目的となる csv ファイルは tmp ディレクトリに以下のような形式で配置されているとする\n1 2 3 4  tmp ├── 1.csv ├── 2.csv └── 3.csv   各ファイルはこのような形式で保存されています。\n1 2 3 4  id name created 1 John 2020/09/10 2 bob 2020/09/10 3 taro 2020/09/11   以下の Python スクリプトを実行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import pathlib import pandas as pd def contcat_csv(f_path:str): # pathlibのitedir()で対象とするディレクトリのCSVファイル一覧をジェネレーターとして取得 csvs = [pd.read_csv(str(path)) for path in pathlib.Path(f_path).glob(\u0026#39;*.csv\u0026#39;)] # そのファイル一覧をPandasで読み込んで、pd.concat()で連結してDataFrameとして返す return pd.concat(csvs, sort=False) df = contcat_csv(\u0026#39;tmp\u0026#39;) # 連結されたDataFrameをCSVとして保存する df.to_csv(\u0026#39;concat.csv\u0026#39;, index=False)   concat.csv を確認してみると\n1 2 3 4 5 6 7 8 9 10 11 12 13  id name created 1 John 2020/09/10 2 bob 2020/09/10 3 taro 2020/09/11 4 hanako 2020/09/12 1 John 2020/09/10 2 bob 2020/09/10 3 taro 2020/09/11 4 hanako 2020/09/12 1 John 2020/09/10 2 bob 2020/09/10 3 taro 2020/09/11 4 hanako 2020/09/12   無事連結された CSV を取得することができました\n追記  2020-09-12: @siumachi さんからご指摘を受け、CSV ファイル以外がディレクトリに混入していても問題ないように変更しました   拝見しました。for でディレクトリを走査するときは pathlib.Path.glob(\u0026quot;*.csv\u0026rdquo;)を使った方が、csv 以外のファイルが混入したときの対策になると思ったのですが、いかがでしょう https://docs.python.org/ja/3/library/pathlib.html#pathlib.Path.glob\n ","permalink":"https://shunyaueta.com/posts/2020-09-09/","summary":"目的 複数の同じフォーマットの CSV ファイルが特定のディレクトリに配置されており、その CSV ファイル群を一つの CSV ファイルに連結したい\n今回は、Python の Pandas と pathlib を使って上記の目的を実現します。\n実行環境 1 2 3 4 5 6 7  In [1]: import pandas as pd In [2]: pd.__version__ Out[2]: \u0026#39;1.1.2 In [3]: import sys ...: print(sys.version) 3.8.2 (default, Jul 19 2020, 07:23:27) [Clang 11.0.3 (clang-1103.0.32.62)]   目的となる csv ファイルは tmp ディレクトリに以下のような形式で配置されているとする\n1 2 3 4  tmp ├── 1.csv ├── 2.csv └── 3.csv   各ファイルはこのような形式で保存されています。","title":"pandas を使って特定のディレクトリのCSVファイルをすべて連結して一つのCSVファイルを作成"},{"content":"MLOps の査読付き国際会議 2020 USENIX Conference on Operational Machine Learning (略称 OpML\u0026rsquo;20)に論文が採択されたので、登壇してきた。\nPodcast でも紹介しました。\n#1 MLOps の国際会議 OpML20 について at just4fun.fm\nMLOps の査読付き国際会議と OpML の立ち位置 機械学習エンジニアリング・MLOps の領域の会議でも一番有名なものとして 2018 年に発足したMLSysがあります。(ちなみに最初は SysML という名前でした) このカンファレンスの傾向としては、アカデミアの研究者主体の発足経緯からアカデミアからインダストリーへの橋渡し的立ち位置となっています。 具体的には、発表者はアカデミアの方が大半でハードウェアから、モデルの OSS 公開など幅広く機械学習エンジニアリング・MLOps の周辺領域をカバーしています。\nOpML はその一年後に、USENIXが母体の会議として MLOps を軸にした会議として誕生しました。 USENIX は SRECON、OSDI などを開催している団体です。 学術的なスタイルに則り、先端的な計算機システムの成果を論文として公開されています。MLSys と対称的にこちらはインダストリーからアカデミアへの橋渡し的立ち位置となっています。発表内容は企業での発表者が多く、実際の運用で得られた各企業の MLOps のベストプラクティスなどがメインで話されています。 個人的には OpML のほうが、MLOps のど真ん中を主体に置いているので MLSys よりも盛り上がってほしいなと思っています。\nOpML\u0026rsquo;19 がどのような様子だったかは、以下の記事がわかりやすいです。\n OpML ‘19 参加レポート The first conference of Operational Machine Learning: OpML ‘19  自分自身、機械学習エンジニアリングや MLOps 周りのカンファレンス情報などを追いかけていますが、この分野で査読付きかつ論文として残せる形式の国際会議は主に上記の２つの認識です。\nKDD や COLING、NAACL などの国際会議でもインダストリートラックが常設されるようになって久しいですが、最近ではインダストリートラックだけではなく、積極的に実応用前提のワークショップ(ECNLP at ACL2020, IRS2020 at KDD2020など)が開催されており機械学習の理論と実応用の融合が進んでいます。\nOpML\u0026rsquo;20 への投稿と採択で得られたもの OpML\u0026rsquo;20 では下記２つの発表枠があり、投稿者がどちらかを選んで発表を行います。\n 査読付きで 20m の口頭発表 2 ページの査読付き論文+20m の口頭発表  OpML20 で推奨されるトピックでも、自分たちが持っているネタで\n New model introduction into production (e.g., staging, A/B test)\n において新規性(Novelty)があると考えて、ここからストーリーを組み立てていきました。\nスケジュール感として投稿締切が 2020/02/25 で、その 1 ヶ月前の 1 月末から毎日 1 時間、Google Calendar で時間を抑えて同僚と集中的に論文の執筆を行いました。 最初にガッと 3 ページほど書いて、その後洗練させて 2 ページに圧縮して投稿しました。 あらめて添削や執筆をともに行ってくれた同僚たちに感謝します。\nそして投稿の 1 ヶ月後に通知メールが来て採択を知りました。 添削を何度も繰り返して時間が迫るなかなんとか投稿できたという状態で、とりあえず投稿できて良かったなと感じていた最中だったので、採択通知が来て本当におどきました。\n査読システムの良い点として、自分たちの投稿内容がその会議で発表足り得るものかがレビュアーからレビューされることです。 自分自身機械学習エンジニアとして働いていますが、その成果が査読を通して同じ分野で働いているエキスパートの第三者に認められたという事実が自分の仕事への自信に繋がりました。\nその後 Reviwer の方に指摘された点を意識しつつ修正を行って、Camera Ready を無事に提出しました。 また今回は急遽オンライン開催へと変更されたので、動画投稿も必須になり初めての収録もなかなか難産でしたが無事提出することができました。\nOpML\u0026rsquo;20 で採択された論文 採択された内容は以下のページにまとまっているので、もしご興味があればご覧ください。 内容を簡単にまとめると、\n C2C での出品商品への監視タスクで機械学習を導入して、Recall を改善 Human In The Loop と組み合わせたバックテストにより、リリース時の機械学習モデルの劣化を事前防止 不均衡なデータ分布を考慮したモデルリリース後の新たな A/B テスト手法の考案による、意思決定の高速化  です。\nAuto Content Moderation in C2C e-Commerce\n発表資料\n 講演動画\n 当日はオンライン開催で、発表内容は事前に YouTube で公開され、その後 Slack 上でリアルタイムに発表者に質問ができる Ask Me Anything (AMA)セッションが開催されました。 また、今回はオンライン開催になった影響で参加費は無料となりました。 AMA セッションで Chair の方がファシリテーターとなり、予想以上に活発な議論が生み出され驚きました。\nオンライン開催になり、発表内容はすべて YouTube で公開され、アメリカへの渡航の必要がなくなり気軽に参加できるのは非常に良い流れだと思います。\n一方で、日本からの参加だと深夜 01:00-02:30 の参加だったのでその点のみ非常に辛かったです。 懇親会などの偶発性を持った出会いなどはオンラインだとやはり、難しく感じました。\n面白かった OpML\u0026rsquo;20 の発表まとめ OpML\u0026rsquo;20 の発表一覧はこちらから確認できます。\n OpML\u0026rsquo;20 Conference Program  ここから面白かった発表を抜粋して紹介させていただきます。\nTwitter でもまとめていたのですが、記録のために分散させておきたいのでこちらでも記載しておきます\nRunway - Model Lifecycle Management at Netflix  https://www.usenix.org/conference/opml20/presentation/cepoi  Runway という Netflix でのモデルのライフサイクルマネジメントツールの紹介 NetFlix で実際にあった課題\n モデルを誰が作ったか、現在どんなモデルがあるのか、特徴量、変換、データはどんなものが使われているか モデルのリリース、検証、モニタリングやアラートが標準化されていない 計算機資源の無駄遣いなど  上記の課題を解決するために Runway が作成された。\n Runway は現状 A/B テストやカナリーテストは提供できていない integration 周りは Jenkins を使って実現している  k8s を明示的に言及していないってことは多分使っていなさそうではある。 計画段階だが、Netflix で開発している機械学習のワークフローエンジンMETAFLOWと Runway のインテグレーションも視野に入れてるらしい。\n一つの会社でこんだけ機械学習社内ツールが乱立してるのは競争力が非常に高いと感じた\nデータリネージなどの質問もされている\nガバナンスまわりを考えると、大企業だとデータリネージの必要性も最近わかるようになってきた。\nMore Data Science, Less Engineering: A Netflix Original  https://www.usenix.org/conference/opml20/presentation/goyal  MLOPs NYC で公開された MetaFlow の話。 スライドでの Data Scientist の苦悩を描いたイラストが秀逸で面白かったです。\nMETAFLOW の Data Scientst のスキルを最大限に活かす思想は非常に共感できるので、どんどん発展してほしいです。\n最近では R にも対応して驚きましたが、METAFLOW の思想であるMore Data Science, Less Engineering: A Netflix Originalに基づいていて非常に的確な対応ですね・\ninside NVIDIA’s AI Infrastructure for Self-driving Cars  https://www.usenix.org/conference/opml20/presentation/farabet   自動運転のためにデータの収集からモデルのデプロイ・シミュレーションまで一気通貫で開発 実世界での Ground Truth を考える  多種多様なデータ(LIDAR, Sensor, GPS. etc.) 100 種類以上の外的なシグナルの活用    スケールするデプロイのための、データの生成から、モデルの学習 → モデルの監査からシミュレーション・ハードウェアでの shadow patch を終えて、Prod にデプロイする流れ\nAutomating Operations with ML   https://www.usenix.org/conference/opml20/presentation/underwood\n  オペレーションでの機械学習活用について Google によるセッション\n  ほとんどの場合、機械学習の維持コストのほうが実際のインパクトより高い\n  単純なヒューリスティックは柔軟性に欠けるが機械学習と同等に役に立つ\n  発表者の Todd さんは他にも SRECON2019 に All of Our ML Ideas Are Bad (and We Should Feel Bad) となかなか刺激的なタイトルで、SRE の分野で機械学習の実応用はなかなかうまくいなかったぜと共有している\u0026hellip;\nHow ML Breaks: A Decade of Outages for One Large ML Pipeline  https://www.usenix.org/conference/opml20/presentation/papasian  日本語に訳すと、 機械学習システムの壊れ方:15 年以上経過した ML パイプラインがどのようにしてぶっ壊れてきたか\nモデルの前提として\n 15 年以上まえから開発された機械学習パイプライン 10 年以上前からポストモーメンタムが行われている 定期的に 1 時間かけてモデルを新規データで学習 新モデルは学習後即、CD される   失敗の要因を 19 種類にまとめた。オーケストレーションの失敗、CPU 不足などなど   個人的には\n NVIDIA の inside NVIDIA’s AI Infrastructure for Self-driving Cars Google の How ML Breaks: A Decade of Outages for One Large ML Pipeline  が骨太で非常に面白い発表でした\nMLOps という言葉が誕生して久しいですが、機械学習システムの開発と運用に置いて、何をやるべきなのかが構造化されてきています。\n書籍では、日本語で読める良書として、仕事ではじめる機械学習 や最近では、Googler の方がオライリーで出版予定のMachine Learning Design Patterns など非常に充実しています。\nこれからも、機械学習と現実世界の橋渡しとしてこの領域はどんどん掘り進められていくと思います。\nMLOps はデータの活用を前提にしたシステム開発というとてもエキサイティングです。\nもし OpML の発表が面白いと感じた方は、来年の OpML21 の口頭発表にも申し込んでみてはいかがでしょうか?\n","permalink":"https://shunyaueta.com/posts/2020-09-06/","summary":"MLOps の査読付き国際会議 2020 USENIX Conference on Operational Machine Learning (略称 OpML\u0026rsquo;20)に論文が採択されたので、登壇してきた。\nPodcast でも紹介しました。\n#1 MLOps の国際会議 OpML20 について at just4fun.fm\nMLOps の査読付き国際会議と OpML の立ち位置 機械学習エンジニアリング・MLOps の領域の会議でも一番有名なものとして 2018 年に発足したMLSysがあります。(ちなみに最初は SysML という名前でした) このカンファレンスの傾向としては、アカデミアの研究者主体の発足経緯からアカデミアからインダストリーへの橋渡し的立ち位置となっています。 具体的には、発表者はアカデミアの方が大半でハードウェアから、モデルの OSS 公開など幅広く機械学習エンジニアリング・MLOps の周辺領域をカバーしています。\nOpML はその一年後に、USENIXが母体の会議として MLOps を軸にした会議として誕生しました。 USENIX は SRECON、OSDI などを開催している団体です。 学術的なスタイルに則り、先端的な計算機システムの成果を論文として公開されています。MLSys と対称的にこちらはインダストリーからアカデミアへの橋渡し的立ち位置となっています。発表内容は企業での発表者が多く、実際の運用で得られた各企業の MLOps のベストプラクティスなどがメインで話されています。 個人的には OpML のほうが、MLOps のど真ん中を主体に置いているので MLSys よりも盛り上がってほしいなと思っています。\nOpML\u0026rsquo;19 がどのような様子だったかは、以下の記事がわかりやすいです。\n OpML ‘19 参加レポート The first conference of Operational Machine Learning: OpML ‘19  自分自身、機械学習エンジニアリングや MLOps 周りのカンファレンス情報などを追いかけていますが、この分野で査読付きかつ論文として残せる形式の国際会議は主に上記の２つの認識です。","title":"MLOps の国際会議 OpML'20 に、機械学習を活用した商品監視の改善に関する論文が採択されたので登壇してきた"},{"content":"リストを構築する際に Python ではリスト内包表記とジェネレータ式の２種類が存在する。 今回、リスト構築時にメモリ使用量にどれだけ差異が発生するのか調査をしてみた。 メモリ使用量の調査には、memory_profilerというパッケージを使用した。\nまず、２つのリストのデカルト積のタプルを表示するプログラムでの比較\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  from memory_profiler import profile @profile def main(): \u0026#34;\u0026#34;\u0026#34; Comparision List comprehension VS generator memory usage \u0026#34;\u0026#34;\u0026#34; colors = \u0026#34;colors\u0026#34; * 1000 sizes = \u0026#34;S\u0026#34; * 100 for shirts in ((color, size) for color in colors for size in sizes): print(shirts) [print((color, size)) for color in colors for size in sizes] if __name__ == \u0026#34;__main__\u0026#34;: main()   1 2 3 4 5 6 7 8 9 10 11 12 13 14  Filename: src/listcomp_vs_generator.py Line # Mem usage Increment Line Contents ================================================ 4 10.5 MiB 10.5 MiB @profile 5 def main(): 6 \u0026#34;\u0026#34;\u0026#34; 7 Comparision List comprehension VS generator memory usage 8 \u0026#34;\u0026#34;\u0026#34; 9 10.5 MiB 0.0 MiB colors = \u0026#34;colors\u0026#34; * 1000 10 10.5 MiB 0.0 MiB sizes = \u0026#34;S\u0026#34; * 100 11 10.5 MiB 0.0 MiB for shirts in ((color, size) for color in colors for size in sizes): 12 10.5 MiB 0.0 MiB print(shirts) 13 15.1 MiB 0.1 MiB [print((color, size)) for color in colors for size in sizes]   次に、1000x1000 のデカルト積を作成した場合\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  from memory_profiler import profile @profile def main(): \u0026#34;\u0026#34;\u0026#34; Comparision List comprehension VS generator memory usage \u0026#34;\u0026#34;\u0026#34; colors = \u0026#34;colors\u0026#34; * 1000 sizes = \u0026#34;S\u0026#34; * 1000 ((color, size) for color in colors for size in sizes) [(color, size) for color in colors for size in sizes] if __name__ == \u0026#34;__main__\u0026#34;: main()   1 2 3 4 5 6 7 8 9 10 11  Line # Mem usage Increment Line Contents ================================================ 4 10.5 MiB 10.5 MiB @profile 5 def main(): 6 \u0026#34;\u0026#34;\u0026#34; 7 Comparision List comprehension VS generator memory usage 8 \u0026#34;\u0026#34;\u0026#34; 9 10.5 MiB 0.0 MiB colors = \u0026#34;colors\u0026#34; * 1000 10 10.5 MiB 0.0 MiB sizes = \u0026#34;S\u0026#34; * 1000 11 10.5 MiB 0.0 MiB ((color, size) for color in colors for size in sizes) 12 434.6 MiB 5.8 MiB [(color, size) for color in colors for size in sizes]   結果を見ると一目瞭然で、ジェネレータ式の場合だとイテレータプロトコルが要素を一つ一つ作成するので、メモリ使用量もリスト内包表記と比べると 40 倍以上と一目瞭然の差になった\n機械学習でも大規模なデータを扱うことが多い、なのでジェネレータ式を意識して書いてメモリ使用量を抑えていきたい。\n","permalink":"https://shunyaueta.com/posts/2020-08-23/","summary":"リストを構築する際に Python ではリスト内包表記とジェネレータ式の２種類が存在する。 今回、リスト構築時にメモリ使用量にどれだけ差異が発生するのか調査をしてみた。 メモリ使用量の調査には、memory_profilerというパッケージを使用した。\nまず、２つのリストのデカルト積のタプルを表示するプログラムでの比較\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  from memory_profiler import profile @profile def main(): \u0026#34;\u0026#34;\u0026#34; Comparision List comprehension VS generator memory usage \u0026#34;\u0026#34;\u0026#34; colors = \u0026#34;colors\u0026#34; * 1000 sizes = \u0026#34;S\u0026#34; * 100 for shirts in ((color, size) for color in colors for size in sizes): print(shirts) [print((color, size)) for color in colors for size in sizes] if __name__ == \u0026#34;__main__\u0026#34;: main()   1 2 3 4 5 6 7 8 9 10 11 12 13 14  Filename: src/listcomp_vs_generator.","title":"Python の内包表記とジェネレータ式のメモリ使用量比較"},{"content":"はじめに コーディングの腕をもっと磨きたいなと思ったので、以下の記事を参考に始めてみた\n https://qiita.com/e869120/items/f1c6f98364d1443148b3   全部で 44 問ありますが、最後の 4 問は競プロとはあまり関係ないので、ITP1_1-A から ITP1_10-D までの 40 問を解くことをお勧めします。\n まずは最初におすすめされた、AOJ の ITP1_1-A から ITP1_10-D までの 40 問を解いてみた 無料でこのサービスが提供されてるの素晴らしい 標準入力、出力の整形が少し手間取ったけど、あとは愚直に解いていった\n http://judge.u-aizu.ac.jp/onlinejudge/  感想としては、\n やってみたら、意外と楽しい。特に自分で諦めずに試行錯誤して、オンラインで一発で AC もらえるとめちゃくちゃ嬉しい テストケースに通る、すなわち正しい、それが書けたら達成感がある 何かしらのお題に沿って、コードを書くという動機ができるので、書くことに慣れたい場合も有用そう  toggl で時間計測しながら、やって見直してみたら 15h46m 費やしていた。大体 1 問 25m くらい\n次の目標、\n AtCoder で水色を目指す!!! データ構造周りや、アルゴリズム周りはまだまだ弱いのでそこらへんを抑えていきたい  当面は、以下の２つに投資していきます\n 機械学習だけに縛られない、SWE としてスキル底上げ 機械学習関係の確固たる基礎知識と実装力  以下に自分が書いた回答例を放流しておきます。\n Rule  15 分試行錯誤しても、緒がわからない場合は諦める わからなかったとき、もっと上手な書き方は以下を参考にしました https://qiita.com/cmtennis1042/items/5f1e7f071081176e857f  ITP1_1_A: Hello World 1  print(\u0026#39;Hello world\u0026#39;)   ITP1_1_B: X Cubic 1 2  x = input() print(x ** 3)   ITP1_1_C: Rectangle 1 2  a, b = map(int, input().split()) print(a*b, 2*(a+b))   ITP1_1_D: Watch 1 2  second = int(input()) print(second//(60**2),\u0026#39;:\u0026#39;,(second%(60**2))//60,\u0026#39;:\u0026#39;,second%60,sep=\u0026#39;\u0026#39;)   // は整数の商を算出する\nITP1_2_A: Small, Large, or Equal 1 2 3 4 5 6 7  a, b = map(int, input().split()) if a \u0026gt; b: print(\u0026#34;a \u0026gt; b\u0026#34;) elif a == b: print(\u0026#34;a == b\u0026#34;) elif a \u0026lt; b: print(\u0026#34;a \u0026lt; b\u0026#34;)   ITP1_2_B: Range 1 2 3 4 5  a, b, c = map(int, input().split()) if a \u0026lt; b and b \u0026lt; c: print(\u0026#34;Yes\u0026#34;) else: print(\u0026#34;No\u0026#34;)   ITP1_2_C: Sorting Three Numbers 1 2 3 4 5  l = list(map(int, input().split())) l.sort() print(l[0],l[1],l[2], sep=\u0026#39; \u0026#39;) #best print(*three_numbers)   ITP1_2_D: Circle in a Rectangle 1 2 3 4 5  w,h,x,y,r = map(int, input().split()) if (x+r) \u0026gt; w or (x-r) \u0026lt; 0 or (y+r)\u0026gt;h or (y-r) \u0026lt;0: print(\u0026#34;No\u0026#34;) else: print(\u0026#34;Yes\u0026#34;)   一発で通って、めちゃくちゃ興奮した!!\nITP1_3_A: Print Many Hello World 1  [print(\u0026#34;Hello World\u0026#34;) for _ in range(1000)]   ##　ITP1_3_B: Print Test Cases\n1 2 3 4 5 6 7 8  a = [] while True: n = int(input()) if n == 0: break else: a.append(n) [print(f\u0026#39;Case {i+1}: {num}\u0026#39;) for i, num in enumerate(a)]   ITP1_3_C: Swapping Two Numbers 1 2 3 4 5 6 7 8 9 10  pairs = [] while True: a, b = map(int,input().split()) if (a, b) == (0, 0): break else: pairs.append([a, b]) for pair in pairs: pair.sort() print(*pair)   ITP1_3_D: How Many Divisors? 1 2 3 4 5 6 7 8 9 10 11 12  a, b, c = map(int, input().split()) cnt = 0 for i in range(b - a + 1): if c % (a+i) == 0: cnt += 1 print(cnt) # Best for k in range(a,b+1): if c%k==0: cnt+=1 print(cnt)   ITP1_4_A: A / B Problem 1 2  a, b = map(int, input().split()) print(\u0026#39;%i%i%-5f\u0026#39; % (a//b, a%b, a/b))   ITP1_4_B: Circle 1 2 3  import math r = float(input()) print(\u0026#39;%-5f%-5f\u0026#39; % (math.pi*r*r, 2* math.pi * r))   ITP1_4_C: Simple Calculator 1 2 3 4 5 6 7 8 9 10 11 12 13  while True: a, op, b = input().split() a, b = map(int, [a, b]) if op == \u0026#39;?\u0026#39;: break elif op == \u0026#34;+\u0026#34;: print(a+b) elif op == \u0026#34;-\u0026#34;: print(a-b) elif op == \u0026#34;*\u0026#34;: print(a*b) elif op == \u0026#34;/\u0026#34;: print(a//b)   ITP1_4_D: Min, Max and Sum \u0026lsquo;\u0026lsquo;\u0026lsquo;python n = int(input()) a = list(map(int, input().split())) print(min(a), max(a), sum(a)) '\u0026rsquo;\u0026rsquo;\nITP1_5_A: Print a Rectangle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  a = [] while True: h, w = map(int, input().split()) if (h, w ) == (0, 0): break else: a.append([h,w]) for pair in a: for i in range(pair[0]): print(\u0026#34;#\u0026#34; * pair[1]) print(\u0026#39;\u0026#39;) #Best while True: h, w = map(int, input().split()) if (h, w) == (0, 0): break [print(\u0026#39;#\u0026#39;*w) for i in range(h)] print()   ITP1_5_B: Print a Frame 1 2 3 4 5 6 7 8 9 10 11 12 13  while True: h, w = map(int, input().split()) if (h, w) == (0, 0): break else: for i in range(h): if i == 0: print(\u0026#39;#\u0026#39; * w, sep=\u0026#39;\u0026#39;) elif i == (h-1): print(\u0026#39;#\u0026#39; * w) print() else: print(f\u0026#34;#\u0026#34;, \u0026#34;.\u0026#34; * (w - 2), \u0026#34;#\u0026#34;, sep=\u0026#39;\u0026#39;)   ITP1_5_C: Print a Chessboard 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  while True: h, w = map(int, input().split()) if (h, w) == (0, 0): break else: for i in range(h): line = \u0026#39;\u0026#39; if i % 2 == 0: for j in range(w): if j % 2 == 0: line+=\u0026#39;#\u0026#39; else: line+=\u0026#39;.\u0026#39; else: for j in range(w): if j % 2 == 0: line+=\u0026#39;.\u0026#39; else: line+=\u0026#39;#\u0026#39; print(line) print()   ITP1_5_D: Structured Programming C++のコードで 3 がつくものというロジックが読めなかった。。。10 で割ってあまりが 3 だけだと、31 とかがエッジケースで通らなかった。。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  n = int(input()) l = \u0026#39;\u0026#39; for i in range(1,n+1): if i % 3 == 0: l+=\u0026#39; \u0026#39;+str(i) elif i % 10 == 3: l+=\u0026#39; \u0026#39;+str(i) print(l) # AC N=int(input()) for i in range(1,N+1): if i%3==0 or \u0026#34;3\u0026#34; in str(i): print(\u0026#34; {}\u0026#34;.format(i),end=\u0026#34;\u0026#34;) print()   ITP1_6_A: Reversing Numbers 1 2 3  n = int(input()) a = list(map(int, input().split())) print(*reversed(a))   ITP1_6_B: Finding Missing Cards 1 2 3 4 5 6 7 8 9 10 11  all_cards = [(s, n) for s in [\u0026#39;S\u0026#39;, \u0026#39;H\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;] for n in range(1, 14)] n = int(input()) hold_cards = [] for i in range(n): suit, num = input().split() num = int(num) hold_cards.append((suit, num)) for card in all_cards: if card not in set(hold_cards): print(*card)   in はO(n)なので注意が必要\nITP1_6_C: Official House 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  n = int(input()) pos = [] pairs = {} for i in range(n): b,f,r,v = map(int, input().split()) pos.append((b,f,r)) pairs[(b,f,r)] = int(v) values = [[[0 for _ in range(10)] for _ in range(3)] for _ in range(4)] # 4 buildings for b in range(4): # 3 floors for f in range(3): # 10 rooms for r in range(10): if (b + 1, f + 1, r + 1) in set(pos): print(pairs[(b+1, f+1, r+1)]) values[b][f][r] += pairs[(b+1, f+1, r+1)] for b in range(4): # 3 floors for f in range(3): # 10 rooms print(\u0026#39; \u0026#39;,end=\u0026#39;\u0026#39;) print(*values[b][f]) if b != 3: print(\u0026#39;#\u0026#39;*20) # modify n = int(input()) inputs = [list(map(int, input().split())) for _ in range(n)] values = [[[0 for _ in range(10)] for _ in range(3)] for _ in range(4)] for i in inputs: values[i[0]-1][i[1]-1][i[2]-1] += i[3] for b in range(4): for f in range(3): print(\u0026#39; \u0026#39;, end=\u0026#39;\u0026#39;) print(*values[b][f]) if b != 3: print(\u0026#39;#\u0026#39;*20)   入力時に、位置情報:値 を pair で持つ辞書を作って全部屋を for で回して、マッチした時にその部屋の値を変更していくようにした。 1 つめの test case は成功したが、２つ目の test case で同じ位置情報が入力された時に、辞書のkeyが上書きされてうまく動かなくなった。 解決法として、入力値を全てリストで保持、その入力値を使って、直接その要素の値を逐次計算するようにした\nITP1_6_D: Matrix Vector Multiplication 1 2 3 4 5 6 7 8  n, m = map(int, input().split()) mat = [list(map(int, input().split())) for _ in range(n)] vec = [int(input()) for _ in range(m)] ans = [0]*n for i, row in enumerate(mat): for j in range(m): ans[i] += row[j] * vec[j] [print(ans[i]) for i in range(n)]   ITP1_7_A: Grading 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  while True: m,f,r = map(int, input().split()) if (m, f, r) == (-1, -1, -1): break if m == -1 or f == -1 or (m+f) \u0026lt; 30: print(\u0026#39;F\u0026#39;) elif (m+f) \u0026gt;= 80: print(\u0026#39;A\u0026#39;) elif 80 \u0026gt; (m+f) \u0026gt;= 65: print(\u0026#39;B\u0026#39;) elif 65 \u0026gt; (m+f) \u0026gt;= 50: print(\u0026#39;C\u0026#39;) elif 50 \u0026gt; (m+f) \u0026gt;= 30: if r \u0026gt;= 50: print(\u0026#39;C\u0026#39;) else: print(\u0026#39;D\u0026#39;)   ITP1_7_B: How many ways? 全探索で探索してみたけど、O(N^3)かかるかつ、選択されたものが再度選択されてしまう。set()で扱っても、探索候補からキューとして扱わないといけないことには気づいたがどう扱えばいいか考えつかず断念 その後、回答例を見ると、たしかにループで、外側とかぶらないように変数管理すればうまく重複排除できることに気がついた\u0026hellip;!\n1 2 3 4 5 6 7 8 9 10 11  while True: n, x = map(int, input().split()) if (n, x) == (0, 0): break cnt = 0 for i in range(1, n + 1): for j in range(i+1, n + 1): for k in range(j+1, n + 1): if i + j + k == x: cnt+=1 print(cnt)   ITP1_7_C: Spreadsheet 1 2 3 4 5 6 7 8 9 10 11  r, c = map(int, input().split()) table = [list(map(int, input().split())) for _ in range(r)] c_sum = [0]*c for i in range(r): for j in range(c): c_sum[j] += table[i][j] for i in table: print(*i, end=\u0026#39; \u0026#39;) print(sum(i)) print(*c_sum, end=\u0026#39; \u0026#39;) print(sum(c_sum))   ITP1_7_D: Matrix Multiplication 1 2 3 4 5 6 7 8 9 10 11 12  n, m, l = map(int, input().split()) mat_a = [list(map(int, input().split())) for _ in range(n)] mat_b = [list(map(int, input().split())) for _ in range(m)] for i in range(n): row = [] for j in range(l): tmp = 0 for k in range(m): tmp += mat_a[i][k]*mat_b[k][j] row.append(tmp) print(*row)   ITP1_8_A: Toggling Cases 1 2 3 4 5 6 7 8  input_str = input() for i in input_str: if i.islower(): print(i.upper(), end=\u0026#39;\u0026#39;) else: print(i.lower(), end=\u0026#39;\u0026#39;) print()   ITP1_8_B: Sum of Numbers 1 2 3 4 5  while True: a = input() if a == \u0026#39;0\u0026#39;: break print(sum(list(map(int, list(a)))))   ITP1_8_C: Counting Characters 入力が複数行で何行入力されるか不明な場合は、sys.stdin.read()を使う\nhttps://qiita.com/kyuna/items/8ee8916c2f4e36321a1c#1-%E3%81%BE%E3%81%A8%E3%82%81\nreadline()だと一行ずつ読み込む\n1 2 3 4 5 6 7  import sys s = sys.stdin.read().lower() a = {chr(i): 0 for i in range(ord(\u0026#39;a\u0026#39;), ord(\u0026#39;z\u0026#39;)+1)} for i in s: if ord(i) in range(ord(\u0026#39;a\u0026#39;), ord(\u0026#39;z\u0026#39;)+1): a[i] += 1 [print(f\u0026#39;{k} : {v}\u0026#39;) for k, v in a.items()]   ITP1_8_D: Ring 1 2 3 4 5 6 7  s = input() p = input() s = s * 2 if s.find(p) != -1: print(\u0026#39;Yes\u0026#39;) else: print(\u0026#39;No\u0026#39;)   ITP1_9_A: Finding a Word 1 2 3 4 5 6 7 8  w = input() sentents = \u0026#39;\u0026#39; while True: s = input() if s == \u0026#39;END_OF_TEXT\u0026#39;: break sentents += s + \u0026#39; \u0026#39; print(sentents.lower().split().count(w))   ITP1_9_B: Shuffle 1 2 3 4 5 6 7 8 9  while True: d = input() if d == \u0026#39;-\u0026#39;: break m = int(input()) h = [int(input()) for _ in range(m)] for i in h: d = d[i:] + d[:i] print(d)   ITP1_9_C: Card Game 1 2 3 4 5 6 7 8 9 10 11 12 13  n = int(input()) cards = [input().split() for _ in range(n)] p_t = 0 p_h = 0 for card in cards: if card[0] == card[1]: p_t += 1 p_h += 1 elif card[0] \u0026gt; card[1]: p_t += 3 else: p_h += 3 print(p_t, p_h)   ITP1_9_D: Transformation  スライス表記で、l[start:end:step]を理解できるとかける end の記法が前から違和感あったんだけど、 @reto_nayuta さんにしていただいた説明がとてもわかり易かった!! 感謝  1 2 3 4  1-start というより 0-start exclusive、つまり [start, end) の右半開区間と解釈するのが自然かと ・i から長さ k 取る→a[i, i+k] ・i を境界に2つに分ける（二分探索等）→a[:i], a[i:] ・range の挙動とも一致→a[i:j] == [a[k] for k in range(i, j)]（R/Ruby/pandas/Rust は arr[range] 記法）   https://twitter.com/reto_nayuta/status/1290125805167632385\n1 2 3 4 5 6 7 8 9 10 11  s = input() n = int(input()) orders = [input().split() for _ in range(n)] for order in orders: a, b = map(int, order[1:3]) if order[0] == \u0026#39;print\u0026#39;: print(s[a:b+1]) elif order[0] == \u0026#39;replace\u0026#39;: s = s[0:a] + order[3] + s[b+1:] elif order[0] == \u0026#39;reverse\u0026#39;: s = s[0:a] + s[a:b+1][::-1] + s[b + 1:]   ITP1_10_D: Distance II 1 2 3  import math x1, y1, x2, y2 = map(float, input().split()) print(math.hypot(x2 - x1, y2 - y1))   ITP1_10_B: Triangle 1 2 3 4 5 6 7 8  import math a, b, C = map(float, input().split()) theta = math.radians(C) h = b*math.sin(theta) S = (a*h)/2 c = math.sqrt(a**2+b**2-2*a*b*math.cos(theta)) L = a+b+c print(S, L, h, sep=\u0026#34;\\n\u0026#34;)   ITP1_10_C: Standard Deviation 1 2 3 4 5 6 7 8  import math while True: n = int(input()) if n == 0: break s = list(map(float, input().split())) m = sum(s) / n print(math.sqrt((sum([(i - m)** 2 for i in s]) / n)))   ITP1_10_D: Distance II チェビシェフ距離、学生の頃を思い出して懐かしかった。 zipできれいに書けた\n1 2 3 4 5 6 7  n = int(input()) x = list(map(int, input().split())) y = list(map(int, input().split())) print(sum([abs(x-y) for x, y in zip(x, y)])) print(sum([abs(x - y) ** 2 for x, y in zip(x, y)]) ** (1/2) ) print(sum([abs(x - y) ** 3 for x, y in zip(x, y)]) ** (1/3) ) print(max([abs(x-y) for x, y in zip(x, y)]))   ","permalink":"https://shunyaueta.com/posts/2020-08-04/","summary":"はじめに コーディングの腕をもっと磨きたいなと思ったので、以下の記事を参考に始めてみた\n https://qiita.com/e869120/items/f1c6f98364d1443148b3   全部で 44 問ありますが、最後の 4 問は競プロとはあまり関係ないので、ITP1_1-A から ITP1_10-D までの 40 問を解くことをお勧めします。\n まずは最初におすすめされた、AOJ の ITP1_1-A から ITP1_10-D までの 40 問を解いてみた 無料でこのサービスが提供されてるの素晴らしい 標準入力、出力の整形が少し手間取ったけど、あとは愚直に解いていった\n http://judge.u-aizu.ac.jp/onlinejudge/  感想としては、\n やってみたら、意外と楽しい。特に自分で諦めずに試行錯誤して、オンラインで一発で AC もらえるとめちゃくちゃ嬉しい テストケースに通る、すなわち正しい、それが書けたら達成感がある 何かしらのお題に沿って、コードを書くという動機ができるので、書くことに慣れたい場合も有用そう  toggl で時間計測しながら、やって見直してみたら 15h46m 費やしていた。大体 1 問 25m くらい\n次の目標、\n AtCoder で水色を目指す!!! データ構造周りや、アルゴリズム周りはまだまだ弱いのでそこらへんを抑えていきたい  当面は、以下の２つに投資していきます\n 機械学習だけに縛られない、SWE としてスキル底上げ 機械学習関係の確固たる基礎知識と実装力  以下に自分が書いた回答例を放流しておきます。\n Rule  15 分試行錯誤しても、緒がわからない場合は諦める わからなかったとき、もっと上手な書き方は以下を参考にしました https://qiita.com/cmtennis1042/items/5f1e7f071081176e857f  ITP1_1_A: Hello World 1  print(\u0026#39;Hello world\u0026#39;)   ITP1_1_B: X Cubic 1 2  x = input() print(x ** 3)   ITP1_1_C: Rectangle 1 2  a, b = map(int, input().","title":"AOJの「ITP I」40問をPythonで解いた"},{"content":"If you want to write UnitTest when using stdin in Python. Pytest provide setattr function in monkeypatch\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  from io import StringIO import sys def divide(): input = sys.stdin.readline return list(input()) def gather(): input = sys.stdin.readline return sum(list(map(int, input().split()))) def test_divide(monkeypatch): monkeypatch.setattr(\u0026#39;sys.stdin\u0026#39;, StringIO(\u0026#39;abc\u0026#39;)) assert divide() == [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;] def test_gather(monkeypatch): monkeypatch.setattr(\u0026#39;sys.stdin\u0026#39;, StringIO(\u0026#39;1 2 3\u0026#39;)) assert gather() == 6   Reference  Monkeypatching/mocking modules and environments I want to use stdin in a pytest test https://gist.github.com/GenevieveBuckley/efd16862de9e2fe7adfd2bf2bef93e02  ","permalink":"https://shunyaueta.com/posts/2020-07-25/","summary":"If you want to write UnitTest when using stdin in Python. Pytest provide setattr function in monkeypatch\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  from io import StringIO import sys def divide(): input = sys.stdin.readline return list(input()) def gather(): input = sys.stdin.readline return sum(list(map(int, input().split()))) def test_divide(monkeypatch): monkeypatch.setattr(\u0026#39;sys.stdin\u0026#39;, StringIO(\u0026#39;abc\u0026#39;)) assert divide() == [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;] def test_gather(monkeypatch): monkeypatch.","title":"How to write the UnitTest with stdin at Pytest"},{"content":"Machine Learning Casual Talks 第 12 回を開催しました。 前回から少し開きがあり、7 ヶ月ぶりの開催となりました。\nhttps://mlct.connpass.com/event/172550/\n今回の個人的なテーマはベストプラクティスとアンチパターンです。\n @keigohtr さんには、AWS の各種サービスを使った機械学習実験基盤をアベジャの適用事例と重ね合わせて、説得力のあるベストプラクティスを語っていただきました。 @yuzutas0 さんには、機械学習の前に、データのマネジメントがいかに必要かを語っていただきました。建設的に改善していこうぜという未来が語られていて、個人的にお話を依頼した甲斐がありました 同僚の @overs_5121 さんには、メルカリ : TensorFlow Lite で、気付きにくい便利機能をユーザーに提唱  の裏話や、適用までの泥臭い事例をお話していただきました。  登壇者の皆様、改めて登壇の依頼をご快諾いただきありがとうございました。\nまた、コロナウイルスの影響もあり試験的ですが完全なオンライン開催となりました。 配信面は今回は完全に @chezou さんに頼らせていただきました。 プロフェッショナルな配信ありがとうございました！ 配信のベストプラクティスや様子などは、こちらを御覧ください\nGoogle Meet と YouTube Live でオンラインミートアップの配信をした\n勉強会の資料と動画  資料ページ Machine Learning Casual Talks #12 - YouTube  所感としては、以前から配信 NG の発表以外は積極的に YouTube で公開していたのだが、参加者の皆様からはオンライン開催でありがたいと声が大きく、個人的に驚きました。\n自分が思うに、オンライン参加も配信動画を後から見るのも、リアルタイムで質問ができないこと以外は大きな差異が無いと思っていたのだが、参加者側からすると大きく異なるようで新鮮だった。\nオンライン勉強会開催側のコツ 最低でも\n 配信者 司会者 配信の監視を行う監視者  の 3 役がいないとオンライン開催は難しいことがわかった\nライブ配信視聴者数は、以下のような遷移となりました。 500 人参加申込みがあり、最大視聴者数が 252 人とギリギリ 5 割を超えました。\n今までのオフラインでの開催は 6-8 割くらいだったので、それと比較すると上出来かなと思います。 また、オンライン開催はオフライン開催と比べて会場の確保コストや懇親会おじさんの発生などを抑えられるので、その点もありがたかったです。 オフライン開催だと、会場撤収が効率的に終わっても 22:00、家に帰ると 23:00 なので、イベント主催者にとっても開催しやすい気がしますね。\n改善点としては、質問の数が少なめだったことと、オフラインでの交流を補うような要素(パネルディスカッションなど）をもう少し入れたいなと思っています。\nオンライン開催は、開催コストが高くて継続できないというイベント主催者あるあるの問題を解決する一つのきっかけにもなるんじゃないのかなぁと思いました。\nでは、また次回の MLCT 開催をお待ち下さい!!\n","permalink":"https://shunyaueta.com/posts/2020-06-13/","summary":"Machine Learning Casual Talks 第 12 回を開催しました。 前回から少し開きがあり、7 ヶ月ぶりの開催となりました。\nhttps://mlct.connpass.com/event/172550/\n今回の個人的なテーマはベストプラクティスとアンチパターンです。\n @keigohtr さんには、AWS の各種サービスを使った機械学習実験基盤をアベジャの適用事例と重ね合わせて、説得力のあるベストプラクティスを語っていただきました。 @yuzutas0 さんには、機械学習の前に、データのマネジメントがいかに必要かを語っていただきました。建設的に改善していこうぜという未来が語られていて、個人的にお話を依頼した甲斐がありました 同僚の @overs_5121 さんには、メルカリ : TensorFlow Lite で、気付きにくい便利機能をユーザーに提唱  の裏話や、適用までの泥臭い事例をお話していただきました。  登壇者の皆様、改めて登壇の依頼をご快諾いただきありがとうございました。\nまた、コロナウイルスの影響もあり試験的ですが完全なオンライン開催となりました。 配信面は今回は完全に @chezou さんに頼らせていただきました。 プロフェッショナルな配信ありがとうございました！ 配信のベストプラクティスや様子などは、こちらを御覧ください\nGoogle Meet と YouTube Live でオンラインミートアップの配信をした\n勉強会の資料と動画  資料ページ Machine Learning Casual Talks #12 - YouTube  所感としては、以前から配信 NG の発表以外は積極的に YouTube で公開していたのだが、参加者の皆様からはオンライン開催でありがたいと声が大きく、個人的に驚きました。\n自分が思うに、オンライン参加も配信動画を後から見るのも、リアルタイムで質問ができないこと以外は大きな差異が無いと思っていたのだが、参加者側からすると大きく異なるようで新鮮だった。\nオンライン勉強会開催側のコツ 最低でも\n 配信者 司会者 配信の監視を行う監視者  の 3 役がいないとオンライン開催は難しいことがわかった\nライブ配信視聴者数は、以下のような遷移となりました。 500 人参加申込みがあり、最大視聴者数が 252 人とギリギリ 5 割を超えました。","title":"Machine Learning Casual Talks # 12 を開催しました"},{"content":"自走プログラマーを読み終えた。\n読み始めたきっかけとして、自分は機械学習エンジニアとして現在働いているが、できることの幅を広げるために最近はソフトウェアエンジニアとしてのスキルをもっと伸ばしたいと考えている。\n自走プログラマーは、Python を使ったアプリケーション開発のアンチパターンとベストプラクティスを例示して学ぶことができる書籍で、今回の自分の状況にすごくフィットしていて楽しく学習することができた。\nPython 独特のはまりどころは、Kindle: The Hitchhiker’s Guide to Python, The Hitchhiker’s Guide to Python でも数多く参照されていて、こっちも後から読んでおきたいなと思いました。\n次は、ちゃんとした Pythonista になれるように、Fluent Python を読みます。@ynqa さん、以前この本を教えて下さり、ありがとうございました。\n長らく積ん読になっていますが、毎日読み進めていきます。\n20 歳頃の寝る間を惜しんで、ウェブアプリを開発していたときのワクワク感が徐々に蘇ってきた気がしています。\nある程度書けるようになってきたら、なにかアプリとか作って公開したいなと思っています！\n","permalink":"https://shunyaueta.com/posts/2020-05-10/","summary":"自走プログラマーを読み終えた。\n読み始めたきっかけとして、自分は機械学習エンジニアとして現在働いているが、できることの幅を広げるために最近はソフトウェアエンジニアとしてのスキルをもっと伸ばしたいと考えている。\n自走プログラマーは、Python を使ったアプリケーション開発のアンチパターンとベストプラクティスを例示して学ぶことができる書籍で、今回の自分の状況にすごくフィットしていて楽しく学習することができた。\nPython 独特のはまりどころは、Kindle: The Hitchhiker’s Guide to Python, The Hitchhiker’s Guide to Python でも数多く参照されていて、こっちも後から読んでおきたいなと思いました。\n次は、ちゃんとした Pythonista になれるように、Fluent Python を読みます。@ynqa さん、以前この本を教えて下さり、ありがとうございました。\n長らく積ん読になっていますが、毎日読み進めていきます。\n20 歳頃の寝る間を惜しんで、ウェブアプリを開発していたときのワクワク感が徐々に蘇ってきた気がしています。\nある程度書けるようになってきたら、なにかアプリとか作って公開したいなと思っています！","title":"自走プログラマーを読み終えた"},{"content":" Upstream  Upstream はそのシステムが依存しているジョブ Upstream のデザインが変わることで、システムも影響をうける   Downstream  Downstream はそのシステムが影響を与える影響を与える部分    例えば、Web Application などでは、データベースは Downstream となる\ne.g. Web service→ Databese という流れでデータが作成される\nReferences  https://reflectoring.io/upstream-downstream/ https://softwareengineering.stackexchange.com/questions/71080/what-does-downstream-upstream-design-mean/83686 https://en.wikipedia.org/wiki/Upstream_(software_development) https://en.wikipedia.org/wiki/Downstream_(software_development)  ","permalink":"https://shunyaueta.com/posts/2020-04-27/","summary":" Upstream  Upstream はそのシステムが依存しているジョブ Upstream のデザインが変わることで、システムも影響をうける   Downstream  Downstream はそのシステムが影響を与える影響を与える部分    例えば、Web Application などでは、データベースは Downstream となる\ne.g. Web service→ Databese という流れでデータが作成される\nReferences  https://reflectoring.io/upstream-downstream/ https://softwareengineering.stackexchange.com/questions/71080/what-does-downstream-upstream-design-mean/83686 https://en.wikipedia.org/wiki/Upstream_(software_development) https://en.wikipedia.org/wiki/Downstream_(software_development)  ","title":"ソフトウェア開発における Upstream と Downstream の意味"},{"content":"例えば以下のように、デフォルト引数で初期化を行い、文字列を追加する関数があるとする。\n1 2 3  def append_to(values=[]): values.append(\u0026#34;Hoge\u0026#34;) return values   期待する振る舞いとしては。\n1 2 3 4 5 6 7 8 9 10  In [14]: def append_to(values=[]): ...: values.append(\u0026#34;Hoge\u0026#34;) ...: return values ...: In [17]: append_to() Out[17]: [\u0026#39;Hoge\u0026#39;] In [18]: append_to() Out[18]: [\u0026#39;Hoge\u0026#39;]   と関数呼び出しごとに、values は空のリストに初期化されるので上記のように返ってきてほしい\nだが、実際に表示されるのは\n1 2 3 4 5 6 7 8 9 10  In [14]: def append_to(values=[]): ...: values.append(\u0026#34;Hoge\u0026#34;) ...: return values ...: In [17]: append_to() Out[17]: [\u0026#39;Hoge\u0026#39;] In [18]: append_to() Out[18]: [\u0026#39;Hoge\u0026#39;, \u0026#39;Hoge\u0026#39;]   である。\n実際に内部で何が起きているかというと\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  In [23]: def append_to(values=[]): ...: values.append(\u0026#34;Hoge\u0026#34;) ...: return values ...: In [24]: pinfo append_to Signature: append_to(values=[]) Docstring: \u0026lt;no docstring\u0026gt; File: ~/\u0026lt;ipython-input-23-4530a91351ab\u0026gt; Type: function In [25]: append_to() Out[25]: [\u0026#39;Hoge\u0026#39;] In [26]: pinfo append_to Signature: append_to(values=[\u0026#39;Hoge\u0026#39;]) Docstring: \u0026lt;no docstring\u0026gt; File: ~/\u0026lt;ipython-input-23-4530a91351ab\u0026gt; Type: function In [27]: append_to() Out[27]: [\u0026#39;Hoge\u0026#39;, \u0026#39;Hoge\u0026#39;] In [28]: pinfo append_to Signature: append_to(values=[\u0026#39;Hoge\u0026#39;, \u0026#39;Hoge\u0026#39;]) Docstring: \u0026lt;no docstring\u0026gt; File: ~/\u0026lt;ipython-input-23-4530a91351ab\u0026gt; Type: function   が起きている。 pinfo は ipython 上で、オブジェクトの情報が確認できる便利コマンドです。 関数呼び出しごとに、デフォルト引数の values が上書きされていっていることがわかります。 これは、Python のデフォルト引数が、関数が定義されたときのみ評価され、毎回毎回評価されるわけではない。(Ruby は評価される) ここでわかるのは、mutable\n対処方法 関数が呼び出されるごとに新しいオブジェクトを作成する。 何も設定されていないときは、デフォルト引数を使うことで検知する\n1 2 3 4 5  def append_to(values=None): if values is None: values = [] values.append(\u0026#34;Hoge\u0026#34;) return values   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  In [34]: def append_to(values=None): ...: if values is None: ...: values = [] ...: values.append(\u0026#34;Hoge\u0026#34;) ...: return values ...: In [35]: append_to() Out[35]: [\u0026#39;Hoge\u0026#39;] In [36]: pinfo append_to Signature: append_to(values=None) Docstring: \u0026lt;no docstring\u0026gt; File: ~/\u0026lt;ipython-input-34-8e047f793784\u0026gt; Type: function In [37]: append_to() Out[37]: [\u0026#39;Hoge\u0026#39;] In [38]: pinfo append_to Signature: append_to(values=None) Docstring: \u0026lt;no docstring\u0026gt; File: ~/\u0026lt;ipython-input-34-8e047f793784\u0026gt; Type: function   想定通りの動きになった\nRef  https://docs.python-guide.org/writing/gotchas/ http://moqada.hatenablog.com/entry/20090206/1233935560  ","permalink":"https://shunyaueta.com/posts/2020-04-26/","summary":"例えば以下のように、デフォルト引数で初期化を行い、文字列を追加する関数があるとする。\n1 2 3  def append_to(values=[]): values.append(\u0026#34;Hoge\u0026#34;) return values   期待する振る舞いとしては。\n1 2 3 4 5 6 7 8 9 10  In [14]: def append_to(values=[]): ...: values.append(\u0026#34;Hoge\u0026#34;) ...: return values ...: In [17]: append_to() Out[17]: [\u0026#39;Hoge\u0026#39;] In [18]: append_to() Out[18]: [\u0026#39;Hoge\u0026#39;]   と関数呼び出しごとに、values は空のリストに初期化されるので上記のように返ってきてほしい\nだが、実際に表示されるのは\n1 2 3 4 5 6 7 8 9 10  In [14]: def append_to(values=[]): ...: values.append(\u0026#34;Hoge\u0026#34;) ...: return values ...: In [17]: append_to() Out[17]: [\u0026#39;Hoge\u0026#39;] In [18]: append_to() Out[18]: [\u0026#39;Hoge\u0026#39;, \u0026#39;Hoge\u0026#39;]   である。","title":"Pythonの関数のデフォルト引数はmutable(上書きされる)"},{"content":"[抄訳] What’s your ML test score? A rubric for ML production systemsで紹介した論文の続編があったので読んでみました。\n 注意)この翻訳記事は原著論文の著者陣からレビューはされていません Shunya Ueta, are providing a translation and abridgment, which has not been reviewed by the authors.  Change log  2021/02/03  ML Test Score を簡単に計算できるGoogle Spread Sheets を公開   2020/06/24  著者の Eric Breck さんに連絡をし、抄訳の公開を快諾していただきました。ありがとうございます。 完全な citation 情報を追記しました。 この翻訳記事が著者のレビューを受けていないことを追記しました。    Citation  Eric Breck, Shanqing Cai, Michael Salib, . The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction. In 2017 IEEE International Conference on Big Data (Big Data) (pp. 1123-1132). IEEE.\n 大規模なオフラインの機械学習実験は注目されているが、反対にオンラインでの信頼性がある機械学習システムの開発は難しく、技術的負債が溜まりやすい\n図1 では、左側が伝統的なシステムのテストとモニタリング、右側は、機械学習システムのテストとモニタリングである 機械学習システムのテストとモニタリングが複雑になる要因として、コードだけではなく、動的に決定されるデータの質とモデルの多種多様な設定に依存するからである\nTL; DR;  機械学習システムの信頼性を評価する28の実行可能なテスト項目とスコアリング方法を提案する。 Google 内部の調査では、調査対象の 80%のチームが 28 のテスト項目の１つさえも行っていなかった。。(エンジニアリングに長けている Google 内部でさえも十分に行われてはいない) もし手持ちの機械学習システムの ML Test Score を計算したい場合は、簡単に計算可能な Google Spread Sheets を公開します。  閲覧権限のみ与えているので、File-\u0026gt; Make a copy を選択して手元にコピーしてお使いください。 ML Test Score template - Googpe Spread Sheets    TEST FOR DATA AND FEATURES  Data 1: 期待する特徴量は全てスキーマで管理され、読み込み可能か?  How: このスキーマは学習データの統計値を計算することに有用である。これらを可視化することで事前のバイアスを検知したり、Fasets1などのツールを用いた統計値の可視化もとても有用である   Data 2: 全ての特徴量は有用か?  全ての特徴量は、追加すればするほどコストになる。独立した全ての特徴量は有用だろうか? How: 特徴量削除や目的変数と特徴量の相関などを見てみよう   Data 3: 特徴量のコストは高くないか?  How: 推論速度や RAM の使用率だけを見るのではなく、その特徴量に関連するデータの依存先や不安定性なども考慮する   Data 4: 特徴量は要件に準拠しているか?  一般的には、機械学習システムにデータが送られる際には単一のリソースからデータを取得することが前提ですが、実験の際には知らずしらずのうちに精度を向上させるために特徴量をどんどん追加してしまいがちである How: \u0008 学習データのリソースがプロダクション環境から逸脱していないか監視する   Data 5: データパイプラインは適切なプライバシーコントロールはされているか?  How: データパイプライン構築の時には権限管理を厳密に行う   Data 6: 新しい特徴量は素早く追加可能か?  新しいアイデアを素早く試せるチームは強い。世界規模、またはトラフィックが多い機械学習システムでも、1-2 ヶ月で新しい機能を追加することが可能である の。Data 5 と相反する形にはなっている。   Data 7: 全ての特徴量生成コードはテストされているか?  特徴量生成のコードは一見シンプルでユニットテストなどは必要そうに見えないかもしれないが、ここで発生するバグは発見することはとても難しい    TESTS FOR MODEL DEVELOPMENT  Model 1: 全てのモデルはレビューされ、リポジトリに格納されている  バージョン管理され、過去の実験との比較や再現実験などを可能にする   Model 2: オフラインの疑似指標はオンラインメトリクスに相関しているか?  How: A/B テストで意図的にモデルのスコアを劣化させてテストする   Model 3: 全てのハイパーパラメータはチューニングされているか?  学習率・NN の層の数・次元数など数多くのハイパーパラメータがあり、予測精度に大きな影響を与える How: グリッドサーチや確率的なパラメータチューニングをおこなう   Model 4: 古くなった(陳腐)モデルの影響は把握されているか?  例えば学習パイプラインが失敗し、更新されなかったモデルをここで古びたモデルと呼ぶ。モデルが更新されないことでどれくらい予測精度に影響を与えるか理解することで、再学習の適切な期間も決定できる。ほとんどのモデルは外部世界の要因により更新しなければならない。 How: 小規模な A/B テストにより、新モデルと旧モデルを比較することでモデルのリフレッシュネスがどれくらい重要か計測することができる   Model 5: 単純なモデルは必ずしも良くはない  少ない特徴量で単純な線形モデルがベースラインとして使われることがあるが、パイプライン構築のコストとその利益のトレードオフを常に考えていくべきである   Model 6: 全ての重要なデータの部分集合でもモデルの質は十分なものになっているか?  How: 多様な基準でデータを抜き出し、モデルの精度が変化しないかを確認する。例えば、時間帯や性別、年齢などの基準でデータを抜き出してもモデルの精度は同一か？   Model 7: 包括性を考慮したモデルになっているか?  機械学習システムでは Fairness などが最近課題となっている。例えば、Bolukbasi らは文字埋め込みにおいて性別の差が予測において不適切な影響を発生させていることがわかった。 How: 特定のグループ間で、実験をおこない結果が異なっていないか確認する。    TESTS FOR ML INFRASTRUCTURE  Infra 1: 学習は再現性があるか?  理想としては、同じデータで学習を行った際には同じ結果が期待される。しかし不幸なことに非凸な問題(e.g. Deep Learning)などの学習は再現不可能なことがめずらしくない。シードの固定などをしても必ず再現性が担保されるわけでもない How: モデルのアンサンブルなどは有効である   Infra 2: モデルの仕様はユニットテストされているか?  ユニットテストは外部依存性を排除し、素早く実行可能でなければならない。だが実際はモデルの学習には非常に時間がかかり、計算機資源も必須である How: まずモデルのテストはAPIとしてのテストとアルゴリズムとして正しいかのテストの２つに分解して考える。(著者が執筆当時 OSS としての公開を計画中らしい\u0026hellip;DATA VALIDATION FOR MACHINE LEARNING を指しているのだろうか？) 学習データをフルに活用したモデルの開発は律速になりがちである。そのためベストプラクティスとしては、ランダムに生成した入力データに対して、一つのステップで勾配降下法が上手く動いているかを確認するなどがある。また、モデルの学習過程でチェックポイントを作成しておくことで、学習ジョブが死んだとしても復旧可能になる 機械学習アルゴリズムが正しいかどうかのテストとして、例えばアルゴリズムの特定の部分の計算が正しく実行されているかを確認することができます。別の解決策としては、数回の反復のみを計算し損失関数が減少しているかどうかを確認する。 また、モデルのテストを行う際には黄金のテストを避けるべきである。例えば、部分的に学習したモデルと前回のモデルの結果を比較するのはメンテナンスすることが非常に難しい。そして上記のテストはテストが失敗した時に洞察をすることが難しい。   Infra 3: 全ての機械学習パイプラインは結合テストされているか?  完璧な機械学習パイプラインとは主に、学習データの構築 → 特徴生成 → モデル学習 → モデル検証 → サービングシステムへのデプロイから構成される。単一のソフトウェアエンジニアチームは、特定の箇所にのみ集中して開発を行う。そして、その開発によりどこかが壊れパイプライン全体が動かなくなることも珍しくはない。そのため、完全に自動化された結合テストを定常的に行うべきである。 How: 結合テストをモデルまたはサービングのリリース前に CI として実行する。結合テスト全体がうまく言っているかを目的として高速な結合テストのために、部分的な学習データやシンプルなデータを扱うことを推奨する。また、別途プロダクションに限りなく近づけたミラーリング環境での結合テストも行う   Infra 4: モデルの品質は、サービングする前に検証されているか?  この検証段階で、モデルの受け入れ判定を行う How: 同じ検証データを用いて、モデル間の比較を行う。   Infra 5: 一つのサンプルに対する学習、サービングの段階ごとの計算をデバッグ可能か?  モデルの学習を段階ごとに観測する。特定のツールを使うことで観測可能である。例: TensorFlow Debugger   Infra 6: カナリーリリースによって、モデルはプロダクション環境下で検証されているか?  オフラインテストをいくら広範囲に行ったとしても、プロダクション環境下でのパフォーマンスを保証してくれない。現実世界では、予測不可能な問題が起きる。そのため、常にプロダクション環境下への新モデルのリリースはリスクを生じる。 一つの再帰的な問題として、カナリーはモデルの生成物とインフラのミスマッチを把握するのに有用である。モデリングのコードは、サービングのコードと比較して高頻度に変更される。新しいモデリングのコードで、サービングのシステムが動かないことは避けておきたい  図 2 で示すように、Day1 のモデルから Day2 のモデルに更新された際に、そのオペレーターに対応していないサービングシステムが混在することもあり得る How: ミスマッチの問題を解決するために、一つの取組みとしてプロダクション環境下のサービングのバイナリとインフラに対して新モデルの読み込みを行い、予測ができるかの検証を行う。また一般的にモデルのマイグレーションはカナリリリースにより、新モデルの挙動が問題ないことを確認し、徐々に新モデルの比率を上げて、旧モデルを減らす。そして最終的に旧モデルをゼロにすることでマイグレーションを完了させる   Infra 7: モデルは、安全かつ高速に前のバージョンに戻せるか?  モデルのロールバックが可能かどうかは主にインシデント発生時の対応方法として有用である。モデルのロールバックは、平常時にも常日頃から備えておくべきである。    MONITORING TESTS FOR ML あなたの機械学習システムが、リリース時に正しく動いたかどうかだけではなく、機械学習モデルが正常に稼働し続けることを把握することは非常に重要です。 標準的なアプローチとしては、ダッシュボードを作成し、関連するグラフや統計情報を可視化し、確認できるようにします。そして、その値が大きく動いたときには自動的に担当のチームにアラートが飛ぶようにします。 機械学習システムでは、サービングシステム、学習パイプライン、入力データの監視が非常に重要です。 また、インシデントの対応として機械学習システムが主に行うのはシステムのロールバックではなく、モデルのロールバックである。\n Monitor 1. 依存先が変化した結果は通知されるか?  機械学習システムは、入力データに強く依存する。学習データや推論時に入力データの傾向や使用が変化した際にも把握しておかないといけない How: 開発者チームは、モデルが使用するデータを生み出す依存先をリストアップし依存先に対するアナウンスなどを常に把握しておく。また、依存先のオーナーは、そのデータが機械学習システムに使われていることを知っておく必要もある。   Monitor 2. 学習とサービングの入力時、両者は普遍性を保っているか？  学習データとサービングデータの一貫性を監視することは非常に重要です。機械学習のモデルの振る舞いの異変を把握することは難しいですが、データの普遍性が保たれていないことはまず第一にできる取り組みです。 How: スキーマ管理されたデータを取り扱うようにし、その監視も行う。例えば、TensorFlow Data Validation では、スキーマのズレを監視することが可能である。また、このアラートの閾値は適切に設定しないと、Google 内の機械学習システム開発者チームはこのアラートを無視をするようになった逸話もある2。   Monitor 3. 学習時とサービング時の特徴量計算は同一か？  同一の入力に対しても学習時と推論時の一貫性を保っていない、training / serving skewと呼ばれる問題がある。  How: この問題を解決するためには、実際のサービングシステムへのトラフィックを保管することが肝心である。サービングシステムへの入力データを保管することで、特徴量生成の比較も容易に行うことができる。確認すべき項目として、特徴量は、必ず学習時と推論時は同一でなければならない。 別のアプローチとしては、学習データの統計値と推論時に入力されたデータの統計値を比較することも有用なアプローチである。典型的な統計値としては、最大・最小・平均・欠損値の割合などが挙げられる。   Monitor 4. モデルは極端に古い状態ではないか？  Model 4でも述べたが、モデルの古さは予測精度において重要な指標である。なので、プロダクションで運用されているモデルがどれくらい古いかは監視する必要しておこう。以外にも更新頻度の低いモデルはメンテナンスコストが高い。想像してみよう、例えば一人のエンジニアが手作業で年に 1,2 回再学習されるモデルがあったとする。もしそのエンジニアがチームを離れることになれば、このモデルの再学習の再現は困難になるだろう。たとえ、ドキュメントなどを残していても時間経過により、それも意味を成さなくなる。 How: 週次、もしくはもっと頻繁にモデルの再学習パイプラインを実行する。プロダクション環境下のモデルの古さは常に監視し、どのレベルの古さが予測精度に影響を与えるかを基準にアラート条件を策定する。   Monitor 5. モデルは数値的に安定しているか?  How: モデルから NaN や無限の値が出力されていないか監視する。   Monitor 6. このモデルは学習速度や、サービングレイテンシー、スループット、RAM 使用率に影響を与えるようなメモリリークやデグレーションなどは発生していないか?  DNN の学習は、計算コストが非常に高く計算機コストの増大はスケーリングのボトルネックにもなる。 How: 計算使用の監視は、一般的なモニタリングでも重要な指標です。コードのコンポーネントやバージョンごとにモニタリングをするだけではなく、データやモデルも軸にした監視を行う。   Monitor 7. サービングシステムへの予測品質は測定可能か？  How: データの変更や設定の変更などによる予測品質が低下していないことを確認するためのいくつかの方法を説明する 例えば、特定のタスクなどでは、データのラベルが即座に手に入る（広告クリックなど）。このラベルデータを使って、実際のサービングシステムの予測性能をほぼリアルタイムに算出可能である。このようにヒトを介したモデルの評価機構などは Human In The Loop ともよばれる。 この仕組によって、定期的に学習のための新たなデータが取得できるようになる。    文化の変化 技術的負債は定量化が難しい。また負債を返済するための優先順位付けもやどれくらい改善されたかの測定も難しい。ここで提案した試験項目は定量化した機械学習のテストスコアを提供する。これは、測定可能かつ改善可能なものである。 この指標は、機械学習システムの開発者はどこを改善すればより信頼性のある機械学習システムを作ることができるかのガイドラインとなる。この方針は Google の Test Cerfiedの仕組みを基に発案され、信頼性向上のためのはしごを提供する。\n ML Test Score の計算方法  手動で確認され、ドキュメントに結果をまとめ配布されている場合は、0.5ポイント 反復的に自動的に検証されていたら1.0ポイント 4 つの章で、独立して各ポイントの合計を計算 各章のスコアを集計し、最小の値を最終的な ML Test Score とする    なぜ最小の値を選択する理由として、全ての章が重要であり、すべての章を考慮したスコアを計算するべきだからである。また全てのテスト項目は同一の価値を持つ。 以下に、ML Test Score からその機械学習システムがどのレベルに達しているかの説明を示す\n   ポイント 説明     0 プロダクションレベルというよりも、研究プロジェクトの一種   (0,1] 総合的にテストはされていないが、可能な限り信頼性向上に努めている   (1,2] 基礎的なプロジェクトの要求事項は通過した。しかし、信頼性向上のためのさらなる投資が必要とされる   (2,3] 適切なテストがされている、だが更に自動化の余地が残っている   (3,5] 信頼性の高い自動化されたテストとモニタリングレベル。ミッションクリティカルな状況でも問題はない   \u0026gt;5 卓越したレベルの機械学習システム    Rubic の現実世界の機械学習システムへの適用と実際に起きたこと 我々は Google 内部の機械学習に携わるチームのために ML Test Score Certified Program を開発した。 36 チームにインタビューや Rubric を通じた評価を行い、その統計値を以下の図にまとめた。 Rubric の重要性 Google 内の事例では、グローバル・サービスがローカライズされた際に、特定の単一の国に対して粗悪な予測を提供していないかどうかのチェックリストとして役立ったり、各機械学習システムの評価の標準化の普及に役立った。最終的に ML Test Score が普及をしていく中で、あるチームは training / serving skew の危険性を認識し、特徴生成の部分にテストを実施したり、手動テストを自動テストに切り替えていったりしていきました。\n依存性の問題 機械学習システムのデータの依存性の問題として、開発チームが Upstream サービスから提供されるデータの完全なる理解のオーナーシップを失ってしまう傾向もあります。\nフレームワークの重要性 結合テストは、サービングシステムには適合されていることが多いが、学習パイプラインに適用されていることは稀である。それは、アドホックなスクリプトや手動実行に依存している部分が多いからである。\nモデルのカナリリリース(Infra 6) は、頻繁に多くのチームで既に実装されていた。だがカナリリリースに面白い２つの興味深い問題が浮き彫りになった。\n カナリリリースにより、サービングできない問題や数値的にモデルが安定していないなど多くの問題が見つかるが、実際には単体テストや結合テストで発見することが可能である 既存のフレームワークでカナリリリースを実行しているチームは、好意的な印象をカナリリリースに対してもっているが、独自実装が必要だったチームは二度とやりたくないと言っていた  おそらく、一番重要かつ、最も実装されていないテストの一つがtraining / serving skew (Moniro 3)だ。実際この問題は非常に発見が難しいが、TFX などのフレームワークによりこの課題を解決可能である。\nRublic の評価検証 Google 内で開発チームに対して、この指標が有用かそうでないかの評価を行った。 興味深い結果として、画像や音声のみを扱うチームは Data の章は多くの部分が適切ではないと答えた。 だが、人的検査やLIMEなどの解析は音声・画像のみのデータでも重要だと確認できた。 例えば、そのような検査は分布の不均衡や不確かな影響を受けている学習データを明らかにした。\n教師あり学習は、ラベル付きデータを必要とする。しかし、特定のドメインではそもそもデータセットが手に入らなかったり、取得コストが非常に高い。 あるグループはとても巨大で多様性があるデータセットを所有しており、人力でラベルを付けていくことは不可能に見えた。彼らは、ヒューリスティックなシステムを構築して、それを活用して機械学習モデルを構築しました。（機械学習の専門家はこの学習モデルは狂っていてうまくいくわけがないと言っていました）\n実際にヒトが評価を行ってみるとヒューリスティックシステムの性能は一貫して良いが、そこから学習した機械学習システムのほうが更に良かった。しかし、ここで基本的なヒューリスティックシステムに対する指標を我々が用意できていないことが露呈した。ラベリングコストが高いことは、学習モデルの定量的評価も難しいことを意味し、Model4, Infra 4の検査も困難になる。\n  https://pair-code.github.io/facets/ \u0026#x21a9;\u0026#xfe0e;\n https://research.google/pubs/pub46484/ \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://shunyaueta.com/posts/2020-04-25/","summary":"[抄訳] What’s your ML test score? A rubric for ML production systemsで紹介した論文の続編があったので読んでみました。\n 注意)この翻訳記事は原著論文の著者陣からレビューはされていません Shunya Ueta, are providing a translation and abridgment, which has not been reviewed by the authors.  Change log  2021/02/03  ML Test Score を簡単に計算できるGoogle Spread Sheets を公開   2020/06/24  著者の Eric Breck さんに連絡をし、抄訳の公開を快諾していただきました。ありがとうございます。 完全な citation 情報を追記しました。 この翻訳記事が著者のレビューを受けていないことを追記しました。    Citation  Eric Breck, Shanqing Cai, Michael Salib, . The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction.","title":"機械学習システムの信頼性を数値化し、技術的負債を解消する論文「 The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction」"},{"content":"NIPS206 にて開催された Reliable Machine Learning in the Wild - NIPS 2016 Workshop (2016) という、現実世界でどうやって信頼性の高い機械学習に取り組んでいくかについてのワークショップがある。\nここで Google から発表された What’s your ML test score? A rubric for ML production systems がとても面白く、身になるものが多かったのでメモがてら抄訳を残しておく。\n PDF Slide 発表動画もワークショップページにて公開されています。  change logs  2021-04-25  この原著論文の完全版になっている論文の抄訳を新たに公開しています。 [抄訳]: The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction     概略  現実世界のプロダクションシステムで機械学習を使う際に、機械学習の研究実験と異なる、小さなもしくは小さくない問題がある テストとモニタリングはプロダクションレディの機械学習システムにとって必要不可欠 しかし、どれくらいのテストとモニタリングをすれば十分と言えるのだろうか? この論文では、それらの問題を解決する ML Test Score という基準を提案する  Introduciton   Google 内で積み重ねたベストプラクティスをもとに、実行可能なテスト、そしてその機械学習システムがどのていどプロダクションレディなのかを示すスコアシステムを提案\n  このスコアは、機械学習を始めたばかるのチームからエキスパートがあつまるチームまで幅広く適用可能\n  注意: 一般的な Sofware Engineering のベストプラクティスは含んでいない\n  そのかわり、学習とサービングのための Unit Test Coverage の計算方法など機械学習に必要不可欠な点を抑えている\n  ML Test Score の計算方法  各テストの加点基準  1pt: 手動で実行し、その結果を文章として共有済 2pt: CI に組み込まれ、自動的に反復実行済   最終的な ML Score は以下の基準となる  0pt : プロダクション向けというよりも研究プロジェクト 1-2pt : テストが少しはされているが、プロダクションではもっと深刻な罠がある可能性あり 3-4pt : 最初のプロダクションレディへの第一歩。しかし、さらなる投資が必要 5-6pt : 適切なテストがされているが、もっと自動化してみよう 7-10pt : 自動化されたテストとモニタリングが整備されている。重要なシステムでも適切なレベルに達している 12+pt : 卓越した自動化されたテストとモニタリング    前提条件  システムアーキテクチャの前提として、生データから特徴量を抽出し、学習システムに流し込まれる。そして推論のためにサービングされ、その機能は顧客に影響を与える。また、ソースリポジトリや CI を通したテスト、実験のバージョン管理なども可能  特徴量とデータセット  各特徴量の分布が期待した値になっているか?  一例として、特徴 A が 1 から５の値を持つ、または特徴 B の最も普遍期な値として「Harry」と「Potter」が全体の 10%を占める特徴に対して、上記の分布を検証するようなテストを書き、このテストが失敗すれば外部環境が変化しモデルの変更が余儀なくされていることが把握できる   各特徴と目的変数の関係性そして、ペアの相関が独立してあらわれるか?  個の特徴がモデルに使用される際に、   各特徴量のコストは検討されているか?  特徴量のコストとは、各特徴を追加した際の推論速度やメモリ使用率、データ依存 \u0008 やその特徴に依存することによるシステムの不安定性などの検証   手動作業により適切でない特徴量は使用されていないか? データパイプラインの中で、プライバシーコントロールが考慮されているか?  Access Controll の権限は機械学習システムにも適切に付与されているか検証を行う   プロダクションモデルへ新しい特徴量を開発・追加するのに必要な日数はいくらか?  ちょっとしたアイデアをプロダクションで素早く検証できるチームは強い。速さはシステムの改善と外的変化の対応するために重要   学習とサービングで使用される特徴量生成に関わるコードすべてに対するテスト  軽視されがちだが、非常に重要    モデル開発  全てのモデルはコードレビューされリポジトリに格納されているか?  コードレビューは素晴らしい仕組みで、つまらないミスやインシデントに関わるような潜在的なものまで顕在化される   オフラインの疑似指標とオンライン指標の関係性  1%の精度がお客さまの満足度にどうつながるのだろうか? (例: CTR) この検証は意図的に劣化させたモデルなどで小規模の AB テストを通して検証可能   チューニング可能な各ハイパーパラメータのインパクト  グリッドサーチやベイズ最適化は、予測性能を向上させるだけではなく潜在的な信頼性の問題を明らかにできる。例えば、データ並列によるモデルの精度の変化は大きなインパクトである   モデルのフレッシュネス  各モデルが、昨日 VS 先週 VS 過去一年 で学習されたとしてオンラインメトリクスへの影響がどう異なるか? 全てのモデルは外部要求により更新を余儀なくされる。この評価は注意深く行い、決定していくこと   単純なモデルをベースラインとした検証  線形モデルで少ない特徴量のモデルでベースラインを作成しよう。これは、大規模なパイプライン構築やコストのトレードオフの検証にとても効果的である   重要なデータを分割した際のモデル品質の検証  TBD: ちょっと理解しきれない   暗黙的バイアスへの検証  例えば学習データの規模が小規模だと暗黙的なバイアスが掛かってしまう。(例：靴の認識で、男物の靴しか学習データが収集できず、女性の靴のデータがほぼ存在しないなど)    機械学習インフラストラクチャ  モデルの学習の再現性検証  完全に同一データから、２つのモデル作成する。両者のスコア、サンプルに対する予測結果など大きな違いがないか検証を行う   モデルの仕様に関する検証  有用な Assertion として、ロスの低下や学習ジョブがクラッシュしても復元可能など   完全な機械学習パイプラインテストのための結合テスト  良い結合テストとは、もとのデータソースから特徴生成 → 学習 またはサービングまでをカバーする。結合テストは CI を通じて実行し、潜在的な問題をリリース前に発見するために新規リリースの前に検証される   サービング前のモデル品質テスト  サンプルデータを収集して、既知の正しい出力がされるように検証を行い、集計する。\u0008 前バージョンの機械学習モデルの予測結果を用いたバックテストも良い取り組みの一つ   一つのサンプル、もしくはバッチ学習をモデルに送り、学習から予測までの内部状態の変化の監視  少量のデータを流し、モデルの状態がどう変化するか監視するのは有効なアプローチである   本番環境前のカナリアリリースによるテスト  モデリングのコードはサービングコードと違って高頻度に変化する。それにより以前のサービングコードが新しいモデルでは動かない危険性なども発生する。この検証は、モデルがプロダクション環境下で正常に読み込まれるか、正常に推論が稼働するかを確認する。新しいモデルは常に、小規模のトラフィックで検証されるべきである   新しい機械学習モデルから、安全かつ素早く前の機械学習モデルに戻せるか?  モデルのローロバック手続きは、モデルの品質がリリースして良くなかった場合にとても便利である。以前の高性能なモデルに安全かつ素早く戻せる仕組みは非常に重要    機械学習のためのモニタリング指標  学習とサービングの両者にて、イベント発生時の不安定性の検証  モデルの学習時間の不安定性  モデルが頻繁に再学習されるような状況だと、特に問題である   サービング時間の不安定性  client 側の通信が停止したりすると、サービングされなくなる     学習時とサービング時でデータの不変性が保持されているかの検証  例: 特徴量 A と B は、常に同一の非零の要素数を持つ、特徴量 C は常に(0,100)の範囲内かなどを検証する   学習したモデルと、サービングの特徴が完全に同一か  Training/ Serviing Skew とも呼ばれている (Concept Drift とも呼ばれています) 学習したデータと本番のデータが時間経過、もしくは何らかの要因で異なってしまうことが原因です Training/Serving skew は TFX で説明されている   モデルは枯れているか?  使い古されたモデルの場合、パイプラインが停止しても何が原因か把握できる。例えば、重要なテーブルを作成する毎日実行されるジョブがストップしたとして、どんなアラートが良いだろうか? *TBD   学習、もしくはサービング時にNaN または、 無限の値が発生への検証 学習速度、サービング速度、スループットまたは RAM 使用率の検証  スケールする際に機械学習システムの計算機コストは悩みの種である。回帰テストなどで検証されるべきである   予測品質に関する回帰テスト  外部要因により、品質が下がる場合もあるが多くの場合は現実の問題を洗い出すことができる     翻訳でもし怪しい部分やもっとこうしたほうが良いというところがあれば、コメントか @hurutoriya までご連絡ください\n実際読み込んで見ると、信頼性の高いモデルを届けるための秘訣が山盛りで、流石 Google 内のベストプラクティスをまとめたチェックリストですね 実際に開発している機械学習システムがあれば、この Test Score を当てはめてみてどの程度できているのか計測してみるが良さそう\n機械学習エンジニアとして働く自分として、興味のど真ん中にあるようなワークショップだが、2016,2017 年の二回しか開催されておらず、非常に悲しい 逆に言えば、すごく先進的なワークショップでこのワークショップが開催されていたのは凄いことだ\n自分が業務を通じて得た経験やベストプラクティスがすでに論文として世の中に公開されているのはすごく経験になるし、経験から学ぶのではなく歴史から学べるようにしていきたい！ なので、この Workshop 論文の発展形として、The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction が発表されているので読んでみる\nいや、実装寄りの勉強ばかりしていたけど気分転換にこういう知識体系に触れるのすごく楽しい\n","permalink":"https://shunyaueta.com/posts/2020-04-19/","summary":"NIPS206 にて開催された Reliable Machine Learning in the Wild - NIPS 2016 Workshop (2016) という、現実世界でどうやって信頼性の高い機械学習に取り組んでいくかについてのワークショップがある。\nここで Google から発表された What’s your ML test score? A rubric for ML production systems がとても面白く、身になるものが多かったのでメモがてら抄訳を残しておく。\n PDF Slide 発表動画もワークショップページにて公開されています。  change logs  2021-04-25  この原著論文の完全版になっている論文の抄訳を新たに公開しています。 [抄訳]: The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction     概略  現実世界のプロダクションシステムで機械学習を使う際に、機械学習の研究実験と異なる、小さなもしくは小さくない問題がある テストとモニタリングはプロダクションレディの機械学習システムにとって必要不可欠 しかし、どれくらいのテストとモニタリングをすれば十分と言えるのだろうか? この論文では、それらの問題を解決する ML Test Score という基準を提案する  Introduciton   Google 内で積み重ねたベストプラクティスをもとに、実行可能なテスト、そしてその機械学習システムがどのていどプロダクションレディなのかを示すスコアシステムを提案","title":"機械学習システムの信頼性を数値化する論文「 What’s your ML test score? A rubric for ML production systems」"},{"content":"Coursera でHow Google does Machine Learning の講義を修了した\nCertificate はこちら\n7 割が、機械学習プロジェクトの始め方、実際のハマりどころなどが Google 内の実例などに基づいて語られていて面白かった。 特に、\n Secret Sourse ML Sutprise  の部分が、アンチパターンや成功方法などが語られていて実際に実務で機械学習をやっている自分としてはわかる~~~~と共感がすごくできて面白かった。初めて機械学習プロジェクトを担当する PM の方にも良い教材なのではと思いました。\n残りの 3 割は、qwiklab を使って、Notebook を立ち上げたり、Google BQ 叩いたり、Pandas, Google Cloud Vison API など各 ML 系の API を触るといった感じで、初心者過ぎて自分にはレベル感が少しあいませんでしたが、これも非エンジニアの方が機械学習ってこんな感じかと学ぶきっかけにはすごく良さそうです。\n最初の 7 割の部分は、改めてデータ利活用を前提にしたプロジェクトを牽引していく際にここで見つめ直す形になってよかったです。\n最近、Cousera での講義を始める機会があり Andrew Ng 先生の機械学習コースぶりに Coursera をやっているが、勉強のペースメーカーが決められるのと講義内容の質も高いので自分にとってはすごく相性が良い。\n技術書を読むときも同じペースで、実行できないかなと画策したい\n","permalink":"https://shunyaueta.com/posts/2020-04-18/","summary":"Coursera でHow Google does Machine Learning の講義を修了した\nCertificate はこちら\n7 割が、機械学習プロジェクトの始め方、実際のハマりどころなどが Google 内の実例などに基づいて語られていて面白かった。 特に、\n Secret Sourse ML Sutprise  の部分が、アンチパターンや成功方法などが語られていて実際に実務で機械学習をやっている自分としてはわかる~~~~と共感がすごくできて面白かった。初めて機械学習プロジェクトを担当する PM の方にも良い教材なのではと思いました。\n残りの 3 割は、qwiklab を使って、Notebook を立ち上げたり、Google BQ 叩いたり、Pandas, Google Cloud Vison API など各 ML 系の API を触るといった感じで、初心者過ぎて自分にはレベル感が少しあいませんでしたが、これも非エンジニアの方が機械学習ってこんな感じかと学ぶきっかけにはすごく良さそうです。\n最初の 7 割の部分は、改めてデータ利活用を前提にしたプロジェクトを牽引していく際にここで見つめ直す形になってよかったです。\n最近、Cousera での講義を始める機会があり Andrew Ng 先生の機械学習コースぶりに Coursera をやっているが、勉強のペースメーカーが決められるのと講義内容の質も高いので自分にとってはすごく相性が良い。\n技術書を読むときも同じペースで、実行できないかなと画策したい","title":"CourseraでHow Google does Machine Learning の講義を修了した"},{"content":"表題のとおりですが、Getting Started with Google Kubernetes Engine という Coursera の講義を終了しました\n業務で k8s を本格的に使い始め、ちゃんと理解したいな~と思いこのコースを取りました。\n半年前に Kubernetes 完全ガイド impress top gear シリーズ をサラッと読んではいたのですが、やはり手を動かして学んでいないと実際に kubectl command など完全に忘れているし、スキルとして身についていない感半端なかったので、良い機会なので Hands-on が提供うされている Coursera を使って学んでみました。\n個人的にこの講義がめちゃくちゃオススメなのが、 GKE の講義なので Google が提供する qwiklab が使えます。 一時的に GCP プロジェクトが作成され、そこでハンズオンができるのですがこれが実際に手を動かしながら学ぶという形式にすごく良いのと k8s の構築もすべてクラウドでできえるので変に環境構築でハマることなく快適に学習に集中できました。\nもう、これがめちゃくちゃ快適でハンズオンとしてすごく快適に手を動かせなら、k8s の初歩的な概念や Command line などを学べました。\n実際に手を動かしながら学ぶべきものだと思うので、このハンズオン形式の講義はありがたかったです!\n後は学ぶにつれて、 k8s の凄さがわかってきたので理解して使いこなせるようになればスケールするシステムを個人でも作れそうなので、頑張っていきます。\n","permalink":"https://shunyaueta.com/posts/2020-04-12/","summary":"表題のとおりですが、Getting Started with Google Kubernetes Engine という Coursera の講義を終了しました\n業務で k8s を本格的に使い始め、ちゃんと理解したいな~と思いこのコースを取りました。\n半年前に Kubernetes 完全ガイド impress top gear シリーズ をサラッと読んではいたのですが、やはり手を動かして学んでいないと実際に kubectl command など完全に忘れているし、スキルとして身についていない感半端なかったので、良い機会なので Hands-on が提供うされている Coursera を使って学んでみました。\n個人的にこの講義がめちゃくちゃオススメなのが、 GKE の講義なので Google が提供する qwiklab が使えます。 一時的に GCP プロジェクトが作成され、そこでハンズオンができるのですがこれが実際に手を動かしながら学ぶという形式にすごく良いのと k8s の構築もすべてクラウドでできえるので変に環境構築でハマることなく快適に学習に集中できました。\nもう、これがめちゃくちゃ快適でハンズオンとしてすごく快適に手を動かせなら、k8s の初歩的な概念や Command line などを学べました。\n実際に手を動かしながら学ぶべきものだと思うので、このハンズオン形式の講義はありがたかったです!\n後は学ぶにつれて、 k8s の凄さがわかってきたので理解して使いこなせるようになればスケールするシステムを個人でも作れそうなので、頑張っていきます。","title":"Courseraで Getting Started with Google Kubernetes Engine の講義を修了した"},{"content":"pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。\n問題点  そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い  解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。\n実は Google の公式ドキュメントでも推奨されています。\n https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas  方法は以下の２つ。\n google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。\n1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 1  pip install --upgrade google-cloud-bigquery[bqstorage,pandas]   magic command を実行\n1  %load_ext google.cloud.bigquery   後は Jupyter Notebook のセルで以下のコマンドを実行すれば、\n1 2 3 4 5 6 7 8 9 10  %%bigquery df --use_bqstorage_api SELECT CONCAT( \u0026#39;https://stackoverflow.com/questions/\u0026#39;, CAST(id as STRING)) as url, view_count FROM `bigquery-public-data.stackoverflow.posts_questions` WHERE tags like \u0026#39;%google-bigquery%\u0026#39; ORDER BY view_count DESC LIMIT 10   df にマジックコマンドで実行した SQL の実行結果が格納されます! 便利ですね\n2, BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  BigQuery でクエリを実行、実行結果を BigQuery Table へ保存 注)実行結果の容量が巨大なので、保存先は基本的に Big Query Table へ保存するしか選択肢が無い   BigQuery table から GCS へテーブルを CSV として保存  Big Query table からエクスポート時に、ファイルサイズが大きいとエクスポートできないので、分割が必要です。\nhttps://cloud.google.com/bigquery/docs/exporting-data\n保存ファイル名を file-* のようにワイルドカードを指定すると、自動的にひとつのテーブルを複数ファイルに分割して保存してくれる\ngsutil commands で任意のマシンへダウンロードする。\n-m オプションを付け足すと並列ダウンロードが始まるので、複数ファイルダウンロードする場合はおすすめです\nストレスレスなデータ分析ライフを!\n","permalink":"https://shunyaueta.com/posts/2019-10-03/","summary":"pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。\n問題点  そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い  解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。\n実は Google の公式ドキュメントでも推奨されています。\n https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas  方法は以下の２つ。\n google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。\n1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 1  pip install --upgrade google-cloud-bigquery[bqstorage,pandas]   magic command を実行","title":"遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む"},{"content":"Pytorch 1.2 からは公式に Tensorboard がサポートされている\nTensorboard とは、学習の状況を可視化できる TensorFlow Family の一種\nJupyte Notebook 上で学習状況を確認したい場合に Tensorboard をそのまま表示して確認できれば楽なので、試してみる\n sample code: https://pytorch.org/docs/stable/tensorboard.html  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import torch import torchvision from torch.utils.tensorboard import SummaryWriter from torchvision import datasets, transforms # Writer will output to ./runs/ directory by default writer = SummaryWriter() transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) trainset = datasets.MNIST(\u0026#39;mnist_train\u0026#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) model = torchvision.models.resnet50(False) # Have ResNet model take in grayscale rather than RGB model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) images, labels = next(iter(trainloader)) grid = torchvision.utils.make_grid(images) writer.add_image(\u0026#39;images\u0026#39;, grid, 0) writer.add_graph(model, images) writer.close()   その結果を Jupyter 上で確認したい場合、以下のマジックコマンドを実行する\n https://www.tensorflow.org/tensorboard/r2/tensorboard_in_notebooks  1 2  %load_ext tensorboard %tensorboard --logdir runs   結果を 1 画面で完結して見せたい場合に便利そう\n完了!\n","permalink":"https://shunyaueta.com/posts/2019-09-25/","summary":"Pytorch 1.2 からは公式に Tensorboard がサポートされている\nTensorboard とは、学習の状況を可視化できる TensorFlow Family の一種\nJupyte Notebook 上で学習状況を確認したい場合に Tensorboard をそのまま表示して確認できれば楽なので、試してみる\n sample code: https://pytorch.org/docs/stable/tensorboard.html  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import torch import torchvision from torch.utils.tensorboard import SummaryWriter from torchvision import datasets, transforms # Writer will output to ./runs/ directory by default writer = SummaryWriter() transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.","title":"Jupyter Notebook上にTensorboard を わずか2行で表示させる"},{"content":"1. Generate SSH config file using gcloud command line 1  gcloud compute config-ssh   https://cloud.google.com/sdk/gcloud/reference/compute/config-ssh\nYou cant get ssh config for your Google Compute Engine project!\nNotice: you need choose target GCP project before run below command.\n1  gcloud config set project \u0026lt;your-project-id\u0026gt;   2. Install Remote SSH extention in Visual Studio Code. https://code.visualstudio.com/blogs/2019/07/25/remote-ssh\n3. Press ⇧⌘P \u0026amp; Select target connection in Visual Studio Code! Finaly you can connect in Visual Studio Code. Welcome to VS code when you write the code in SSH connection.\n","permalink":"https://shunyaueta.com/posts/2019-09-24/","summary":"1. Generate SSH config file using gcloud command line 1  gcloud compute config-ssh   https://cloud.google.com/sdk/gcloud/reference/compute/config-ssh\nYou cant get ssh config for your Google Compute Engine project!\nNotice: you need choose target GCP project before run below command.\n1  gcloud config set project \u0026lt;your-project-id\u0026gt;   2. Install Remote SSH extention in Visual Studio Code. https://code.visualstudio.com/blogs/2019/07/25/remote-ssh\n3. Press ⇧⌘P \u0026amp; Select target connection in Visual Studio Code! Finaly you can connect in Visual Studio Code.","title":"How to connect the Google Compute Engine via Visual Studio Code"},{"content":"@pseudo_finite さんから\n「ビジネスでインパクトが出せるデータサイエンティストになるためには」\nをご恵贈していただいたので、感想をここに記します。\n 経営システム誌に寄稿したものができました。30 部あるので欲しい方はお声がけください。\n私の 10 年間の経験を整理して中堅のデータサイエンティスト向けに書いたものになります。批判的なフィードバックなどいただけると嬉しいです。(@pseudo_finite) tweet January 18, 2019\n 批評 1. データサイエンティストが力を発揮する場  データサイエンティストとして成果を発揮するには、事業ドメイン・そしてデータの規模と質に依存する\n 圧倒的に同意です。自分も現職に就職する際には、データ規模・質・種類や社内のデータに関する文化などを考慮して会社を選びました。 最後の一文も完全に同意で、良いデータさえあれば基本的に問題は解きやすく簡単になると思っています。\n2. 課題設定  データサイエンティストの仕事の肝は適切な課題設定\n 本質的な課題設定とはそもそもなんなのかと考えてみます。\nここで、本質的な課題設定を解明するための大きな障壁になるのは、\n 自社が事業会社か ドッグフーディングできるサービスを運営しているか  どうかではないでしょうか?\n日常的に自社のサービスを使っていると、顧客視点での改善点や課題点などを見つけることができる。 また、サービスをより深く知ることで深い考察や客観的な観察をすることができる。 スタートアップ界隈では浸透している リーンスタートアップの考え方は、本質的な課題の発見に非常に相性が良いと思っています。\nまた、データ分析では、単なる集計や相関ではなく、顧客がどんな状況で何をしたいのかを考えてユーザーリサーチをすることも非常に重要です。 ジョブ理論 イノベーションを予測可能にする消費のメカニズム で語られているトピックですが、非常に勉強になります。\n3. 解決方法の設定  自然なモデリングと実現可能性のあるモデリング\n 自分はデータサイエンティストではなく、機械学習エンジニアとして働いているので、その立場からの視点です。 実感するのはまず何よりも実装力が大事だと思います。\n実装ができるからこそ、実験ができる。その実験から知見を得て改善のサイクルが回り始める。\n関連する暦本先生の tweet が面白かったので、ご参考まで\n深層学習時代になってますますですが研究者のコード書きは開発というより未解決の問題や仮説に決着をつける行為、サイエンスにおける実験そのものなのでコード書かない人がどうやって研究してるのか想像つかないです🤔 https://t.co/mojhTqHqmx\n\u0026mdash; Jun Rekimoto : 暦本純一 (@rkmt) September 16, 2019  4. 検証  施策実行後の検証は必須\n 個人的には自分が最も重要だと思う点はここである。 確かに施策が成功したら、燃え尽きたくなる気持ちはわかるが、なぜ成功したのかを解明して再現性を担保しなければ知見としてストックされない。 そして知見の溜め込みの速さ・多さこそがビジネスとしての優位性につながるのではないのだろうか? これこそ、まさに科学的思考の本幹ですね。\n5. 育成  データサイエンティストの育成は非常に難しい\n 育成の点は、自分も最近考えていたことですが、\n 例えば研究室のセミナーや論文の赤入れなどで議論をしたからこそ科学的思考方法が身についたのか?  を考えていました。\n僕の結論では、 強い相関はあれど研究室での議論により全員が身科学的思考方法を会得するのは難しいのではないかと思っています。 (もちろん全員が身につけることこそ、研究室の本懐だと思います)\nSoftware Enginnering やアカデミアの世界では、レビュー文化を体験してあくまで内容に関する批判であり、 フィードバックを受け入れて改善する姿勢を身に着けていることも大事だなと最近感じています。\nTeam Geek ―Google のギークたちはいかにしてチームを作るのか  の書籍で語られる HRT の精神ですね。\nまとめ 自分も機械学習エンジニアとして働きはじめて、2 年弱が経過しようとしている。 シニアクラスになるためには、まだまだ不足点がたくさんあるが俯瞰的に見直せる良い機会になる良い予稿だった。\n岩永次郎さんが書かれている続編的な位置づけの\nデータサイエンティストが実務を通して経験すべきこと\nが、更に踏み込んだ内容になっているのでこの記事が面白いと感じた人はぜひご覧ください。\n","permalink":"https://shunyaueta.com/posts/2019-09-23/","summary":"@pseudo_finite さんから\n「ビジネスでインパクトが出せるデータサイエンティストになるためには」\nをご恵贈していただいたので、感想をここに記します。\n 経営システム誌に寄稿したものができました。30 部あるので欲しい方はお声がけください。\n私の 10 年間の経験を整理して中堅のデータサイエンティスト向けに書いたものになります。批判的なフィードバックなどいただけると嬉しいです。(@pseudo_finite) tweet January 18, 2019\n 批評 1. データサイエンティストが力を発揮する場  データサイエンティストとして成果を発揮するには、事業ドメイン・そしてデータの規模と質に依存する\n 圧倒的に同意です。自分も現職に就職する際には、データ規模・質・種類や社内のデータに関する文化などを考慮して会社を選びました。 最後の一文も完全に同意で、良いデータさえあれば基本的に問題は解きやすく簡単になると思っています。\n2. 課題設定  データサイエンティストの仕事の肝は適切な課題設定\n 本質的な課題設定とはそもそもなんなのかと考えてみます。\nここで、本質的な課題設定を解明するための大きな障壁になるのは、\n 自社が事業会社か ドッグフーディングできるサービスを運営しているか  どうかではないでしょうか?\n日常的に自社のサービスを使っていると、顧客視点での改善点や課題点などを見つけることができる。 また、サービスをより深く知ることで深い考察や客観的な観察をすることができる。 スタートアップ界隈では浸透している リーンスタートアップの考え方は、本質的な課題の発見に非常に相性が良いと思っています。\nまた、データ分析では、単なる集計や相関ではなく、顧客がどんな状況で何をしたいのかを考えてユーザーリサーチをすることも非常に重要です。 ジョブ理論 イノベーションを予測可能にする消費のメカニズム で語られているトピックですが、非常に勉強になります。\n3. 解決方法の設定  自然なモデリングと実現可能性のあるモデリング\n 自分はデータサイエンティストではなく、機械学習エンジニアとして働いているので、その立場からの視点です。 実感するのはまず何よりも実装力が大事だと思います。\n実装ができるからこそ、実験ができる。その実験から知見を得て改善のサイクルが回り始める。\n関連する暦本先生の tweet が面白かったので、ご参考まで\n深層学習時代になってますますですが研究者のコード書きは開発というより未解決の問題や仮説に決着をつける行為、サイエンスにおける実験そのものなのでコード書かない人がどうやって研究してるのか想像つかないです🤔 https://t.co/mojhTqHqmx\n\u0026mdash; Jun Rekimoto : 暦本純一 (@rkmt) September 16, 2019  4. 検証  施策実行後の検証は必須","title":"ビジネスでインパクトが出せるデータサイエンティストになるには"},{"content":"When you need to concat same size image to make figure.\nskimage \u0026amp; numpy combination is too powerfull to concat images.\nThis is sample script.\n1 2 3 4 5 6 7 8  from skimage import data, io import numpy as np img = skimage.data.astronaut() imgs= [img for i in range(10)] skimage.io.imsave(\u0026#34;sample_h.png\u0026#34;,np.hstack(imgs)) skimage.io.imsave(\u0026#34;sample_v.png\u0026#34;,np.vstack(imgs))   After that you can get below images.\n    Via Gist: https://gist.github.com/hurutoriya/fedf059ad3db5c67b16d8d5dd6d3df70\n","permalink":"https://shunyaueta.com/posts/2019-06-17/","summary":"When you need to concat same size image to make figure.\nskimage \u0026amp; numpy combination is too powerfull to concat images.\nThis is sample script.\n1 2 3 4 5 6 7 8  from skimage import data, io import numpy as np img = skimage.data.astronaut() imgs= [img for i in range(10)] skimage.io.imsave(\u0026#34;sample_h.png\u0026#34;,np.hstack(imgs)) skimage.io.imsave(\u0026#34;sample_v.png\u0026#34;,np.vstack(imgs))   After that you can get below images.\n    Via Gist: https://gist.github.com/hurutoriya/fedf059ad3db5c67b16d8d5dd6d3df70","title":"How to concat image using skimage"},{"content":" Hugo 0.32から page bundle が使用可能に この機能で画像ファイルを以下のファイル構成で構築できる  1 2 3  - hoge/ - index.md - hoge.png   これにより、markdown とアセットファイルが同一ディレクトリ内に収まるのでアセットファイルの管理が簡単になる\n hugo new で特定のエディタを開くには?  1  hugo new posts/hoge.md --editor=\u0026#34;code\u0026#34;    作成時にslug に日付を含める 今回は2020-09-09の形式で slug を作成する  1  hugo new posts/$(date \u0026#39;+%Y-%m-%d\u0026#39;)/index.md    page をビルドして結果を確認する  1  hugo server    下書きも含めてビルドする  1  hugo server -D   ","permalink":"https://shunyaueta.com/posts/2019-06-16/","summary":" Hugo 0.32から page bundle が使用可能に この機能で画像ファイルを以下のファイル構成で構築できる  1 2 3  - hoge/ - index.md - hoge.png   これにより、markdown とアセットファイルが同一ディレクトリ内に収まるのでアセットファイルの管理が簡単になる\n hugo new で特定のエディタを開くには?  1  hugo new posts/hoge.md --editor=\u0026#34;code\u0026#34;    作成時にslug に日付を含める 今回は2020-09-09の形式で slug を作成する  1  hugo new posts/$(date \u0026#39;+%Y-%m-%d\u0026#39;)/index.md    page をビルドして結果を確認する  1  hugo server    下書きも含めてビルドする  1  hugo server -D   ","title":"Hugo Tips"},{"content":"MLCT #10 を開催しました。\nMachine Learning Casual Talks とは\n実サービスにおける機械学習の経験をカジュアルに語り合おう\nを目的としたコミュニティです。\nスポンサー 前回と同じく本イベントは株式会社メルカリにスポンサーとして会場と懇親会の提供を受け開催することができました。 スポンサー依頼を快諾いただきありがとうございました!\n配信動画はこちら!\n Sli.do がパネルディスカッションでめっちゃ便利な件 今回は、パネルディスカッションで sli.do をスクリーンにフルスクリーンで表示してモデレーションを行いました。\n手元のスマートフォンで質問をハイライトして、回答を終えたものはアーカイブという運用でしたが、とても快適なのでみなさんぜひお試しください。 スクリーンでの表示画面が SPA で同期されているので、手元のスマートフォンで更新すればリアルタイムで同期されるのがとても便利です。\n","permalink":"https://shunyaueta.com/posts/2019-06-15/","summary":"MLCT #10 を開催しました。\nMachine Learning Casual Talks とは\n実サービスにおける機械学習の経験をカジュアルに語り合おう\nを目的としたコミュニティです。\nスポンサー 前回と同じく本イベントは株式会社メルカリにスポンサーとして会場と懇親会の提供を受け開催することができました。 スポンサー依頼を快諾いただきありがとうございました!\n配信動画はこちら!\n Sli.do がパネルディスカッションでめっちゃ便利な件 今回は、パネルディスカッションで sli.do をスクリーンにフルスクリーンで表示してモデレーションを行いました。\n手元のスマートフォンで質問をハイライトして、回答を終えたものはアーカイブという運用でしたが、とても快適なのでみなさんぜひお試しください。 スクリーンでの表示画面が SPA で同期されているので、手元のスマートフォンで更新すればリアルタイムで同期されるのがとても便利です。","title":"Machine Learning Casual Talks #10 を開催しました"},{"content":"Machine Learning Casual Talks 第 8 回の開催を無事終えました\nMLCT とは\n実サービスにおける機械学習の経験をカジュアルに語り合う\nコミュニティです\nスポンサー 今回も同じく本イベントは株式会社メルカリにスポンサーとして会場と懇親会を提供を受け開催することができました!\nMachine Learning Casual Talks #8 (2019/01/28 18:30〜)\n当日の配信動画はこちら\n当日の発表資料はすべてこちらにあります\nMachine Learning Casual Talks #8 - 資料一覧 - connpass\nエムスリー 西場さん\nBEDORE すみのさん\nTL;DR;  エムスリーの西場さん、すべてをこなす toB の機械学習サービス、システムアーキテクチャデザインかなり考えないとキツイ 懇親会での 🍣 の需給予測失敗しかけた  次回挑戦したいこと 今回会場撤収時に有志の参加者、登壇者の方が撤収作業を手伝っていただき非常に助かりました。次回は有志で会場撤収ボランティアの参加枠を作ろうかなと思いました。運営コストを下げるのは、継続で一番大事だなと思っているので、お手伝いいただいた皆様ありがとうございました。助かるという感情が出てくる前に、素直にめちゃくちゃ嬉しかったです!\n参加率も 8 割を超えていて欠席率が非常に少なかったのも継続していきたい\n","permalink":"https://shunyaueta.com/posts/2019-02-02/","summary":"Machine Learning Casual Talks 第 8 回の開催を無事終えました\nMLCT とは\n実サービスにおける機械学習の経験をカジュアルに語り合う\nコミュニティです\nスポンサー 今回も同じく本イベントは株式会社メルカリにスポンサーとして会場と懇親会を提供を受け開催することができました!\nMachine Learning Casual Talks #8 (2019/01/28 18:30〜)\n当日の配信動画はこちら\n当日の発表資料はすべてこちらにあります\nMachine Learning Casual Talks #8 - 資料一覧 - connpass\nエムスリー 西場さん\nBEDORE すみのさん\nTL;DR;  エムスリーの西場さん、すべてをこなす toB の機械学習サービス、システムアーキテクチャデザインかなり考えないとキツイ 懇親会での 🍣 の需給予測失敗しかけた  次回挑戦したいこと 今回会場撤収時に有志の参加者、登壇者の方が撤収作業を手伝っていただき非常に助かりました。次回は有志で会場撤収ボランティアの参加枠を作ろうかなと思いました。運営コストを下げるのは、継続で一番大事だなと思っているので、お手伝いいただいた皆様ありがとうございました。助かるという感情が出てくる前に、素直にめちゃくちゃ嬉しかったです!\n参加率も 8 割を超えていて欠席率が非常に少なかったのも継続していきたい","title":"Machine Learning Casual Talks #8 を開催しました"},{"content":"Machine Learning Casual Talks 第七回を無事開催しました\nMachine Learning Casual Talks #7 (2018/11/20 18:30〜)\nMLCT とは\n実サービスにおける機械学習の経験をカジュアルに語り合おう\nというコミュニティです。\nスポンサー 今回も同じく本イベントは株式会社メルカリにスポンサーとして会場と懇親会を提供を受け開催することができました。スポンサー依頼を快諾いただきありがとうございました!\n遺伝異常により髪が青く変色してしまったタカヤナギ=サン\nABEJA の機械学習導入事例と辛い話を大田黒さんにお話していただきました!\n今回の勉強会資料は以下にまとまっています。\nMachine Learning Casual Talks #7 — 資料一覧 — connpass\n今回の内容を今北産業\n 機械学習エンジニアとしてのキャリアのお話を タカヤナギ=サン 実世界に根付いた IOT と機械学習サービスはかなり辛い 各社の機械学習エンジニアの定義が揺らいでいるので、世界が壊れる  今回の改善点 参加枠の多様性  絶対参加するぞ枠 一般参加枠 初回参加枠 SNS 枠 Blog 枠  と今までは一つの枠で扱っていたものを、5 つの枠に分散して用意してみました。\nなぜかというとドタキャンやノーショーの方の影響で本当に参加したい方や初回参加の方の機会が喪失してしまうのはいただけないので、それを解決したなと思ったのが始まりです。\n初回参加枠を設けることで、新規参加者が増えて内輪感が解消されるのも狙ってみました。その影響か前回と比較して 2 割ほど参加率が増えてよかったです :)\nパネルディスカッション 登壇者 2 名と僕がモデレーターを行い、パネルディスカッションを行いました。単なる発表保の質疑応答時よりも話が盛り上がってなによりでした~\n次回予告 次回 MLCT 第 8 回は 2019/01/28 に参加予定です!\n現実世界での機械学習の辛みを共有したい・語りたいという方はゼヒご参加ください~\nMachine Learning Casual Talks #8 (2019/01/28 18:30〜)\n","permalink":"https://shunyaueta.com/posts/2018-12-15/","summary":"Machine Learning Casual Talks 第七回を無事開催しました\nMachine Learning Casual Talks #7 (2018/11/20 18:30〜)\nMLCT とは\n実サービスにおける機械学習の経験をカジュアルに語り合おう\nというコミュニティです。\nスポンサー 今回も同じく本イベントは株式会社メルカリにスポンサーとして会場と懇親会を提供を受け開催することができました。スポンサー依頼を快諾いただきありがとうございました!\n遺伝異常により髪が青く変色してしまったタカヤナギ=サン\nABEJA の機械学習導入事例と辛い話を大田黒さんにお話していただきました!\n今回の勉強会資料は以下にまとまっています。\nMachine Learning Casual Talks #7 — 資料一覧 — connpass\n今回の内容を今北産業\n 機械学習エンジニアとしてのキャリアのお話を タカヤナギ=サン 実世界に根付いた IOT と機械学習サービスはかなり辛い 各社の機械学習エンジニアの定義が揺らいでいるので、世界が壊れる  今回の改善点 参加枠の多様性  絶対参加するぞ枠 一般参加枠 初回参加枠 SNS 枠 Blog 枠  と今までは一つの枠で扱っていたものを、5 つの枠に分散して用意してみました。\nなぜかというとドタキャンやノーショーの方の影響で本当に参加したい方や初回参加の方の機会が喪失してしまうのはいただけないので、それを解決したなと思ったのが始まりです。\n初回参加枠を設けることで、新規参加者が増えて内輪感が解消されるのも狙ってみました。その影響か前回と比較して 2 割ほど参加率が増えてよかったです :)\nパネルディスカッション 登壇者 2 名と僕がモデレーターを行い、パネルディスカッションを行いました。単なる発表保の質疑応答時よりも話が盛り上がってなによりでした~\n次回予告 次回 MLCT 第 8 回は 2019/01/28 に参加予定です!","title":"Machine Learning Casual Talks #7 を開催しました"},{"content":"機械学習の信頼性が熱いよねというお話\n柚餅子 さんの発表風景\n2018/09/25 の MLCT #6 を開催しました。\nMLCT とは\n実務における機械学習の話や経験をカジュアルに語り合おう\nというコミュニティです。\nスポンサー 今回も同じく本イベントは株式会社メルカリにスポンサーとして会場と懇親会を提供を受け開催することができました。スポンサー依頼を快諾いただきありがとうございました!\nMachine Learning Casual Talks #6 (2018/09/25 19:00〜)\n発表資料一覧 👇(スライドと配信動画) Machine Learning Casual Talks #6 - 資料一覧 - connpass\n柚餅子さん  リブセンスにおける機械学習システムの信頼性エンジニアリング SRE の考えを機械学習システムに取り入れるというお話ですが、筋が良さそう。特に SLO 周りはうちでも取り入れないなぁと思いました  Naomichi Agata さん  ユーザーフィードバックと機械学習 半教師あり学習で解くというアプローチは非常に筋が良さそうで気になった。技術書典の書籍も気になる 👀  gamella さん  マーケット予測モデルの PCDA の回し方 ms 単位のデータを学習データにして株価の UP/DOWN を予測する。。。。適用するドメインの難易度が鬼ゲーすぎて、ハラハラしそうだけど解きがいがありそう  @yu-ya4 さん  Big Query ML を使ってみた話 さらっと BQML を試して成果が出ましたと言っていたが、良い問題を探し出す嗅覚がすごいなと思いました。実際 BQ だけで過不足なくモデリングが終わるなら理想の世界ですね~  Kosuke Kitahara さん  発表資料は後日公開されます。謎の力により Youtube 配信はされていません  KPT Keep  動画配信を問題なく完了できた 魅力ある発表内容を維持できた  Problem  参加率が低かった。前回は 65%程度の参加率でしたが、今回は雨の影響もありますが 40% と低くなっていた 倍率も毎回 1.5–2 倍程度なのでこれだと補欠枠で参加できなかった人が申し訳ないので、もっと熱意ある人が参加できるような仕組みを考えています  Try  懇親会で議論できる場の形成を狙っていたが、知人同士で固まってしまう傾向が見られたので自然と議論できるようなオープンスペースをやってみる  [](https://twitter.com/komiya_atsushi/status/1044554279527178242) というわけでありがとうございました!\n   回数を重ねるごとに運営もこなれてきましたが、内輪感が出ないように頑張っていきたいなと思います!\n次回の開催は 2018/11/20 を計画しているのでよろしくおねがいします 🙇\n","permalink":"https://shunyaueta.com/posts/2018-10-14/","summary":"機械学習の信頼性が熱いよねというお話\n柚餅子 さんの発表風景\n2018/09/25 の MLCT #6 を開催しました。\nMLCT とは\n実務における機械学習の話や経験をカジュアルに語り合おう\nというコミュニティです。\nスポンサー 今回も同じく本イベントは株式会社メルカリにスポンサーとして会場と懇親会を提供を受け開催することができました。スポンサー依頼を快諾いただきありがとうございました!\nMachine Learning Casual Talks #6 (2018/09/25 19:00〜)\n発表資料一覧 👇(スライドと配信動画) Machine Learning Casual Talks #6 - 資料一覧 - connpass\n柚餅子さん  リブセンスにおける機械学習システムの信頼性エンジニアリング SRE の考えを機械学習システムに取り入れるというお話ですが、筋が良さそう。特に SLO 周りはうちでも取り入れないなぁと思いました  Naomichi Agata さん  ユーザーフィードバックと機械学習 半教師あり学習で解くというアプローチは非常に筋が良さそうで気になった。技術書典の書籍も気になる 👀  gamella さん  マーケット予測モデルの PCDA の回し方 ms 単位のデータを学習データにして株価の UP/DOWN を予測する。。。。適用するドメインの難易度が鬼ゲーすぎて、ハラハラしそうだけど解きがいがありそう  @yu-ya4 さん  Big Query ML を使ってみた話 さらっと BQML を試して成果が出ましたと言っていたが、良い問題を探し出す嗅覚がすごいなと思いました。実際 BQ だけで過不足なくモデリングが終わるなら理想の世界ですね~  Kosuke Kitahara さん  発表資料は後日公開されます。謎の力により Youtube 配信はされていません  KPT Keep  動画配信を問題なく完了できた 魅力ある発表内容を維持できた  Problem  参加率が低かった。前回は 65%程度の参加率でしたが、今回は雨の影響もありますが 40% と低くなっていた 倍率も毎回 1.","title":"Machine Learning Casual Talks #6 を開催しました"},{"content":"2018/07/13 に MLCT #5 を開催してきたお話\nOpening Talk by Aki Ariga\nMachine Learning Casual Talks #5 (2018/07/13 19:30〜)\n本イベントは株式会社メルカリにスポンサーとして会場と懇親会を提供を受け開催することができました。スポンサー依頼を快諾いただきありがとうございました。\n発表資料はこちら 👉 here (YouTube 配信もあります)\nこの記事では、技術的なお話というよりも開催に至るまでの話をメインに書いていきます。\nStart 構想開始時期は 2018/04 頃に考えていて、弊社開催の\nMLOps Nightと呼ばれるイベントの準備を行っているときに、社内だけではなく\n社外の人の機械学習の辛い話をうんうんと頷きながら聞きたいなぁ\nと思ったのが事の始まりです。\nそのあと、とりあえず日程と発表者は事前に集めておかねばと思い @hagino3000 さんにラブコールを送っていた。\n発表依頼の様子\nchezou さんとの出会いと MLCT 復活の狼煙 その後、\n 勉強会の名前どうしよう 🤔 運営の方針どうすべきか 🤔  を迷いつつ時間が過ぎていき業務の一環として機械学習工学キックオフシンポジウム に参加していたら、そういえば Aki Ariga さんって MLCT 開催してたよな、あの勉強会すごく参考になること多かったから復活できないかなと思い始め、気がついたら懇親会で hagino3000 さんに chezou さんを紹介してもらい\n「MLCT 復活させたいです!!! 場所と運営準備は僕主体でやります!」\nと提案したら、あっさりと快諾され運営者に混ぜてもらえることになりました\nメッセージ投げかけから 1 分で承認される\nあらためて、突然飛び込んできた見知らぬ人物の運営への参加を快諾してくださった、 @chezou さん、 @tetsuroito さん、 @komiya_atsushi さんありがとうございました 🙇\nまた社内にはコミュニティを運営する人たちが集まる #z-organizer という夢のような Slack チャンネルがあり、そこの方々からも今回の運営準備や行動理念など沢山の助言をいただきました! ありがとうございました!\nMLCT #5 開催の感想 定員は 100 名で募集したところ、応募者が 200 人強、実参加者は 71 人と盛況にイベントを開催することができました。\n運営初回ということもあり、発表者は事前に僕と @hagino3000 さん、同僚の @hnakagawa さん, @ynqa さんにお願いして数を揃えました。 イベント告知後には @soonraah さん、 @overs_5121 さんに LT 参加応募をいただき定員に達してホッと覚えがあります。\n結果的に、機械学習の技術的な話から運用・先進的なミドルウェアの紹介などバランスが取れたコンテンツを提供できる形になって最高のスタートを切れたんじゃないかと思いました(自画自賛)\nスポンサーである株式会社メルカリからの ドリンクと懇親会の提供の様子\n会場の様子\nまとめ MLCT はこれから 2 ヶ月ごとに定期開催していく予定です。日本の機械学習コミュニティをどんどん盛り上げていけたらなと思っています!\n司会のときにも言いましたが、\n 機械学習エンジニア界隈に知り合いがほぼ存在していないので\n発表したい方絶賛大募集中です!\n では、これからもよろしくお願いします!!\n","permalink":"https://shunyaueta.com/posts/2018-07-15/","summary":"2018/07/13 に MLCT #5 を開催してきたお話\nOpening Talk by Aki Ariga\nMachine Learning Casual Talks #5 (2018/07/13 19:30〜)\n本イベントは株式会社メルカリにスポンサーとして会場と懇親会を提供を受け開催することができました。スポンサー依頼を快諾いただきありがとうございました。\n発表資料はこちら 👉 here (YouTube 配信もあります)\nこの記事では、技術的なお話というよりも開催に至るまでの話をメインに書いていきます。\nStart 構想開始時期は 2018/04 頃に考えていて、弊社開催の\nMLOps Nightと呼ばれるイベントの準備を行っているときに、社内だけではなく\n社外の人の機械学習の辛い話をうんうんと頷きながら聞きたいなぁ\nと思ったのが事の始まりです。\nそのあと、とりあえず日程と発表者は事前に集めておかねばと思い @hagino3000 さんにラブコールを送っていた。\n発表依頼の様子\nchezou さんとの出会いと MLCT 復活の狼煙 その後、\n 勉強会の名前どうしよう 🤔 運営の方針どうすべきか 🤔  を迷いつつ時間が過ぎていき業務の一環として機械学習工学キックオフシンポジウム に参加していたら、そういえば Aki Ariga さんって MLCT 開催してたよな、あの勉強会すごく参考になること多かったから復活できないかなと思い始め、気がついたら懇親会で hagino3000 さんに chezou さんを紹介してもらい\n「MLCT 復活させたいです!!! 場所と運営準備は僕主体でやります!」\nと提案したら、あっさりと快諾され運営者に混ぜてもらえることになりました\nメッセージ投げかけから 1 分で承認される\nあらためて、突然飛び込んできた見知らぬ人物の運営への参加を快諾してくださった、 @chezou さん、 @tetsuroito さん、 @komiya_atsushi さんありがとうございました 🙇","title":"Machine Learning Casual Talks #5 を開催しました"},{"content":"イベント運営者必見 sli.do の使い方\nsli.do のディスプレイ画面\nsli.do という便利な質問投稿・回答サービスがあります。このサービスですが、イベント運営者あるあるの\n オフラインだと活発に質問が出ない Twitter は盛り上がってるが実際の反応はわからない Google Form はいまいち回答率が悪い  などの問題点を解決してくれるサービスです\nSlido - Audience Interaction Made Easy\n基本的な使い方は以下の公式動画がサクッとまとめられていて分かりやすいです\nこれ見れば sli.do の機能は全て俯瞰できる\nUI が良く操作に迷うことはないので、各機能のスクショを貼りつつ紹介していきます\nイベント設定 設定画面\nイベントの名前や、 短縮コード(イベントのハッシュタグにしておくと分かりやすい)を設定して導線をわかりやすくできる\n投票機能 無料版だと 3 つの投票までできる。大きなイベントでなければ十分。もちろん回答結果はシークレットにもできます。\n3 つの投票機能\n複数選択肢\n自由記入式\n星によるレーティング\n各投票機能はアクティブにすると参加者は一つだけ投票可能になる\n参加者からの質問・回答結果のライブ表示 右上のトグルボタンをクリックすると、投票結果をライブ表示できる。勉強会の発表中にサイドディスプレイがあれば常時表示しておくとライブ感が出て良いと思う\nライブ画面への切り替え\n質問一覧\n回答ライブ画面\n上部のスイッチ画面から次の投票に切り替えることができる\n回答解析機能 管理画面から回答のインフォグラフィックを生成することもできる\nといたせりつくせりの機能が提供されています。\nまとめると\n イベント参加者からのオープンな質問投稿(匿名・非匿名) 各質問・回答のライブ表示 運営者からのサーベイ(イベントの感想など)  の 3 点が sli.do では使えます\nTips 唯一惜しい機能としては、イベント管理者が単一ユーザーでしか管理できない点ですが共同アカウント作れば大丈夫そうです。\nHow do I add more admins to my event?\nGoogle Slides でも QA 機能ありませんかとかありますが、sli.do に比べて Open ではないので sli.do を選択しています。あと Google Slides は 1 スライド:1Question でまとめるのが難しい\n","permalink":"https://shunyaueta.com/posts/2018-06-17/","summary":"イベント運営者必見 sli.do の使い方\nsli.do のディスプレイ画面\nsli.do という便利な質問投稿・回答サービスがあります。このサービスですが、イベント運営者あるあるの\n オフラインだと活発に質問が出ない Twitter は盛り上がってるが実際の反応はわからない Google Form はいまいち回答率が悪い  などの問題点を解決してくれるサービスです\nSlido - Audience Interaction Made Easy\n基本的な使い方は以下の公式動画がサクッとまとめられていて分かりやすいです\nこれ見れば sli.do の機能は全て俯瞰できる\nUI が良く操作に迷うことはないので、各機能のスクショを貼りつつ紹介していきます\nイベント設定 設定画面\nイベントの名前や、 短縮コード(イベントのハッシュタグにしておくと分かりやすい)を設定して導線をわかりやすくできる\n投票機能 無料版だと 3 つの投票までできる。大きなイベントでなければ十分。もちろん回答結果はシークレットにもできます。\n3 つの投票機能\n複数選択肢\n自由記入式\n星によるレーティング\n各投票機能はアクティブにすると参加者は一つだけ投票可能になる\n参加者からの質問・回答結果のライブ表示 右上のトグルボタンをクリックすると、投票結果をライブ表示できる。勉強会の発表中にサイドディスプレイがあれば常時表示しておくとライブ感が出て良いと思う\nライブ画面への切り替え\n質問一覧\n回答ライブ画面\n上部のスイッチ画面から次の投票に切り替えることができる\n回答解析機能 管理画面から回答のインフォグラフィックを生成することもできる\nといたせりつくせりの機能が提供されています。\nまとめると\n イベント参加者からのオープンな質問投稿(匿名・非匿名) 各質問・回答のライブ表示 運営者からのサーベイ(イベントの感想など)  の 3 点が sli.do では使えます\nTips 唯一惜しい機能としては、イベント管理者が単一ユーザーでしか管理できない点ですが共同アカウント作れば大丈夫そうです。\nHow do I add more admins to my event?","title":"イベント運営に便利なsli.do の使いこなしかた"},{"content":"データサイエンティストとデータエンジニアの定義とその誤解による悲劇、そしてそれを救う存在である機械学習エンジニア\n紹介記事 Data engineers vs. data scientists\n紹介記事を同僚から教えてもらい、面白かったので抄訳した\n [](https://twitter.com/chezou/status/980349709339394048) \u0026gt; Aki Ariga さんが言及していた記事と方向性が同一で面白かった。\n Data Scientists : ビジネスサイドを理解し、他者にわかりやすく可視化と言語化できる職能。そして高度な数学的知識に基づいたモデリングやアルゴリズム提案スキルも持っている。Data Scientists には高度な Programming skill は必ずしも必須ではない、なぜならモデリングやアルゴリズムを実装するためにプログラミングを習得した人が多いからだ。システムデザインや Programming スキルは、Software Engineer や DataEngineer からみると見れたものではない(そしてそうでなくてはならない、なぜならスペシャリストだから)\nData Engineer : 分散プログラミングを意識して構築できる職能。DE は卓越したプログラミングスキルとシステム構成力を持つ。定義 : つまりビッグデータに対してシステム的に解決できるスキル。クラスタ設計までが Data Engineer の役割であり運用(Ops)はやらない\nfrom : https://www.oreilly.com/ideas/data-engineers-vs-data-scientists\n Data Scientists と Data Engineer の互いの特化したスキルは補完しあってこそ輝く。 Data Scientist がデータパイプラインを作ると悲劇が起きてしまう。多くの企業が Data Scientist を Data Engineer として雇っているが、それは Data Scientists のスペックを活かしきれず、20–30%の効率で働かせてしまっている。そしてその ROI はめちゃくちゃ悪い。Data Scientists は適切なツールと選択肢を熟知していない(そして Data Engineer はシステムデザインと熟知しているのでミスは侵さない) e.g. 実際著者が聞いたこんな話がある。 Data Scientists が Apache Spark を使って 10GB のデータ処理を行うのに 1 回 15m の時間がかかっていた。(だが RDBMS を使えば、10ms で終わる) Data Scientist は彼らの流儀を疑うこと無く 1 日に 16 回 Spark の処理を実行しており、15mx16=240m つまり 4h の時間を無駄にしてる。RDBMS を使えば、160ms で終わるというのに… Data Scientist が頑張ってシステムを構築するが、職能の限界で Data Engineer しか作れないシステムなので時間とお金の浪費になった 実情 : Data Scientist として雇われたのに、Data Engineer として働かざるを得ない人がほとんどだ 理想的な人材配置 Case : 初期の組織: 2–3 人の Data Engineer : DataScientist Group Case : 更に複雑な事に取り組みたい 4–5 人の Data Engineer : 1 Data Scientist Data Engineer change to Data Scientist の王道 → それが新しい職種 : Machine Learning Engineer!!  from : https://www.oreilly.com/ideas/data-engineers-vs-data-scientists\n Machine Learning Engineer は両方の職種の経験がある。Machine Learning Engineer は Data Scientist が規律を守っていないコードのラストワンマイルを守り、データパイプラインを作る職種である。 (ここで バランスを取るように Data Engineer のディスりが突然入る) Data Engineer は白黒、0–1 の世界が好きなので推測の世界(DS)が好きではない。そのため、Machine Learning Engineer は Data Scientists ,Data Engineer のふたつにまたがる存在である 時代の流れにより、最適化や機械学習のパッケージが揃いつつあり、既存の有名なアルゴリズムは簡単に使える。また Google Auto ML, Data Robot のように、Data Scientist の領域も代替可能な tools が普及しつつある 結論: Data Scientist, Data Engineer の役割が判明した今、組織の構造変革が必要 Data Engineer を雇って Data Scientist の代わりにデータパイプラインのシステム構築させれば皆幸せになる。  ","permalink":"https://shunyaueta.com/posts/2018-04-24/","summary":"データサイエンティストとデータエンジニアの定義とその誤解による悲劇、そしてそれを救う存在である機械学習エンジニア\n紹介記事 Data engineers vs. data scientists\n紹介記事を同僚から教えてもらい、面白かったので抄訳した\n [](https://twitter.com/chezou/status/980349709339394048) \u0026gt; Aki Ariga さんが言及していた記事と方向性が同一で面白かった。\n Data Scientists : ビジネスサイドを理解し、他者にわかりやすく可視化と言語化できる職能。そして高度な数学的知識に基づいたモデリングやアルゴリズム提案スキルも持っている。Data Scientists には高度な Programming skill は必ずしも必須ではない、なぜならモデリングやアルゴリズムを実装するためにプログラミングを習得した人が多いからだ。システムデザインや Programming スキルは、Software Engineer や DataEngineer からみると見れたものではない(そしてそうでなくてはならない、なぜならスペシャリストだから)\nData Engineer : 分散プログラミングを意識して構築できる職能。DE は卓越したプログラミングスキルとシステム構成力を持つ。定義 : つまりビッグデータに対してシステム的に解決できるスキル。クラスタ設計までが Data Engineer の役割であり運用(Ops)はやらない\nfrom : https://www.oreilly.com/ideas/data-engineers-vs-data-scientists\n Data Scientists と Data Engineer の互いの特化したスキルは補完しあってこそ輝く。 Data Scientist がデータパイプラインを作ると悲劇が起きてしまう。多くの企業が Data Scientist を Data Engineer として雇っているが、それは Data Scientists のスペックを活かしきれず、20–30%の効率で働かせてしまっている。そしてその ROI はめちゃくちゃ悪い。Data Scientists は適切なツールと選択肢を熟知していない(そして Data Engineer はシステムデザインと熟知しているのでミスは侵さない) e.g. 実際著者が聞いたこんな話がある。 Data Scientists が Apache Spark を使って 10GB のデータ処理を行うのに 1 回 15m の時間がかかっていた。(だが RDBMS を使えば、10ms で終わる) Data Scientist は彼らの流儀を疑うこと無く 1 日に 16 回 Spark の処理を実行しており、15mx16=240m つまり 4h の時間を無駄にしてる。RDBMS を使えば、160ms で終わるというのに… Data Scientist が頑張ってシステムを構築するが、職能の限界で Data Engineer しか作れないシステムなので時間とお金の浪費になった 実情 : Data Scientist として雇われたのに、Data Engineer として働かざるを得ない人がほとんどだ 理想的な人材配置 Case : 初期の組織: 2–3 人の Data Engineer : DataScientist Group Case : 更に複雑な事に取り組みたい 4–5 人の Data Engineer : 1 Data Scientist Data Engineer change to Data Scientist の王道 → それが新しい職種 : Machine Learning Engineer!","title":"[抄訳] Data engineers vs. data scientists"},{"content":"Colabratory 上で 日本語に対する NLP をしたいときありませんか？\n1 2 3 4 5  # install MeCab neologd !apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab \u0026gt; /dev/null !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git \u0026gt; /dev/null !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 !pip install mecab-python3 \u0026gt; /dev/null   インストールに成功しました。\nGoogle Colabratory も以下で公開しているので参考にしてみてください\ncolab-mecab-ipadic-NEologd.ipynb\n","permalink":"https://shunyaueta.com/posts/2018-04-23/","summary":"Colabratory 上で 日本語に対する NLP をしたいときありませんか？\n1 2 3 4 5  # install MeCab neologd !apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab \u0026gt; /dev/null !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git \u0026gt; /dev/null !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 !pip install mecab-python3 \u0026gt; /dev/null   インストールに成功しました。\nGoogle Colabratory も以下で公開しているので参考にしてみてください\ncolab-mecab-ipadic-NEologd.ipynb","title":"Google Colaboratory で Mecab-ipadic-Neologd を使用可能にする"},{"content":"eBay が ARCore を使った商品の梱包測定機能を提供しているので試した\n梱包測定の仕組みとしては、ARCore(今回は Pixel2 XL で試した)で平面検出を行って、そこに eBay のダンボールオブジェクトを設置することで、ダンボールに入るかどうかを判定できる。\n下のダンボールアイコンを選択して、検出された平面をタップするとダンボールオブジェクトを設置できる\n今回例として用いた MBP の空箱だと ARCore が空箱自体を平面と認識してしまうという罠があるので、床で平面検知を終えてから商品を置くという裏技が必要\nまとめ  ARCore を用いた AR 機能をすぐ試せる組織体制なのは凄い。ハッカソンで作ったのかな? 実用性は 🙅、平面検出しかしてないので、荷物に合わせて最適なダンボールを選ぶのは結局ユーザー。そこまで自動化してこそ革新的な機能になるのではなかろうか  Reference eBay uses augmented reality to help sellers find the right box for their product\n","permalink":"https://shunyaueta.com/posts/2018-04-16/","summary":"eBay が ARCore を使った商品の梱包測定機能を提供しているので試した\n梱包測定の仕組みとしては、ARCore(今回は Pixel2 XL で試した)で平面検出を行って、そこに eBay のダンボールオブジェクトを設置することで、ダンボールに入るかどうかを判定できる。\n下のダンボールアイコンを選択して、検出された平面をタップするとダンボールオブジェクトを設置できる\n今回例として用いた MBP の空箱だと ARCore が空箱自体を平面と認識してしまうという罠があるので、床で平面検知を終えてから商品を置くという裏技が必要\nまとめ  ARCore を用いた AR 機能をすぐ試せる組織体制なのは凄い。ハッカソンで作ったのかな? 実用性は 🙅、平面検出しかしてないので、荷物に合わせて最適なダンボールを選ぶのは結局ユーザー。そこまで自動化してこそ革新的な機能になるのではなかろうか  Reference eBay uses augmented reality to help sellers find the right box for their product","title":"eBayのAR測定機能を試してみた"},{"content":"Google, Facebook の機械学習基盤情報をまとめました\nPodcast でも紹介しました\n#2 Facebook と Google の機械学習基盤について at just4fun.fm\n社内の勉強会で Google, Facebook が提供する機械学習基盤に関する論文を紹介したので、その資料を公開します\n機械学習をサービスとして提供開始すると、継続的な学習やプロダクション環境での機械学習の提供はモデル構築以外にもいろいろと考える問題が多くなります ¹\n[1] Hidden technical debt in Machine learning systems NIPS’15\n要するに機械学習をサービスとして届けるには、実はめちゃんこ大変なんだよという話なんですが、みんな同じ問題にぶち当たります。\nそのためプロダクションレディなレベルで機械学習を提供できるプラットフォームを各社が提案しておりその中でも Google, Facebook の事例を提供します。\nTL; DR;  FBLearner: MLaaS の事例として最初に読むべき論文、MLaaS をどのような戦略で提供しているかを抽象的にまなべるため、鳥瞰図として読みましょう TFX は逆に機械学習基盤が必要とする技術スタックや要件などを詳細に説明しており、教科書的な立ち位置です。社内で機械学習基盤を内製したい場合に詳しく読み込むこと必須 両社とも MLaaS をスケーラブルに提供する環境ができており、サービスのコアテクノロジーになっている  Google : Tensorflow Expand TFX: A tensor flow-based production-scale machine learning platform\n Facebook : FBLeaner Applied machine learning at facebook a datacenter infrastructure perspective HPCA18\n ONNX を見ていると TensorFlow VS. ONNX 陣営が出来つつありどちらのプラットフォームに乗るかの戦略が大事だなと思います。\nGoogle はモバイル上の ML から超大規模分散学習まで TF シリーズで提供、実際のサービング環境も TFServing というスタックを提供しはじめていて個人的に TensorFlow が何歩も先にいっているという所感\n(逆に TensorFlow 一強状態になると、競争状態がなくなるので是非 ONNX 陣営には頑張ってほしい)\n","permalink":"https://shunyaueta.com/posts/2018-04-09/","summary":"Google, Facebook の機械学習基盤情報をまとめました\nPodcast でも紹介しました\n#2 Facebook と Google の機械学習基盤について at just4fun.fm\n社内の勉強会で Google, Facebook が提供する機械学習基盤に関する論文を紹介したので、その資料を公開します\n機械学習をサービスとして提供開始すると、継続的な学習やプロダクション環境での機械学習の提供はモデル構築以外にもいろいろと考える問題が多くなります ¹\n[1] Hidden technical debt in Machine learning systems NIPS’15\n要するに機械学習をサービスとして届けるには、実はめちゃんこ大変なんだよという話なんですが、みんな同じ問題にぶち当たります。\nそのためプロダクションレディなレベルで機械学習を提供できるプラットフォームを各社が提案しておりその中でも Google, Facebook の事例を提供します。\nTL; DR;  FBLearner: MLaaS の事例として最初に読むべき論文、MLaaS をどのような戦略で提供しているかを抽象的にまなべるため、鳥瞰図として読みましょう TFX は逆に機械学習基盤が必要とする技術スタックや要件などを詳細に説明しており、教科書的な立ち位置です。社内で機械学習基盤を内製したい場合に詳しく読み込むこと必須 両社とも MLaaS をスケーラブルに提供する環境ができており、サービスのコアテクノロジーになっている  Google : Tensorflow Expand TFX: A tensor flow-based production-scale machine learning platform\n Facebook : FBLeaner Applied machine learning at facebook a datacenter infrastructure perspective HPCA18","title":"Google, Facebookが提供する機械学習基盤まとめ"},{"content":"メルカリが主催する機械学習のミートアップに参加してきたので備忘録がてらメモ書きです。書きなぐったメモなので意訳として捉えて下さい\n発表者の顔は隠しています。なにか問題があればお伝え下さい。\nTeam AI Meetup #1 (2018/02/13 19:00〜)\nTwitter: #mercari_ai\n山口さん  Image Net と mercari の持ってるデータセットは似てるのでいけるのでは! ルカリはクラス数 over 1100 始期の推定値 Top5 29.3% エラー例 :画像からサイズを推定しないと男女のシューズを区別不可(人間でも不可能なエラー例が多い) 意外と認識がうまくいく事例がかなりある(クラス設計の影響で推定が上手く出来ていないらしい) データセットはユーザーが作成してるので最高、画像が正方形なのも Good 学習は GPU,推論は CPU 環境下で行っている 画像検索自体はプロトタイプは出来ているが、実運用は計算量やリアルタイム性を担保するのが難しいので一旦保留中  大筋は以下の記事と資料で把握できます。k8s で運用しているなどの実運用の構成が今回の発表では差分として語られていました。\n画像での商品検索に向けて - Mercari Engineering Blog\n【Mercari Summer Internship】商品画像の色推定を行いました! - Mercari Engineering Blog\n工藤さん  運用を継続すると色んな問題が出てきたので、それを解決する基盤環境を作成しはじめた e.g. 学習データのバージョン管理、モデルのデプロイ, etc 最終的には OSS としての公開も考えているとのこと  メルカリの今年 1 年間の機械学習の取り組みとこれから - Mercari Engineering Blog\nML Ops Study (仮) #1の発表内容も今回の機械学習の運用周りの Tips が共有されているので興味のある方はオススメです\n機械学習基盤といえば、最近Polyaxonという OSS が公開されてました。\nPolyaxon\nQ.A.セッション  Q. 論文などは参考にされますか？ A. ある程度枯れた手法をメインに実装している TFX(KDD2017)など Google が出している機械学習の基盤関係の論文は非常に参考している   Q. 機械学習のプロジェクトをどう進めているか? A. ある程度の機械学習についての啓蒙が必要 必ずデモを見せて期待値のすり合わせを行っている 何もしらない人は機械学習に対して過度な期待ではなく間違った期待をしてしまっているのでそこのすり合わせは重要 e.g. データは無いが機械学習でなんとかして! など雑感 : 質疑応答でも機械学習を実運用したい場合にどう運用しているかにフォーカスした質問がメインでためになる質問が多かったので良かった。メルカリさんが今回のような知見を共有してくれるのは本当に尊敬できます。  ","permalink":"https://shunyaueta.com/posts/2018-02-13/","summary":"メルカリが主催する機械学習のミートアップに参加してきたので備忘録がてらメモ書きです。書きなぐったメモなので意訳として捉えて下さい\n発表者の顔は隠しています。なにか問題があればお伝え下さい。\nTeam AI Meetup #1 (2018/02/13 19:00〜)\nTwitter: #mercari_ai\n山口さん  Image Net と mercari の持ってるデータセットは似てるのでいけるのでは! ルカリはクラス数 over 1100 始期の推定値 Top5 29.3% エラー例 :画像からサイズを推定しないと男女のシューズを区別不可(人間でも不可能なエラー例が多い) 意外と認識がうまくいく事例がかなりある(クラス設計の影響で推定が上手く出来ていないらしい) データセットはユーザーが作成してるので最高、画像が正方形なのも Good 学習は GPU,推論は CPU 環境下で行っている 画像検索自体はプロトタイプは出来ているが、実運用は計算量やリアルタイム性を担保するのが難しいので一旦保留中  大筋は以下の記事と資料で把握できます。k8s で運用しているなどの実運用の構成が今回の発表では差分として語られていました。\n画像での商品検索に向けて - Mercari Engineering Blog\n【Mercari Summer Internship】商品画像の色推定を行いました! - Mercari Engineering Blog\n工藤さん  運用を継続すると色んな問題が出てきたので、それを解決する基盤環境を作成しはじめた e.g. 学習データのバージョン管理、モデルのデプロイ, etc 最終的には OSS としての公開も考えているとのこと  メルカリの今年 1 年間の機械学習の取り組みとこれから - Mercari Engineering Blog\nML Ops Study (仮) #1の発表内容も今回の機械学習の運用周りの Tips が共有されているので興味のある方はオススメです","title":"メルカリのTeam AI Meetup #1 に参加してきた #mercari_ai"},{"content":"2018 年 1 月の振り返りをざっくりと\nKeep  Mercari Competition にて初 Kaggle コンペ参加。 Best 116 位まで言ったんですが、NLP 周りの知見が無いのと、Kaggle 周りの力不足で現在は 317 位… ブロンズまで引き戻せるか…  [](https://twitter.com/hurutoriya/status/952573577357770753) ぬか喜びしている図\n    Coursera で Ng 先生のML コースを始めた。あと 5 日位で無事終わりそう。とても分かりやすい(何故もっと早く受けなかったのか…) 終わったら、How to Win a Data Science Competition: Learn from Top Kagglersか Ng 先生の deeplearning.ai を始める予定 本を 2 冊読めた。案外読む始めるのがハードル高いだけで、読み始めれば 1 日 30 分*5 日で読み終わることがわかったので今度から通勤途中に読むことにした。これで月 4 冊読んでいくようにしたい。2–3 月はメタ学習関連の本を読み進めていく。  学力の経済学\n [](https://twitter.com/hurutoriya/status/960525598610268160)\n お金持ちになれる黄金の羽根の拾い方\n [](https://twitter.com/hurutoriya/status/952046931307458560)\n  Duolingo で英語学習を再開した。学習アプリとしてとても出来が良い。(名詞の複数形や冠詞などの細かい間違いを都度指摘されるのが好き。モバイルアプリ(iOS)は問題が重複して出題されるので PC メインで利用)  100%達成するぞ! Linkedin に成績が反映されなくなったのは残念。おそらく言語学習は継続性がないと意味がないのでそれを考慮してシステム変更したんだろうな\n 住信 SBI ネット銀行を開設した。ネット銀行最高。 Google Family **を始めた。**予定管理はTimeTreeを使ってデザインや使い勝手は不安が無かったんですが、Google Calendar に比べてアプリのインテグレーションや持続性(日本のベンチャー企業なのでいつまでそのサービスが持つのか)が不安になったので Google Family に乗り換えました。  Problem  Kaggle のゴールドの壁を痛感。もっと精進せねば 論文読みが止まってしまってた。機械学習基盤とプロダクション環境における機械学習関連の論文をテーマにしてまとめて書き上げる  Try  02 月から仕事が始まるので(初社会人、入社完了したらまた振り返り Blog を書く)、社会人としてのメタスキルを実践できるようにしておく(e.g. 入社一年目の教科書 ) 日本語ニュース(TechCrunch Japan)を読むのを廃止、英語ニュースに切り替え 英会話学習を始める、リスニング力も強化する(Podcast) 対外発表の実績を積んでいく  ","permalink":"https://shunyaueta.com/posts/2018-02-09/","summary":"2018 年 1 月の振り返りをざっくりと\nKeep  Mercari Competition にて初 Kaggle コンペ参加。 Best 116 位まで言ったんですが、NLP 周りの知見が無いのと、Kaggle 周りの力不足で現在は 317 位… ブロンズまで引き戻せるか…  [](https://twitter.com/hurutoriya/status/952573577357770753) ぬか喜びしている図\n    Coursera で Ng 先生のML コースを始めた。あと 5 日位で無事終わりそう。とても分かりやすい(何故もっと早く受けなかったのか…) 終わったら、How to Win a Data Science Competition: Learn from Top Kagglersか Ng 先生の deeplearning.ai を始める予定 本を 2 冊読めた。案外読む始めるのがハードル高いだけで、読み始めれば 1 日 30 分*5 日で読み終わることがわかったので今度から通勤途中に読むことにした。これで月 4 冊読んでいくようにしたい。2–3 月はメタ学習関連の本を読み進めていく。  学力の経済学\n [](https://twitter.com/hurutoriya/status/960525598610268160)\n お金持ちになれる黄金の羽根の拾い方\n [](https://twitter.com/hurutoriya/status/952046931307458560)\n  Duolingo で英語学習を再開した。学習アプリとしてとても出来が良い。(名詞の複数形や冠詞などの細かい間違いを都度指摘されるのが好き。モバイルアプリ(iOS)は問題が重複して出題されるので PC メインで利用)  100%達成するぞ!","title":"2018.01 KPT"},{"content":"2018 年の抱負です。\nWorks 機械学習エンジニアとして六本木で働く事になりました。機械学習エンジニアとしてスペシャリストになれるように周辺スキルを伸ばしていきます。\n Kaggle Master に到達(1 gold medal, 2 silver medals)  初参加のコンペもずるずる下降してしまいブロンズ圏外に…\n Coursera, Udacity (MOOC)で CV, Robotics 周辺を学習していく Github で 50 star over の OSS を作る(CV or ML) 勉強会に登壇 or 主催 する  メタ学習(学習の効率化)について小話\nうまくなる技術¹などのメタ学習関連の本を色々と読み漁って学習のコツをいろいろ勉強してるんですが、外的指標(Github Star, Kaggle Rank, Toggl tracking,Coursera Certificate)を目的にすると人間気持ちよく継続できるらしいのでそれに沿った行動をするよう心がけています。後 5–6 冊メタ学習の本を読み終えてまとまったらまた記事にする。\nサンシャイン丸の内さん¹や ふろむださん¹の記事も参考になるのでオススメです\n個人的に最近心に響いたのはのじゃろりおじさんの勉強の姿勢です。\n３ D や unity の勉強方法について。：ねこみみメモ\n おそらく、やり続けて成功した人は「やり続ければ報われる！」と言うと思うのですが、年齢や経済状況や自分の才能を疑ったり等……現実は難しいと思います。\n私は偶然このタイミングで「オリジナルモデルの Vyoutuber を出せた」から幸運に恵まれただけで、純粋な技術ではおそらく就職は無理だったのではないかと思います。2018~19 年で見切りつけて諦めなきゃなと思ってたぐらいです。\nなので「やり続ければ報われる」とはとても言えません。ただ**「やり続けて報われなくてもいいと思える事は、やり続けた方がいい」**とは思います。\n他者の評価がどうであれ、やてって楽しくて満足できるのであればある意味常に成功している状態で、やればやるほど成功続きなわけです。\n English  DMM 英会話 TOEFLE で 90 点、英語で働けるレベルを目指す 英語 Blog 記事を定期的に書く  Health  カラダステーションで内臓脂肪が少し高めになってたので走る(筋トレしかやってなかったツケが…) 筋肉量も左右でバラツキ(利き手側じゃないほうが 100g 程度筋肉量が少ない)があるので整える  筋トレはケトルベル 16kg で色々とやってるので継続する。\n筋トレとランニング楽しむために Garmin Vivoactive3¹ を注文しました。\n使えない筋肉だと意味が無いのでプリズナートレーニング²の本を読み込んで、筋肉全体の神経郡を発達させるような自重トレーニングをはじめました。\nプロテインは人工甘味料が入っていない以下の 2 種類で固定されました。\n過度な甘みは無く水単体でも飲めるくらいには自然な甘さなのでオススメです。\n Optimum Nutrition, ゴールドスタンダード、100%ホエイ、ナチュラル、チョコレート、4.8 ポンド (2.18 kg) Dymatize Nutrition, ISO 100 加水分解化、100%ホエイプロテインアイソレート、グルメチョコレート、5 Lbs (2.3 kg)  Other  本を毎月 3 冊読む(毎月 Blog で紹介)  ","permalink":"https://shunyaueta.com/posts/2018-01-29/","summary":"2018 年の抱負です。\nWorks 機械学習エンジニアとして六本木で働く事になりました。機械学習エンジニアとしてスペシャリストになれるように周辺スキルを伸ばしていきます。\n Kaggle Master に到達(1 gold medal, 2 silver medals)  初参加のコンペもずるずる下降してしまいブロンズ圏外に…\n Coursera, Udacity (MOOC)で CV, Robotics 周辺を学習していく Github で 50 star over の OSS を作る(CV or ML) 勉強会に登壇 or 主催 する  メタ学習(学習の効率化)について小話\nうまくなる技術¹などのメタ学習関連の本を色々と読み漁って学習のコツをいろいろ勉強してるんですが、外的指標(Github Star, Kaggle Rank, Toggl tracking,Coursera Certificate)を目的にすると人間気持ちよく継続できるらしいのでそれに沿った行動をするよう心がけています。後 5–6 冊メタ学習の本を読み終えてまとまったらまた記事にする。\nサンシャイン丸の内さん¹や ふろむださん¹の記事も参考になるのでオススメです\n個人的に最近心に響いたのはのじゃろりおじさんの勉強の姿勢です。\n３ D や unity の勉強方法について。：ねこみみメモ\n おそらく、やり続けて成功した人は「やり続ければ報われる！」と言うと思うのですが、年齢や経済状況や自分の才能を疑ったり等……現実は難しいと思います。\n私は偶然このタイミングで「オリジナルモデルの Vyoutuber を出せた」から幸運に恵まれただけで、純粋な技術ではおそらく就職は無理だったのではないかと思います。2018~19 年で見切りつけて諦めなきゃなと思ってたぐらいです。\nなので「やり続ければ報われる」とはとても言えません。ただ**「やり続けて報われなくてもいいと思える事は、やり続けた方がいい」**とは思います。\n他者の評価がどうであれ、やてって楽しくて満足できるのであればある意味常に成功している状態で、やればやるほど成功続きなわけです。\n English  DMM 英会話 TOEFLE で 90 点、英語で働けるレベルを目指す 英語 Blog 記事を定期的に書く  Health  カラダステーションで内臓脂肪が少し高めになってたので走る(筋トレしかやってなかったツケが…) 筋肉量も左右でバラツキ(利き手側じゃないほうが 100g 程度筋肉量が少ない)があるので整える  筋トレはケトルベル 16kg で色々とやってるので継続する。","title":"2018年の抱負"},{"content":"Kevin J. Shih, Saurabh Singh, Derek Hoiem, “Where To Look: Focus Regions for Visual Question Answering”, in CVPR2016 link\nSummry\nを読んだので、軽くメモ。\nVQA(Visual Question Answer) 画像に対する質問に対して応答するタスクに対し、その質問クエリに対して画像のどの領域に注目すべきかのモデルの学習方法について論じた論文。\nContribution  VQA datasetに対して、提案手法を適用。従来手法を全て上回った。 画像に対して CNN を用いて物体領域の検出を行った後にベクトル化、質問クエリはword2vecを用いてベクトル化を行う。 その 2 つのベクトルを用いて内積計算により重み付けを行うことで、どの領域に注目すべきかを計算する。  Comments 引用文献の訳 9 割が 2014–2015(直近 2 年間)で発表された論文で、改めてこの分野の最先端を駆け抜けるのは凄まじい能力が必要になるなと思いました。\nそして相変わらず CVPR の論文のネーミングセンスは良いですね。(ジャケ買いならぬジャケ読み)\n単純な質問なら、人間でも瞬間的に解答可能な物が多いなと感じた。\nfig. 1\nセマンティックな疑問(Fig.1 雨は降っていますか?)の場合、人間に注目した場合は傘をさしているから雨と判断しても良いがもっと広い範囲で画像を見てみると空は快晴なので人間に注目するのは筋が悪く VQA はとても難しくチャレンジングな問題だと書かれていた。(それでも充分すごい領域に到達しているなと思うが)\n","permalink":"https://shunyaueta.com/posts/2018-01-18/","summary":"Kevin J. Shih, Saurabh Singh, Derek Hoiem, “Where To Look: Focus Regions for Visual Question Answering”, in CVPR2016 link\nSummry\nを読んだので、軽くメモ。\nVQA(Visual Question Answer) 画像に対する質問に対して応答するタスクに対し、その質問クエリに対して画像のどの領域に注目すべきかのモデルの学習方法について論じた論文。\nContribution  VQA datasetに対して、提案手法を適用。従来手法を全て上回った。 画像に対して CNN を用いて物体領域の検出を行った後にベクトル化、質問クエリはword2vecを用いてベクトル化を行う。 その 2 つのベクトルを用いて内積計算により重み付けを行うことで、どの領域に注目すべきかを計算する。  Comments 引用文献の訳 9 割が 2014–2015(直近 2 年間)で発表された論文で、改めてこの分野の最先端を駆け抜けるのは凄まじい能力が必要になるなと思いました。\nそして相変わらず CVPR の論文のネーミングセンスは良いですね。(ジャケ買いならぬジャケ読み)\n単純な質問なら、人間でも瞬間的に解答可能な物が多いなと感じた。\nfig. 1\nセマンティックな疑問(Fig.1 雨は降っていますか?)の場合、人間に注目した場合は傘をさしているから雨と判断しても良いがもっと広い範囲で画像を見てみると空は快晴なので人間に注目するのは筋が悪く VQA はとても難しくチャレンジングな問題だと書かれていた。(それでも充分すごい領域に到達しているなと思うが)","title":"Where To Look: Focus Regions for Visual Question Answering (CVPR2016)を読んだ"},{"content":"Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ\nMikel Rodriguez, Josef Sivic, Ivan Laptev, Jean-Yves Audibert, “Data-driven Crowd Analysis in Videos”, in ICCV2011.\nProject Page\nを読んだので、メモです。\nSummary\ntl;dr  高密度な群集内の個人を追跡を転移学習によって精度を向上させる手法  Contribution  追跡の精度を転移学習によって向上させた 転移学習を行うためのデータセットとそのフレームワークを考案  論文内では、転移学習の例としてマラソンAの群集を対象に追跡する際に、以下の流れで転移学習を行う。\n 大域的な群衆状況のマッチング : 同じようなシーンを探索(この場合 DB 内にあるマラソン動画) 局所的な群衆状況のマッチング : 1でマッチした動画においてオプティカルフローが類似するパッチを探索して転移学習  また、Rare Events(デモの最中に群集を横断するカメラマンなど、群衆の流れに対して同調しない動きを行う人物)に対しても実験を行い評価。\nComments 転移学習は自分のイメージだと、自然言語処理のイメージ(一般的な文書を学習したモデルを法律文書に対して適用するなど)しかなかったので新鮮な気持ちで読めた。\n動画なら転移学習を行ったとしても、直感的に良い特徴を学べそうなので、良い仮説を立てている論文でした。\n最後に示されてる個人追跡における平均誤検出の単位がpixelだが、Ground-Truth と提案手法の追跡軌跡の重複度具合を見てると誤検出が更に高そうに見えるけどどうなんでしょうか？\n(テストデータのみ学習が 58.82、転移学習を行った提案手法だと 46.88になっていてもっと相対的な差が出てくるはず?)\n","permalink":"https://shunyaueta.com/posts/2018-01-17/","summary":"Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ\nMikel Rodriguez, Josef Sivic, Ivan Laptev, Jean-Yves Audibert, “Data-driven Crowd Analysis in Videos”, in ICCV2011.\nProject Page\nを読んだので、メモです。\nSummary\ntl;dr  高密度な群集内の個人を追跡を転移学習によって精度を向上させる手法  Contribution  追跡の精度を転移学習によって向上させた 転移学習を行うためのデータセットとそのフレームワークを考案  論文内では、転移学習の例としてマラソンAの群集を対象に追跡する際に、以下の流れで転移学習を行う。\n 大域的な群衆状況のマッチング : 同じようなシーンを探索(この場合 DB 内にあるマラソン動画) 局所的な群衆状況のマッチング : 1でマッチした動画においてオプティカルフローが類似するパッチを探索して転移学習  また、Rare Events(デモの最中に群集を横断するカメラマンなど、群衆の流れに対して同調しない動きを行う人物)に対しても実験を行い評価。\nComments 転移学習は自分のイメージだと、自然言語処理のイメージ(一般的な文書を学習したモデルを法律文書に対して適用するなど)しかなかったので新鮮な気持ちで読めた。\n動画なら転移学習を行ったとしても、直感的に良い特徴を学べそうなので、良い仮説を立てている論文でした。\n最後に示されてる個人追跡における平均誤検出の単位がpixelだが、Ground-Truth と提案手法の追跡軌跡の重複度具合を見てると誤検出が更に高そうに見えるけどどうなんでしょうか？\n(テストデータのみ学習が 58.82、転移学習を行った提案手法だと 46.88になっていてもっと相対的な差が出てくるはず?)","title":"Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ"},{"content":"群衆解析の手法に興味があるので、サーベイの結果を放流しておきます。\nJing Shao, Chen Change Loy, Kai Kang, and Xiaogang Wang, “Slicing Convolutional Neural Network for Crowd Video Understanding”, in CVPR, 2016.\nProject Page\nSummary\n一言説明 時系列・空間的特徴から CNN で特徴を学習、群衆の動画に対してstate-of-the-artを達成\n3 個の CNN を用いて下記の３つの特徴を表現学習\n xy- : 空間的特徴 xt- : x 軸の時系列特徴 yt- : y 軸の時系列特徴  Comments Dataset としてWWW Crowd Dataset\nが公開されている。10,000 本の群衆の動画を収集公開しているとのこと。\nDemo Movie\n 紹介動画を見てみたら分かるが、群衆の動画というよりも数が増大した結果一般的な画像認識のデモ動画になっている Jing Shaoさんは CVPR2014 から群衆解析のための descriptor を提案したりしてたんだけど、2016 年から Deep な手法での群衆解析の研究をやっているのは手が早いなと 所属グループは ISLVRC2015 の物体認識タスクで優勝した香港大学のグループ Multimedia Laboratory The Chinese University of Hong Kong データセット、実装コードを必ず公開しているのは尊敬、またそれくらいやらないとトップには通過しないんだろうな CNN のアーキテクチャ毎の比較実験と考察をかなり入念に行っていた。数年後には各データのフォーマットに合わせたベストな DNN のアーキテクチャが決まってくるんじゃないだろうか  ","permalink":"https://shunyaueta.com/posts/2018-01-16/","summary":"群衆解析の手法に興味があるので、サーベイの結果を放流しておきます。\nJing Shao, Chen Change Loy, Kai Kang, and Xiaogang Wang, “Slicing Convolutional Neural Network for Crowd Video Understanding”, in CVPR, 2016.\nProject Page\nSummary\n一言説明 時系列・空間的特徴から CNN で特徴を学習、群衆の動画に対してstate-of-the-artを達成\n3 個の CNN を用いて下記の３つの特徴を表現学習\n xy- : 空間的特徴 xt- : x 軸の時系列特徴 yt- : y 軸の時系列特徴  Comments Dataset としてWWW Crowd Dataset\nが公開されている。10,000 本の群衆の動画を収集公開しているとのこと。\nDemo Movie\n 紹介動画を見てみたら分かるが、群衆の動画というよりも数が増大した結果一般的な画像認識のデモ動画になっている Jing Shaoさんは CVPR2014 から群衆解析のための descriptor を提案したりしてたんだけど、2016 年から Deep な手法での群衆解析の研究をやっているのは手が早いなと 所属グループは ISLVRC2015 の物体認識タスクで優勝した香港大学のグループ Multimedia Laboratory The Chinese University of Hong Kong データセット、実装コードを必ず公開しているのは尊敬、またそれくらいやらないとトップには通過しないんだろうな CNN のアーキテクチャ毎の比較実験と考察をかなり入念に行っていた。数年後には各データのフォーマットに合わせたベストな DNN のアーキテクチャが決まってくるんじゃないだろうか  ","title":"Slicing Convolutional Neural Network for Crowd Video Understanding (CVPR2016)を読んだ"},{"content":"Jupyter notebook をご利用の皆さん、朗報です。\n例えば、下記の２つの notebook の差分を比較したい際に、\n nb_1.ipynb nb_2.ipynb  diffコマンドを用いると下記のような結果になってしまいます。\n1 2  diff nb_1.ipynb nb_2.ipynb [master] 14c14 \u0026amp;lt; “image/png”: “iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\\nAAAAAElFTkSuQmCC\\n”, — - \u0026amp;gt; “image/png”: “iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\\nJIb+P2c9rHcimwD6AAAAAElFTkSuQmCC\\n”, 16c16 \u0026amp;lt; “\u0026amp;lt;matplotlib.figure.Figure at 0x10ba48cc0\u0026amp;gt;” — - \u0026amp;gt; “\u0026amp;lt;matplotlib.figure.Figure at 0x11037dcc0\u0026amp;gt;” 34c34 \u0026amp;lt; “ax.fill(x, y, zorder=10,facecolor=’green’)\\n”, — - \u0026amp;gt; “ax.fill(x, y, zorder=10,facecolor=’black’)\\n”,`   diffは単純な文字列の差分比較を行うだけなので、notebook がjsonで管理されており、matplotlibで生成される出力結果の保存内容にも差異がでるのでこうなってしまいます。\njupyter/nbdimeを用いることで、notebook 形式の json をパースしたうえでの差分比較が可能になります。\n提供されるコマンド一覧\n nbdiff : ノートブックの差分比較をターミナルで行う nbdiff-web : Notebook の差分をブラウザで行う  簡単な実行結果\n差分比較 ターミナル上での差分比較 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  nbdiff nb_1.ipynb nb_2.ipynb nbdiff nb_1.ipynb nb_2.ipynb — — nb_1.ipynb 2017–02–02 11:00:48 +++ nb_2.ipynb 2017–02–02 11:01:07 ## inserted before /cells/0/outputs/0: - output: - output_type: display_data - data: - image/png: iVBORw0K…\u0026amp;lt;snip base64, md5=052a34b732d64628…\u0026amp;gt; - text/plain: \u0026amp;lt;matplotlib.figure.Figure at 0x11037dcc0\u0026amp;gt;``## deleted /cells/0/outputs/0: * output: * output_type: display_data * data: * image/png: iVBORw0K…\u0026amp;lt;snip base64, md5=657f3d90cea1a3d0…\u0026amp;gt; * text/plain: \u0026amp;lt;matplotlib.figure.Figure at 0x10ba48cc0\u0026amp;gt;`## modified /cells/0/source: @@ -8,6 +8,6 @@ y = np.sin(4 * np.pi * x) * np.exp(-5 * x)`fig, ax = plt.subplots()``ax.fill(x, y, zorder=10,facecolor=’green’)zorder=10,facecolor=’black’) ax.grid(True, zorder=5) plt.show()`   Web ブラウザ上での差分比較 1  nbdiff-web nb_1.ipynb nb_2.ipynb [master] [I nbdimeserver:274] Listening on 127.0.0.1, port 51809 [I nbdiffweb:48] URL: [http://127.0.0.1:51809/diff?base=nb_1.ipynb\u0026amp;amp;remote=nb_2.ipynb](http://127.0.0.1:51809/diff?base=nb_1.ipynb\u0026amp;amp;remote=nb_2.ipynb) [I web:1971] 200 GET /diff?base=nb_1.ipynb\u0026amp;amp;remote=nb_2.ipynb (127.0.0.1) 15.43ms [I web:1971] 200 GET /static/nbdime.js?v=ce2758430f38b0ad261242e2b658a8e4 (127.0.0.1) 228.07ms [I web:1971] 200 POST /api/diff (127.0.0.1) 41.45ms [W web:1971] 404 GET /favicon.ico (127.0.0.1) 0.79ms`   Web ブラウザ上での画面\nGit との連携 nbdime - diffing and merging of Jupyter Notebooks - nbdime 0.4.1 documentation\nGit の環境下で、複数人で notebook を編集する機会がある人は便利そう\n(Notebook 上で複数人でガンガン編集してると、Git の差分比較を行っても混沌としそうだが…)\nそう考えるとJupyter Driveがいいのか。\nJupyterを確認すると、Jupyter に関するパッケージが活発に開発されていて中々楽しい 😄\n","permalink":"https://shunyaueta.com/posts/2018-01-15/","summary":"Jupyter notebook をご利用の皆さん、朗報です。\n例えば、下記の２つの notebook の差分を比較したい際に、\n nb_1.ipynb nb_2.ipynb  diffコマンドを用いると下記のような結果になってしまいます。\n1 2  diff nb_1.ipynb nb_2.ipynb [master] 14c14 \u0026amp;lt; “image/png”: “iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt8VPWd//HXh4SLEBAUCSgoqNiKl3pJvdRtGxRW0BZs\\nvVRsvbRa+qs/u9vtbr1su/669mFbtr/t2u2DR1u26k+7damFqiyiKMigaEFABImAhIRLuF8TApIL\\n+fz+yOCOIZDJzJk5M3Pez8cjD+ZMvud8P18nvnPynTnfY+6OiIhES5ewCxARkexT+IuIRJDCX0Qk\\nghT+IiIRpPAXEYkghb+ISAQp/EVEIkjhLyISQQp/EZEIKg67gGPp37+/Dx06NOX9Dxw4QK9evYIr\\nKA9EbcxRGy9ozFGRzpiXLl26y91P6bChu6f9BYwB1gCVwIPHaHML8D5QATzT0TEvvfRST8e8efPS\\n2j8fRW3MURuvu8YcFemMGVjiSeR22mf+ZlYETAZGAzXAYjOb4e7vJ7QZDjwEXOXue81sQLr9iohI\\n6oKY878MqHT3KndvBKYC49u0+SYw2d33Arj7jgD6FRGRFJmnuaqnmd0EjHH3e+LbtwOXu/t9CW2e\\nBz4ArgKKgB+5+8vtHGsiMBGgtLT00qlTp6ZcV319PSUlJSnvn4+iNuaojRc05qhIZ8wjR45c6u5l\\nHbXL1hu+xcBwoBwYDLxuZhe4+77ERu4+BZgCUFZW5uXl5Sl3GIvFSGf/fBS1MUdtvKAxR0U2xhzE\\ntM9mYEjC9uD4c4lqgBnu3uTu1bT+FTA8gL5FRCQFQYT/YmC4mQ0zs27ArcCMNm2ep/WsHzPrD5wD\\nVAXQt4iIpCDt8Hf3ZuA+YDawCnjW3SvM7BEzGxdvNhvYbWbvA/OA77v77nT7FhGR1AQy5+/us4BZ\\nbZ57OOGxA9+Lf4mISMhy9grffLFuzzoeW/gYFTsrOKvfWdz76Xu5eNDFYZclInJcCv80PLfqOb76\\n56/yYfOHAMxbP48n3n2Cn17zU77/me9jZiFXKCLSPi3slqJ51fO4ZdotHwX/ES3ewgNzHuCnC34a\\nUmUiIh1T+Kdg98HdTJg+geaW5mO2+cFrP2DGmrYfehIRyQ0K/xTc/+r9bD+wvcN233jhG+w4oJUs\\nRCT3KPw7afm25Tz57pNJtd394W6+N1sfcBKR3KPw76R/nv/POMmvh/SH9/7AW5veymBFIiKdp/Dv\\nhDW71vDc6uc6vd8Dcx4g3QX0RESCpPDvhF+9/auU9luwcQFzquYEXI2ISOoU/kk60HiAp5c/nfL+\\nj77xaIDViIikR+GfpOmrprO/cX/K+8/fMJ/FmxcHWJGISOoU/kn6/Yrfp32MxxY9FkAlIiLpU/gn\\nYXv9dl6rfi3t4/yp4k9sq98WQEUiIulR+CfhudXP0eItaR+nqaWJJ5cld42AiEgmKfyT8OdVfw7s\\nWP/xzn8E8otERCQdCv8O1B6qZd76eYEdr3pfNbH1scCOJyKSCoV/B15Z98pxF3BLxVPLnwr0eCIi\\nnRVI+JvZGDNbY2aVZvZgO9+/y8x2mtm78a97gug3G16qfCnwY057fxoHGg8EflwRkWSlHf5mVgRM\\nBsYCI4AJZjainaZ/dPeL4l+/S7ffbHB3Xq58OfDjHmw6yAtrXgj8uCIiyQrizP8yoNLdq9y9EZgK\\njA/guKGr2FnB1vqtGTn2M+89k5HjiogkI4jwPw3YlLBdE3+urRvNbIWZTTOzIQH0m3Fzq+Zm7Niv\\nrHuFPR/uydjxRUSOJ1v38P1v4L/cvcHMvgU8BVzdtpGZTQQmApSWlhKLxVLusL6+Pq39AZ5d+Wxa\\n+x9PU0sTk16YxNiBYwM7ZhBjzidRGy9ozFGRjTEHEf6bgcQz+cHx5z7i7rsTNn8H/Et7B3L3KcAU\\ngLKyMi8vL0+5qFgsRjr7t3gLqxatSnn/ZLx3+D0mlU8K7HjpjjnfRG28oDFHRTbGHMS0z2JguJkN\\nM7NuwK3Ax25ea2aDEjbHAZlN1QC8t/099h7am9E+5lTNofZQbUb7EBFpT9rh7+7NwH3AbFpD/Vl3\\nrzCzR8xsXLzZ35hZhZktB/4GuCvdfjPtjY1vZLyPppYmXlz7Ysb7ERFpK5A5f3efBcxq89zDCY8f\\nAh4Koq9seXPTm1np5/nVz3PbBbdlpS8RkSN0he8xvLkxO+H/UuVLNDQ3ZKUvEZEjFP7tqKmrYVPd\\npo4bBqC+sV5r/YhI1in827GoZlFW+9PVviKSbQr/diysWZjV/l5c+yLuntU+RSTaFP7tWLwlu/fa\\n3Vi7kZU7Vma1TxGJNoV/G4dbDrNky5Ks9ztr7ayOG4mIBETh38bqXas50JT95ZZnVSr8RSR7FP5t\\nvLP1nVD6fWvTW7raV0SyRuHfxtKtS0Ppt7mlmbnVmVtFVEQkkcK/jbDO/AFmV84OrW8RiRaFf4IW\\nb+Hdbe+G1v/sdbP1kU8RyQqFf4KqvVXsb9wfWv8bajfwwe4PQutfRKJD4Z8gzLP+I16tejXsEkQk\\nAhT+CZZvWx52CQp/EckKhX+CFTtWhF0C86rn0dzSHHYZIlLgFP4JVmwPP/z3N+7n7c1vh12GiBQ4\\nhX9cXUMd6/etD7sMAOZW6fP+IpJZCv+4ih0VYZfwEc37i0imBRL+ZjbGzNaYWaWZPXicdjeamZtZ\\nWRD9BimXVtVcWLOQA43ZX19IRKIj7fA3syJgMjAWGAFMMLMR7bTrDfwtkN07pSQpl8K/qaUpa/cQ\\nFpFoCuLM/zKg0t2r3L0RmAqMb6fdj4FJwKEA+gxcxc7cmfYBzfuLSGYFEf6nAYk3vK2JP/cRM7sE\\nGOLuLwbQX0a8v/P9sEv4mHnr54VdgogUsOJMd2BmXYBfAHcl0XYiMBGgtLSUWCyWcr/19fVJ77+/\\naT9b67em3FcmLN2ylJlzZlJSXJL0Pp0ZcyGI2nhBY46KbIw5iPDfDAxJ2B4cf+6I3sD5QMzMAAYC\\nM8xsnLt/7JZZ7j4FmAJQVlbm5eXlKRcVi8VIdv+3Nr0Fb6XcVUa00AKnQ/k55Unv05kxF4KojRc0\\n5qjIxpiDmPZZDAw3s2Fm1g24FZhx5JvuXuvu/d19qLsPBRYCRwV/mFbtXBV2Ce2aV62pHxHJjLTD\\n392bgfuA2cAq4Fl3rzCzR8xsXLrHz4ZVu3I0/DXvLyIZEsicv7vPAma1ee7hY7QtD6LPIK3etTrs\\nEtr17rZ32XdoH3179A27FBEpMLrCl9wNf8d5Y8MbYZchIgUo8uHf0NxA9b7qsMs4pvkb5oddgogU\\noMiHf+WeSlq8Jewyjim2PhZ2CSJSgCIf/mt2rwm7hONatm0ZtYdqwy5DRApM5MM/1++Z2+ItWudH\\nRAKn8M/x8AeYv17z/iISLIV/HoT/6xtfD7sEESkwkQ//tXvWhl1Ch5ZsWaL1/UUkUJEO/9pDtew4\\nsCPsMjrU3NLMwpqFYZchIgUk0uFfuacy7BKS9voGTf2ISHAiHf75MOVzhOb9RSRI0Q7/3fkT/gtr\\nFtJ4uDHsMkSkQEQ6/Cv35s+0z6HmQyzZkjOrYItInot0+K/bsy7sEjpFi7yJSFCiHf578yz8Nyr8\\nRSQYkQ3/+sZ6ttVvC7uMTlmwcUFOL0InIvkjsuFftbcq7BI6rbahlpU7VoZdhogUAIV/ntG8v4gE\\nIZDwN7MxZrbGzCrN7MF2vv+/zOw9M3vXzBaY2Ygg+k1H3oa/5v1FJABph7+ZFQGTgbHACGBCO+H+\\njLtf4O4XAf8C/CLdftOVr+G/YOMC3D3sMkQkzwVx5n8ZUOnuVe7eCEwFxic2cPe6hM1eQOjpla/h\\nv3n/ZtbvWx92GSKS54oDOMZpwKaE7Rrg8raNzOx/A98DugFXt3cgM5sITAQoLS0lFoulXFR9ff1x\\n91+5OX/fOJ0yewrXDrz2qOc7GnOhidp4QWOOimyMOYjwT4q7TwYmm9ltwA+BO9tpMwWYAlBWVubl\\n5eUp9xeLxTjW/i3ewo4Fub+a57Hs7rW73bEdb8yFKGrjBY05KrIx5iCmfTYDQxK2B8efO5apwA0B\\n9JuybfXbaDjcEGYJadGbviKSriDCfzEw3MyGmVk34FZgRmIDMxuesHk9EOqKatV7q8PsPm2rd61m\\n54GdYZchInks7fB392bgPmA2sAp41t0rzOwRMxsXb3afmVWY2bu0zvsfNeWTTYXwhqlu6i4i6Qhk\\nzt/dZwGz2jz3cMLjvw2in6AUQvgv2LiAGz4Z6uyZiOSxSF7hWyjhLyKSqmiGf+36sEtI29KtSznY\\ndDDsMkQkT0Uz/AvgzL+5pZm3N78ddhkikqciF/4t3sLG2o1hlxEILfImIqmKXPhv3b+1YO6Fq8/7\\ni0iqIhf+G2o3hF1CYP5S8xeaW5rDLkNE8lD0wn9f4YR/fWM9K7avCLsMEclD0Qv/AjrzB3hzoy72\\nEpHOi1z4F8qbvUdo3l9EUhG58C+0M/83Nr6hm7uISKdFLvwL7cx/W/02qvfl90J1IpJ9Cv8CoM/7\\ni0hnRSr8aw/VUtdQ13HDPKN5fxHprEiFfyGe9YPCX0Q6T+FfAD7Y/QE7DuTvbSlFJPsiFf6b6jZ1\\n3ChPaYlnEemMSIV/oZ75g970FZHOCST8zWyMma0xs0oze7Cd73/PzN43sxVmNtfMzgii384q5DN/\\nzfuLSGekHf5mVgRMBsYCI4AJZjaiTbNlQJm7XwhMA/4l3X5Tsam2cMN/2bZlHGzWzV1EJDlBnPlf\\nBlS6e5W7NwJTgfGJDdx9nrsfSaaFwOAA+u20Qp72afEWKuoqwi5DRPJEEOF/GpB4Sl0Tf+5Y7gZe\\nCqDfTmnxFjbv35ztbrPqvdr3wi5BRPJEcTY7M7OvAWXA54/x/YnARIDS0lJisVjKfdXX139s/72N\\newvmJi7HsmzPsrT+m+Wbtq9xFGjM0ZCNMQcR/puBIQnbg+PPfYyZjQJ+AHze3RvaO5C7TwGmAJSV\\nlXl5eXnKRcViMRL3X7plKfwl5cPlhQ8OfsCVf3Ul3Yu7h11KVrR9jaNAY46GbIw5iGmfxcBwMxtm\\nZt2AW4EZiQ3M7GLgt8A4dw/laqSaupowus2qxpZGlmxZEnYZIpIH0g5/d28G7gNmA6uAZ929wswe\\nMbNx8WY/B0qAP5nZu2Y24xiHy5hC/phnotc3vB52CSKSBwKZ83f3WcCsNs89nPB4VBD9pCMKZ/7Q\\n+nn/h3go7DJEJMdF5grfQv+kzxFvbnqTwy2Hwy5DRHJcZMI/Kmf+dQ11LN++POwyRCTHKfwLkOb9\\nRaQjkQh/d2dzXTSmfQDmb5gfdgkikuMiEf57D+3lw+YPwy4ja17f8Dot3hJ2GSKSwyIR/lE66wfY\\n8+EeKnZonR8RObZohH9EPumTSFM/InI80Qj/iJ35g8JfRI4vGuEfxTP/9fNx97DLEJEcFY3wj+CZ\\n/86DO1m9a3XYZYhIjopG+EfwzB8gtj4WdgkikqMiEf5b9m8Ju4RQzFs/L+wSRCRHRSL8o3rmP3+D\\n5v1FpH0FH/5Nh5vYcSCUWwiEbseBHazatSrsMkQkBxV8+G+r3xZ2CaGaV62pHxE5WsGHf1Tn+4/Q\\nvL+ItKfgwz+q8/1HxNbHtM6PiByl4MN/6/6tYZcQqt0f7mbljpVhlyEiOSaQ8DezMWa2xswqzezB\\ndr7/OTN7x8yazeymIPpMVtSnfQDmVs0NuwQRyTFph7+ZFQGTgbHACGCCmY1o02wjcBfwTLr9ddaW\\neoW/5v1FpK0gzvwvAyrdvcrdG4GpwPjEBu6+3t1XAFmffI76tA+0zvs3tzSHXYaI5JDiAI5xGrAp\\nYbsGuDyVA5nZRGAiQGlpKbFYLOWi6uvricVirN22NuVjFIr9jfuZMnMKI/q0/YMsvx15jaNEY46G\\nbIw5iPAPjLtPAaYAlJWVeXl5ecrHisVilJeXU/t2bUDV5bc9ffdQ/rnysMsI1JHXOEo05mjIxpiD\\nmPbZDAxJ2B4cfy50Dc0N7P5wd9hl5IS51XrTV0T+RxDhvxgYbmbDzKwbcCswI4Djpi3qV/cmemvT\\nWxxoPBB2GSKSI9IOf3dvBu4DZgOrgGfdvcLMHjGzcQBm9mkzqwFuBn5rZlm5wezWer3Ze0Tj4Ube\\n2PhG2GWISI4IZM7f3WcBs9o893DC48W0TgdllT7p83FzquYw5uwxYZchIjmgoK/w1Zn/x71a9WrY\\nJYhIjijo8NfVvR+3YvsKvQ8iIkCBh7+C7mivrtPZv4gUePhr2udomvoRESjw8NeZ/9FeWfeKlngW\\nkcIOf33a52jbD2xnxfYVYZchIiEr2PA/7Icje+/ejrxc+XLYJYhIyAo2/Gubajnsh8MuIycp/EUk\\npxZ2C9Kexj1hl5Cz3tz0JnUNdfTp3ifsUiTBwaaDvL/zfVbvWk313mq21m9lz4d7ONB0gKbDTZgZ\\n9XvrOXPfmfQ/oT+Deg/ijBPP4KyTzuITJ3+CXt16hT0EySMK/whqbmnm1XWvcuOIG8MuJdIONR9i\\nXvU8Zq+bzfwN83lv+3tJ/bW6YPeCdp8/q99ZXDzoYi4/7XKuHHwlZaeW0b24e9BlS4FQ+EfUS5Uv\\nKfxD0OItvFb9Gk8tf4rnVz9PfWN9YMdet3cd6/auY9r70wDoUdyDzwz5DKOGjeLas6/looEX0cUK\\ndqZXOqlgw39v496wS8hps9bOwt0xs7BLiYSDTQd5ctmT/HLRL1m7Jzs3GDrUfIjXql/jterX+MfX\\n/pHSXqVcP/x6xn1iHKPPGk3Prj2zUofkpoINf535H9/W+q0s27aMSwZdEnYpBe1Q8yF+vfjX/OzN\\nn4X+6bPtB7bzxLtP8MS7T9Cza0/Gnj2Wm0bcxPXDr6d3996h1ibZp/CPsJkfzFT4Z4i7M33VdP7h\\nlX9gQ+2GsMs5ysGmg0xfNZ3pq6bTo7gH1w+/nq+c9xWuP+d6/UUQEQU7Aajw79jMD2aGXUJBqt5b\\nzZg/jOHmP92ck8Hf1qHmQ0xfNZ1bpt3CgJ8P4Kt//iovfvAijYcbwy5NMqhww79J4d+RxVsW6yro\\nALk7v178a87/9fm8su6VsMtJyYGmAzzz3jN84b++wKB/HcS3/vtbxNbHONyia2YKTcGGv97wTc6L\\na18Mu4SCsOvgLsZNHce9s+7lYNPBsMsJxJ4P9zDlnSmMfGokpz92On/38t+xqGYR7h52aRKAQMLf\\nzMaY2RozqzSzB9v5fncz+2P8+4vMbGgQ/R5LQ3MD+5v3Z7KLgvHCmhfCLiHvLaxZyMW/vbigp9G2\\n7N/CY4se44rHr2DYL4dx/6v38/bmt/WLII+lHf5mVgRMBsYCI4AJZjaiTbO7gb3ufjbwb8CkdPs9\\nnrA/VZFP5lTN0Y3d0/D4O4/zuSc/R01dTdilZM2G2g38/K2fc/nvLmfoL4fy3Ze/y/z182luaQ67\\nNOmEIM78LwMq3b3K3RuBqcD4Nm3GA0/FH08DrrEMfsBcSzkn71DzIWavmx12GXnncMth/n7233PP\\nf99DU0tT2OWEZmPtRn656JeUP1XOwP87kDueu4NnK55l36F9YZcmHQjio56nAZsStmuAy4/Vxt2b\\nzawWOBnYFUD/R6lrqKN/t/50694tE4fPWY0NjSmNefHmxVx+WtuXLPftbNjJ5rrNWe/3hK4n8KPY\\nj5i+ajqn9j41q32n+hpny9zqucytnktxl2KuGHwFf33mXzP6rNH0KO5B0+HUfkmG9TqHoYt1oW+P\\nvlnpK6c+529mE4GJAKWlpcRisZSOU0QRT17wJCUlJQFWl/vq6+tTGvP+pv0MfWwozZ6Hf7YvzG53\\nvYp68ej5j/Llvl/my5d8Obudk/prHJo6qFpWxcHmg6ysW8nyfctZWbeSNfvX0NDSkPxxsvw6B6mr\\ndaVft36c1O0kTup2Uuvjrq3/9uvWj75d+3Ji1xPp27Uvfbr2ociKqK+vTzn/khVE+G8GhiRsD44/\\n116bGjMrBk4Edrc9kLtPAaYAlJWVeXl5ecpFxWIx0tk/H6Uz5qu3X523H0/Mlv49+zP7a7NDvTAu\\nn3+ur+O6jx43tzSzcsdKlmxZwjtb32H59uWs3LGSuoa6ECvsnO5F3RnUexCn9j6VQSWDGFTS+vjU\\n3qcyqHfr9qDegzj5hJM7vYxKNl7nIMJ/MTDczIbRGvK3Are1aTMDuBP4C3AT8JrrYwI55aZzb1L4\\nH0dpr1Lm3jGX8wacF3YpBaG4SzEXDbyIiwZe9LHna+pqWL1rNWt3r2Xd3nWs37eeipoKar2WHQd2\\nZPweHX269+HkE06mf8/+nNLrFAb0GsCAngMoLSllYMnAj74GlQyi3wn9MlpLpqUd/vE5/PuA2UAR\\n8IS7V5jZI8ASd58BPA783swqgT20/oKQHHLDJ2/g2y9+WzfAaUdpr1Lm3TmPc085N+xSCt7gPoMZ\\n3Gcwo84c9dFzR86CW7yFXQd3sevgLnYf3M2+Q/vY37if+sZ6Pmz6kIbDDTQebuRwy+GP7lPdxbrQ\\ntagrXbt0pXtxd04oPoETup5ASbcS+nTvQ+9uvenbo+9HX12LuoY19KwLZM7f3WcBs9o893DC40PA\\nzUH0JZlxSq9TKB9aztzquWGXklP69+zP3DvmKvhzQBfr0nom3mtA2KUUhIK9wlc675bzbgm7hJzS\\nt0dfXr39VU31SEFS+MtHbjz3Roq75NQHwELTs2tPZk6YedSctEihUPjLR07ueTKjzxwddhmhK+5S\\nzLSbp3HV6VeFXYpIxij85WMmnD8h7BJCZRiPj3ucscPHhl2KSEYp/OVjvnTulyJ9M49Hr36UOz51\\nR9hliGScwl8+pqRbCeM/0XZppmj41qXf4qHPPhR2GSJZofCXo0TxzHfs2WOZfN3ksMsQyRqFvxxl\\n9JmjGVQyKOwysubC0gv5401/pKhLUdiliGSNwl+OUtSliNsvvD3sMrJiYMlAZk6YSe/uvcMuRSSr\\nFP7SrrsvuTvsEjKuR3EPXrj1BYacOKTjxiIFRuEv7Trn5HP47OmfDbuMjHpy/JNcdtplYZchEgqF\\nvxzTNy/5ZtglZMwPP/tDbj1f6wtKdCn85ZhuPu9mTjrhpLDLCNyXPvklHhn5SNhliIRK4S/H1KO4\\nB1+/6OthlxGoT5V+it9/6fedvrmGSKFR+Mtx3fvpe+lihfFjckrPU5gxYQa9uvUKuxSR0BXG/9WS\\nMWf2O5Prh18fdhlp69qlK9Nvmc7pJ54edikiOUHhLx367hXfDbuEtE2+bjKfPaOwP70k0hkKf+nQ\\n1cOuzut17b9z2Xf45qWF+8klkVSkFf5mdpKZvWpma+P/tntHYzN72cz2mdnMdPqT8Nz/mfvDLiEl\\no88czb9d+29hlyGSc9I9838QmOvuw4G58e32/ByIxnoBBeqW827hzH5nhl1Gp5xz8jk8e/OzWrNH\\npB3phv944Kn446eAG9pr5O5zgf1p9iUhKupSxEN/lT/LHffr0Y+ZE2bSt0ffsEsRyUnm7qnvbLbP\\n3fvGHxuw98h2O23LgX9w9y8c53gTgYkApaWll06dOjXl2urr6ykpKUl5/3yU6TE3tzRz++Lb2XZo\\nW8b6CEKxFTPpgklc0u+SsEsJnH6uoyGdMY8cOXKpu5d11K7Du3Wb2RxgYDvf+kHihru7maX+m6T1\\nGFOAKQBlZWVeXl6e8rFisRjp7J+PsjHmR/s9yt0zcnvRt9984TcFuzCdfq6jIRtj7nDax91Hufv5\\n7Xy9AGw3s0EA8X93ZLRaCd2dn7qTc/ufG3YZx/TgVQ8WbPCLBCndOf8ZwJ3xx3cCL6R5PMlxRV2K\\nmDRqUthltOu2C27jJ9f8JOwyRPJCuuH/M2C0ma0FRsW3MbMyM/vdkUZm9gbwJ+AaM6sxs2vT7FdC\\n9MVPfJFrhl0Tdhkfc/Wwq3ly/JNas0ckSR3O+R+Pu+8GjkoBd18C3JOwrUsrC8yvxv6KT/3mUzS1\\nNIVdChcPvJjnvvIc3Yq6hV2KSN7QFb6SknNPOZfvf+b7YZfBOSefw8tfe5k+3fuEXYpIXlH4S8r+\\n6fP/xCf7fzK0/gf2GMic2+cwoNeA0GoQyVcKf0lZj+IePH3D0xR3SWv2MCWnn3g6v7jwF7r/rkiK\\nFP6Slk+f9ml+PPLHWe1zaN+hxO6MMeiEQVntV6SQKPwlbQ9c9QBfPOeLWenr3P7n8sbX32BYv2FZ\\n6U+kUCn8JW1mxh++/AcuGHBBRvu5ashVLPjGAgb3GZzRfkSiQOEvgejdvTcvf+1lhvXNzBn57Rfe\\nztw75hbkDeVFwqDwl8Cc2vtU5t05j7NPOjuwY3Yr6sa/j/l3nv7S03Qv7h7YcUWiTuEvgTqj7xm8\\n+Y03uXLwlWkf68LSC1l0zyK+c/l3AqhMRBIp/CVwA3oNYP5d87n/M/dTZJ2/kUqf7n2YNGoSS765\\nJK9vHymSyxT+khFdi7oyafQk3vnWO1w//HqMjtfcOaXnKfzwsz+k6m+quP+q++la1DULlYpEU/av\\nzpFIubD0QmbeNpN1e9Yx7f1pvL7xddbuXktdQx09intw+omnc8mgS7j2rGsZdeYoBb5Ilij8JSvO\\nOuksHvirB3iAB8IuRUTQtI+ISCQp/EVEIkjhLyISQQp/EZEISiv8zewkM3vVzNbG/+3XTpuLzOwv\\nZlZhZivM7Cvp9CkiIulL98z/QWCuuw8H5sa32zoI3OHu5wFjgMfMrG+a/YqISBrSDf/xwFPxx08B\\nN7Rt4O4fuPva+OMtwA7glDT7FRGRNKQb/qXuvjX+eBtQerzGZnYZ0A1Yl2a/IiKSBnP34zcwmwMM\\nbOdbPwCecve+CW33uvtR8/7x7w0CYsCd7r7wGG0mAhMBSktLL506dWoyY2hXfX09JSUlKe+fj6I2\\n5qiNFzTmqEhnzCNHjlzq7mUdtesw/I+7s9kaoNzdtx4Jd3f/RDvt+tAa/D9x92lJHnsnsCHl4qA/\\nsCuN/fNR1MYctfGCxhwV6Yz5DHfvcGo93eUdZgB3Aj+L//tC2wZm1g14Dng62eAHSKb44zGzJcn8\\n9iskURtz1MYLGnNUZGPM6c75/wwYbWZrgVHxbcyszMx+F29zC/A54C4zezf+pXV6RURClNaZv7vv\\nBq5p5/lLXv8NAAADtElEQVQlwD3xx/8J/Gc6/YiISLAK+QrfKWEXEIKojTlq4wWNOSoyPua03vAV\\nEZH8VMhn/iIicgx5Hf5mNsbM1phZpZkdtbSEmXU3sz/Gv7/IzIZmv8pgJTHm75nZ+/F1lOaa2Rlh\\n1Bmkjsac0O5GM3Mzy/tPhiQzZjO7Jf5aV5jZM9muMWhJ/GyfbmbzzGxZ/Of7ujDqDIqZPWFmO8xs\\n5TG+b2b27/H/HivM7JJAC3D3vPwCimi9UvhMWq8aXg6MaNPmXuA38ce3An8Mu+4sjHkk0DP++NtR\\nGHO8XW/gdWAhUBZ23Vl4nYcDy4B+8e0BYdedhTFPAb4dfzwCWB923WmO+XPAJcDKY3z/OuAlwIAr\\ngEVB9p/PZ/6XAZXuXuXujcBUWtcaSpS49tA04Boz6/hO4rmrwzG7+zx3PxjfXAgMznKNQUvmdQb4\\nMTAJOJTN4jIkmTF/E5js7nsB3H1HlmsMWjJjdqBP/PGJwJYs1hc4d38d2HOcJuNpvT7KvXVVhL7x\\ni2kDkc/hfxqwKWG7Jv5cu23cvRmoBU7OSnWZkcyYE91N65lDPutwzPE/h4e4+4vZLCyDknmdzwHO\\nMbM3zWyhmY3JWnWZkcyYfwR8zcxqgFnAd7JTWmg6+/97p+gG7gXKzL4GlAGfD7uWTDKzLsAvgLtC\\nLiXbimmd+imn9a+7183sAnffF2pVmTUB+H/u/q9mdiXwezM7391bwi4sH+Xzmf9mYEjC9uD4c+22\\nMbNiWv9U3J2V6jIjmTFjZqNoXXhvnLs3ZKm2TOlozL2B84GYma2ndW50Rp6/6ZvM61wDzHD3Jnev\\nBj6g9ZdBvkpmzHcDzwK4+1+AHrSugVOokvr/PVX5HP6LgeFmNiy+ftCttK41lOjI2kMANwGvefyd\\nlDzV4ZjN7GLgt7QGf77PA0MHY3b3Wnfv7+5D3X0ore9zjPPWq8zzVTI/28/TetaPmfWndRqoKptF\\nBiyZMW8kvqKAmZ1La/jvzGqV2TUDuCP+qZ8rgFr/nyX005a30z7u3mxm9wGzaf2kwBPuXmFmjwBL\\n3H0G8DitfxpW0vrGyq3hVZy+JMf8c6AE+FP8ve2N7j4utKLTlOSYC0qSY54N/LWZvQ8cBr7vrcut\\n5KUkx/z3wH+Y2d/R+ubvXfl8Mmdm/0XrL/D+8fcx/g/QFcDdf0Pr+xrXAZW03hHx64H2n8f/7URE\\nJEX5PO0jIiIpUviLiESQwl9EJIIU/iIiEaTwFxGJIIW/iEgEKfxFRCJI4S8iEkH/H6iRjqvW7TK6\\nAAAAAElFTkSuQmCC\\n”, — - \u0026amp;gt; “image/png”: “iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\\nJIb+P2c9rHcimwD6AAAAAElFTkSuQmCC\\n”, 16c16 \u0026amp;lt; “\u0026amp;lt;matplotlib.figure.Figure at 0x10ba48cc0\u0026amp;gt;” — - \u0026amp;gt; “\u0026amp;lt;matplotlib.figure.Figure at 0x11037dcc0\u0026amp;gt;” 34c34 \u0026amp;lt; “ax.fill(x, y, zorder=10,facecolor=’green’)\\n”, — - \u0026amp;gt; “ax.fill(x, y, zorder=10,facecolor=’black’)\\n”,`   diffは単純な文字列の差分比較を行うだけなので、notebook がjsonで管理されており、matplotlibで生成される出力結果の保存内容にも差異がでるのでこうなってしまいます。\njupyter/nbdimeを用いることで、notebook 形式の json をパースしたうえでの差分比較が可能になります。\n提供されるコマンド一覧\n nbdiff : ノートブックの差分比較をターミナルで行う nbdiff-web : Notebook の差分をブラウザで行う  簡単な実行結果\n差分比較 ターミナル上での差分比較 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  nbdiff nb_1.","title":"Jupyter Notebookの差分を明瞭に確認する事ができるpackage : nbdime"},{"content":"スタンディングディスカッション形式での会話を評価した研究\nSummary Slide\nX Alameda-Pineda, Y Yan, E Ricci, O Lanz, N Sebe\n“Analyzing Free-standing Conversational Groups: A Multimodal Approach”, in 2015 ACM Multimedia Conference\nlink\nを読んだので、軽くメモ。\nマルチモーダル系の論文初めて読んだんですが、まとめるとコントリビューションが 5 つあると主張。\nContribution  音声・近接情報、そして監視カメラからの身体と頭の姿勢推定からマルチモーダルに解析 フリースタンディングディスカッションを身体・頭の姿勢推定から解析 カメラと音声・近接センサーからなるマルチモーダルな解析するためのフレームワークを提案 ラベリングされてないデータに対する行列補間問題の考案 SALSA(データ・セット)を公開・評価  SALSA というポスターセッションの動画と音声のデータも公開されている\n SALSA: Synergetic sociAL Scene Analysis\n 動画は Google Drive で公開されていて時代の波を感じる。\n データセットを公開 論文も読みやすい 新しい行列補完計画法(アルゴリズム)を提案 実問題に取り組む  と盛り沢山な内容で面白かった。\nスライド内のリンクは Google Slide で共有しているのでこちらを参照すると便利です。\nX Alameda-Pineda, Y Yan, E Ricci, O Lanz, N Sebe \u0026quot;Analyzing Free-standing Conversational Groups: A…\n","permalink":"https://shunyaueta.com/posts/2018-01-14/","summary":"スタンディングディスカッション形式での会話を評価した研究\nSummary Slide\nX Alameda-Pineda, Y Yan, E Ricci, O Lanz, N Sebe\n“Analyzing Free-standing Conversational Groups: A Multimodal Approach”, in 2015 ACM Multimedia Conference\nlink\nを読んだので、軽くメモ。\nマルチモーダル系の論文初めて読んだんですが、まとめるとコントリビューションが 5 つあると主張。\nContribution  音声・近接情報、そして監視カメラからの身体と頭の姿勢推定からマルチモーダルに解析 フリースタンディングディスカッションを身体・頭の姿勢推定から解析 カメラと音声・近接センサーからなるマルチモーダルな解析するためのフレームワークを提案 ラベリングされてないデータに対する行列補間問題の考案 SALSA(データ・セット)を公開・評価  SALSA というポスターセッションの動画と音声のデータも公開されている\n SALSA: Synergetic sociAL Scene Analysis\n 動画は Google Drive で公開されていて時代の波を感じる。\n データセットを公開 論文も読みやすい 新しい行列補完計画法(アルゴリズム)を提案 実問題に取り組む  と盛り沢山な内容で面白かった。\nスライド内のリンクは Google Slide で共有しているのでこちらを参照すると便利です。\nX Alameda-Pineda, Y Yan, E Ricci, O Lanz, N Sebe \u0026quot;Analyzing Free-standing Conversational Groups: A…","title":"Analyzing Free-standing Conversational Groups: A Multimodal Approach (ACMMM15) を読んだ"},{"content":"Python でアニメーションを作成したかったのでメモ\nGaussian Kernel GIF Animation\n当然ながら、HTML5 の Video は再生されないので GIF に変換した結果が以下。\nこれで HTML5 で再生される。\n**GIF**で表示する方法として %matplotlib nbagg\nというオプションが存在しているが、Kernel が busy 状態を何度も繰り返すので、自分は mp4 で出力するようにした。\n実験結果も以前は GIF で保存してたが、最近は全てmp4で管理するようにした。\nあと、**np.linspace()**が iterable ではないので、イマイチな書き方になった。。\n**np.arange()**を使うべきなのか…\nReferences  Embedding Matplotlib Animations in Jupyter Notebooks Jupyter 上で matplotlib のアニメーションを再生する - Qiita  ","permalink":"https://shunyaueta.com/posts/2018-01-13/","summary":"Python でアニメーションを作成したかったのでメモ\nGaussian Kernel GIF Animation\n当然ながら、HTML5 の Video は再生されないので GIF に変換した結果が以下。\nこれで HTML5 で再生される。\n**GIF**で表示する方法として %matplotlib nbagg\nというオプションが存在しているが、Kernel が busy 状態を何度も繰り返すので、自分は mp4 で出力するようにした。\n実験結果も以前は GIF で保存してたが、最近は全てmp4で管理するようにした。\nあと、**np.linspace()**が iterable ではないので、イマイチな書き方になった。。\n**np.arange()**を使うべきなのか…\nReferences  Embedding Matplotlib Animations in Jupyter Notebooks Jupyter 上で matplotlib のアニメーションを再生する - Qiita  ","title":"PythonでGaussian Kernelのアニメーションを作成"},{"content":"Affective Computing: 計算機と人間の感情や情緒の関係性を考える領域\nMIT Media Lab Affective Computing Group のプロジェクト。\n2 年前に MIT Media Lab へ訪問した際に、色々と見せてもらったけどかなり野心的な事に取り組んでいて感動してた。(デバイスも自分たちでプロトタイプを作りまくっていて、Deploy or Dieの意思を感じ取れる)\n論文のまとめスライドは以下\nOne Slide Summary\nHCI 系の論文は初めてまともに読んだんですが、\n 実験デザインが一番難しそう。人と計算機の関係を研究するので、必然的に人間の感性をどう評価する話にもつながってきてる? 手法に重きを置くというよりも、問題に重きを置いている印象  普段自分は、数値線形代数や機械学習、コンピュータビジョンを主にやっていて、精度をどれだけ出せるか、正解率をどれだけ向上や、速度をどれだけ上げれるかを理論的に保証、提案したりしていますが、面白い問題や興味深いデータを集めることも大事だなと思った。\n実験計画法¹もかなり重要で、メンターの人から実験計画法の成り立ちを教えてもらってとてもおもしろかった。\n","permalink":"https://shunyaueta.com/posts/2018-01-12/","summary":"Affective Computing: 計算機と人間の感情や情緒の関係性を考える領域\nMIT Media Lab Affective Computing Group のプロジェクト。\n2 年前に MIT Media Lab へ訪問した際に、色々と見せてもらったけどかなり野心的な事に取り組んでいて感動してた。(デバイスも自分たちでプロトタイプを作りまくっていて、Deploy or Dieの意思を感じ取れる)\n論文のまとめスライドは以下\nOne Slide Summary\nHCI 系の論文は初めてまともに読んだんですが、\n 実験デザインが一番難しそう。人と計算機の関係を研究するので、必然的に人間の感性をどう評価する話にもつながってきてる? 手法に重きを置くというよりも、問題に重きを置いている印象  普段自分は、数値線形代数や機械学習、コンピュータビジョンを主にやっていて、精度をどれだけ出せるか、正解率をどれだけ向上や、速度をどれだけ上げれるかを理論的に保証、提案したりしていますが、面白い問題や興味深いデータを集めることも大事だなと思った。\n実験計画法¹もかなり重要で、メンターの人から実験計画法の成り立ちを教えてもらってとてもおもしろかった。","title":"Call center stress recognition with person-specific models を読んだ"},{"content":"べき乗法と独立成分分析を用いたマルチスケールに頑強なクラスタリング手法の提案\n べき乗法では近似固有ベクトル(Pseudo-eigenvectors)は複数の真の固有ベクトルの線形結合からなる。有益でない固有ベクトルも含まれるのでいかにそれを除去するか Fig.1 で言ってることがいまいちわからない。ｃ,ｄも変化がないように見える。論文で言及してる stripe が何を指してるかが不明 分かった。赤・青・黒の３つのクラスタで分かれている。最初は黒色がクラスタ境界面に見えた。論文内での multi scale というのは、データの幾何的な分布を指している？ 固有値計算は O(n³）かかるから計算量が~~って毎回言われるが、行列の状態に依存するので毎回最悪の場合を主張されてるも困る ZP self-turining spectral clustering をなぜ対抗馬にして比較してるのかは謎。 ICA¹を行ったあとにその行列に対して回転処理を行い情報量の最小化を狙う クラスタ数が多い場合、PIC では良い結果が出づらい multi scale なデータの場合標準の spectral clustering では失敗することが多々ある fig.2 (a) 見ればわかるがk-means では分離が困難 fig.2(b) ４－５本目の固有ベクトルを基底ベクトルにもつ空間ではクラスタのオーバーラップが少ない つまり１－５本目の固有ベクトル全て含めてクラスタリングすれば結果が良好になるのではないか？（仮説） fig.2 © ４回べき乗法を行った。結果が似てる。（縦軸が何を表してるかは不明）。四回が上から四本求めたのか、一番上の固有ベクトルを４回求めたのか分からない fig. (d) 提案手法を適用。k-means で分離可能 行列Vをp回べき乗法を行って構築 E=MVの最小化を行う ICA を最小化するために Jacobi Rotation を用いる。探索には貪欲法を採用  Contribution  マルチスケール（多種多様な）データ分布に対してクラスタリングが可能 計算時間は従来（ncut)と同等  感想  PIC(Power Itetaion Clusterg)をまさか拡張してくるとは思わなかったなので、興味深い論文。(実装して再現実験したけど、Early stopping の段階で高確率でクラスタリングが失敗していて使い物にならなかった印象があるので…) データの分布が多様な場合、上位数本のベクトルではなく、そこから更に下の固有ベクトルに着目したほうが結果が良好になるのは面白い KDD の論文、相変わらず読みやすかった。  ","permalink":"https://shunyaueta.com/posts/2018-01-11/","summary":"べき乗法と独立成分分析を用いたマルチスケールに頑強なクラスタリング手法の提案\n べき乗法では近似固有ベクトル(Pseudo-eigenvectors)は複数の真の固有ベクトルの線形結合からなる。有益でない固有ベクトルも含まれるのでいかにそれを除去するか Fig.1 で言ってることがいまいちわからない。ｃ,ｄも変化がないように見える。論文で言及してる stripe が何を指してるかが不明 分かった。赤・青・黒の３つのクラスタで分かれている。最初は黒色がクラスタ境界面に見えた。論文内での multi scale というのは、データの幾何的な分布を指している？ 固有値計算は O(n³）かかるから計算量が~~って毎回言われるが、行列の状態に依存するので毎回最悪の場合を主張されてるも困る ZP self-turining spectral clustering をなぜ対抗馬にして比較してるのかは謎。 ICA¹を行ったあとにその行列に対して回転処理を行い情報量の最小化を狙う クラスタ数が多い場合、PIC では良い結果が出づらい multi scale なデータの場合標準の spectral clustering では失敗することが多々ある fig.2 (a) 見ればわかるがk-means では分離が困難 fig.2(b) ４－５本目の固有ベクトルを基底ベクトルにもつ空間ではクラスタのオーバーラップが少ない つまり１－５本目の固有ベクトル全て含めてクラスタリングすれば結果が良好になるのではないか？（仮説） fig.2 © ４回べき乗法を行った。結果が似てる。（縦軸が何を表してるかは不明）。四回が上から四本求めたのか、一番上の固有ベクトルを４回求めたのか分からない fig. (d) 提案手法を適用。k-means で分離可能 行列Vをp回べき乗法を行って構築 E=MVの最小化を行う ICA を最小化するために Jacobi Rotation を用いる。探索には貪欲法を採用  Contribution  マルチスケール（多種多様な）データ分布に対してクラスタリングが可能 計算時間は従来（ncut)と同等  感想  PIC(Power Itetaion Clusterg)をまさか拡張してくるとは思わなかったなので、興味深い論文。(実装して再現実験したけど、Early stopping の段階で高確率でクラスタリングが失敗していて使い物にならなかった印象があるので…) データの分布が多様な場合、上位数本のベクトルではなく、そこから更に下の固有ベクトルに着目したほうが結果が良好になるのは面白い KDD の論文、相変わらず読みやすかった。  ","title":"FUSE: Full Spectral Clustering(KDD2016) を読んだ"},{"content":"PWA と FireBase を試してみたかった\nfirebase init で現れる画面、テンション上がる\nGithub Pages + CloudFlare で独自ドメインの shunyaueta.com をホスティングしてたんですが、Firebase でホスティングできると聞いて Firebase に移行しました。\nPWA にしたのは完全に趣味です。\nTL;DR;  Web App を作ってる人は manifest.json を設置するだけでも Android の使用感が改善されそう 独自ドメインでお手軽に SSL ホスティングしたいなら Firebase hosting めっちゃおすすめです(1GB のホスティングは無料) FireBase Hosting だけだと Firebase 本来の旨味は味わえません  PWA 1 年前ですが、簡潔に PWA の事が書かれています\nプログレッシブウェブアプリ詳解 ─ 過去・現在・未来\n左: PWA 化以前 右: PWA 化以降\nServiceWorker と manifest.json,あとは\u0026lt;meta name=”theme-color”\u0026gt;を指定すると PWA のスコアが 100 点になる 🎉\nmanihest.json によるホームアイコン作成誘導\nFavicon の各画像の生成は下記のサイトが便利でした。要求される解像度毎の画像(Favicon,Home icon, Apple home icon)が生成されて mahifest.json も同梱されます。(画像の圧縮率も選べる)\nFavicon Generator for all platforms: iOS, Android, PC/Mac\u0026hellip;\n後は、\nLearn how to build a PWA in 5 minutes\nを参考に最小実装で実現しました。(Web 通知は無意味だから止めました)\nCSS は Skelton を採用しました。(css only 実装で js がなくてシンプルで良い)\ndhg/Skeleton\nWeb App やってる人は、実装コスト 1 時間程度で済むので皆 manifest.json を設置するべき!!!\n [](https://twitter.com/agektmr/status/894753854373351424) kitamura さんの受け売りです\n Web App Manifest については下記の記事が分かりやすいです。\nWeb App Manifest で Web アプリをインストール可能に - Qiita\nというか皆 Chrome の中の人の記事だった(どうりで詳しいわけですね…)\nホスティング Github Pages+CloudFlare(独自ドメイン運用)で SSL 認証された Web Page のホスティングを実現していました。\nFireBase では、FireBase Hostingを使うことで独自ドメインでもデフォルトで https で Web サイトがホスティングされます(最高)\nやり方は以下を参考\nDeploy Your Site | Firebase\nめっちゃ簡単にデブロイできます…!!\n成果物 Shunya Ueta | Engineer\n所感 今回は静的かつ小規模な Web サイトを PWA 化したので、恩恵殆ど無いですがまともな WebApp だと恩恵めっちゃ受けると思います。(Twitter Lite や日経電子版の web サイトなど)\n How we built Twitter Lite いまさら聞けない PWA と AMP - Qiita  ","permalink":"https://shunyaueta.com/posts/2018-01-09/","summary":"PWA と FireBase を試してみたかった\nfirebase init で現れる画面、テンション上がる\nGithub Pages + CloudFlare で独自ドメインの shunyaueta.com をホスティングしてたんですが、Firebase でホスティングできると聞いて Firebase に移行しました。\nPWA にしたのは完全に趣味です。\nTL;DR;  Web App を作ってる人は manifest.json を設置するだけでも Android の使用感が改善されそう 独自ドメインでお手軽に SSL ホスティングしたいなら Firebase hosting めっちゃおすすめです(1GB のホスティングは無料) FireBase Hosting だけだと Firebase 本来の旨味は味わえません  PWA 1 年前ですが、簡潔に PWA の事が書かれています\nプログレッシブウェブアプリ詳解 ─ 過去・現在・未来\n左: PWA 化以前 右: PWA 化以降\nServiceWorker と manifest.json,あとは\u0026lt;meta name=”theme-color”\u0026gt;を指定すると PWA のスコアが 100 点になる 🎉\nmanihest.json によるホームアイコン作成誘導\nFavicon の各画像の生成は下記のサイトが便利でした。要求される解像度毎の画像(Favicon,Home icon, Apple home icon)が生成されて mahifest.","title":"サイトのPWA化、ホスティングをGithub PagesからFirebaseへ移行"},{"content":"pyadmin4 で Heroku 上の DB に接続する記事が日本語になかったので、メモ\n接続前の準備 Heroku にログインして、対象の App の DB のページへ\nHeroku App DB page\nそこから DB のセッティングページにある credential ボタンをクリック\nclick credential button\nそこに記載されている各種情報を pgadmin4 に入力して Heroku 上の DB に接続する\nCopy information\npgadmin4 で Heroku の DB に接続 以下のページから pgAdmin4 をダウンロード\nDownload\nそこからアプリを開くと下記の画面になるので、Add new Serverをクリック\nClick Add New Server\nHeroku 上の DB の情報を入力していく。Server の名前は適当で大丈夫です。\n接続されるとこんな感じになります。\nQuery Tool Query Toolを使うことで Heroku 上の DB に対して SQL クエリを投げる事ができます。\nQuery Toolは上部のツールバーからアクセス可能です。 注意) 左カラムのテーブルをクリックした後でないとアクティブになりません。\nSQL の実行結果\n雷ボタンで SQL を実行可能 ↓ ボタンで SQL 結果をローカルに CSV 形式で保存できます。\n","permalink":"https://shunyaueta.com/posts/2017-12-27/","summary":"pyadmin4 で Heroku 上の DB に接続する記事が日本語になかったので、メモ\n接続前の準備 Heroku にログインして、対象の App の DB のページへ\nHeroku App DB page\nそこから DB のセッティングページにある credential ボタンをクリック\nclick credential button\nそこに記載されている各種情報を pgadmin4 に入力して Heroku 上の DB に接続する\nCopy information\npgadmin4 で Heroku の DB に接続 以下のページから pgAdmin4 をダウンロード\nDownload\nそこからアプリを開くと下記の画面になるので、Add new Serverをクリック\nClick Add New Server\nHeroku 上の DB の情報を入力していく。Server の名前は適当で大丈夫です。\n接続されるとこんな感じになります。\nQuery Tool Query Toolを使うことで Heroku 上の DB に対して SQL クエリを投げる事ができます。\nQuery Toolは上部のツールバーからアクセス可能です。 注意) 左カラムのテーブルをクリックした後でないとアクティブになりません。","title":"HerokuのDBにpgadmin4で接続してローカルにデータをダウンロードする"},{"content":"自己符号化器と Spectral Clusteing の関連性を示した論文\n Paper link  Author  Fei Tian : http://home.ustc.edu.cn/~tianfei/ 中国科学技術大学 Ph.D １年生 MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。 この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%) MSRA @ ILSVRC2015 資料  Motivation (研究背景・動機)  Deep Learning が数多くの応用でめざましい成果をあげている。 音声認識 画像認識 自然言語処理 DL において Clustering に関する適切な調査が行われてない。 論文の目的として、DL における Clustering の調査を行う  概要  Graph Clustering はクラスタリングでも重要な手法の一つ Graph Clustering の応用 Image segmentation Community Detection VLSI Design 嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能 自己符号化器と Spectral Clustering の類似性 Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。 自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する 計算量 : 対象とするグラフは n 個のノードを持つ EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量 自己符号化器 : ノードがスパースな際は計算量は O(kn) スパース表現 : データが大きくなるならスパース性を有効活用したい EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない 自己符号化器 : スパース性を用いるのは容易  既存研究で未検証な事柄、何を解決・解明したいのか？  Graph Clustering のための Deep Learning の活用方法と調査 自己符号化器と Spectral Clustering の類似性とその比較・検証  Method (提案手法)  GraphEncoder(for graph clustering.) グラフラプラシアン(L=D−1S)に対してスパース自己符号化器(Sparse-AutoEncoders)を通した最終層に k-meas クラスタリングを行う AutoEncoder は入力層・隠れ層・出力層の 3 層を Stacked する。 X(j)=D−1S の列ベクトルを各ユニットに入力し、隠れ層の活性化関数 h(j)を得て、X(j+1)=h(j)と Γ 回(層の階数分)更新する。  Evaluation (検証方法・評価方法・優位性)  実世界のグラフデータに対して NMI によってクラスタリングの評価実験を行う。 データの種類 ワイン ニュース記事 タンパク質構造グラフ  以下の三種で比較\n Spectral Clustering k-means Sparse-AutoEncoders(Graph-Encoder)  次元の減少推移\n層を重ねる毎に NMI が向上している。\nConclusion (結論・貢献)  Deep Learn と Graph Clustering の関係性、結果を調査。 GraphEncoder の嬉しいところ 隠れ層の次元は入力層の次元より低い。これは全てのエッジが必須ではないことを直感的に示す。 エッジの除去を行いグラフ表現を更に明確にするために、浅い層から深い層へ。 EVD の計算量は最速でも O(n2.367)で、グラフは密なグラフ表現。(Toeplitz Matirix) GraphEncoder は O(ncd)、d は隠れ層の最大次元、c はグラフの平均次元。(例: 各ノードが k 本のエッジを持つ類似度グラフの場合 c=k。ソーシャルグラフで表すと、c は友達の平均の数を示す。) EVD は並列化が困難。確率的勾配降下法(SGD)は EVD と比べると並列化が容易である。  提案手法の限界(残された課題) 実行時間の比較が行われていないが、あくまでこの論文の価値は DL と Graph Clusetering の関連性を示しているのが価値なのでそこは許して下さいって感じ。### Comments (疑問点・わからなかったところ・議論)\n トップカンファレンスを年 2 本、2nd tier を 1 本 1st で出せるのは、どうやればそのレベルに到達するんだ? トレンドに乗った良い論文。 Good Writing. 内容もシンプルなので 90 分でサクッと気持よく読めた。論文読むより、スパース自己符号化の勉強に時間取られた。 Corollary2 で ~ symmetrix matrix D−1S って言ってる割に行列の対称性は保証されてないので 3.1 全般が怪しい、辻褄があってない。  ","permalink":"https://shunyaueta.com/posts/2017-12-23/","summary":"自己符号化器と Spectral Clusteing の関連性を示した論文\n Paper link  Author  Fei Tian : http://home.ustc.edu.cn/~tianfei/ 中国科学技術大学 Ph.D １年生 MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。 この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%) MSRA @ ILSVRC2015 資料  Motivation (研究背景・動機)  Deep Learning が数多くの応用でめざましい成果をあげている。 音声認識 画像認識 自然言語処理 DL において Clustering に関する適切な調査が行われてない。 論文の目的として、DL における Clustering の調査を行う  概要  Graph Clustering はクラスタリングでも重要な手法の一つ Graph Clustering の応用 Image segmentation Community Detection VLSI Design 嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能 自己符号化器と Spectral Clustering の類似性 Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。 自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する 計算量 : 対象とするグラフは n 個のノードを持つ EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量 自己符号化器 : ノードがスパースな際は計算量は O(kn) スパース表現 : データが大きくなるならスパース性を有効活用したい EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない 自己符号化器 : スパース性を用いるのは容易  既存研究で未検証な事柄、何を解決・解明したいのか？  Graph Clustering のための Deep Learning の活用方法と調査 自己符号化器と Spectral Clustering の類似性とその比較・検証  Method (提案手法)  GraphEncoder(for graph clustering.","title":"“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ"},{"content":"JupyterNotebook をリモートサーバー上で公開して、どこでも研究開発。\n講義で Jupyterhub を利用するお話です。\nGIF 画像は下記の記事で知ったtqdmというパッケージを使いたくなったので載せてみた。\n 私が選ぶ 2015 年の”新しい”Python モジュール トップ 5 IPython データサイエンスクックブック ―対話型コンピューティングと可視化のためのレシピ集  IPython データサイエンスクックブックをキッカケに研究室でも JupyterNotebook の凄さを皆が知り、MATLAB の kernel を通して利用を始めたりしています。自分は Python2→MATLAB→MATLAB \u0026amp; Python3 という流れで移り変わっています。\nJupyterNotebook をリモートサーバー上で公開 コードは以下の通りです。特に問題なく公開することができました。\n環境  CentOS 7 系  下記の記事を参考にセットアップする。\n pyenv と virtualenv で環境構築  今回は pyenv を使って Python3.5.1 でホストしています。\n昨日この記事を読んで、Anaconda がオススメされているので今度セットアップするときに使ってみよう。\n Running a public notebook server | JupyterNotebook Docment  こまかい設定等は以下の記事で説明されています。IPython Notebook を対象にした記事ですが、ほとんど一緒なので問題ありません。(config.py 自体がコメントで丁寧に各設定が記述されています。)\n IPython notebook サーバーを立ち上げる ipython notebook をリモートサーバ上で動かす。  参考記事  iPython notebook で研究開発生活  Juptyerhub : 講義で Jupyter を利用する。 JupyterNotebook を講義でも活用できるようにならないかなと先生と探していたのですが、Jupyternotebook を公開するだけだとユーザー管理が不可能です。例えば ~tarou/というディレクトリで jupyternotebook を公開すると~tarou/に notebook が沢山できだれがどのノートを作ったのかが把握できないという問題点があります。\nその問題点を解消するのがJupyterHubです。\n Jupyterhub  Jupyterhub は標準のユーザー認証でPAMを利用しているので、Cent OS のユーザー名とパスワードでログインすることができます。\n2016/01/29 現時点で、PAM の認証方式だと上手く認証されるときとされないときが存在しています。\nPAM 以外の認証方法としてはAuthで記述されているが以下の\n GitHub/BitBucket/MediaWiki/CILogon OAuth Google OAuth Local+Google OAuth (from within a Docker container) LDAP Authenticator  が用意されている。大学で LADP サーバーが公開されているならそれと連携するのが一番だが、Github などを認証に使うのも良いんじゃないかと。\n[余談]Jupyterhub の記事を探していて見つけたこの記事が面白い。UC バークレーの Ph.D の学生が教育用にどう Jupyterhub を活用しているか。そして運用の段階でどのような問題がでてきたかが書かれている。海外の大学は進んでいるな~\n Deploying JupyterHub for Education  参考記事  CentOS6.6(64bit)に JupyterHub をインストールする  ","permalink":"https://shunyaueta.com/posts/2017-12-22/","summary":"JupyterNotebook をリモートサーバー上で公開して、どこでも研究開発。\n講義で Jupyterhub を利用するお話です。\nGIF 画像は下記の記事で知ったtqdmというパッケージを使いたくなったので載せてみた。\n 私が選ぶ 2015 年の”新しい”Python モジュール トップ 5 IPython データサイエンスクックブック ―対話型コンピューティングと可視化のためのレシピ集  IPython データサイエンスクックブックをキッカケに研究室でも JupyterNotebook の凄さを皆が知り、MATLAB の kernel を通して利用を始めたりしています。自分は Python2→MATLAB→MATLAB \u0026amp; Python3 という流れで移り変わっています。\nJupyterNotebook をリモートサーバー上で公開 コードは以下の通りです。特に問題なく公開することができました。\n環境  CentOS 7 系  下記の記事を参考にセットアップする。\n pyenv と virtualenv で環境構築  今回は pyenv を使って Python3.5.1 でホストしています。\n昨日この記事を読んで、Anaconda がオススメされているので今度セットアップするときに使ってみよう。\n Running a public notebook server | JupyterNotebook Docment  こまかい設定等は以下の記事で説明されています。IPython Notebook を対象にした記事ですが、ほとんど一緒なので問題ありません。(config.py 自体がコメントで丁寧に各設定が記述されています。)\n IPython notebook サーバーを立ち上げる ipython notebook をリモートサーバ上で動かす。  参考記事  iPython notebook で研究開発生活  Juptyerhub : 講義で Jupyter を利用する。 JupyterNotebook を講義でも活用できるようにならないかなと先生と探していたのですが、Jupyternotebook を公開するだけだとユーザー管理が不可能です。例えば ~tarou/というディレクトリで jupyternotebook を公開すると~tarou/に notebook が沢山できだれがどのノートを作ったのかが把握できないという問題点があります。","title":"JupyterNotebookをリモートサーバー上で公開して、どこでも研究開発 \u0026 講義でJupyterhubを利用する"},{"content":"TensorFlow 無双\nTensorFlow Lite meets CoreML!!\n個人的にいま興味ある分野のうちの一つがスマホで動く機械学習なんですが、昨日 TensorFlow Lite が CoreML でサポートされるというアナウンスがありました!\nAnnouncing Core ML support in TensorFlow Lite\nCoreML の最大の利点は iPhone のアーキテクチャを最大限に利用した推論の高速化なので、Google も何かしらの手を打ってくると思っていましたがまさかそのまま CoreML にサポートされたのは驚きです。\n個人的に keras2, Caffe¹だけがサポートされてる今の状態は選択肢が少なくて微妙だなと思っていたので良いことだと思います。\n少し横道にそれますが、ONNX と呼ばれる Machine Leaning のモデルを相互変換できるプロジェクトも立ち上がっているので、近いうちにフレームワーク間の差異は消えていき、書きたいフレームワークで書き、動かしたい環境にモデルを変換して運用するという流れになる未来がくるかもしれません。\nONNX: Open Neural Network Exchange Format\nPixel²も iPhone8³以降に搭載されている A11 チップに機械学習の計算を高速化させるチップが採用されているのでこれから Machine Learning on Mobile はドンドン加速していくとおもいます。iOS11 の吉田さんが担当している CoreML の章を見てましたが、利点と欠点が明快に知れるのでオススメです。\niOS 11 Programming - PEAKS\nTensorFlow Lite もデフォルトで Android をサポートしているので、こりゃほんとにプロダクション環境だと TensorFlow 一択になりつつありますね\nにしても TensorFlow の勢いはほんとに凄い…\n","permalink":"https://shunyaueta.com/posts/2017-12-06/","summary":"TensorFlow 無双\nTensorFlow Lite meets CoreML!!\n個人的にいま興味ある分野のうちの一つがスマホで動く機械学習なんですが、昨日 TensorFlow Lite が CoreML でサポートされるというアナウンスがありました!\nAnnouncing Core ML support in TensorFlow Lite\nCoreML の最大の利点は iPhone のアーキテクチャを最大限に利用した推論の高速化なので、Google も何かしらの手を打ってくると思っていましたがまさかそのまま CoreML にサポートされたのは驚きです。\n個人的に keras2, Caffe¹だけがサポートされてる今の状態は選択肢が少なくて微妙だなと思っていたので良いことだと思います。\n少し横道にそれますが、ONNX と呼ばれる Machine Leaning のモデルを相互変換できるプロジェクトも立ち上がっているので、近いうちにフレームワーク間の差異は消えていき、書きたいフレームワークで書き、動かしたい環境にモデルを変換して運用するという流れになる未来がくるかもしれません。\nONNX: Open Neural Network Exchange Format\nPixel²も iPhone8³以降に搭載されている A11 チップに機械学習の計算を高速化させるチップが採用されているのでこれから Machine Learning on Mobile はドンドン加速していくとおもいます。iOS11 の吉田さんが担当している CoreML の章を見てましたが、利点と欠点が明快に知れるのでオススメです。\niOS 11 Programming - PEAKS\nTensorFlow Lite もデフォルトで Android をサポートしているので、こりゃほんとにプロダクション環境だと TensorFlow 一択になりつつありますね\nにしても TensorFlow の勢いはほんとに凄い…","title":"CoreMLがTensorFlow Liteをサポート"},{"content":"You can intuitively lean Power Iteration by Visualization Power.\nAnimation of Power Iteration by MATLAB\nPower iteration - Wikipedia\nFinally you put a command line\n1  \u0026gt; convert -layers optimize -loop 0 -delay 40 eigenvector*.png anim.gif   ","permalink":"https://shunyaueta.com/posts/2017-12-05/","summary":"You can intuitively lean Power Iteration by Visualization Power.\nAnimation of Power Iteration by MATLAB\nPower iteration - Wikipedia\nFinally you put a command line\n1  \u0026gt; convert -layers optimize -loop 0 -delay 40 eigenvector*.png anim.gif   ","title":"Visualized Approximate Eigen Vector by Power Iteration on 3 dimensions."},{"content":"応用数理研究者が機械学習界に進出していく研究\nyoutube clip\n応用数理界隈ではクラシックな解き方で PageRank が解かれているので最新の数値計算手法に置き換えることで劇的にパフォーマンスが向上して 10 年前のパフォーマンスの鎖を解き放った論文\n KDD2015 Best student paper award Slide(PDF) Poster(PDF) Spectral network analysis  wenleix/EdgePPR\n Presentation Movie is uploaded in Youtube.  Author\n W. Xie Ph.D Candidate at Cornell University http://wenleix.github.io/ iterative computation on big graph data D. Bindel http://www.cs.cornell.edu/~bindel/ http://www.cs.cornell.edu/~bindel/talks.html He is frequently research activ like ideal young researcher. Nonlinear eigenvalue problem. Alan J. Demers Prof. Johannes Gehrke http://www.cs.cornell.edu/johannes/ Prof. VLDB,SIGMOD,KDD  Motivation  ページランクは重要な指標。遷移確率を求めるにはランダムサーファーモデル(ランダムウォーク)が必要。 様々な高速解法が提案されている。  Reseatch Question  しかし 10 年以上前、PageRank の黎明期から Personalization based の手法は問題がある。 一般的な PageRank の解法の説明、その後に Model Reduction をベースにした約 5 倍の性能を誇る提案手法を説明。 提案手法の性能によって、パフォーマンス上のボトムネックは消えた。  Proposed Method  PageRank 初期:WEBPage のランクに使用 → 現在:推薦、ソーシャルネットワーク Random Suffer Model Transition : α の確率で Random Walk(滞在ノードから無作為に遷移) Teleporting : 1−α Random Junmp(滞在ノードに依存せずに全てのノードを対象に無作為遷移) x(t+1)=αPxt+(1−α)v,where P=AD−1 v is represents telepotation probabilitie xt Walker の確率分布 サーファーが来る確率 = RandomWalk の確率_RandomWalk による遷移確率 + RandomJump の確率_RandomJump の遷移確率 = α× RandomWalk による遷移確率 + (1−α)×RandomJump の遷移確率 = α×+(1−α)×v x: Stationary vector(不動ベクトル、定常ベクトル、不動点定理など) x が定常状態になった際に、x の確率分布が PageRank を表す。 x(t),x(t+1)が同一(残差が無い)だと仮定することで、次式の線形方程式を解くことで Pagerank を求める。 Mx=b,where M=(I−αP),b=(1−α)v.  Standard PageRank  グラフの構造(幾何性)を利用 多くのグラフでは、ノードやエッジに重みが存在する  Personalized PageRank  PageRank を特定のユーザやクエリに合わせたランク Node wighted graph Mx(w)=(1−α)v(w),w∈ℝ𝕕 w: 特定の個人、クエリに合わせたパラメータ Edge weighted graph M(w)x(w)=(I−α)v,w∈ℝ𝕕 Node Weight, Edge Weight 共に Personalized PageRank は重要だが、２つの問題がある。 Node Weight に関する論文:多い Edge Weight:少ない 計算コストが高い Edge weighted Personalized PageRank における x(w)を求める高速手法を提案 提案手法は計算量が準線形的。 予備知識 xt+1:べき乗法 Mx=b:ヤコビ法 x(w)=(1−α)M−1Vw. v≈Vwwhere V∈ℝ𝕟×𝕕 M−1V:O(n) 、x(w)の再構築に O(d)、PageRank を求めるには O(nd)が要求される。 v が疎なら計算量は準線形へ。  MODEL RESUCTION METHOD  k 次元の次元の削減された空間を構築 k 次元を近似するために k 個の等式が必要 次元削減の問題を解く(PageRank のベクトルは一部分が重要だという仮説)   Reduced Space Construction {w(j)}rj=1(ベクトルの集合)に対応する PageRank {x(j)}rj=1(ベクトルの集合)を求める。 w は乱数によって決定する k 次元の空間 υ を探索する  ","permalink":"https://shunyaueta.com/posts/2017-12-04/","summary":"応用数理研究者が機械学習界に進出していく研究\nyoutube clip\n応用数理界隈ではクラシックな解き方で PageRank が解かれているので最新の数値計算手法に置き換えることで劇的にパフォーマンスが向上して 10 年前のパフォーマンスの鎖を解き放った論文\n KDD2015 Best student paper award Slide(PDF) Poster(PDF) Spectral network analysis  wenleix/EdgePPR\n Presentation Movie is uploaded in Youtube.  Author\n W. Xie Ph.D Candidate at Cornell University http://wenleix.github.io/ iterative computation on big graph data D. Bindel http://www.cs.cornell.edu/~bindel/ http://www.cs.cornell.edu/~bindel/talks.html He is frequently research activ like ideal young researcher. Nonlinear eigenvalue problem. Alan J. Demers Prof. Johannes Gehrke http://www.cs.cornell.edu/johannes/ Prof. VLDB,SIGMOD,KDD  Motivation  ページランクは重要な指標。遷移確率を求めるにはランダムサーファーモデル(ランダムウォーク)が必要。 様々な高速解法が提案されている。  Reseatch Question  しかし 10 年以上前、PageRank の黎明期から Personalization based の手法は問題がある。 一般的な PageRank の解法の説明、その後に Model Reduction をベースにした約 5 倍の性能を誇る提案手法を説明。 提案手法の性能によって、パフォーマンス上のボトムネックは消えた。  Proposed Method  PageRank 初期:WEBPage のランクに使用 → 現在:推薦、ソーシャルネットワーク Random Suffer Model Transition : α の確率で Random Walk(滞在ノードから無作為に遷移) Teleporting : 1−α Random Junmp(滞在ノードに依存せずに全てのノードを対象に無作為遷移) x(t+1)=αPxt+(1−α)v,where P=AD−1 v is represents telepotation probabilitie xt Walker の確率分布 サーファーが来る確率 = RandomWalk の確率_RandomWalk による遷移確率 + RandomJump の確率_RandomJump の遷移確率 = α× RandomWalk による遷移確率 + (1−α)×RandomJump の遷移確率 = α×+(1−α)×v x: Stationary vector(不動ベクトル、定常ベクトル、不動点定理など) x が定常状態になった際に、x の確率分布が PageRank を表す。 x(t),x(t+1)が同一(残差が無い)だと仮定することで、次式の線形方程式を解くことで Pagerank を求める。 Mx=b,where M=(I−αP),b=(1−α)v.","title":"Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrier を読んだ"},{"content":"機械学習の研究してる人は全員読んだ方がいい。そう断言できるぐらい良い内容が書かれています。 ICML2012 で発表された最近の機械学習に関する研究の問題点を論じた論文。\nSummaly\n「あなたは現実世界で役立つデータに対して機械学習の研究を行っていますか?」\n著者は NASA の JPL(ジェット推進研究所)、カリフォルニア工科大学に所属する Kiri L.\nKiri L. Wagstaff\n発表動画を探してみたんですが、ICML2012 で発表した際のビデオはサーバのクラッシュにより喪失したとのこと。\n The video for my controversial ICML 2012 talk is no longer available (lost in a server crash). However, you can read the original paper: Machine Learning that Matters (pdf, 6 pages, 234K) and see the slides from a subsequent invited AAAI talk: Challenges for Machine Learning Impact on the Real World (1.6M). PowerPoint は下記リンク先に配布されています。http://www.wkiri.com/research/papers/wagstaff-MLmatters-slides-AAAI.pptx\n 時間のある人は是非原著論文を読んでみてください!\n概要 最近の機械学習は問題を社会・科学世界から持ち込むための深刻なギャップがある。 データセットに明確な制約をかけた物を私達は調査し、それを評価指標のために利用する。 データセットがある問題領域に戻って考えよう。 機械学習が持つインパクト(影響)を増加するためは私達の研究で何を変えればいいだろう？ 我々は機械学習の問題について焦点と議論を交わす。\n大事なところは全て下記の記事が抑えてくれてるので、時間の無駄だと思うので訳すのはやめます。\n＊下記の記事がキッカケでこの論文を知りました。 日本語で詳しくこの論文が紹介されていて、この記事を読めば大事な事が僕のメモよりはるかに分かりやすく書かれています。\nMachine Learning that Matters(ICML 2012) 読んだ - 糞ネット弁慶\nコメント 機械学習に関する意味ある応用と理論[¹] というよりも、科学と工学の研究の違いについて、考える後押しをくれた論文でした。 大学院で科学寄りの講義を受けていると、「じゃあ、工学の意味ってなんだろう?」と考える事が多くなりました。\n今まで見聞きした中で工学の説明で一番しっくりきたのは、昔東大工学部のサイトに掲載されていた下記の言葉です。\n「工学とは意思を実現する学問である」\n他にこの論文について取り上げられている記事。 優秀な研究者がエンジニアに流れているのは日本だけじゃないのかも\n上記の記事の統計学者ブラッドリー・エフロンが述べている言葉は、じんわりと自分に染み渡ってきました。\n 計算機科学の世界には、我々がデータを計算機に入れさえすれば正しい答えが出てくるのだという誤解が向けられている。いまやさまざまな分野で、例えば天文学などがそうだが、多くの部分を計算機とシミュレーションに負うようになってきていて、彼らは自然そのものではなく、シミュレーションなどを見て議論をするようになっている。これは我々統計学者の状況と同じだ。彼らはどんどん二次科学に近付いている。これは、彼らも私たちと同じトラブルに見舞われていること意味する。「何が正しくて何が間違っているのか」。私はたまに思うのだが、科学の歴史の中で、まず目に見える簡単な問題から解かれていった。そこには確率も統計も必要がなかった。科学はそういう簡単な問題をどんどん征服していっているのではないかとも思える。今科学は非常に複雑な問題について研究をし始めている。「自然の中の科学」と「自然を越えた科学」の両方がある中で、我々は２番目のものに向かっているのではないだろうか。\n 引用しか残っておらず、本体記事の方は消えているのが残念です。\n","permalink":"https://shunyaueta.com/posts/2017-12-01/","summary":"機械学習の研究してる人は全員読んだ方がいい。そう断言できるぐらい良い内容が書かれています。 ICML2012 で発表された最近の機械学習に関する研究の問題点を論じた論文。\nSummaly\n「あなたは現実世界で役立つデータに対して機械学習の研究を行っていますか?」\n著者は NASA の JPL(ジェット推進研究所)、カリフォルニア工科大学に所属する Kiri L.\nKiri L. Wagstaff\n発表動画を探してみたんですが、ICML2012 で発表した際のビデオはサーバのクラッシュにより喪失したとのこと。\n The video for my controversial ICML 2012 talk is no longer available (lost in a server crash). However, you can read the original paper: Machine Learning that Matters (pdf, 6 pages, 234K) and see the slides from a subsequent invited AAAI talk: Challenges for Machine Learning Impact on the Real World (1.6M). PowerPoint は下記リンク先に配布されています。http://www.","title":"Machine Learning that Matters (ICML2012) を読んだ"},{"content":"アイデアは面白い… けど easy drawing ではない\nJupyter 使ってると作図も Jupyter 上で完結させたいなぁ~って思うときがあるんですが、スクリプトで作図はけっこう辛いものがあります\nそのため Jupyter 上でフリースタイルに作図できる機能ないかなと探してたら egal という面白そうな拡張機能があったので使ってみました\nuclmr/egal\negal GIF animation\n以下のリポジトリから $pip3 install git+https://github.com/uclmr/egal.gi\nでクローンしてきて $jupyter nbextension install --py egal $jupyter nbextension enable --py egal\nで拡張機能を有効にして使えるようになります。\nブラシアイコンをクリックすると新たなセルが生成される\nボタンをクリックすると各オブジェクトの詳細なプロパティが調整できる\nフレーム毎にオブジェクトを設定してアニメーションっぽくもできる\n5–6 分使ってみて感じましたが、めちゃくちゃ操作がしづらい…\nやはりブラウザ上での図形作成はめちゃくちゃストレスたまるので、ローカルで keynote 使って図形作成したほうがマシな感じです。\n遊んだ結果を notebook で github にアップしておきました。\n残念ながら SVG が Github 上ではレンダリングされないので残念な感じになっております\u0026hellip; ローカルにクローンしてきて egal を有効にしておくと見れます。\nhurutoriya/notebook\n結論 Jupyter で全てを完結させるのは難しい\n","permalink":"https://shunyaueta.com/posts/2017-11-22/","summary":"アイデアは面白い… けど easy drawing ではない\nJupyter 使ってると作図も Jupyter 上で完結させたいなぁ~って思うときがあるんですが、スクリプトで作図はけっこう辛いものがあります\nそのため Jupyter 上でフリースタイルに作図できる機能ないかなと探してたら egal という面白そうな拡張機能があったので使ってみました\nuclmr/egal\negal GIF animation\n以下のリポジトリから $pip3 install git+https://github.com/uclmr/egal.gi\nでクローンしてきて $jupyter nbextension install --py egal $jupyter nbextension enable --py egal\nで拡張機能を有効にして使えるようになります。\nブラシアイコンをクリックすると新たなセルが生成される\nボタンをクリックすると各オブジェクトの詳細なプロパティが調整できる\nフレーム毎にオブジェクトを設定してアニメーションっぽくもできる\n5–6 分使ってみて感じましたが、めちゃくちゃ操作がしづらい…\nやはりブラウザ上での図形作成はめちゃくちゃストレスたまるので、ローカルで keynote 使って図形作成したほうがマシな感じです。\n遊んだ結果を notebook で github にアップしておきました。\n残念ながら SVG が Github 上ではレンダリングされないので残念な感じになっております\u0026hellip; ローカルにクローンしてきて egal を有効にしておくと見れます。\nhurutoriya/notebook\n結論 Jupyter で全てを完結させるのは難しい","title":"Jupyter上でSVGのイラストやアニメーションが作成できるプラグイン egel"},{"content":"OpenCV と MobileNet を使って物体検出を行った\nObject Detection with OpenCV dnn modules and MobileNetSSD on Jupyter Notebook\nIntroduction 物体検出を Deep Leaning と OpenCV を用いて行う\nOpenCV 3.3 からdnnモジュールが正式にリリースされた\n The main news is that we promoted DNN module from opencv_contrib to the main repository, improved and accelerated it a lot. An external BLAS implementation is not needed anymore. For GPU there is experimental DNN acceleration using Halide (http://halide-lang.org_). The detailed information about the module can be found in our wiki: Deep Learning in OpenCV.\n_https://opencv.org/opencv-3-3.html\n 今回はその dnn モジュールを使って物体検出を行う\n OpenCV dnn modules Official Sample  この記事は主にpyimagesearch の記事を参考に行いました。詳細な解説がありがたいです Deep Leaning を基にした物体検出で有名な手法は以下の 3 つである\n Faster R-CNN : 2015 YOLO : 2015 SSDs : 2015  物体検出を行う際の DNN のアーキテクチャでは主に VGG か ResNet が用いられる。しかし欠点としてこれらのアーキテクチャは非常に大きく 200–500MB のサイズになってしまう\n物体検出において速度、精度、メモリの３つのバランスを考慮する場合、どのアーキテクチャを選ぶべきかを Google が論文としてまとめている\n[1611.10012] Speed/accuracy trade-offs for modern convolutional object detectors\n一行でまとめると、基本的に速度と精度はトレードオフであり、最速は SSDs MobileNet, 最高精度は Faster R-CNN w/Inception Resnet at stride 8\n今回は省メモリかつ速度が早い SSDs MobileNet を用いて物体検出を行った\nDemo on JupyterNotebook\nCode hurutoriya/yolov2_api\nStudy  Print デバッグをやめて、logging module に切り替えた。出力が綺麗になっていい感じ OpenCV だけで DNN を使えるのは結構便利(学習済モデルとかは必須)  Future Work  YOLO v2 も C++のサンプルがあるので、Python でも動くようにしたい Web App に組み込めるくらい速そうなので、やってみましょう  ","permalink":"https://shunyaueta.com/posts/2017-11-14/","summary":"OpenCV と MobileNet を使って物体検出を行った\nObject Detection with OpenCV dnn modules and MobileNetSSD on Jupyter Notebook\nIntroduction 物体検出を Deep Leaning と OpenCV を用いて行う\nOpenCV 3.3 からdnnモジュールが正式にリリースされた\n The main news is that we promoted DNN module from opencv_contrib to the main repository, improved and accelerated it a lot. An external BLAS implementation is not needed anymore. For GPU there is experimental DNN acceleration using Halide (http://halide-lang.org_). The detailed information about the module can be found in our wiki: Deep Learning in OpenCV.","title":"OpenCV 3.3から使えるDNNモジュールを使って物体検出"},{"content":"DEMO GitHub でコードを公開してます。\nhurutoriya/face_detector_api\nDjango の勉強は、基本的なイントロダクションとしてオフィシャルサイトのドキュメントが充実しているのでオススメです。\npyimagesearch の Blog 記事で最小限の構成で顔検出を行う API サーバーを作る記事があり、今回はそれを基本に作成した。\n以下所感\n Django は Rails と比べるとそんなにレールが敷かれていない 日本語の記事がほぼ存在しないので、英語の記事を読む良い練習になった OpenCV や Scikit-lean がそのまま動くのは相当魅力的で、サーバからのレスポンスが帰ってきた時には地味に感動 API 設計や非同期処理なんかの知識が全く足りない  次の課題 今回の発展形として django-rest-framework を使って、モデルを組み込んで作り上げて Google Apps Engine 上で公開してみよう。 REST Framework はこの記事2を参考に画像をアップロードできる雛形は作り上げた。 後は OpenCV で処理を施す部分を書き上げたらいけそう。\ndjango-rest-framework で使える管理画面\nReferences  hurutoriya/face_detector_api Django REST Framework を使って爆速で API を実装する,ChristianKreuzberger/django-rest-imageupload-example Creating a face detection API with Python and OpenCV (in just 5 minutes) Django 1.11 Documentation Django REST framework is a powerful and flexible toolkit for building Web APIs.  ","permalink":"https://shunyaueta.com/posts/2017-11-13/","summary":"DEMO GitHub でコードを公開してます。\nhurutoriya/face_detector_api\nDjango の勉強は、基本的なイントロダクションとしてオフィシャルサイトのドキュメントが充実しているのでオススメです。\npyimagesearch の Blog 記事で最小限の構成で顔検出を行う API サーバーを作る記事があり、今回はそれを基本に作成した。\n以下所感\n Django は Rails と比べるとそんなにレールが敷かれていない 日本語の記事がほぼ存在しないので、英語の記事を読む良い練習になった OpenCV や Scikit-lean がそのまま動くのは相当魅力的で、サーバからのレスポンスが帰ってきた時には地味に感動 API 設計や非同期処理なんかの知識が全く足りない  次の課題 今回の発展形として django-rest-framework を使って、モデルを組み込んで作り上げて Google Apps Engine 上で公開してみよう。 REST Framework はこの記事2を参考に画像をアップロードできる雛形は作り上げた。 後は OpenCV で処理を施す部分を書き上げたらいけそう。\ndjango-rest-framework で使える管理画面\nReferences  hurutoriya/face_detector_api Django REST Framework を使って爆速で API を実装する,ChristianKreuzberger/django-rest-imageupload-example Creating a face detection API with Python and OpenCV (in just 5 minutes) Django 1.11 Documentation Django REST framework is a powerful and flexible toolkit for building Web APIs.","title":"Djangoで顔認識の結果をJSONで返す最小構成のAPIサーバーを作った"},{"content":"Latex の煩わしい点が全て解決される Mac のソフトウェアです。\n 自動補完 synctex 対応(コマンド+クリックで PDF、tex ファイル同期) 自動タイプセット Texpad · Smoothest way to write LaTeX  以前は atom+latexmk で Latex を扱ってましたが、texpad の方が断トツに使い心地が最高です。\n購入方法 Appstore か、クレジットカードで Appstore を経由せずに買うかの２つの方法があります。\nAppstore はだいぶ前に更新を停止しているみたいなので、クレジットカードを持っている人はクレジットカードを使って MacAppstore を経由せずに購入することをオススメします。\n$24.99 しましたが、それ以上の価値があるソフトウェアです。\n2 週間の無料体験期間があるので是非お試し下さい。\n下記のスクリーンショットのように、beamer も texpad で動くのは感動モノ。\nbeamer on texpad\n日本語で TexPad を扱いたいときは下記のスクリプトを読み込んだら、bibtex のバグが治ります。\n","permalink":"https://shunyaueta.com/posts/2017-10-08/","summary":"Latex の煩わしい点が全て解決される Mac のソフトウェアです。\n 自動補完 synctex 対応(コマンド+クリックで PDF、tex ファイル同期) 自動タイプセット Texpad · Smoothest way to write LaTeX  以前は atom+latexmk で Latex を扱ってましたが、texpad の方が断トツに使い心地が最高です。\n購入方法 Appstore か、クレジットカードで Appstore を経由せずに買うかの２つの方法があります。\nAppstore はだいぶ前に更新を停止しているみたいなので、クレジットカードを持っている人はクレジットカードを使って MacAppstore を経由せずに購入することをオススメします。\n$24.99 しましたが、それ以上の価値があるソフトウェアです。\n2 週間の無料体験期間があるので是非お試し下さい。\n下記のスクリーンショットのように、beamer も texpad で動くのは感動モノ。\nbeamer on texpad\n日本語で TexPad を扱いたいときは下記のスクリプトを読み込んだら、bibtex のバグが治ります。","title":"TexPadのおかげでLatex人生が変わりました"},{"content":"株式会社 ABEJA という会社に 2013/8/25~2013/9/26 間に 2 週間弱インターンシップに参加してきました。\n2013 年当時のサイトのスクショ\nWhat’S ABEJA? ABEJA について詳しく知りたい方は下記のリンクを見るとわかりやすいです。\n マーケティングから決済まで、人認証技術 × ビジネスプロデュースで未来を変える ABEJA の挑戦【連載:NEO ジェネ！】 いまそこにある、リアルタイムデータ解析。ぼくらの暮らしを変える、日本のスタートアップ 3 社  この会社をはじめて知ったのは、ミクシィでのインターンシップに参加してた時に別グループでインターンしていた知り合いの人に教えてもらったのがキッカケです。 機械学習や画像解析を用いて、現実世界の問題を解決して更にビジネスにまで昇華させている点に痺れました。\n本来なら一ヶ月近く参加予定だったんですが、もう一つ挑戦していたリクルートのインターンに運良く合格したので結果的には 2 週間弱という短めのインターンになりました。\nWhat do you doing in ABEJA intern? インターンの内容は Python を使って社内用のツールをゴニョゴニョしてました。\n 郷に従います。(StyleGuide 見てなかった…) / 他 2 コメント http://t.co/5I0nThs4rU “PEP 8 — Style Guide for Python Code” http://t.co/in7q9lD6cy\u0026gt; — UEDA (@hurutoriya) 2014, 9 月 25\n 普段は matlab か Ruby 書いてるんですが、普段書いてない言語を書くとあまりよろしくないコードになったので StyleGuide 導入しました。\n _“vim で python 開発するとき pyflakes + PEP8 = flake8 が便利 _http://t.co/ga33wqUytL_\u0026gt; — UEDA (@hurutoriya) 2014, 9 月 25\n ABEJA のエンジニアの人たちは、全員が自分の強みを持っていて凄く良いチームが出来上がっていました! 一緒に働くことができて心の底から心地よかったです。\nランチや晩御飯は他のインターン生や社員さんと色んな話ができて最高でした。(ご馳走していただいた方々ありがとうございました) 人事の方が他のインターン生や社員さんとの交流の機会を積極的に設けてくれたのが大きかったです!!\n中間報告会で「ビジネスサイドの人とご飯食べに行きたいです!!」と要望を出したら時間調整をしてくれてありがたかったです!!\n行列の次元圧縮とその応用が僕の研究分野なのですが、今回のインターンで色んなアドバイスが聞けて研究に対してかなりモチベーションが上がりました。研究内容について相談することができるのは、本当にありがたかったです。\nお客さんが来たり誰かが営業に行ったりした時に、全員から挨拶の言葉が出ているので”めっちゃ元気があるし、この会社雰囲気最高だな!!“と肌で感じてました。\nまとめ インターンでは実務面もそうですが、人生についても色々と真剣に考える機会が与えられました。今まで良くも悪くも惰性的に人生の進路を決めてきたので、今回のインターンシップで色々とこれから先の事を考えてみるキッカケを与えれくれたのは新鮮な体験でした。\n全員が一つの意思を持って突き進んでいる姿はとても美しいです!!\nメンターである@toshitanianさん、今回のインターンに誘ってくれてありがとうございました!!\n","permalink":"https://shunyaueta.com/posts/2014-08-23/","summary":"株式会社 ABEJA という会社に 2013/8/25~2013/9/26 間に 2 週間弱インターンシップに参加してきました。\n2013 年当時のサイトのスクショ\nWhat’S ABEJA? ABEJA について詳しく知りたい方は下記のリンクを見るとわかりやすいです。\n マーケティングから決済まで、人認証技術 × ビジネスプロデュースで未来を変える ABEJA の挑戦【連載:NEO ジェネ！】 いまそこにある、リアルタイムデータ解析。ぼくらの暮らしを変える、日本のスタートアップ 3 社  この会社をはじめて知ったのは、ミクシィでのインターンシップに参加してた時に別グループでインターンしていた知り合いの人に教えてもらったのがキッカケです。 機械学習や画像解析を用いて、現実世界の問題を解決して更にビジネスにまで昇華させている点に痺れました。\n本来なら一ヶ月近く参加予定だったんですが、もう一つ挑戦していたリクルートのインターンに運良く合格したので結果的には 2 週間弱という短めのインターンになりました。\nWhat do you doing in ABEJA intern? インターンの内容は Python を使って社内用のツールをゴニョゴニョしてました。\n 郷に従います。(StyleGuide 見てなかった…) / 他 2 コメント http://t.co/5I0nThs4rU “PEP 8 — Style Guide for Python Code” http://t.co/in7q9lD6cy\u0026gt; — UEDA (@hurutoriya) 2014, 9 月 25\n 普段は matlab か Ruby 書いてるんですが、普段書いてない言語を書くとあまりよろしくないコードになったので StyleGuide 導入しました。","title":"機械学習・コンピュータビジョンを活かしたビジネスを手掛ける株式会社ABEJAでインターンしてきた"},{"content":"株式会社ミクシイに 2013/8/9~2013/9/20 までインターンしてきました。 2013 年当時のスクリーンショットを探してたが、無かったので 2017/12/24 のを撮ってきた。良い意味であまり変わってない(ライさんの謎のくるまアプリとか)\nDive into mixi\nなぜインターンに行こうと思ったのか 僕は高専から筑波大学の情報系に今年度から編入したんですが、\n編入してから同期の編入生や先輩、内部生の人達に会って 「このままじゃダメだ、もっと面白く!もっと楽しく生きる!!」 と感じたので参加しました。\n…つまり、面白そうな事をしたかったので参加しました。 選考過程 エントリーリーシートを提出、希望する配属先はここで提出します。 2 回の面接があるのですが、つくばから渋谷への道のりは遠いので一回にしてくれました。ありがとうございます！！\n面接の服装は、スーツじゃなくて良いので楽です！(むしろスーツはやめてねと言われました)\n自己紹介と自分のこれまでの成果物を面接官に説明した後に、質疑応答を 30 分、時間が 20 分ほど余ったので面接官の方から「気になってることをなんでも聞いて下さい」と言われて質問をしていると面接がいつの間にか終わりました。\n合否発表は翌日に連絡がきて無事合格することができました、嬉しい!!\n配属先 僕は第一希望のDeployGateに配属されました。\nDeployGate チームは社員二人、インターン三人という謎構成でした。\n 「DepoyGate」とは、ミクシィ社の新規事業第 1 弾として登場した Android アプリ提供者向けのテスト版アプリ配信サービスです。 アプリをワイヤレスで配布することができ、アップデータ配信や動作ログがリアルタイムに取得できるので、プロジェクトメンバー全員が常に最新版アプリに触れることができます。 2012 年 9 月のサービス開始から、世界 93 ヵ国 3600 以上のアプリ開発に利用され、そのユーザーの 76％は英語圏というグローバルなサービスでもあります。 「DepoyGate」エンジニアからの発案で生まれたサービスです。 最新の技術に触れ、一緒に開発を進めていきたいという、あなたのチャレンジをお待ちしております。### メリット\n 今回のミクシィインターンは 6 週間を超える長期のインターンで実際に提供されているサービスの開発に参加すること t ができます。\nデメリット 基本的に無いです。\n強いて言うなら夏休みが 4/5 ほど持ってかれたことですね。\n何をやった？ 基本的に Github に issue に上がってるチケットを消化していく感じでした。 自分でこれがあったら便利だなと思う機能を追加したり、不便な箇所を改修したりして、GO サインが出たら Depoloy していきます。\n今年度のインターン生は何人? 内定生のインターンを除くと両手で数えられるほどでした。\n北は北海道、南は沖縄まで全国各地からインターン生が集結しました。\n昼ごはん イノベーションセンターの人たちと基本的に食べに行きました。渋谷のご飯は高いけど美味しいです^^ IRC で他のインターン生を誘ってご飯に行くこともありました。\nIRC のインターンシップチャンネルが全く盛り上がらないの、隣の T さんと裏工作を行い盛り上げてました。\nLT ９月の上旬にミクシィ社内で LT が行われて、初めての LT をした!!\n感動した事  Github でのソースコードを管理 僕が DeployGate の開発に参加するのに、要した時間はわずか 10 分です。 Collabrator に登録してもらい、clone をしてディレクトリの設定を終えたら開発が開始できる状態になります。 わずか１０分で開発に参加できる世界にバンザイです！！ DeployGate のミッションでもある「開発以外の見えないコストをなくす」に通じるところがありますね。   *注)僕はもともと自分のMacにRailsとgitの環境があったのでここまでスムーズに参加することができました。 後からDeployGateに来たインターン生の人はRailsの環境構築から始めても二時間弱程度で終わりました。 自分の書いたコードが世界中で走る!! DeployGate は世界規模のサービスです。 2012 年 9 月のサービス開始から、世界 93 ヵ国 3600 以上のアプリ開発に利用され、そのユーザーの 76％は英語圏というグローバルなサービスでもあります。 自分の書いたコードが世界中の人に使われるという経験を味わえました。 Github の Pull Request を用いたコードレビュー DeploGate チーム(主にメンターの方)にガッツリレビューして貰うことができます 正常に動くコード(これがめちゃくちゃ難しい)のために Pull Request でのコードレビューをしてくれました。 コーディングの作法や効率の良い実装方法などが行単位でレビューされるので、自分のプログラミングスキルがメキメキと上がっていったと勘違いしちゃうくらいには革新的でした(てへっ!! 自分の書いたコードがレビューされると言う事自体が少ないと思いますが、経験できてよかったと思います 皆、技術が大好き インターンのチームメイトも社員さんも、めっちゃくちゃ技術大好き!!って人たちばっかで四六時中お話してても飽きなくて凄くワクワクして楽しかったです。  まとめ(感じたこと・得たこと) 箇条書きで\n テストは大事 Deploy するとなると簡単にはいかない… イノセンの人たちはギラギラしてて最高に面白い! エンジニアリングとかビジネスとかの垣根なんか関係なく、面白い事をやればいい 面白いことを嗅ぎつけて飛びついていこう!!  最後に  ライさん、キョロさん  ライさんのおかげでインターンを無事に達成することができました。僕の拙いコー ドを何回もレビューしてくれて本当に助かりました。Deploy されるまでの険しい道程 をサポートしていただきありがとうございました。\nキョロさんには本当に色々とお世話になりました(ここでは書ききれないほどに)。キ ョロさんがいなければ、僕のインターンの通勤は苦痛でしかありませんでした。\n へんてこさん、たなさん 詰まった時に聞いたら、的確なアドバイスをくれるのがへんてこさんです。Android も Rails も両方共扱えて純粋に凄いなと感じました。ごはんに毎回誘っていただき有難うございました。 T さんは僕より 4 週間ほど遅れて DeoloyGate のインターンに参加されましたが、さくさくっと合流できてて凄いと思います。CSS おじさんといってすいませんでした。京都に寄った際にはまた、ご飯食べに行きましょう。 二人のおかげで楽しく開発ができました。ありがとうございました。 イノセンの皆さん イノセンの人たちは、僕のミクシィに対するイメージを変えてくれました。 今の僕のミクシィのイメージは「最高にクールでワクワクさせてくれる存在」です!! インターン同期の皆さん チームごとに分かれているのでお話できる機会は余りありませんでしたが、昼ごはん食べにいけれて楽しかったです! 人事の社員さま このインターンに巡り合わせてくれてありがとうございました。  ここまで読んで頂いてありがとうございました!! 関連リンク  mixi のインターンにいってきた 希望を信じた Android アプリ開発者を泣かせたくない，最後まで笑顔でいてほしい．ので，株式会社ミクシィでインターンしてきた SDN インターン  ","permalink":"https://shunyaueta.com/posts/2013-08-21/","summary":"株式会社ミクシイに 2013/8/9~2013/9/20 までインターンしてきました。 2013 年当時のスクリーンショットを探してたが、無かったので 2017/12/24 のを撮ってきた。良い意味であまり変わってない(ライさんの謎のくるまアプリとか)\nDive into mixi\nなぜインターンに行こうと思ったのか 僕は高専から筑波大学の情報系に今年度から編入したんですが、\n編入してから同期の編入生や先輩、内部生の人達に会って 「このままじゃダメだ、もっと面白く!もっと楽しく生きる!!」 と感じたので参加しました。\n…つまり、面白そうな事をしたかったので参加しました。 選考過程 エントリーリーシートを提出、希望する配属先はここで提出します。 2 回の面接があるのですが、つくばから渋谷への道のりは遠いので一回にしてくれました。ありがとうございます！！\n面接の服装は、スーツじゃなくて良いので楽です！(むしろスーツはやめてねと言われました)\n自己紹介と自分のこれまでの成果物を面接官に説明した後に、質疑応答を 30 分、時間が 20 分ほど余ったので面接官の方から「気になってることをなんでも聞いて下さい」と言われて質問をしていると面接がいつの間にか終わりました。\n合否発表は翌日に連絡がきて無事合格することができました、嬉しい!!\n配属先 僕は第一希望のDeployGateに配属されました。\nDeployGate チームは社員二人、インターン三人という謎構成でした。\n 「DepoyGate」とは、ミクシィ社の新規事業第 1 弾として登場した Android アプリ提供者向けのテスト版アプリ配信サービスです。 アプリをワイヤレスで配布することができ、アップデータ配信や動作ログがリアルタイムに取得できるので、プロジェクトメンバー全員が常に最新版アプリに触れることができます。 2012 年 9 月のサービス開始から、世界 93 ヵ国 3600 以上のアプリ開発に利用され、そのユーザーの 76％は英語圏というグローバルなサービスでもあります。 「DepoyGate」エンジニアからの発案で生まれたサービスです。 最新の技術に触れ、一緒に開発を進めていきたいという、あなたのチャレンジをお待ちしております。### メリット\n 今回のミクシィインターンは 6 週間を超える長期のインターンで実際に提供されているサービスの開発に参加すること t ができます。\nデメリット 基本的に無いです。\n強いて言うなら夏休みが 4/5 ほど持ってかれたことですね。\n何をやった？ 基本的に Github に issue に上がってるチケットを消化していく感じでした。 自分でこれがあったら便利だなと思う機能を追加したり、不便な箇所を改修したりして、GO サインが出たら Depoloy していきます。","title":"ミクシィにインターンしてきた"},{"content":"著者 上田隼也 (a.k.a. @hurutoriya) のブログです。 機械学習の実応用や検索技術領域に興味があります。\nSoftware Engineer として働いています。 Opinions are my own.\n Linkedin GitHub Twitter Speaker Deck Google Scholar  活動履歴 主な登壇歴  2020.08.07 Auto Content Moderation in C2C e-Commerce at OpML20  機械学習による C2C マーケットプレイスでの商品監視改善成果が MLOps の査読付き国際会議 OpML\u0026rsquo;20 にて登壇  YouTube Slide      執筆  2021.08.18 機械学習エンジニアの学会での論文発表のススメ。応募から査読通過までの流れ  Offers magazine というメディアに、業務での実績の論文化について寄稿   2020.08.07 Auto Content Moderation in C2C e-Commerce at OpML20  機械学習による C2C マーケットプレイスでの商品監視改善成果が MLOps の査読付き国際会議 OpML\u0026rsquo;20 にて論文が採択 PDF   2020.07.08 データ分析の進め方及び AI・機械学習導入の指南 ～データ収集・前処理・分析・評価結果の実務レベル対応～  マルチモーダルによる規約違反出品検知への応用と運用について を共著にて執筆    Misc  Machine Learning Casual Talks  第 5 回から運営者として、機械学習の実応用をテーマにしたコミュニティを運営しています。   パターン認識と機械学習の勉強会 @筑波大学  大学在学時に 2015.03-2017.03 まで主催していた筑波大学で開催される機械学習に関する勉強会    ","permalink":"https://shunyaueta.com/about/","summary":"著者 上田隼也 (a.k.a. @hurutoriya) のブログです。 機械学習の実応用や検索技術領域に興味があります。\nSoftware Engineer として働いています。 Opinions are my own.\n Linkedin GitHub Twitter Speaker Deck Google Scholar  活動履歴 主な登壇歴  2020.08.07 Auto Content Moderation in C2C e-Commerce at OpML20  機械学習による C2C マーケットプレイスでの商品監視改善成果が MLOps の査読付き国際会議 OpML\u0026rsquo;20 にて登壇  YouTube Slide      執筆  2021.08.18 機械学習エンジニアの学会での論文発表のススメ。応募から査読通過までの流れ  Offers magazine というメディアに、業務での実績の論文化について寄稿   2020.08.07 Auto Content Moderation in C2C e-Commerce at OpML20  機械学習による C2C マーケットプレイスでの商品監視改善成果が MLOps の査読付き国際会議 OpML\u0026rsquo;20 にて論文が採択 PDF   2020.","title":"このサイトについて"}]