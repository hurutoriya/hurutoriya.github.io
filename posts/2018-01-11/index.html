<!doctype html><html><head><script data-ad-client=ca-pub-8604812913439531 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>FUSE: Full Spectral Clustering(KDD2016) を読んだ - hurutoriya</title><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:image" content><meta property="og:title" content="FUSE: Full Spectral Clustering(KDD2016) を読んだ"><meta property="og:description" content="べき乗法と独立成分分析を用いたマルチスケールに頑強なクラスタリング手法の提案 べき乗法では近似固有ベクトル(Pseudo-eigenvectors)は複数の真の固有ベクトルの線形結合からなる。有益でない固有ベクトルも含まれるのでいかにそれを除"><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2018-01-11/"><meta property="article:published_time" content="2018-01-11T17:30:28+00:00"><meta property="article:modified_time" content="2018-01-11T17:30:28+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="FUSE: Full Spectral Clustering(KDD2016) を読んだ"><meta name=twitter:description content="べき乗法と独立成分分析を用いたマルチスケールに頑強なクラスタリング手法の提案 べき乗法では近似固有ベクトル(Pseudo-eigenvectors)は複数の真の固有ベクトルの線形結合からなる。有益でない固有ベクトルも含まれるのでいかにそれを除"><script src=https://shunyaueta.com/js/feather.min.js></script><link href=https://shunyaueta.com/css/fonts.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://shunyaueta.com/css/main.css><link rel=stylesheet type=text/css href=https://shunyaueta.com/css/dark.css media="(prefers-color-scheme: dark)"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-39994406-11"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-39994406-11');</script></head><body><div class=content><header><div class=main><a href=https://shunyaueta.com/>hurutoriya</a></div><nav><a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a></nav></header><main><article><div class=title><h1 class=title>FUSE: Full Spectral Clustering(KDD2016) を読んだ</h1><div class=meta>Posted on Jan 11, 2018</div></div><section class=body><p>べき乗法と独立成分分析を用いたマルチスケールに頑強なクラスタリング手法の提案</p><p><img src=/posts/2018-01-13/images/1.png alt=image></p><ul><li>べき乗法では近似固有ベクトル(Pseudo-eigenvectors)は複数の真の固有ベクトルの線形結合からなる。有益でない固有ベクトルも含まれるのでいかにそれを除去するか</li><li>Fig.1 で言ってることがいまいちわからない。ｃ,ｄも変化がないように見える。論文で言及してる stripe が何を指してるかが不明
分かった。赤・青・黒の３つのクラスタで分かれている。最初は黒色がクラスタ境界面に見えた。論文内での multi scale というのは、データの幾何的な分布を指している？</li><li>固有値計算は O(n³）かかるから計算量が~~って毎回言われるが、行列の状態に依存するので毎回最悪の場合を主張されてるも困る</li><li>ZP self-turining spectral clustering をなぜ対抗馬にして比較してるのかは謎。</li><li>ICA<a href=http://www.kecl.ntt.co.jp/icl/signal/sawada/mypaper/subspace2010rev.pdf>¹</a>を行ったあとにその行列に対して回転処理を行い情報量の最小化を狙う</li><li>クラスタ数が多い場合、PIC では良い結果が出づらい
multi scale なデータの場合標準の spectral clustering では失敗することが多々ある</li><li>fig.2 (a) 見ればわかるが<em>k</em>-means では分離が困難</li><li>fig.2(b) ４－５本目の固有ベクトルを基底ベクトルにもつ空間ではクラスタのオーバーラップが少ない</li><li>つまり１－５本目の固有ベクトル全て含めてクラスタリングすれば結果が良好になるのではないか？（仮説）</li><li>fig.2 © ４回べき乗法を行った。結果が似てる。（縦軸が何を表してるかは不明）。四回が上から四本求めたのか、一番上の固有ベクトルを４回求めたのか分からない</li><li>fig. (d) 提案手法を適用。k-means で分離可能 行列<strong>V</strong>を<em>p</em>回べき乗法を行って構築 <strong>E=MV</strong>の最小化を行う</li><li>ICA を最小化するために Jacobi Rotation を用いる。探索には貪欲法を採用</li></ul><h3 id=contribution>Contribution</h3><ul><li>マルチスケール（多種多様な）データ分布に対してクラスタリングが可能</li><li>計算時間は従来（ncut)と同等</li></ul><h3 id=感想>感想</h3><ul><li>PIC(Power Itetaion Clusterg)をまさか拡張してくるとは思わなかったなので、興味深い論文。(実装して再現実験したけど、Early stopping の段階で高確率でクラスタリングが失敗していて使い物にならなかった印象があるので…)</li><li>データの分布が多様な場合、上位数本のベクトルではなく、そこから更に下の固有ベクトルに着目したほうが結果が良好になるのは面白い</li><li>KDD の論文、相変わらず読みやすかった。</li></ul></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/machine-learning>Machine Learning</a></li><li><a href=/tags/paper>Paper</a></li><li><a href=/tags/kdd>KDD</a></li></ul></nav></div></article></main><h3>See Also</h3><ul><li><a href=/posts/2017-12-04/>Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrier を読んだ</a></li><li><a href=/posts/2017-12-23/>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ</a></li><li><a href=/posts/2017-12-01/>Machine Learning that Matters (ICML2012) を読んだ</a></li><li><a href=/posts/2017-12-06/>CoreMLがTensorFlow Liteをサポート</a></li></ul><footer><hr><a class=soc href=https://github.com/hurutoriya title=GitHub><i data-feather=github></i></a>|<a class=soc href=https://twitter.com/hurutoriya/ title=Twitter><i data-feather=twitter></i></a>|⚡️
2021 © Shunya Ueta | <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></footer><script>feather.replace()</script></div></body></html>