<!doctype html><html lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme-color content="dark"><title>FUSE: Full Spectral Clustering(KDD2016) を読んだ | Shunya Ueta</title><link rel=stylesheet href=/sass/main.min.1a3290acca8b3fc92df85ea9200859476d6c80d59009c89a749300f3ce7a7a67.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-39994406-11','auto');ga('send','pageview');}</script><meta property="og:title" content="FUSE: Full Spectral Clustering(KDD2016) を読んだ"><meta property="og:description" content="べき乗法と独立成分分析を用いたマルチスケールに頑強なクラスタリング手法の提案
 べき乗法では近似固有ベクトル(Pseudo-eigenvectors)は複数の真の固有ベクトルの線形結合からなる。有益でない固有ベクトルも含まれるのでいかにそれを除去するか Fig.1 で言ってることがいまいちわからない。ｃ,ｄも変化がないように見える。論文で言及してる stripe が何を指してるかが不明 分かった。赤・青・黒の３つのクラスタで分かれている。最初は黒色がクラスタ境界面に見えた。論文内での multi scale というのは、データの幾何的な分布を指している？ 固有値計算は O(n³）かかるから計算量が~~って毎回言われるが、行列の状態に依存するので毎回最悪の場合を主張されてるも困る ZP self-turining spectral clustering をなぜ対抗馬にして比較してるのかは謎。 ICA¹を行ったあとにその行列に対して回転処理を行い情報量の最小化を狙う クラスタ数が多い場合、PIC では良い結果が出づらい multi scale なデータの場合標準の spectral clustering では失敗することが多々ある fig.2 (a) 見ればわかるがk-means では分離が困難 fig.2(b) ４－５本目の固有ベクトルを基底ベクトルにもつ空間ではクラスタのオーバーラップが少ない つまり１－５本目の固有ベクトル全て含めてクラスタリングすれば結果が良好になるのではないか？（仮説） fig.2 © ４回べき乗法を行った。結果が似てる。（縦軸が何を表してるかは不明）。四回が上から四本求めたのか、一番上の固有ベクトルを４回求めたのか分からない fig. (d) 提案手法を適用。k-means で分離可能 行列Vをp回べき乗法を行って構築 E=MVの最小化を行う ICA を最小化するために Jacobi Rotation を用いる。探索には貪欲法を採用  Contribution  マルチスケール（多種多様な）データ分布に対してクラスタリングが可能 計算時間は従来（ncut)と同等  感想  PIC(Power Itetaion Clusterg)をまさか拡張してくるとは思わなかったなので、興味深い論文。(実装して再現実験したけど、Early stopping の段階で高確率でクラスタリングが失敗していて使い物にならなかった印象があるので…) データの分布が多様な場合、上位数本のベクトルではなく、そこから更に下の固有ベクトルに着目したほうが結果が良好になるのは面白い KDD の論文、相変わらず読みやすかった。"><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2018-01-11/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:published_time" content="2018-01-11T17:30:28+00:00"><meta property="article:modified_time" content="2021-12-13T01:09:13+09:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="FUSE: Full Spectral Clustering(KDD2016) を読んだ"><meta name=twitter:description content="べき乗法と独立成分分析を用いたマルチスケールに頑強なクラスタリング手法の提案
 べき乗法では近似固有ベクトル(Pseudo-eigenvectors)は複数の真の固有ベクトルの線形結合からなる。有益でない固有ベクトルも含まれるのでいかにそれを除去するか Fig.1 で言ってることがいまいちわからない。ｃ,ｄも変化がないように見える。論文で言及してる stripe が何を指してるかが不明 分かった。赤・青・黒の３つのクラスタで分かれている。最初は黒色がクラスタ境界面に見えた。論文内での multi scale というのは、データの幾何的な分布を指している？ 固有値計算は O(n³）かかるから計算量が~~って毎回言われるが、行列の状態に依存するので毎回最悪の場合を主張されてるも困る ZP self-turining spectral clustering をなぜ対抗馬にして比較してるのかは謎。 ICA¹を行ったあとにその行列に対して回転処理を行い情報量の最小化を狙う クラスタ数が多い場合、PIC では良い結果が出づらい multi scale なデータの場合標準の spectral clustering では失敗することが多々ある fig.2 (a) 見ればわかるがk-means では分離が困難 fig.2(b) ４－５本目の固有ベクトルを基底ベクトルにもつ空間ではクラスタのオーバーラップが少ない つまり１－５本目の固有ベクトル全て含めてクラスタリングすれば結果が良好になるのではないか？（仮説） fig.2 © ４回べき乗法を行った。結果が似てる。（縦軸が何を表してるかは不明）。四回が上から四本求めたのか、一番上の固有ベクトルを４回求めたのか分からない fig. (d) 提案手法を適用。k-means で分離可能 行列Vをp回べき乗法を行って構築 E=MVの最小化を行う ICA を最小化するために Jacobi Rotation を用いる。探索には貪欲法を採用  Contribution  マルチスケール（多種多様な）データ分布に対してクラスタリングが可能 計算時間は従来（ncut)と同等  感想  PIC(Power Itetaion Clusterg)をまさか拡張してくるとは思わなかったなので、興味深い論文。(実装して再現実験したけど、Early stopping の段階で高確率でクラスタリングが失敗していて使い物にならなかった印象があるので…) データの分布が多様な場合、上位数本のベクトルではなく、そこから更に下の固有ベクトルに着目したほうが結果が良好になるのは面白い KDD の論文、相変わらず読みやすかった。"></head><body class=dark><nav class=navbar><div class=container><div class=flex><div><a class=brand href=/><span class=emoji>🦅</span>
Shunya Ueta</a></div><div class=flex><a href=/articles/>All posts</a>
<button id=dark-mode-button></button></div></div></div></nav><main><div class=container><article><header class=article-header><div class=thumb><div><h1>FUSE: Full Spectral Clustering(KDD2016) を読んだ</h1><div class=post-meta><div>By on <time>2018.01.11</time> (Last updated:<time>2021.12.13</time>)</div><div class=tags><a href=/tags/machinelearning/>machinelearning</a>
<a href=/tags/paper/>paper</a></div></div></div></div></header></article><div class=article-post><p>べき乗法と独立成分分析を用いたマルチスケールに頑強なクラスタリング手法の提案</p><p><img src=/posts/2018-01-13/images/1.png alt=image></p><ul><li>べき乗法では近似固有ベクトル(Pseudo-eigenvectors)は複数の真の固有ベクトルの線形結合からなる。有益でない固有ベクトルも含まれるのでいかにそれを除去するか</li><li>Fig.1 で言ってることがいまいちわからない。ｃ,ｄも変化がないように見える。論文で言及してる stripe が何を指してるかが不明
分かった。赤・青・黒の３つのクラスタで分かれている。最初は黒色がクラスタ境界面に見えた。論文内での multi scale というのは、データの幾何的な分布を指している？</li><li>固有値計算は O(n³）かかるから計算量が~~って毎回言われるが、行列の状態に依存するので毎回最悪の場合を主張されてるも困る</li><li>ZP self-turining spectral clustering をなぜ対抗馬にして比較してるのかは謎。</li><li>ICA<a href=http://www.kecl.ntt.co.jp/icl/signal/sawada/mypaper/subspace2010rev.pdf>¹</a>を行ったあとにその行列に対して回転処理を行い情報量の最小化を狙う</li><li>クラスタ数が多い場合、PIC では良い結果が出づらい
multi scale なデータの場合標準の spectral clustering では失敗することが多々ある</li><li>fig.2 (a) 見ればわかるが<em>k</em>-means では分離が困難</li><li>fig.2(b) ４－５本目の固有ベクトルを基底ベクトルにもつ空間ではクラスタのオーバーラップが少ない</li><li>つまり１－５本目の固有ベクトル全て含めてクラスタリングすれば結果が良好になるのではないか？（仮説）</li><li>fig.2 © ４回べき乗法を行った。結果が似てる。（縦軸が何を表してるかは不明）。四回が上から四本求めたのか、一番上の固有ベクトルを４回求めたのか分からない</li><li>fig. (d) 提案手法を適用。k-means で分離可能 行列<strong>V</strong>を<em>p</em>回べき乗法を行って構築 <strong>E=MV</strong>の最小化を行う</li><li>ICA を最小化するために Jacobi Rotation を用いる。探索には貪欲法を採用</li></ul><h2 id=contribution>Contribution</h2><ul><li>マルチスケール（多種多様な）データ分布に対してクラスタリングが可能</li><li>計算時間は従来（ncut)と同等</li></ul><h2 id=感想>感想</h2><ul><li>PIC(Power Itetaion Clusterg)をまさか拡張してくるとは思わなかったなので、興味深い論文。(実装して再現実験したけど、Early stopping の段階で高確率でクラスタリングが失敗していて使い物にならなかった印象があるので…)</li><li>データの分布が多様な場合、上位数本のベクトルではなく、そこから更に下の固有ベクトルに着目したほうが結果が良好になるのは面白い</li><li>KDD の論文、相変わらず読みやすかった。</li></ul></div><h2>Support</h2><div class=container><div class=posts><div class=post><div class=post-row><a href=https://www.buymeacoffee.com/hurutoriya target=_blank>☕️ Buy me a cofee: お読みくださりありがとうございます。
こちらから ☕ を一杯支援していただけると、ブログ執筆のモチベーションに繋がります ✨</a></div></div></div></div><h2>See Also</h2><ul><li><a href=/posts/2017-12-23/>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ</a></li><li><a href=/posts/2017-12-04/>Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrier を読んだ</a></li><li><a href=/posts/2017-12-01/>Machine Learning that Matters (ICML2012) を読んだ</a></li><li><a href=/posts/2017-12-06/>CoreMLがTensorFlow Liteをサポート</a></li><li><a href=/posts/2017-11-14/>OpenCV 3.3から使えるDNNモジュールを使って物体検出</a></li></ul></div><div class=container><nav class="flex container suggested"><a rel=prev href=/posts/2018-01-09/ title="Previous post (older)"><span>Previous</span>
サイトのPWA化、ホスティングをGithub PagesからFirebaseへ移行</a>
<a rel=next href=/posts/2018-01-12/ title="Next post (newer)"><span>Next</span>
Call center stress recognition with person-specific models を読んだ</a></nav></div></main></main><footer class="footer flex"><section class=container><nav class=footer-links><a href=/about/>About</a>
<a href=/index.xml>RSS</a>
<a href=https://www.buymeacoffee.com/hurutoriya>Support</a></nav></section><script async src=/js/features.min.a94f58a30ad2560de728e080d87f75c60cf806fd1b3d5f4815f1a1a02c0d1859.js></script></footer></body></html>