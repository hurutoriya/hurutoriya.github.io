<!doctype html><html lang=ja><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む | hurutoriya</title><meta name=title content="遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む"><meta name=description content="pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。
問題点 そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い 解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。
実は Google の公式ドキュメントでも推奨されています。
https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas 方法は以下の２つ。
google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー 1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 pip install --upgrade google-cloud-bigquery[bqstorage,pandas] magic command を実行
%load_ext google.cloud.bigquery 後は Jupyter Notebook のセルで以下のコマンドを実行すれば、"><meta name=keywords content="gcp,bigquery,pandas,python,"><meta property="og:title" content="遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む"><meta property="og:description" content="pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。
問題点 そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い 解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。
実は Google の公式ドキュメントでも推奨されています。
https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas 方法は以下の２つ。
google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー 1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 pip install --upgrade google-cloud-bigquery[bqstorage,pandas] magic command を実行
%load_ext google.cloud.bigquery 後は Jupyter Notebook のセルで以下のコマンドを実行すれば、"><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2019-10-03/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-10-03T23:52:54+09:00"><meta property="article:modified_time" content="2019-10-03T23:52:54+09:00"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む"><meta name=twitter:description content="pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。
問題点 そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い 解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。
実は Google の公式ドキュメントでも推奨されています。
https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas 方法は以下の２つ。
google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー 1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 pip install --upgrade google-cloud-bigquery[bqstorage,pandas] magic command を実行
%load_ext google.cloud.bigquery 後は Jupyter Notebook のセルで以下のコマンドを実行すれば、"><meta itemprop=name content="遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む"><meta itemprop=description content="pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。
問題点 そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い 解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。
実は Google の公式ドキュメントでも推奨されています。
https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas 方法は以下の２つ。
google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー 1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 pip install --upgrade google-cloud-bigquery[bqstorage,pandas] magic command を実行
%load_ext google.cloud.bigquery 後は Jupyter Notebook のセルで以下のコマンドを実行すれば、"><meta itemprop=datePublished content="2019-10-03T23:52:54+09:00"><meta itemprop=dateModified content="2019-10-03T23:52:54+09:00"><meta itemprop=wordCount content="144"><meta itemprop=image content="https://shunyaueta.com/ogp.jpg"><meta itemprop=keywords content="gcp,bigquery,pandas,python,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-JMJRQJT0Q3"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JMJRQJT0Q3")</script></head><body><header><a href=/ class=title><h2>hurutoriya</h2></a><nav><a href=/index.xml>RSS</a>
<a href=/about/>このサイトについて</a></nav></header><main><h1>遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む</h1><p><i><time datetime=2019-10-03 pubdate>2019-10-03</time></i></p><content><p><a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_gbq.html>pandas.read_gbq</a> 便利ですよね。
クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。
Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。</p><h2 id=問題点>問題点</h2><ul><li>そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い</li></ul><p>解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。</p><p>実は Google の公式ドキュメントでも推奨されています。</p><ul><li><a href=https://cloud.google.com/bigquery/docs/pandas-gbq-migration>https://cloud.google.com/bigquery/docs/pandas-gbq-migration</a></li><li><a href=https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas>https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas</a></li></ul><p>方法は以下の２つ。</p><ol><li><code>google-cloud-bigquery</code> をインストールして、マジックコマンドで Google BQ を実行</li><li>BQ 実行 →BigQuery table として保存 →GCS へ保存 → <code>gsutil</code> でマシンへコピー</li></ol><p>1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も <code>pandas.rad_gbq</code> よりも高速です
2 番目はそもそも実行結果が巨大な場合で、目安としては<code>1GB以上</code>なら 2 番目の方法を使えば楽です。</p><h2 id=1-google-cloud-bigquery-をインストールしてjupyter-notebook-のマジックコマンドで-google-bq-を実行>1, <code>google-cloud-bigquery</code> をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pip install <span style=color:#f92672>--</span>upgrade google<span style=color:#f92672>-</span>cloud<span style=color:#f92672>-</span>bigquery[bqstorage,pandas]
</span></span></code></pre></div><p>magic command を実行</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>load_ext google<span style=color:#f92672>.</span>cloud<span style=color:#f92672>.</span>bigquery
</span></span></code></pre></div><p>後は Jupyter Notebook のセルで以下のコマンドを実行すれば、</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>bigquery df <span style=color:#f92672>--</span>use_bqstorage_api
</span></span><span style=display:flex><span>SELECT
</span></span><span style=display:flex><span>  CONCAT(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;https://stackoverflow.com/questions/&#39;</span>,
</span></span><span style=display:flex><span>    CAST(id <span style=color:#66d9ef>as</span> STRING)) <span style=color:#66d9ef>as</span> url,
</span></span><span style=display:flex><span>  view_count
</span></span><span style=display:flex><span>FROM <span style=color:#960050;background-color:#1e0010>`</span>bigquery<span style=color:#f92672>-</span>public<span style=color:#f92672>-</span>data<span style=color:#f92672>.</span>stackoverflow<span style=color:#f92672>.</span>posts_questions<span style=color:#960050;background-color:#1e0010>`</span>
</span></span><span style=display:flex><span>WHERE tags like <span style=color:#e6db74>&#39;</span><span style=color:#e6db74>%g</span><span style=color:#e6db74>oogle-bigquery%&#39;</span>
</span></span><span style=display:flex><span>ORDER BY view_count DESC
</span></span><span style=display:flex><span>LIMIT <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><p><code>df</code> にマジックコマンドで実行した SQL の実行結果が格納されます!
便利ですね</p><h2 id=2-bq-実行-bigquery-table-として保存-gcs-へ保存--gsutil-でマシンへコピー>2, BQ 実行 →BigQuery table として保存 →GCS へ保存 → <code>gsutil</code> でマシンへコピー</h2><ul><li>BigQuery でクエリを実行、実行結果を BigQuery Table へ保存</li><li>注)実行結果の容量が巨大なので、保存先は基本的に Big Query Table へ保存するしか選択肢が無い</li></ul><p><img src=/posts/2019-10-03/images/export-to-bqtable.png alt="can&amp;rsquo;t export large file as one file"></p><ul><li>BigQuery table から GCS へテーブルを CSV として保存</li></ul><p>Big Query table からエクスポート時に、ファイルサイズが大きいとエクスポートできないので、分割が必要です。</p><p><img src=/posts/2019-10-03/images/cant-export-onefile.png alt="can&amp;rsquo;t export large file as one file"></p><p><a href=https://cloud.google.com/bigquery/docs/exporting-data>https://cloud.google.com/bigquery/docs/exporting-data</a></p><p>保存ファイル名を <code>file-*</code> のようにワイルドカードを指定すると、自動的にひとつのテーブルを複数ファイルに分割して保存してくれる</p><p><code>gsutil</code> commands で任意のマシンへダウンロードする。</p><p><code>-m</code> オプションを付け足すと並列ダウンロードが始まるので、複数ファイルダウンロードする場合はおすすめです</p><p>ストレスレスなデータ分析ライフを!</p><h2>関連しているかもしれない記事</h2><ul><li><a href=/posts/2019-09-25/>Jupyter Notebook上にTensorboard を わずか2行で表示させる</a></li><li><a href=/posts/2019-09-24/>How to connect the Google Compute Engine via Visual Studio Code</a></li><li><a href=/posts/2019-06-17/>How to concat image using skimage</a></li><li><a href=/posts/2018-04-23/>Google Colaboratory で Mecab-ipadic-Neologd を使用可能にする</a></li><li><a href=/posts/2018-01-15/>Jupyter Notebookの差分を明瞭に確認する事ができるpackage : nbdime</a></li></ul><h2>Support</h2>記事をお読みくださりありがとうございます。
このウェブサイトの運営を支援していただける方を募集しています。
もしよろしければ、<a href=https://www.buymeacoffee.com/hurutoriya>Buy Me a Coffee</a> からサポート(投げ銭)していただけると、記事の執筆、情報発信のモチベーションに繋がります✨<p>--</p>記事を楽しめましたか？
<a href=/index.xml>RSS</a>で更新情報を配信しているので、お好きなフィードリーダーで購読してみてください。
また、記事へのリアクションやコメントなどを、以下のGitHub アカウントを利用したコメントシステムからしていただけると、執筆の励みになります。</content><p><a href=https://shunyaueta.com/tags/gcp/>#gcp</a>
<a href=https://shunyaueta.com/tags/bigquery/>#bigquery</a>
<a href=https://shunyaueta.com/tags/pandas/>#pandas</a>
<a href=https://shunyaueta.com/tags/python/>#python</a></p><script src=https://giscus.app/client.js data-repo=hurutoriya/hurutoriya.github.io data-repo-id="MDEwOlJlcG9zaXRvcnk2MDgyNTY1Nw==" data-category=Comments data-category-id=DIC_kwDOA6AgOc4CAQTX data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=dark data-lang=ja crossorigin=anonymous async></script></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>