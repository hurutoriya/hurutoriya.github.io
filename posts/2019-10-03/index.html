<!doctype html><html lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme-color content="dark"><title>遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む | Shunya Ueta</title><link rel=stylesheet href=/sass/main.min.b81b77bfef641cdef5a77f4a7bb221547def6cea9b73ab29b5fed4c98707f2a8.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-39994406-11','auto');ga('send','pageview');}</script><meta property="og:title" content="遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む"><meta property="og:description" content="pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。
問題点  そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い  解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。
実は Google の公式ドキュメントでも推奨されています。
 https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas  方法は以下の２つ。
 google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 1  pip install --upgrade google-cloud-bigquery[bqstorage,pandas]   magic command を実行"><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2019-10-03/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:published_time" content="2019-10-03T23:52:54+09:00"><meta property="article:modified_time" content="2021-12-28T23:39:18+09:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む"><meta name=twitter:description content="pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。
問題点  そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い  解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。
実は Google の公式ドキュメントでも推奨されています。
 https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas  方法は以下の２つ。
 google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 1  pip install --upgrade google-cloud-bigquery[bqstorage,pandas]   magic command を実行"></head><body class=dark><nav class=navbar><div class=container><div class=flex><div><a class=brand href=/><span class=emoji>🦅</span>
Shunya Ueta</a></div><div class=flex><a href=/articles/>All posts</a>
<button id=dark-mode-button></button></div></div></div></nav><main><div class=container><article><header class=article-header><div class=thumb><div><h1>遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む</h1><div class=post-meta><div>By Shunya Ueta on <time>2019.10.03</time> (Last updated:<time>2021.12.28</time>)</div><div class=tags><a href=/tags/pandas/>pandas</a>
<a href=/tags/python/>python</a>
<a href=/tags/bigquery/>bigquery</a>
<a href=/tags/gcp/>gcp</a></div></div></div></div></header></article><div class=article-post><p><a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_gbq.html>pandas.read_gbq</a> 便利ですよね。
クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。
Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。</p><h2 id=問題点>問題点</h2><ul><li>そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い</li></ul><p>解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。</p><p>実は Google の公式ドキュメントでも推奨されています。</p><ul><li><a href=https://cloud.google.com/bigquery/docs/pandas-gbq-migration>https://cloud.google.com/bigquery/docs/pandas-gbq-migration</a></li><li><a href=https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas>https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas</a></li></ul><p>方法は以下の２つ。</p><ol><li><code>google-cloud-bigquery</code> をインストールして、マジックコマンドで Google BQ を実行</li><li>BQ 実行 →BigQuery table として保存 →GCS へ保存 → <code>gsutil</code> でマシンへコピー</li></ol><p>1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も <code>pandas.rad_gbq</code> よりも高速です
2 番目はそもそも実行結果が巨大な場合で、目安としては<code>1GB以上</code>なら 2 番目の方法を使えば楽です。</p><h2 id=1-google-cloud-bigquery-をインストールしてjupyter-notebook-のマジックコマンドで-google-bq-を実行>1, <code>google-cloud-bigquery</code> をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=n>pip</span> <span class=n>install</span> <span class=o>--</span><span class=n>upgrade</span> <span class=n>google</span><span class=o>-</span><span class=n>cloud</span><span class=o>-</span><span class=n>bigquery</span><span class=p>[</span><span class=n>bqstorage</span><span class=p>,</span><span class=n>pandas</span><span class=p>]</span>
</code></pre></td></tr></table></div></div><p>magic command を実行</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=o>%</span><span class=n>load_ext</span> <span class=n>google</span><span class=o>.</span><span class=n>cloud</span><span class=o>.</span><span class=n>bigquery</span>
</code></pre></td></tr></table></div></div><p>後は Jupyter Notebook のセルで以下のコマンドを実行すれば、</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=o>%%</span><span class=n>bigquery</span> <span class=n>df</span> <span class=o>--</span><span class=n>use_bqstorage_api</span>
<span class=n>SELECT</span>
  <span class=n>CONCAT</span><span class=p>(</span>
    <span class=s1>&#39;https://stackoverflow.com/questions/&#39;</span><span class=p>,</span>
    <span class=n>CAST</span><span class=p>(</span><span class=nb>id</span> <span class=k>as</span> <span class=n>STRING</span><span class=p>))</span> <span class=k>as</span> <span class=n>url</span><span class=p>,</span>
  <span class=n>view_count</span>
<span class=n>FROM</span> <span class=sb>`bigquery-public-data.stackoverflow.posts_questions`</span>
<span class=n>WHERE</span> <span class=n>tags</span> <span class=n>like</span> <span class=s1>&#39;</span><span class=si>%g</span><span class=s1>oogle-bigquery%&#39;</span>
<span class=n>ORDER</span> <span class=n>BY</span> <span class=n>view_count</span> <span class=n>DESC</span>
<span class=n>LIMIT</span> <span class=mi>10</span>
</code></pre></td></tr></table></div></div><p><code>df</code> にマジックコマンドで実行した SQL の実行結果が格納されます!
便利ですね</p><h2 id=2-bq-実行-bigquery-table-として保存-gcs-へ保存--gsutil-でマシンへコピー>2, BQ 実行 →BigQuery table として保存 →GCS へ保存 → <code>gsutil</code> でマシンへコピー</h2><ul><li>BigQuery でクエリを実行、実行結果を BigQuery Table へ保存</li><li>注)実行結果の容量が巨大なので、保存先は基本的に Big Query Table へ保存するしか選択肢が無い</li></ul><p><img src=/posts/2019-10-03/images/export-to-bqtable.png alt="can&rsquo;t export large file as one file"></p><ul><li>BigQuery table から GCS へテーブルを CSV として保存</li></ul><p>Big Query table からエクスポート時に、ファイルサイズが大きいとエクスポートできないので、分割が必要です。</p><p><img src=/posts/2019-10-03/images/cant-export-onefile.png alt="can&rsquo;t export large file as one file"></p><p><a href=https://cloud.google.com/bigquery/docs/exporting-data>https://cloud.google.com/bigquery/docs/exporting-data</a></p><p>保存ファイル名を <code>file-*</code> のようにワイルドカードを指定すると、自動的にひとつのテーブルを複数ファイルに分割して保存してくれる</p><p><code>gsutil</code> commands で任意のマシンへダウンロードする。</p><p><code>-m</code> オプションを付け足すと並列ダウンロードが始まるので、複数ファイルダウンロードする場合はおすすめです</p><p>ストレスレスなデータ分析ライフを!</p></div><h2>Support</h2><div class=container><div class=posts><div class=post><div class=post-row><a href=https://www.buymeacoffee.com/hurutoriya target=_blank>☕️ Buy me a cofee: お読みくださりありがとうございます。
こちらから ☕ を一杯支援していただけると、ブログ執筆のモチベーションに繋がります ✨</a></div></div></div></div><h2>See Also</h2><ul><li><a href=/posts/2019-09-25/>Jupyter Notebook上にTensorboard を わずか2行で表示させる</a></li><li><a href=/posts/2019-09-24/>How to connect the Google Compute Engine via Visual Studio Code</a></li><li><a href=/posts/2019-06-17/>How to concat image using skimage</a></li><li><a href=/posts/2018-04-23/>Google Colaboratory で Mecab-ipadic-Neologd を使用可能にする</a></li><li><a href=/posts/2018-01-15/>Jupyter Notebookの差分を明瞭に確認する事ができるpackage : nbdime</a></li></ul></div><div class=container><nav class="flex container suggested"><a rel=prev href=/posts/2019-09-25/ title="Previous post (older)"><span>Previous</span>
Jupyter Notebook上にTensorboard を わずか2行で表示させる</a>
<a rel=next href=/posts/2020-04-12/ title="Next post (newer)"><span>Next</span>
Courseraで Getting Started with Google Kubernetes Engine の講義を修了した</a></nav></div></main></main><footer class="footer flex"><section class=container><nav class=footer-links><a href=/about/>About</a>
<a href=/index.xml>RSS</a>
<a href=https://www.buymeacoffee.com/hurutoriya>Support</a></nav></section><script async src=/js/features.min.a94f58a30ad2560de728e080d87f75c60cf806fd1b3d5f4815f1a1a02c0d1859.js></script></footer></body></html>