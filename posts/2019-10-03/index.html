<!doctype html><html lang=ja dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む | Shunya Ueta</title><meta name=keywords content="gcp,bigquery,pandas,python"><meta name=description content="pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。
問題点  そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い  解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。
実は Google の公式ドキュメントでも推奨されています。
 https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas  方法は以下の２つ。
 google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 1  pip install --upgrade google-cloud-bigquery[bqstorage,pandas]   magic command を実行"><meta name=author content="Shunya Ueta"><link rel=canonical href=https://shunyaueta.com/posts/2019-10-03/><link crossorigin=anonymous href=/assets/css/stylesheet.min.e21185e6c4b43ff34c81666f70aa4f80140274057866888c0a5c28addc9b7fd2.css integrity="sha256-4hGF5sS0P/NMgWZvcKpPgBQCdAV4ZoiMClwordybf9I=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://shunyaueta.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shunyaueta.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shunyaueta.com/favicon-32x32.png><link rel=apple-touch-icon href=https://shunyaueta.com/apple-touch-icon.png><link rel=mask-icon href=https://shunyaueta.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.67.1"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-39994406-11','auto');ga('send','pageview');}</script><meta property="og:title" content="遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む"><meta property="og:description" content="pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。
問題点  そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い  解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。
実は Google の公式ドキュメントでも推奨されています。
 https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas  方法は以下の２つ。
 google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 1  pip install --upgrade google-cloud-bigquery[bqstorage,pandas]   magic command を実行"><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2019-10-03/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-10-03T23:52:54+09:00"><meta property="article:modified_time" content="2022-01-20T00:07:47+09:00"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む"><meta name=twitter:description content="pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。
問題点  そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い  解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。
実は Google の公式ドキュメントでも推奨されています。
 https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas  方法は以下の２つ。
 google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 1  pip install --upgrade google-cloud-bigquery[bqstorage,pandas]   magic command を実行"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む","item":"https://shunyaueta.com/posts/2019-10-03/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む","name":"遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む","description":"pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。\n問題点  そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い  解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。\n実は Google の公式ドキュメントでも推奨されています。\n https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas  方法は以下の２つ。\n google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。\n1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 1  pip install --upgrade google-cloud-bigquery[bqstorage,pandas]   magic command を実行","keywords":["gcp","bigquery","pandas","python"],"articleBody":"pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。\n問題点  そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い  解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。\n実は Google の公式ドキュメントでも推奨されています。\n https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas  方法は以下の２つ。\n google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。\n1, google-cloud-bigquery をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行 1  pip install --upgrade google-cloud-bigquery[bqstorage,pandas]   magic command を実行\n1  %load_ext google.cloud.bigquery   後は Jupyter Notebook のセルで以下のコマンドを実行すれば、\n1 2 3 4 5 6 7 8 9 10  %%bigquery df --use_bqstorage_api SELECT CONCAT( 'https://stackoverflow.com/questions/', CAST(id as STRING)) as url, view_count FROM `bigquery-public-data.stackoverflow.posts_questions` WHERE tags like '%google-bigquery%' ORDER BY view_count DESC LIMIT 10   df にマジックコマンドで実行した SQL の実行結果が格納されます! 便利ですね\n2, BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  BigQuery でクエリを実行、実行結果を BigQuery Table へ保存 注)実行結果の容量が巨大なので、保存先は基本的に Big Query Table へ保存するしか選択肢が無い   BigQuery table から GCS へテーブルを CSV として保存  Big Query table からエクスポート時に、ファイルサイズが大きいとエクスポートできないので、分割が必要です。\nhttps://cloud.google.com/bigquery/docs/exporting-data\n保存ファイル名を file-* のようにワイルドカードを指定すると、自動的にひとつのテーブルを複数ファイルに分割して保存してくれる\ngsutil commands で任意のマシンへダウンロードする。\n-m オプションを付け足すと並列ダウンロードが始まるので、複数ファイルダウンロードする場合はおすすめです\nストレスレスなデータ分析ライフを!\n","wordCount":"156","inLanguage":"ja","datePublished":"2019-10-03T23:52:54+09:00","dateModified":"2022-01-20T00:07:47+09:00","author":{"@type":"Person","name":"Shunya Ueta"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://shunyaueta.com/posts/2019-10-03/"},"publisher":{"@type":"Organization","name":"Shunya Ueta","logo":{"@type":"ImageObject","url":"https://shunyaueta.com/favicon.ico"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href=https://shunyaueta.com/ accesskey=h title="Shunya Ueta (Alt + H)">Shunya Ueta</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://shunyaueta.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://shunyaueta.com/about title=About><span>About</span></a></li><li><a href=https://www.getrevue.co/profile/hurutoriya title=Newsletter><span>Newsletter</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む</h1><div class=post-meta><span title="2019-10-03 23:52:54 +0900 +0900">October 3, 2019</span>&nbsp;·&nbsp;Shunya Ueta</div></header><div class=post-content><p><a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_gbq.html>pandas.read_gbq</a> 便利ですよね。
クレデンシャルファイルを認証画面からコピペすれば Jupyter Notebook 上で簡単に認証され、Google BigQuery が実行されてその結果がそのままデータフレームとして扱えます。
Jupyter Notebook と Google BigQuery を連携させたいときは愛用していました(過去形)。</p><h2 id=問題点>問題点<a hidden class=anchor aria-hidden=true href=#問題点>#</a></h2><ul><li>そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが凄い</li></ul><p>解決方法として、Google BigQuery で巨大なデータをダウンロードする方法について書きます。</p><p>実は Google の公式ドキュメントでも推奨されています。</p><ul><li><a href=https://cloud.google.com/bigquery/docs/pandas-gbq-migration>https://cloud.google.com/bigquery/docs/pandas-gbq-migration</a></li><li><a href=https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas>https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas</a></li></ul><p>方法は以下の２つ。</p><ol><li><code>google-cloud-bigquery</code> をインストールして、マジックコマンドで Google BQ を実行</li><li>BQ 実行 →BigQuery table として保存 →GCS へ保存 → <code>gsutil</code> でマシンへコピー</li></ol><p>1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も <code>pandas.rad_gbq</code> よりも高速です
2 番目はそもそも実行結果が巨大な場合で、目安としては<code>1GB以上</code>なら 2 番目の方法を使えば楽です。</p><h2 id=1-google-cloud-bigquery-をインストールしてjupyter-notebook-のマジックコマンドで-google-bq-を実行>1, <code>google-cloud-bigquery</code> をインストールして、Jupyter Notebook のマジックコマンドで Google BQ を実行<a hidden class=anchor aria-hidden=true href=#1-google-cloud-bigquery-をインストールしてjupyter-notebook-のマジックコマンドで-google-bq-を実行>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=n>pip</span> <span class=n>install</span> <span class=o>--</span><span class=n>upgrade</span> <span class=n>google</span><span class=o>-</span><span class=n>cloud</span><span class=o>-</span><span class=n>bigquery</span><span class=p>[</span><span class=n>bqstorage</span><span class=p>,</span><span class=n>pandas</span><span class=p>]</span>
</code></pre></td></tr></table></div></div><p>magic command を実行</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=o>%</span><span class=n>load_ext</span> <span class=n>google</span><span class=o>.</span><span class=n>cloud</span><span class=o>.</span><span class=n>bigquery</span>
</code></pre></td></tr></table></div></div><p>後は Jupyter Notebook のセルで以下のコマンドを実行すれば、</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=o>%%</span><span class=n>bigquery</span> <span class=n>df</span> <span class=o>--</span><span class=n>use_bqstorage_api</span>
<span class=n>SELECT</span>
  <span class=n>CONCAT</span><span class=p>(</span>
    <span class=s1>&#39;https://stackoverflow.com/questions/&#39;</span><span class=p>,</span>
    <span class=n>CAST</span><span class=p>(</span><span class=nb>id</span> <span class=k>as</span> <span class=n>STRING</span><span class=p>))</span> <span class=k>as</span> <span class=n>url</span><span class=p>,</span>
  <span class=n>view_count</span>
<span class=n>FROM</span> <span class=sb>`bigquery-public-data.stackoverflow.posts_questions`</span>
<span class=n>WHERE</span> <span class=n>tags</span> <span class=n>like</span> <span class=s1>&#39;</span><span class=si>%g</span><span class=s1>oogle-bigquery%&#39;</span>
<span class=n>ORDER</span> <span class=n>BY</span> <span class=n>view_count</span> <span class=n>DESC</span>
<span class=n>LIMIT</span> <span class=mi>10</span>
</code></pre></td></tr></table></div></div><p><code>df</code> にマジックコマンドで実行した SQL の実行結果が格納されます!
便利ですね</p><h2 id=2-bq-実行-bigquery-table-として保存-gcs-へ保存--gsutil-でマシンへコピー>2, BQ 実行 →BigQuery table として保存 →GCS へ保存 → <code>gsutil</code> でマシンへコピー<a hidden class=anchor aria-hidden=true href=#2-bq-実行-bigquery-table-として保存-gcs-へ保存--gsutil-でマシンへコピー>#</a></h2><ul><li>BigQuery でクエリを実行、実行結果を BigQuery Table へ保存</li><li>注)実行結果の容量が巨大なので、保存先は基本的に Big Query Table へ保存するしか選択肢が無い</li></ul><p><img loading=lazy src=/posts/2019-10-03/images/export-to-bqtable.png alt="can&amp;rsquo;t export large file as one file"></p><ul><li>BigQuery table から GCS へテーブルを CSV として保存</li></ul><p>Big Query table からエクスポート時に、ファイルサイズが大きいとエクスポートできないので、分割が必要です。</p><p><img loading=lazy src=/posts/2019-10-03/images/cant-export-onefile.png alt="can&amp;rsquo;t export large file as one file"></p><p><a href=https://cloud.google.com/bigquery/docs/exporting-data>https://cloud.google.com/bigquery/docs/exporting-data</a></p><p>保存ファイル名を <code>file-*</code> のようにワイルドカードを指定すると、自動的にひとつのテーブルを複数ファイルに分割して保存してくれる</p><p><code>gsutil</code> commands で任意のマシンへダウンロードする。</p><p><code>-m</code> オプションを付け足すと並列ダウンロードが始まるので、複数ファイルダウンロードする場合はおすすめです</p><p>ストレスレスなデータ分析ライフを!</p><h2>See Also</h2><ul><li><a href=/posts/2019-09-25/>Jupyter Notebook上にTensorboard を わずか2行で表示させる</a></li><li><a href=/posts/2019-09-24/>How to connect the Google Compute Engine via Visual Studio Code</a></li><li><a href=/posts/2019-06-17/>How to concat image using skimage</a></li><li><a href=/posts/2018-04-23/>Google Colaboratory で Mecab-ipadic-Neologd を使用可能にする</a></li><li><a href=/posts/2018-01-15/>Jupyter Notebookの差分を明瞭に確認する事ができるpackage : nbdime</a></li></ul><h2>Support</h2><a href=https://www.buymeacoffee.com/hurutoriya target=_blank>☕️ Buy me a cofee: お読みくださりありがとうございます。
こちらから ☕ を一杯支援していただけると、ブログ執筆のモチベーションに繋がります ✨</div><footer class=post-footer><ul class=post-tags><li><a href=https://shunyaueta.com/tags/bigquery/>bigquery</a></li><li><a href=https://shunyaueta.com/tags/gcp/>gcp</a></li><li><a href=https://shunyaueta.com/tags/python/>python</a></li><li><a href=https://shunyaueta.com/tags/pandas/>pandas</a></li></ul><nav class=paginav><a class=prev href=https://shunyaueta.com/posts/2020-04-12/><span class=title>« 前のページ</span><br><span>Courseraで Getting Started with Google Kubernetes Engine の講義を修了した</span></a>
<a class=next href=https://shunyaueta.com/posts/2019-09-25/><span class=title>次のページ »</span><br><span>Jupyter Notebook上にTensorboard を わずか2行で表示させる</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=hurutoriya/hurutoriya.github.io data-repo-id="MDEwOlJlcG9zaXRvcnk2MDgyNTY1Nw==" data-category=Comments data-category-id=DIC_kwDOA6AgOc4CAQTX data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=dark data-lang=ja crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://shunyaueta.com/>Shunya Ueta</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z" /></svg></a><script>let menu=document.getElementById('menu')
if(menu){menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>