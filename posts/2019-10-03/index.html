<!doctype html><html><head><script data-ad-client=ca-pub-8604812913439531 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Shunya Ueta (a.k.a hurutoriya) personal website"><link rel="shortcut icon" href=https://shunyaueta.com/favicon.ico><link rel=stylesheet href=/css/style.min.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-39994406-11"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-39994406-11');</script><title>pandas.read_gbq を使わずに、Google BigQueryから高速にData ETL</title></head><body><header id=banner><h2><a href=https://shunyaueta.com/>Software Engineer as Data Scientist</a></h2><nav><ul><li><a href=/ title=posts>posts</a></li><li><a href=/about/ title=about>about</a></li></ul></nav></header><main id=content><article><header id=post-header><h1>pandas.read_gbq を使わずに、Google BigQueryから高速にData ETL</h1><time>2019-10-03</time></header><p><a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_gbq.html>pandas.read_gbq</a> 便利ですよね。
クレデンシャルファイルを認証画面からコピペすれば Jupyter 上でさっと動き、Google Big Query が実行されてその結果がそのままデータフレームとして扱えます。
Jupyter と Google BQ を連携させたいときはいつも使っています</p><h2 id=問題点>問題点</h2><ul><li>そこそこ大きなデータを持ってこようとすると、めちゃくちゃ遅くてストレスが半端ない</li></ul><p>解決方法として、Google Big Query で巨大なデータをダウンロードする方法について書く</p><p>実は Google の公式ドキュメントでも推奨されています</p><ul><li><a href=https://cloud.google.com/bigquery/docs/pandas-gbq-migration>https://cloud.google.com/bigquery/docs/pandas-gbq-migration</a></li><li><a href=https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas>https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas</a></li></ul><p>方法は以下の２つ。</p><ol><li><code>google-cloud-bigquery</code> をインストールして、マジックコマンドで Google BQ を実行</li><li>BQ 実行 →BigQuery table として保存 →GCS へ保存 → <code>gsutil</code> でマシンへコピー</li></ol><p>1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も <code>pandas.rad_gbq</code> よりも高速です
2 番目はそもそも実行結果が巨大な場合で、目安としては<code>1GB以上</code>なら 2 番目の方法を使えば楽です。</p><h3 id=1-google-cloud-bigquery-をインストールしてマジックコマンドで-google-bq-を実行>1, <code>google-cloud-bigquery</code> をインストールして、マジックコマンドで Google BQ を実行</h3><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>pip</span> <span class=n>install</span> <span class=o>--</span><span class=n>upgrade</span> <span class=n>google</span><span class=o>-</span><span class=n>cloud</span><span class=o>-</span><span class=n>bigquery</span><span class=p>[</span><span class=n>bqstorage</span><span class=p>,</span><span class=n>pandas</span><span class=p>]</span>

</code></pre></div><p>magic command を実行</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=o>%</span><span class=n>load_ext</span> <span class=n>google</span><span class=o>.</span><span class=n>cloud</span><span class=o>.</span><span class=n>bigquery</span>
</code></pre></div><p>後は Jupyter Notebook のセルで以下のコマンドを実行すれば、</p><pre><code>%%bigquery df --use_bqstorage_api
SELECT
  CONCAT(
    'https://stackoverflow.com/questions/',
    CAST(id as STRING)) as url,
  view_count
FROM `bigquery-public-data.stackoverflow.posts_questions`
WHERE tags like '%google-bigquery%'
ORDER BY view_count DESC
LIMIT 10
</code></pre><p><code>df</code> にマジックコマンドで実行した SQL の実行結果が格納されます!
便利ですね</p><h3 id=2-bq-実行-bigquery-table-として保存-gcs-へ保存--gsutil-でマシンへコピー>2, BQ 実行 →BigQuery table として保存 →GCS へ保存 → <code>gsutil</code> でマシンへコピー</h3><ul><li><p>BigQuery でクエリを実行、実行結果を BigQuery Table へ保存</p></li><li><p>注)実行結果の容量が巨大なので、保存先は基本的に Big Query Table へ保存するしか選択肢が無い</p></li></ul><p><img src=/posts/2019-10-03/images/export-to-bqtable.png alt="can&rsquo;t export large file as one file"></p><ul><li>BigQuery table から GCS へテーブルを CSV として保存</li></ul><p>Big Query table からエクスポート時に、ファイルサイズが大きいとエクスポートできないので、分割が必要です。</p><p><img src=/posts/2019-10-03/images/cant-export-onefile.png alt="can&rsquo;t export large file as one file"></p><p><a href=https://cloud.google.com/bigquery/docs/exporting-data>https://cloud.google.com/bigquery/docs/exporting-data</a></p><p>保存ファイル名を <code>file-*</code> のようにワイルドカードを指定すると、自動的にひとつのテーブルを複数ファイルに分割して保存してくれる</p><p><code>gsutil</code> commands で任意のマシンへダウンロードする。</p><p><code>-m</code> オプションを付け足すと並列ダウンロードが始まるので、複数ファイルダウンロードする場合はおすすめです</p><p>ストレスレスなデータ分析ライフを!</p></article></main><h3>See Also</h3><ul><li><a href=/posts/2019-09-25/>Tensorboard を わずか2行で Jupyter Notebook上で表示</a></li><li><a href=/posts/2019-09-24/>How to connect the Google Compute Engine via Visual Studio Code</a></li><li><a href=/posts/2019-06-17/>How to concat image using skimage</a></li><li><a href=/posts/2018-01-15/>Jupyter Notebookの差分を明瞭に確認する事ができるpackage : nbdime</a></li><li><a href=/posts/2018-01-13/>PythonでGaussian Kernelのアニメーションを作成</a></li></ul><footer id=footer>Copyright © 2013-2021 Shunya Ueta</footer></body></html>