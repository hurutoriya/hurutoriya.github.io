<!doctype html><html lang=ja dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Search Engineering Newsletter vol.07 | 🦅 hurutoriya</title><meta name=keywords content="newsletter,search"><meta name=description content="Search Engineering Newsletter 7 回目の配信です。 今回のイチオシは、Andre Ng 先生の対談記事「Unbiggen AI」です。
Search Go beyond the search box: Introducing multisearch
Google が導入する、画像検索 → 自然言語による検索の二段階検索である multisaerch。
Google IO 2022 で、強調されていたのは Google Lens と組み合わせて multisaerch をできるようにしたいと提案されていて、なるほど! 確かに日常的に Google Lens を装着するようになれば multisearch の利用状況はものすごく自然だなと思った。 妄想だが、Google Lens がスタンドアロンで動くように機械学習モデルとかは乗せる必要は極論必要なくて、Google Pixel 上に搭載された機械学習モデルで処理を実行、その結果を視覚には Google Lens 、聴覚には Google Pixel Buds という組み合わせもありそうだなと思いを馳せた。
 How our Quality Raters make Search results better - Google Search Help
Google が公開している、検索品質の評価者が検索結果をどのようにかいぜんしているか。仕事でも検索品質の評価ガイドラインを先日書いていたが、かなり参考にさせてもらった。こういう質の高い文章を書けるようになっていきたい。 利用者にこういうことをするといいよとメッセージとして啓蒙する役割もあるのかもしれない。
Our Search Liaison on 25 years of keeping up with search 25 年間検索業界に関わってきた Google 検索の広報担当者である Danny さんのインタビュー記事。良い感じに Google 検索の改善の歴史がまとまっているので、良い記事をたくさん見つけることができた。"><meta name=author content="Shunya Ueta"><link rel=canonical href=https://shunyaueta.com/posts/2022-06-10-1135/><link crossorigin=anonymous href=/assets/css/stylesheet.abc7c82c3d415a6df50430738d1cbcc4c76fea558bc5a0c830d3babf78167a35.css integrity="sha256-q8fILD1BWm31BDBzjRy8xMdv6lWLxaDIMNO6v3gWejU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://shunyaueta.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shunyaueta.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shunyaueta.com/favicon-32x32.png><link rel=apple-touch-icon href=https://shunyaueta.com/apple-touch-icon.png><link rel=mask-icon href=https://shunyaueta.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','G-JMJRQJT0Q3','auto');ga('send','pageview');}</script><meta property="og:title" content="Search Engineering Newsletter vol.07"><meta property="og:description" content="Search Engineering Newsletter 7 回目の配信です。 今回のイチオシは、Andre Ng 先生の対談記事「Unbiggen AI」です。
Search Go beyond the search box: Introducing multisearch
Google が導入する、画像検索 → 自然言語による検索の二段階検索である multisaerch。
Google IO 2022 で、強調されていたのは Google Lens と組み合わせて multisaerch をできるようにしたいと提案されていて、なるほど! 確かに日常的に Google Lens を装着するようになれば multisearch の利用状況はものすごく自然だなと思った。 妄想だが、Google Lens がスタンドアロンで動くように機械学習モデルとかは乗せる必要は極論必要なくて、Google Pixel 上に搭載された機械学習モデルで処理を実行、その結果を視覚には Google Lens 、聴覚には Google Pixel Buds という組み合わせもありそうだなと思いを馳せた。
 How our Quality Raters make Search results better - Google Search Help
Google が公開している、検索品質の評価者が検索結果をどのようにかいぜんしているか。仕事でも検索品質の評価ガイドラインを先日書いていたが、かなり参考にさせてもらった。こういう質の高い文章を書けるようになっていきたい。 利用者にこういうことをするといいよとメッセージとして啓蒙する役割もあるのかもしれない。
Our Search Liaison on 25 years of keeping up with search 25 年間検索業界に関わってきた Google 検索の広報担当者である Danny さんのインタビュー記事。良い感じに Google 検索の改善の歴史がまとまっているので、良い記事をたくさん見つけることができた。"><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2022-06-10-1135/"><meta property="og:image" content="https://shunyaueta.com/posts/2022-01-16/images/1.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-10T11:35:48+09:00"><meta property="article:modified_time" content="2022-10-29T10:30:25+09:00"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/posts/2022-01-16/images/1.png"><meta name=twitter:title content="Search Engineering Newsletter vol.07"><meta name=twitter:description content="Search Engineering Newsletter 7 回目の配信です。 今回のイチオシは、Andre Ng 先生の対談記事「Unbiggen AI」です。
Search Go beyond the search box: Introducing multisearch
Google が導入する、画像検索 → 自然言語による検索の二段階検索である multisaerch。
Google IO 2022 で、強調されていたのは Google Lens と組み合わせて multisaerch をできるようにしたいと提案されていて、なるほど! 確かに日常的に Google Lens を装着するようになれば multisearch の利用状況はものすごく自然だなと思った。 妄想だが、Google Lens がスタンドアロンで動くように機械学習モデルとかは乗せる必要は極論必要なくて、Google Pixel 上に搭載された機械学習モデルで処理を実行、その結果を視覚には Google Lens 、聴覚には Google Pixel Buds という組み合わせもありそうだなと思いを馳せた。
 How our Quality Raters make Search results better - Google Search Help
Google が公開している、検索品質の評価者が検索結果をどのようにかいぜんしているか。仕事でも検索品質の評価ガイドラインを先日書いていたが、かなり参考にさせてもらった。こういう質の高い文章を書けるようになっていきたい。 利用者にこういうことをするといいよとメッセージとして啓蒙する役割もあるのかもしれない。
Our Search Liaison on 25 years of keeping up with search 25 年間検索業界に関わってきた Google 検索の広報担当者である Danny さんのインタビュー記事。良い感じに Google 検索の改善の歴史がまとまっているので、良い記事をたくさん見つけることができた。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Search Engineering Newsletter vol.07","item":"https://shunyaueta.com/posts/2022-06-10-1135/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Search Engineering Newsletter vol.07","name":"Search Engineering Newsletter vol.07","description":"Search Engineering Newsletter 7 回目の配信です。 今回のイチオシは、Andre Ng 先生の対談記事「Unbiggen AI」です。\nSearch Go beyond the search box: Introducing multisearch\nGoogle が導入する、画像検索 → 自然言語による検索の二段階検索である multisaerch。\nGoogle IO 2022 で、強調されていたのは Google Lens と組み合わせて multisaerch をできるようにしたいと提案されていて、なるほど! 確かに日常的に Google Lens を装着するようになれば multisearch の利用状況はものすごく自然だなと思った。 妄想だが、Google Lens がスタンドアロンで動くように機械学習モデルとかは乗せる必要は極論必要なくて、Google Pixel 上に搭載された機械学習モデルで処理を実行、その結果を視覚には Google Lens 、聴覚には Google Pixel Buds という組み合わせもありそうだなと思いを馳せた。\n How our Quality Raters make Search results better - Google Search Help\nGoogle が公開している、検索品質の評価者が検索結果をどのようにかいぜんしているか。仕事でも検索品質の評価ガイドラインを先日書いていたが、かなり参考にさせてもらった。こういう質の高い文章を書けるようになっていきたい。 利用者にこういうことをするといいよとメッセージとして啓蒙する役割もあるのかもしれない。\nOur Search Liaison on 25 years of keeping up with search 25 年間検索業界に関わってきた Google 検索の広報担当者である Danny さんのインタビュー記事。良い感じに Google 検索の改善の歴史がまとまっているので、良い記事をたくさん見つけることができた。","keywords":["newsletter","search"],"articleBody":"Search Engineering Newsletter 7 回目の配信です。 今回のイチオシは、Andre Ng 先生の対談記事「Unbiggen AI」です。\nSearch Go beyond the search box: Introducing multisearch\nGoogle が導入する、画像検索 → 自然言語による検索の二段階検索である multisaerch。\nGoogle IO 2022 で、強調されていたのは Google Lens と組み合わせて multisaerch をできるようにしたいと提案されていて、なるほど! 確かに日常的に Google Lens を装着するようになれば multisearch の利用状況はものすごく自然だなと思った。 妄想だが、Google Lens がスタンドアロンで動くように機械学習モデルとかは乗せる必要は極論必要なくて、Google Pixel 上に搭載された機械学習モデルで処理を実行、その結果を視覚には Google Lens 、聴覚には Google Pixel Buds という組み合わせもありそうだなと思いを馳せた。\n How our Quality Raters make Search results better - Google Search Help\nGoogle が公開している、検索品質の評価者が検索結果をどのようにかいぜんしているか。仕事でも検索品質の評価ガイドラインを先日書いていたが、かなり参考にさせてもらった。こういう質の高い文章を書けるようになっていきたい。 利用者にこういうことをするといいよとメッセージとして啓蒙する役割もあるのかもしれない。\nOur Search Liaison on 25 years of keeping up with search 25 年間検索業界に関わってきた Google 検索の広報担当者である Danny さんのインタビュー記事。良い感じに Google 検索の改善の歴史がまとまっているので、良い記事をたくさん見つけることができた。\nMore helpful product reviews on Search\nGoogle 検索が商品レビューの改善を行った。データを更に構造化したり、画像を表示するようになったらしい。\nGoogle Cloud が Optimization AI をリリース: お客様のルート計画を容易にする Cloud Fleet Routing API\nGCP が Optimization AIをリリース。大規模な最適化問題を API 経由で解決することが可能になる。配送ルートの最適化を行ったり、ワンクリックで Google Map と連携させることの可能らしい。 Chompy さんも過去に配送ルートの最適化に関する記事 1 が面白かったが、デリバリー関係の企業にすごく刺さりそうな API ですね。\n人手による確認を組み込んだ機械学習パイプラインの実現 | メルカリエンジニアリング\nメルペイによる、機械学習モデルの品質を維持するために人による検査を組み込む工夫を紹介した記事。\n 本来、できることならすべての処理を完全自動化し、人が作業するときの作業ミスのリスクを排除することで高効率な機械学習処理運用を実現したいところですが、品質を担保するために、モデル更新 ⇒ 推論処理 の間、 推論処理結果 ⇒ サービスへのリリース の間で次のステップに移ってもよいか関連データを人が確認する作業をしてから進めています。\n 与信関係だからこそ QA は最も重要なのでこの意思決定は非常に良いですね。\n検索アルゴリズム改善における機械学習の活用　〜MLOps について〜 - ぐるなびをちょっと良くするエンジニアブログ\nぐるなびでの機械学習でのランキング調整の実験速度を加速させるためにどのように MLOps を行ってきたかの変遷。GCP が提唱する MLOps レベル2にそって説明されている。AI Platform Pipeline から Vertex AI Pipelines (サーバーレス)に移行した結果、24 時間稼働から 1-2 時間程度の稼働に変わりコストが 9 割近くカットされたというお話がすごかった。\n検索エンジンの More-Like-This クエリとグラフアルゴリズムによる類似記事集約 - Stockmark Tech Blog\n類似記事の基準には埋め込みモデルを使って計算はせずに、Elasicsearch の More Like This クエリによって類似記事を取得して、それらをまとめた実験例\nMachine Learning \u0026 Data Science Multi-language Dataflow pipelines enabled by new, faster architecture\nDataflow で、多言語間での処理を橋渡しできる処理が限定的だが可能になったとのこと。例では、Python の処理内で、Java SDK を利用する例が紹介されている。これが発展すれば、基本的な処理は Java で、機械学習部分は Python でということも可能になる未来が見えた。(管理大変そうだが…)\nAndrew Ng: Unbiggen AI - IEEE Spectrum\nAndrew Ng 先生のインタビュー記事。 面白かった質疑をメモとして書いておきます。\n巨大モデルのその先は?\nNg: 自然言語と比べて、圧縮されてなお動画データは巨大なため、動画にも対応できるか? だが、対象的に小規模なデータから問題を解決したいニーズもあるのが課題。また、巨大モデルでまず先行して NLP 分野が盛り上がったのは、スケーラビリティの問題で、言語データはデータ量は他領域と比べて小さかったからこそ、巨大モデルを構築できたと考えている。 解決方法としては、半導体メーカーが 10 倍の処理能力を提供してくれれば、動画を扱える巨大モデルを構築することができると確信している。\nだが、 そうは言っても、過去 10 年間に起こったこ多くの事例は、非常に大規模なデータセットを持つ消費者向け企業で深層学習が使われたことで、機械学習のそのパラダイムは、消費者向けソフトウェアに多くの経済的価値をもたらしましたが、その規模の取り組みは、大規模なデータセットを持たない他の業界では機能しないことがわかった。\n与太話として面白かったのは、Google Brain を立ち上げた際に、大規模ニューラルネットワークを構築するために、Google 内部のインフラストラクチャを使って構築しようとした際に、偉い人に「インフラのスケールアップで解決するのではなく、モデルのアーキテクチャに目を向けたほうが良い」と水をさされたり、Ng 先生が初めて NeurIPS で CUDA を扱うワークショップを開催した際に、機械学習業界の熟練者が「CUDA は複雑なプログラミングモデルだから、普及しないんじゃないのか? 」とアドバイスしてきたが、今は二つとも主流になっているのが、先見の明がすごかった。\nHow we kept information on Maps reliable in 2021\n2021 年の段階で、Google Map 上での情報をどのように信頼性を保っているか。 COVID-19 の影響で、Google Map の必要性が更にたかまりそれに伴って扱う情報も信頼性を上げるかが急務になったとのこと。\n700 万枚の Google Map のビジネスプロフィールを機械学習により削除、悪意のある 1200 万件以上のビジネスプロフィール作成依頼に対応し、機械学習により 100 万以上の詐欺行為を行うアカウントを無効化した。\n9500 万枚の規約違反のレビューを削除したり、最終的に 1.9 億の画像を削除し、500 万件の低品質な動画(ぼやけてたり、規約違反なもの)を削除。\n規模がすごすぎる。さすが MAU が 10 億超えのサービスは違いますね。\nBigQuery Remote Functions (Preview) をデータパイプラインに組み込んでみました’\nPreview ですが、BigQuery で Cloud Function を呼び出せる Remote Funcitons 機能の紹介記事。BigQuery 内では完結できないような処理を Apache Beam とかではなく、Cloud Function を使って処理を行えるのはお手軽で良さそうです。 BigQuery は UDF で JavaScript などで処理もできますが、Python で形態素解析とかちょっとしたことをできたら夢が広がりますね。\n値段帯が気になったので調べてみたところ\n   呼び出し回数（1 か月あたり） 料金（100 万回あたり）     最初の 200 万回 無料   200 万回を超えた分: $0.40    と書いており、個人用途としては、毎月 200 万回まで無料というのは素晴らしいが、数億規模の行数を RemoteFunction で行うと、即死しそうな気がしたが、どうなんだろうか…?\nMachine Learning at Wolt: Our journey towards MLOps - Wolt Blog\nDoorDash に買収された フードデリバリーサービスの Wolt の MLOps 事例。 主に、ロジスティクス周りで機械学習を活用しており、配送時間予測などが最たる事例。 データサイエンティストによる機械学習モデルのデプロイ方法が集約・管理されておらず、中央集権的にサービス提供ができていなかったので機械学習基盤を刷新した。\nリアルタイムでのフレームワークとしてSeldon-Core を利用。採用理由としては OSS なので、自分たちが欲しい機能をコントリビュートの形式で提案できるところや、V2 Data Plane inference API により、様々な機械学習・深層学習フレームワーク似依存しない API として提供できるのが利点とのこと。\nNOTE: V2 Data Plane inference API は、調べてみると KServe が提案する、特定の機械学習・深層学習フレームワークに依存しない推論・予測を提案しているプロトコルで、GYYP, gRPC 形式で、モデルのメタデータやヘルスチェック、推論をどのように行うかが提案されている。NVIDIA Triton Inference Server3, TensofFlow Serving , ONNX Runtime Server が準拠している。 余談) KServe は、k8s を基盤とした OSS の機械学習基盤である KubeFlow のサービング部分を担当していた KFServing が Kserve という名前に変更された。 k8s ベースで、seldon の Deployment のマニフェストを作れば、REST\u0026 gRPC のサービスがデプロイされるようにしており、かなり抽象化されている。\n面白かったのは、Wolt は内部のアルゴリズムを詳細ではないがどのように動いているかを透明性を目的に公開するページ4があり、透明性の動きとして面白かった。\n感想など Twitter で #searchengineeringnewsletter のハッシュタグでつぶやいていただくか、 Google フォーム での感想投稿をお待ちしております。\n執筆の励みにさせていただきます。\nSearch Engineering Newsletter の購読方法 配信記事が蓄積される RSSを作成しています。\nまた、今までの配信記事一覧もこちらから閲覧できます。\n余談 前回の newsletter をきっかけに、szdr さんから投げ銭をいただきました! ありがとうございます! 良い書籍を世に出していただいたので、本来なら僕から贈りたいところですが…\n  Chompy を支えるアルゴリズムに迫る 過去に詳細な技術記事があったが、削除されてしまったみたいですね。残念 ↩︎\n https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning ↩︎\n GPU に推論を Triton Inference Server でかんたんデプロイ ↩︎\n Transparency at Wolt ↩︎\n   ","wordCount":"418","inLanguage":"ja","image":"https://shunyaueta.com/posts/2022-01-16/images/1.png","datePublished":"2022-06-10T11:35:48+09:00","dateModified":"2022-10-29T10:30:25+09:00","author":{"@type":"Person","name":"Shunya Ueta"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://shunyaueta.com/posts/2022-06-10-1135/"},"publisher":{"@type":"Organization","name":"🦅 hurutoriya","logo":{"@type":"ImageObject","url":"https://shunyaueta.com/favicon.ico"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href=https://shunyaueta.com/ accesskey=h title="🦅 hurutoriya (Alt + H)">🦅 hurutoriya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://shunyaueta.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://shunyaueta.com/about title=About><span>About</span></a></li><li><a href=https://shunyaueta.com/tags/newsletter/ title=Newsletter><span>Newsletter</span></a></li><li><a href=https://shunyaueta.com/index.xml title=RSS><span>RSS</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Search Engineering Newsletter vol.07</h1><div class=post-meta><span title="2022-06-10 11:35:48 +0900 +0900">June 10, 2022</span>&nbsp;·&nbsp;Shunya Ueta</div></header><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2022-01-16/images/1.png alt></figure><div class=post-content><p>Search Engineering Newsletter 7 回目の配信です。
今回のイチオシは、Andre Ng 先生の対談記事「Unbiggen AI」です。</p><h2 id=search>Search<a hidden class=anchor aria-hidden=true href=#search>#</a></h2><p><a href=https://blog.google/products/search/multisearch/>Go beyond the search box: Introducing multisearch</a></p><p>Google が導入する、画像検索 → 自然言語による検索の二段階検索である multisaerch。</p><p>Google IO 2022 で、強調されていたのは Google Lens と組み合わせて multisaerch をできるようにしたいと提案されていて、なるほど! 確かに日常的に Google Lens を装着するようになれば multisearch の利用状況はものすごく自然だなと思った。
妄想だが、Google Lens がスタンドアロンで動くように機械学習モデルとかは乗せる必要は極論必要なくて、Google Pixel 上に搭載された機械学習モデルで処理を実行、その結果を視覚には Google Lens 、聴覚には Google Pixel Buds という組み合わせもありそうだなと思いを馳せた。</p><iframe width=560 height=315 src=https://www.youtube.com/embed/lj0bFX9HXeE title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><a href="https://support.google.com/websearch/answer/9281931?hl=en&utm_source=pocket_mylist">How our Quality Raters make Search results better - Google Search Help</a></p><p>Google が公開している、検索品質の評価者が検索結果をどのようにかいぜんしているか。仕事でも検索品質の評価ガイドラインを先日書いていたが、かなり参考にさせてもらった。こういう質の高い文章を書けるようになっていきたい。
利用者にこういうことをするといいよとメッセージとして啓蒙する役割もあるのかもしれない。</p><p><a href=https://blog.google/products/search/danny-25-years-of-search/>Our Search Liaison on 25 years of keeping up with search</a>
25 年間検索業界に関わってきた Google 検索の広報担当者である Danny さんのインタビュー記事。良い感じに Google 検索の改善の歴史がまとまっているので、良い記事をたくさん見つけることができた。</p><p><a href=https://blog.google/products/search/more-helpful-product-reviews/>More helpful product reviews on Search</a></p><p>Google 検索が商品レビューの改善を行った。データを更に構造化したり、画像を表示するようになったらしい。</p><p><a href=https://cloud.google.com/blog/ja/products/ai-machine-learning/google-cloud-optimization-ai-cloud-fleet-routing-api>Google Cloud が Optimization AI をリリース: お客様のルート計画を容易にする Cloud Fleet Routing API</a></p><p>GCP が <a href=https://cloud.google.com/optimization>Optimization AI</a>をリリース。大規模な最適化問題を API 経由で解決することが可能になる。配送ルートの最適化を行ったり、ワンクリックで Google Map と連携させることの可能らしい。
Chompy さんも過去に配送ルートの最適化に関する記事 <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> が面白かったが、デリバリー関係の企業にすごく刺さりそうな API ですね。</p><p><a href=https://engineering.mercari.com/blog/entry/20220428-06da50715c/>人手による確認を組み込んだ機械学習パイプラインの実現 | メルカリエンジニアリング</a></p><p>メルペイによる、機械学習モデルの品質を維持するために人による検査を組み込む工夫を紹介した記事。</p><blockquote><p>本来、できることならすべての処理を完全自動化し、人が作業するときの作業ミスのリスクを排除することで高効率な機械学習処理運用を実現したいところですが、品質を担保するために、モデル更新 ⇒ 推論処理 の間、 推論処理結果 ⇒ サービスへのリリース の間で次のステップに移ってもよいか関連データを人が確認する作業をしてから進めています。</p></blockquote><p>与信関係だからこそ QA は最も重要なのでこの意思決定は非常に良いですね。</p><p><a href=https://developers.gnavi.co.jp/entry/ml-ops/>検索アルゴリズム改善における機械学習の活用　〜MLOps について〜 - ぐるなびをちょっと良くするエンジニアブログ</a></p><p>ぐるなびでの機械学習でのランキング調整の実験速度を加速させるためにどのように MLOps を行ってきたかの変遷。GCP が提唱する MLOps レベル<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>にそって説明されている。AI Platform Pipeline から Vertex AI Pipelines (サーバーレス)に移行した結果、24 時間稼働から 1-2 時間程度の稼働に変わりコストが 9 割近くカットされたというお話がすごかった。</p><p><a href=https://tech.stockmark.co.jp/blog/similar_articles_ja/>検索エンジンの More-Like-This クエリとグラフアルゴリズムによる類似記事集約 - Stockmark Tech Blog</a></p><p>類似記事の基準には埋め込みモデルを使って計算はせずに、Elasicsearch の More Like This クエリによって類似記事を取得して、それらをまとめた実験例</p><h2 id=machine-learning--data-science>Machine Learning & Data Science<a hidden class=anchor aria-hidden=true href=#machine-learning--data-science>#</a></h2><p><a href=https://cloud.google.com/blog/products/data-analytics/multi-language-sdks-for-building-cloud-pipelines>Multi-language Dataflow pipelines enabled by new, faster architecture</a></p><p>Dataflow で、多言語間での処理を橋渡しできる処理が限定的だが可能になったとのこと。例では、Python の処理内で、Java SDK を利用する例が紹介されている。これが発展すれば、基本的な処理は Java で、機械学習部分は Python でということも可能になる未来が見えた。(管理大変そうだが&mldr;)</p><p><a href=https://spectrum.ieee.org/andrew-ng-data-centric-ai>Andrew Ng: Unbiggen AI - IEEE Spectrum</a></p><p>Andrew Ng 先生のインタビュー記事。
面白かった質疑をメモとして書いておきます。</p><p>巨大モデルのその先は?</p><p>Ng: 自然言語と比べて、圧縮されてなお動画データは巨大なため、動画にも対応できるか? だが、対象的に小規模なデータから問題を解決したいニーズもあるのが課題。また、巨大モデルでまず先行して NLP 分野が盛り上がったのは、スケーラビリティの問題で、言語データはデータ量は他領域と比べて小さかったからこそ、巨大モデルを構築できたと考えている。
解決方法としては、半導体メーカーが 10 倍の処理能力を提供してくれれば、動画を扱える巨大モデルを構築することができると確信している。</p><p>だが、 そうは言っても、過去 10 年間に起こったこ多くの事例は、非常に大規模なデータセットを持つ消費者向け企業で深層学習が使われたことで、機械学習のそのパラダイムは、消費者向けソフトウェアに多くの経済的価値をもたらしましたが、その規模の取り組みは、大規模なデータセットを持たない他の業界では機能しないことがわかった。</p><p>与太話として面白かったのは、Google Brain を立ち上げた際に、大規模ニューラルネットワークを構築するために、Google 内部のインフラストラクチャを使って構築しようとした際に、偉い人に「インフラのスケールアップで解決するのではなく、モデルのアーキテクチャに目を向けたほうが良い」と水をさされたり、Ng 先生が初めて NeurIPS で CUDA を扱うワークショップを開催した際に、機械学習業界の熟練者が「CUDA は複雑なプログラミングモデルだから、普及しないんじゃないのか? 」とアドバイスしてきたが、今は二つとも主流になっているのが、先見の明がすごかった。</p><p><a href=https://blog.google/products/maps/how-we-kept-maps-reliable-2021/>How we kept information on Maps reliable in 2021</a></p><p>2021 年の段階で、Google Map 上での情報をどのように信頼性を保っているか。
COVID-19 の影響で、Google Map の必要性が更にたかまりそれに伴って扱う情報も信頼性を上げるかが急務になったとのこと。</p><p>700 万枚の Google Map のビジネスプロフィールを機械学習により削除、悪意のある 1200 万件以上のビジネスプロフィール作成依頼に対応し、機械学習により 100 万以上の詐欺行為を行うアカウントを無効化した。</p><p>9500 万枚の規約違反のレビューを削除したり、最終的に 1.9 億の画像を削除し、500 万件の低品質な動画(ぼやけてたり、規約違反なもの)を削除。</p><p>規模がすごすぎる。さすが MAU が 10 億超えのサービスは違いますね。</p><p><a href=https://lab.mo-t.com/blog/bq-remote-functions>BigQuery Remote Functions (Preview) をデータパイプラインに組み込んでみました</a>&rsquo;</p><p>Preview ですが、BigQuery で Cloud Function を呼び出せる Remote Funcitons 機能の紹介記事。BigQuery 内では完結できないような処理を Apache Beam とかではなく、Cloud Function を使って処理を行えるのはお手軽で良さそうです。
BigQuery は UDF で JavaScript などで処理もできますが、Python で形態素解析とかちょっとしたことをできたら夢が広がりますね。</p><p>値段帯が気になったので調べてみたところ</p><table><thead><tr><th>呼び出し回数（1 か月あたり）</th><th>料金（100 万回あたり）</th></tr></thead><tbody><tr><td>最初の 200 万回</td><td>無料</td></tr><tr><td>200 万回を超えた分:</td><td>$0.40</td></tr></tbody></table><p>と書いており、個人用途としては、毎月 200 万回まで無料というのは素晴らしいが、数億規模の行数を RemoteFunction で行うと、即死しそうな気がしたが、どうなんだろうか&mldr;?</p><p><a href=https://blog.wolt.com/engineering/2022/04/20/machine-learning-at-wolt-our-journey-towards-mlops/>Machine Learning at Wolt: Our journey towards MLOps - Wolt Blog</a></p><p>DoorDash に買収された フードデリバリーサービスの Wolt の MLOps 事例。
主に、ロジスティクス周りで機械学習を活用しており、配送時間予測などが最たる事例。
データサイエンティストによる機械学習モデルのデプロイ方法が集約・管理されておらず、中央集権的にサービス提供ができていなかったので機械学習基盤を刷新した。</p><p>リアルタイムでのフレームワークとして<a href=https://github.com/SeldonIO/seldon-core>Seldon-Core</a> を利用。採用理由としては OSS なので、自分たちが欲しい機能をコントリビュートの形式で提案できるところや、<a href=https://kserve.github.io/website/modelserving/inference_api/>V2 Data Plane inference API</a> により、様々な機械学習・深層学習フレームワーク似依存しない API として提供できるのが利点とのこと。</p><p><code>NOTE</code>: <code>V2 Data Plane inference API</code> は、調べてみると KServe が提案する、特定の機械学習・深層学習フレームワークに依存しない推論・予測を提案しているプロトコルで、GYYP, gRPC 形式で、モデルのメタデータやヘルスチェック、推論をどのように行うかが提案されている。NVIDIA Triton Inference Server<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, TensofFlow Serving , ONNX Runtime Server が準拠している。
余談) <a href=https://www.kubeflow.org/docs/external-add-ons/kserve/kserve/>KServe </a>は、k8s を基盤とした OSS の機械学習基盤である KubeFlow のサービング部分を担当していた KFServing が Kserve という名前に変更された。
k8s ベースで、seldon の Deployment のマニフェストを作れば、REST& gRPC のサービスがデプロイされるようにしており、かなり抽象化されている。</p><p>面白かったのは、Wolt は内部のアルゴリズムを詳細ではないがどのように動いているかを透明性を目的に公開するページ<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>があり、透明性の動きとして面白かった。</p><h2 id=感想など>感想など<a hidden class=anchor aria-hidden=true href=#感想など>#</a></h2><p>Twitter で <a href="https://twitter.com/hashtag/searchengineeringnewsletter?f=live">#searchengineeringnewsletter</a> のハッシュタグでつぶやいていただくか、
<a href=https://forms.gle/xFgMwRJbeqJxNtfe9>Google フォーム</a> での感想投稿をお待ちしております。</p><p>執筆の励みにさせていただきます。</p><h2 id=search-engineering-newsletter-の購読方法>Search Engineering Newsletter の購読方法<a hidden class=anchor aria-hidden=true href=#search-engineering-newsletter-の購読方法>#</a></h2><p>配信記事が蓄積される <a href=https://shunyaueta.com/tags/newsletter/index.xml>RSS</a>を作成しています。</p><p>また、今までの配信記事一覧も<a href=https://shunyaueta.com/tags/newsletter/>こちら</a>から閲覧できます。</p><h2 id=余談>余談<a hidden class=anchor aria-hidden=true href=#余談>#</a></h2><p>前回の <a href=/posts/2022-05-24-2346/>newsletter</a> をきっかけに、szdr さんから<a href=https://www.buymeacoffee.com/hurutoriya/c/3349194>投げ銭</a>をいただきました! ありがとうございます!
<a href=https://www.lambdanote.com/blogs/news/ir-system>良い書籍</a>を世に出していただいたので、本来なら僕から贈りたいところですが&mldr;</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=https://note.com/chompy/n/nb833abe6cebe>Chompy を支えるアルゴリズムに迫る</a> 過去に詳細な技術記事があったが、削除されてしまったみたいですね。残念 <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><a href=https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning>https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</a> <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p><a href=https://medium.com/nvidiajapan/gpu-for-inference-easy-deploy-by-triton-inference-server-fd2980514af2>GPU に推論を Triton Inference Server でかんたんデプロイ</a> <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p><a href=https://explore.wolt.com/en/hun/transparency>Transparency at Wolt</a> <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section><h2>See Also</h2><ul><li><a href=/posts/2022-05-24-2346/>Search Engineering Newsletter vol.06</a></li><li><a href=/posts/2022-05-02-2350/>Search Engineering Newsletter vol.05</a></li><li><a href=/posts/2022-04-07/>Search Engineering Newsletter vol.04</a></li><li><a href=/posts/2022-03-28/>Search Engineering Newsletter vol.03</a></li><li><a href=/posts/2022-02-09/>Search Engineering Newsletter vol.02</a></li></ul><h2>Support</h2>記事をお読みくださりありがとうございます。このウェブサイトの運営を支援していただける方を募集しています。
もしよろしければ、下のボタンからサポート(投げ銭)していただけると、ブログ執筆、情報発信のモチベーションに繋がります✨
<a href=https://www.buymeacoffee.com/hurutoriya><img src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&emoji=☕&slug=hurutoriya&button_colour=FFDD00&font_colour=000000&font_family=Inter&outline_colour=000000&coffee_colour=ffffff"></a></div><footer class=post-footer><ul class=post-tags><li><a href=https://shunyaueta.com/tags/search/>search</a></li><li><a href=https://shunyaueta.com/tags/newsletter/>newsletter</a></li></ul><nav class=paginav><a class=prev href=https://shunyaueta.com/posts/2022-06-22-0001/><span class=title>« 前のページ</span><br><span>Makefile でコマンドの前に @ を付けると、コマンド自身は表示されず結果のみ表示される</span></a>
<a class=next href=https://shunyaueta.com/posts/2022-06-03-2133/><span class=title>次のページ »</span><br><span>愛用しているツールを更新: Joplin→Obsidian & TickTick → Todoist</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=hurutoriya/hurutoriya.github.io data-repo-id="MDEwOlJlcG9zaXRvcnk2MDgyNTY1Nw==" data-category=Comments data-category-id=DIC_kwDOA6AgOc4CAQTX data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=dark data-lang=ja crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://shunyaueta.com/>🦅 hurutoriya</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z" /></svg></a><script>let menu=document.getElementById('menu')
if(menu){menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script><script>document.querySelectorAll('pre > code').forEach((codeblock)=>{const container=codeblock.parentNode.parentNode;const copybutton=document.createElement('button');copybutton.classList.add('copy-code');copybutton.innerHTML='コピー';function copyingDone(){copybutton.innerHTML='コピーされました!';setTimeout(()=>{copybutton.innerHTML='コピー';},2000);}
copybutton.addEventListener('click',(cb)=>{if('clipboard'in navigator){navigator.clipboard.writeText(codeblock.textContent);copyingDone();return;}
const range=document.createRange();range.selectNodeContents(codeblock);const selection=window.getSelection();selection.removeAllRanges();selection.addRange(range);try{document.execCommand('copy');copyingDone();}catch(e){};selection.removeRange(range);});if(container.classList.contains("highlight")){container.appendChild(copybutton);}else if(container.parentNode.firstChild==container){}else if(codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"){codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);}else{codeblock.parentNode.appendChild(copybutton);}});</script></body></html>