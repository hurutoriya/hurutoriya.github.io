<!doctype html><html lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme-color content="dark"><title>Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ | Shunya Ueta</title><link rel=stylesheet href=/sass/main.min.1a3290acca8b3fc92df85ea9200859476d6c80d59009c89a749300f3ce7a7a67.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-39994406-11','auto');ga('send','pageview');}</script><meta property="og:title" content="Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ"><meta property="og:description" content="Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ
Mikel Rodriguez, Josef Sivic, Ivan Laptev, Jean-Yves Audibert, “Data-driven Crowd Analysis in Videos”, in ICCV2011.
Project Page
を読んだので、メモです。
Summary
tl;dr  高密度な群集内の個人を追跡を転移学習によって精度を向上させる手法  Contribution  追跡の精度を転移学習によって向上させた 転移学習を行うためのデータセットとそのフレームワークを考案  論文内では、転移学習の例としてマラソンAの群集を対象に追跡する際に、以下の流れで転移学習を行う。
 大域的な群衆状況のマッチング : 同じようなシーンを探索(この場合 DB 内にあるマラソン動画) 局所的な群衆状況のマッチング : 1でマッチした動画においてオプティカルフローが類似するパッチを探索して転移学習  また、Rare Events(デモの最中に群集を横断するカメラマンなど、群衆の流れに対して同調しない動きを行う人物)に対しても実験を行い評価。
Comments 転移学習は自分のイメージだと、自然言語処理のイメージ(一般的な文書を学習したモデルを法律文書に対して適用するなど)しかなかったので新鮮な気持ちで読めた。
動画なら転移学習を行ったとしても、直感的に良い特徴を学べそうなので、良い仮説を立てている論文でした。
最後に示されてる個人追跡における平均誤検出の単位がpixelだが、Ground-Truth と提案手法の追跡軌跡の重複度具合を見てると誤検出が更に高そうに見えるけどどうなんでしょうか？
(テストデータのみ学習が 58.82、転移学習を行った提案手法だと 46.88[pixel]になっていてもっと相対的な差が出てくるはず?)"><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2018-01-17/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:published_time" content="2018-01-17T05:55:41+00:00"><meta property="article:modified_time" content="2019-06-16T18:17:47+09:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ"><meta name=twitter:description content="Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ
Mikel Rodriguez, Josef Sivic, Ivan Laptev, Jean-Yves Audibert, “Data-driven Crowd Analysis in Videos”, in ICCV2011.
Project Page
を読んだので、メモです。
Summary
tl;dr  高密度な群集内の個人を追跡を転移学習によって精度を向上させる手法  Contribution  追跡の精度を転移学習によって向上させた 転移学習を行うためのデータセットとそのフレームワークを考案  論文内では、転移学習の例としてマラソンAの群集を対象に追跡する際に、以下の流れで転移学習を行う。
 大域的な群衆状況のマッチング : 同じようなシーンを探索(この場合 DB 内にあるマラソン動画) 局所的な群衆状況のマッチング : 1でマッチした動画においてオプティカルフローが類似するパッチを探索して転移学習  また、Rare Events(デモの最中に群集を横断するカメラマンなど、群衆の流れに対して同調しない動きを行う人物)に対しても実験を行い評価。
Comments 転移学習は自分のイメージだと、自然言語処理のイメージ(一般的な文書を学習したモデルを法律文書に対して適用するなど)しかなかったので新鮮な気持ちで読めた。
動画なら転移学習を行ったとしても、直感的に良い特徴を学べそうなので、良い仮説を立てている論文でした。
最後に示されてる個人追跡における平均誤検出の単位がpixelだが、Ground-Truth と提案手法の追跡軌跡の重複度具合を見てると誤検出が更に高そうに見えるけどどうなんでしょうか？
(テストデータのみ学習が 58.82、転移学習を行った提案手法だと 46.88[pixel]になっていてもっと相対的な差が出てくるはず?)"></head><body class=dark><nav class=navbar><div class=container><div class=flex><div><a class=brand href=/><span class=emoji>🦅</span>
Shunya Ueta</a></div><div class=flex><a href=/articles/>All posts</a>
<button id=dark-mode-button></button></div></div></div></nav><main><div class=container><article><header class=article-header><div class=thumb><div><h1>Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ</h1><div class=post-meta><div>By on <time>2018.01.17</time> (Last updated:<time>2019.06.16</time>)</div><div class=tags><a href=/tags/paper/>paper</a>
<a href=/tags/computervision/>computervision</a></div></div></div></div></header></article><div class=article-post><p>Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ</p><p>Mikel Rodriguez, Josef Sivic, Ivan Laptev, Jean-Yves Audibert, “Data-driven Crowd Analysis in Videos”, in ICCV2011.<br><a href=http://www.di.ens.fr/willow/research/datadriven/>Project Page</a></p><p>を読んだので、メモです。</p><p><img src=/posts/2018-01-17/images/1.png alt=image></p><p>Summary</p><h3 id=tldr>tl;dr</h3><ul><li>高密度な群集内の個人を追跡を転移学習によって精度を向上させる手法</li></ul><h3 id=contribution>Contribution</h3><ul><li>追跡の精度を転移学習によって向上させた</li><li>転移学習を行うためのデータセットとそのフレームワークを考案</li></ul><p>論文内では、転移学習の例として<code>マラソンA</code>の群集を対象に追跡する際に、以下の流れで転移学習を行う。</p><ol><li>大域的な群衆状況のマッチング : 同じようなシーンを探索(この場合 DB 内にあるマラソン動画)</li><li>局所的な群衆状況のマッチング : <code>1</code>でマッチした動画においてオプティカルフローが類似するパッチを探索して転移学習</li></ol><p>また、Rare Events(デモの最中に群集を横断するカメラマンなど、群衆の流れに対して同調しない動きを行う人物)に対しても実験を行い評価。</p><h3 id=comments>Comments</h3><p>転移学習は自分のイメージだと、自然言語処理のイメージ(一般的な文書を学習したモデルを法律文書に対して適用するなど)しかなかったので新鮮な気持ちで読めた。<br>動画なら転移学習を行ったとしても、直感的に良い特徴を学べそうなので、良い仮説を立てている論文でした。</p><p>最後に示されてる個人追跡における平均誤検出の単位が<code>pixel</code>だが、Ground-Truth と提案手法の追跡軌跡の重複度具合を見てると誤検出が更に高そうに見えるけどどうなんでしょうか？<br>(テストデータのみ学習が 58.82、転移学習を行った提案手法だと 46.88[pixel]になっていてもっと相対的な差が出てくるはず?)</p></div><h2>Support</h2><div class=container><div class=posts><div class=post><div class=post-row><a href=https://www.buymeacoffee.com/hurutoriya target=_blank>☕️ Buy me a cofee: お読みくださりありがとうございます。
こちらから ☕ を一杯支援していただけると、ブログ執筆のモチベーションに繋がります ✨</a></div></div></div></div><h2>See Also</h2><ul><li><a href=/posts/2018-01-16/>Slicing Convolutional Neural Network for Crowd Video Understanding (CVPR2016)を読んだ</a></li><li><a href=/posts/2018-01-14/>Analyzing Free-standing Conversational Groups: A Multimodal Approach (ACMMM15) を読んだ</a></li><li><a href=/posts/2018-01-12/>Call center stress recognition with person-specific models を読んだ</a></li><li><a href=/posts/2018-01-11/>FUSE: Full Spectral Clustering(KDD2016) を読んだ</a></li><li><a href=/posts/2017-12-23/>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ</a></li></ul></div><div class=container><nav class="flex container suggested"><a rel=prev href=/posts/2018-01-16/ title="Previous post (older)"><span>Previous</span>
Slicing Convolutional Neural Network for Crowd Video Understanding (CVPR2016)を読んだ</a>
<a rel=next href=/posts/2018-01-18/ title="Next post (newer)"><span>Next</span>
Where To Look: Focus Regions for Visual Question Answering (CVPR2016)を読んだ</a></nav></div></main></main><footer class="footer flex"><section class=container><nav class=footer-links><a href=/about/>About</a>
<a href=/index.xml>RSS</a>
<a href=https://www.buymeacoffee.com/hurutoriya>Support</a></nav></section><script async src=/js/features.min.a94f58a30ad2560de728e080d87f75c60cf806fd1b3d5f4815f1a1a02c0d1859.js></script></footer></body></html>