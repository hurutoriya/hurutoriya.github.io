<!doctype html><html lang=ja><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Google Cloud Pub/Sub に公開された結果はDataflow template を使えばめちゃくちゃ簡単に確認できる | hurutoriya</title><meta name=title content="Google Cloud Pub/Sub に公開された結果はDataflow template を使えばめちゃくちゃ簡単に確認できる"><meta name=description content="PubSub に出力された結果を確認するのって、なかなか手間がかかりませんか?
最近同僚に簡単な確認方法を教えてもらい、感動したのでそれを記事にしました。
確認方法 PubSub のメッセージを出力する Google Cloud Storage bucket を同一 GCP プロジェクトで作成する。 GCP の Pub/Sub ページに移動する 確認したい Pub/Sub topic をクリックする ページ下部にある CREATE SUBSCRIPTION ボタンを押すと選択肢で、Create subscription, Export to BigQuery, Export to Cloud Storageがあり、 Export to Cloud Storageを選択する。 BigQuery、 Google Cloud Storage への吐き出しを行い際に、自動的に subscription が生成される。 Export to Cloud Storage を選択すると、Text 形式か Abro 形式での出力にするかを選択できる。基本的には簡単に確認できる Text 形式を選ぶと良さげ。 選択後、下記のような設定画面が出てくるので情報を埋めていく。基本的には、どこの Google Cloud Storage に出力するかを埋めれば完了。 10m ほどすると Streaming job の Dataflow の起動が完了して、一定期間ごとに Pub/Sub の topic に公開されたデータがテキスト形式で出力され始めます。 出力された GCS の結果を眺めるには、 gsutil コマンドなどを使うのが簡単です。自分はgsutil cat の結果をコピーして VS Code で確認しています。 Cloud Dataflow のテンプレート機能については、端的に説明すると、GUI でパラメータを設定するだけで、Dataflow によるデータ処理が簡単に実行できるようになる機能です。"><meta name=keywords content="gcp,pubsub,dataflow,"><meta property="og:title" content="Google Cloud Pub/Sub に公開された結果はDataflow template を使えばめちゃくちゃ簡単に確認できる"><meta property="og:description" content="PubSub に出力された結果を確認するのって、なかなか手間がかかりませんか?
最近同僚に簡単な確認方法を教えてもらい、感動したのでそれを記事にしました。
確認方法 PubSub のメッセージを出力する Google Cloud Storage bucket を同一 GCP プロジェクトで作成する。 GCP の Pub/Sub ページに移動する 確認したい Pub/Sub topic をクリックする ページ下部にある CREATE SUBSCRIPTION ボタンを押すと選択肢で、Create subscription, Export to BigQuery, Export to Cloud Storageがあり、 Export to Cloud Storageを選択する。 BigQuery、 Google Cloud Storage への吐き出しを行い際に、自動的に subscription が生成される。 Export to Cloud Storage を選択すると、Text 形式か Abro 形式での出力にするかを選択できる。基本的には簡単に確認できる Text 形式を選ぶと良さげ。 選択後、下記のような設定画面が出てくるので情報を埋めていく。基本的には、どこの Google Cloud Storage に出力するかを埋めれば完了。 10m ほどすると Streaming job の Dataflow の起動が完了して、一定期間ごとに Pub/Sub の topic に公開されたデータがテキスト形式で出力され始めます。 出力された GCS の結果を眺めるには、 gsutil コマンドなどを使うのが簡単です。自分はgsutil cat の結果をコピーして VS Code で確認しています。 Cloud Dataflow のテンプレート機能については、端的に説明すると、GUI でパラメータを設定するだけで、Dataflow によるデータ処理が簡単に実行できるようになる機能です。"><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2021-11-05/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-11-05T23:05:22+09:00"><meta property="article:modified_time" content="2021-11-05T23:05:22+09:00"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="Google Cloud Pub/Sub に公開された結果はDataflow template を使えばめちゃくちゃ簡単に確認できる"><meta name=twitter:description content="PubSub に出力された結果を確認するのって、なかなか手間がかかりませんか?
最近同僚に簡単な確認方法を教えてもらい、感動したのでそれを記事にしました。
確認方法 PubSub のメッセージを出力する Google Cloud Storage bucket を同一 GCP プロジェクトで作成する。 GCP の Pub/Sub ページに移動する 確認したい Pub/Sub topic をクリックする ページ下部にある CREATE SUBSCRIPTION ボタンを押すと選択肢で、Create subscription, Export to BigQuery, Export to Cloud Storageがあり、 Export to Cloud Storageを選択する。 BigQuery、 Google Cloud Storage への吐き出しを行い際に、自動的に subscription が生成される。 Export to Cloud Storage を選択すると、Text 形式か Abro 形式での出力にするかを選択できる。基本的には簡単に確認できる Text 形式を選ぶと良さげ。 選択後、下記のような設定画面が出てくるので情報を埋めていく。基本的には、どこの Google Cloud Storage に出力するかを埋めれば完了。 10m ほどすると Streaming job の Dataflow の起動が完了して、一定期間ごとに Pub/Sub の topic に公開されたデータがテキスト形式で出力され始めます。 出力された GCS の結果を眺めるには、 gsutil コマンドなどを使うのが簡単です。自分はgsutil cat の結果をコピーして VS Code で確認しています。 Cloud Dataflow のテンプレート機能については、端的に説明すると、GUI でパラメータを設定するだけで、Dataflow によるデータ処理が簡単に実行できるようになる機能です。"><meta itemprop=name content="Google Cloud Pub/Sub に公開された結果はDataflow template を使えばめちゃくちゃ簡単に確認できる"><meta itemprop=description content="PubSub に出力された結果を確認するのって、なかなか手間がかかりませんか?
最近同僚に簡単な確認方法を教えてもらい、感動したのでそれを記事にしました。
確認方法 PubSub のメッセージを出力する Google Cloud Storage bucket を同一 GCP プロジェクトで作成する。 GCP の Pub/Sub ページに移動する 確認したい Pub/Sub topic をクリックする ページ下部にある CREATE SUBSCRIPTION ボタンを押すと選択肢で、Create subscription, Export to BigQuery, Export to Cloud Storageがあり、 Export to Cloud Storageを選択する。 BigQuery、 Google Cloud Storage への吐き出しを行い際に、自動的に subscription が生成される。 Export to Cloud Storage を選択すると、Text 形式か Abro 形式での出力にするかを選択できる。基本的には簡単に確認できる Text 形式を選ぶと良さげ。 選択後、下記のような設定画面が出てくるので情報を埋めていく。基本的には、どこの Google Cloud Storage に出力するかを埋めれば完了。 10m ほどすると Streaming job の Dataflow の起動が完了して、一定期間ごとに Pub/Sub の topic に公開されたデータがテキスト形式で出力され始めます。 出力された GCS の結果を眺めるには、 gsutil コマンドなどを使うのが簡単です。自分はgsutil cat の結果をコピーして VS Code で確認しています。 Cloud Dataflow のテンプレート機能については、端的に説明すると、GUI でパラメータを設定するだけで、Dataflow によるデータ処理が簡単に実行できるようになる機能です。"><meta itemprop=datePublished content="2021-11-05T23:05:22+09:00"><meta itemprop=dateModified content="2021-11-05T23:05:22+09:00"><meta itemprop=wordCount content="91"><meta itemprop=image content="https://shunyaueta.com/ogp.jpg"><meta itemprop=keywords content="gcp,pubsub,dataflow,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-JMJRQJT0Q3"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JMJRQJT0Q3")</script></head><body><header><a href=/ class=title><h2>hurutoriya</h2></a><nav><a href=/index.xml>RSS</a>
<a href=/about/>このサイトについて</a></nav></header><main><h1>Google Cloud Pub/Sub に公開された結果はDataflow template を使えばめちゃくちゃ簡単に確認できる</h1><p><i><time datetime=2021-11-05 pubdate>2021-11-05</time></i></p><content><p>PubSub に出力された結果を確認するのって、なかなか手間がかかりませんか?</p><p>最近同僚に簡単な確認方法を教えてもらい、感動したのでそれを記事にしました。</p><h2 id=確認方法>確認方法</h2><ol><li>PubSub のメッセージを出力する Google Cloud Storage bucket を同一 GCP プロジェクトで作成する。</li><li>GCP の Pub/Sub ページに移動する</li><li>確認したい Pub/Sub topic をクリックする</li><li>ページ下部にある <code>CREATE SUBSCRIPTION</code> ボタンを押すと選択肢で、<code>Create subscription</code>, <code>Export to BigQuery</code>, <code>Export to Cloud Storage</code>があり、 <code>Export to Cloud Storage</code>を選択する。<ul><li>BigQuery、 Google Cloud Storage への吐き出しを行い際に、自動的に subscription が生成される。</li></ul></li><li><code>Export to Cloud Storage</code> を選択すると、Text 形式か Abro 形式での出力にするかを選択できる。基本的には簡単に確認できる Text 形式を選ぶと良さげ。</li><li>選択後、下記のような設定画面が出てくるので情報を埋めていく。基本的には、どこの Google Cloud Storage に出力するかを埋めれば完了。</li></ol><p><img src=/posts/2021-11-05/images/1.png alt="Export to Cloud Storage config"></p><ol><li>10m ほどすると Streaming job の Dataflow の起動が完了して、一定期間ごとに Pub/Sub の topic に公開されたデータがテキスト形式で出力され始めます。</li><li>出力された GCS の結果を眺めるには、 <code>gsutil</code> コマンドなどを使うのが簡単です。自分は<code>gsutil cat</code> の結果をコピーして VS Code で確認しています。</li></ol><p>Cloud Dataflow のテンプレート機能については、端的に説明すると、GUI でパラメータを設定するだけで、Dataflow によるデータ処理が簡単に実行できるようになる機能です。</p><p>詳しくは、 GCPUG での<a href=https://medium.com/google-cloud-jp/cloud-dataflow-%E3%81%8C%E3%83%86%E3%83%B3%E3%83%97%E3%83%AC%E3%83%BC%E3%83%88%E3%81%AB%E3%82%88%E3%82%8A%E6%B0%97%E8%BB%BD%E3%81%AB%E4%BD%BF%E3%81%88%E3%82%8B%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%83%AC%E3%82%B9%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%81%AB%E9%80%B2%E5%8C%96%E3%81%97%E3%81%9F%E8%A9%B1-f8105ea956d3>Cloud Dataflow がテンプレートにより気軽に使えるサーバーレスのサービスに進化した話
</a>の記事がわかりやすいのでごらんください。</p><h2>関連しているかもしれない記事</h2><ul><li><a href=/posts/2021-09-07/>gcloud commands で Pub/Sub に jsonファイルをメッセージとして公開 (Pusblish) する</a></li><li><a href=/posts/2021-10-04/>CloudComposer のDAGをCircleCIで更新する</a></li><li><a href=/posts/2021-09-29/>GCPのCloud Composer のDAGを素早く・簡単にデバッグする</a></li><li><a href=/posts/2021-03-03/>GKE 上にて Pythonで logger.info() を行うとCloud logging では stderr に保存され、すべてエラーになる問題への対処法</a></li><li><a href=/posts/2019-10-03/>遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む</a></li></ul><h2>Support</h2>記事をお読みくださりありがとうございます。
このウェブサイトの運営を支援していただける方を募集しています。
もしよろしければ、<a href=https://www.buymeacoffee.com/hurutoriya>Buy Me a Coffee</a> からサポート(投げ銭)していただけると、記事の執筆、情報発信のモチベーションに繋がります✨<p>--</p>記事を楽しめましたか？
<a href=/index.xml>RSS</a>で更新情報を配信しているので、お好きなフィードリーダーで購読してみてください。</content><p><a href=https://shunyaueta.com/tags/gcp/>#gcp</a>
<a href=https://shunyaueta.com/tags/pubsub/>#pubsub</a>
<a href=https://shunyaueta.com/tags/dataflow/>#dataflow</a></p><script src=https://giscus.app/client.js data-repo=hurutoriya/hurutoriya.github.io data-repo-id="MDEwOlJlcG9zaXRvcnk2MDgyNTY1Nw==" data-category=Comments data-category-id=DIC_kwDOA6AgOc4CAQTX data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=dark data-lang=ja crossorigin=anonymous async></script></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>