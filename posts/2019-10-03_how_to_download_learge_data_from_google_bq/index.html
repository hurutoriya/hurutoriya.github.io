<!doctype html><html lang=ja prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=description content><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=keywords content="GCP,
BigQuery,
pandas,"><meta property="og:type" content="article"><meta property="og:description" content><meta property="og:title" content="pandas.read_gbq を使わずに、Google BigQueryから高速にData ETL"><meta property="og:site_name" content><meta property="og:image" content><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content><meta property="og:image:height" content><meta property="og:url" content="https://shunyaueta.com/posts/2019-10-03_how_to_download_learge_data_from_google_bq/"><meta property="og:locale" content="ja"><meta property="article:published_time" content="2019-10-03"><meta property="article:modified_time" content="2019-10-03"><meta property="article:tag" content="GCP"><meta property="article:tag" content="BigQuery"><meta property="article:tag" content="pandas"><meta name=twitter:card content="summary"><meta name=twitter:site content="@hurutoriya"><meta name=twitter:creator content="@hurutoriya"><meta name=twitter:title content="pandas.read_gbq を使わずに、Google BigQueryから高速にData ETL |"><meta name=twitter:description content="pandas.read_gbq 便利ですよね。 クレデンシャルファイルを認証画面からコピペすれば Jupyter 上でさっと動き、Google Big Query が実行されてその結果がそのままデータフレームとして扱えます。 Jupyter と Google BQ を連携させたいときはいつも使っています
問題点  そこそこ大きなデータを持ってこようとするとめちゃくちゃ遅くてストレスが半端ない  解決方法として、Google Big Query で巨大なデータをダウンロードする方法について書く
実は Google の公式ドキュメントでも記載されている。
 https://cloud.google.com/bigquery/docs/pandas-gbq-migration https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas  方法は以下の２つ。
 google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 BQ 実行 →BigQuery table として保存 →GCS へ保存 → gsutil でマシンへコピー  1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も pandas.rad_gbq よりも高速です 2 番目はそもそも実行結果が巨大な場合で、目安としては1GB以上なら 2 番目の方法を使えば楽です。
1, google-cloud-bigquery をインストールして、マジックコマンドで Google BQ を実行 pip install --upgrade google-cloud-bigquery[bqstorage,pandas] magic command を実行|"><meta name=twitter:image:src content><meta name=twitter:domain content="https://shunyaueta.com/posts/2019-10-03_how_to_download_learge_data_from_google_bq/"><title>pandas.read_gbq を使わずに、Google BigQueryから高速にData ETL</title><link rel=canonical href=https://shunyaueta.com/posts/2019-10-03_how_to_download_learge_data_from_google_bq/><link rel=stylesheet href=https://unpkg.com/tachyons@4.11.1/css/tachyons.min.css><link rel=stylesheet href=https://shunyaueta.com/css/style.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/highlightjs@9.12.0/styles/github-gist.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-39994406-11','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon href=/apple-touch-icon.png></head><body lang=ja class="sans-serif w-90 w-60-ns center center-ns mv2 mv5-ns" itemscope itemtype=http://schema.org/Article><span class=b>/</span>
<a href=https://shunyaueta.com/ class="b bb bw1 pb1 no-underline black"></a><section id=main class=mt5><h1 itemprop=name id=title>pandas.read_gbq を使わずに、Google BigQueryから高速にData ETL</h1><article itemprop=articleBody id=content class="w-90 lh-copy"><p><a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_gbq.html>pandas.read_gbq</a> 便利ですよね。
クレデンシャルファイルを認証画面からコピペすれば Jupyter 上でさっと動き、Google Big Query が実行されてその結果がそのままデータフレームとして扱えます。
Jupyter と Google BQ を連携させたいときはいつも使っています</p><h2 id=問題点>問題点</h2><ul><li>そこそこ大きなデータを持ってこようとするとめちゃくちゃ遅くてストレスが半端ない</li></ul><p>解決方法として、Google Big Query で巨大なデータをダウンロードする方法について書く</p><p>実は Google の公式ドキュメントでも記載されている。</p><ul><li><a href=https://cloud.google.com/bigquery/docs/pandas-gbq-migration>https://cloud.google.com/bigquery/docs/pandas-gbq-migration</a></li><li><a href=https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas>https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas</a></li></ul><p>方法は以下の２つ。</p><ol><li><code>google-cloud-bigquery</code> をインストールして、マジックコマンドで Google BQ を実行</li><li>BQ 実行 →BigQuery table として保存 →GCS へ保存 → <code>gsutil</code> でマシンへコピー</li></ol><p>1 番目は、Jupyter 上でマジックコマンドで Google BQ が実行できて、速度も <code>pandas.rad_gbq</code> よりも高速です
2 番目はそもそも実行結果が巨大な場合で、目安としては<code>1GB以上</code>なら 2 番目の方法を使えば楽です。</p><h3 id=1-google-cloud-bigquery-をインストールしてマジックコマンドで-google-bq-を実行>1, <code>google-cloud-bigquery</code> をインストールして、マジックコマンドで Google BQ を実行</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>pip install <span style=color:#f92672>--</span>upgrade google<span style=color:#f92672>-</span>cloud<span style=color:#f92672>-</span>bigquery[bqstorage,pandas]

</code></pre></div><p>magic command を実行</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>%</span>load_ext google<span style=color:#f92672>.</span>cloud<span style=color:#f92672>.</span>bigquery
</code></pre></div><p>後は Jupyter Notebook のセルで以下のコマンドぞを実行すれば、</p><pre><code>%%bigquery df --use_bqstorage_api
SELECT
  CONCAT(
    'https://stackoverflow.com/questions/',
    CAST(id as STRING)) as url,
  view_count
FROM `bigquery-public-data.stackoverflow.posts_questions`
WHERE tags like '%google-bigquery%'
ORDER BY view_count DESC
LIMIT 10
</code></pre><p><code>df</code> にマジックコマンドで実行した SQL の実行結果が格納されます!
便利ですね!</p><h3 id=2-bq-実行-bigquery-table-として保存-gcs-へ保存--gsutil-でマシンへコピー>2, BQ 実行 →BigQuery table として保存 →GCS へ保存 → <code>gsutil</code> でマシンへコピー</h3><ol><li>BigQuery でクエリを実行、実行結果を BigQuery Table へ保存</li></ol><ul><li><p>注)実行結果の容量が巨大なので、保存先は基本的に Big Query Table へ保存するしか選択肢が無い</p></li><li><p><img src=/posts/2019-10-03_how_to_download_learge_data_from_google_bq/export-to-bqtable.png alt="can&rsquo;t export large file as one file"></p></li></ul><ol start=2><li>BigQuery table から GCS へテーブルを CSV として保存</li></ol><p>Big Query table からエクスポート時に、ファイルサイズが大きいとエクスポートできないので、分割が必要です。</p><ul><li><img src=/posts/2019-10-03_how_to_download_learge_data_from_google_bq/cant-export-onefile.png alt="can&rsquo;t export large file as one file"></li></ul><p><a href=https://cloud.google.com/bigquery/docs/exporting-data>https://cloud.google.com/bigquery/docs/exporting-data</a></p><p>保存ファイル名を</p><pre><code>file-*
</code></pre><p>のようにワイルドカードを指定すると、自動的にひとつのテーブルを複数ファイルに分割して保存してくれる</p><ol start=3><li><code>gsutil</code> commands で任意のマシンへダウンロードする。</li></ol><p><code>-m</code> オプションを付け足すと並列ダウンロードが始まるので、複数ファイルダウンロードする場合はおすすめ</p><p>ストレスレスなデータ分析ライフを!</p></article></section><footer><div><p class="f6 gray mt6 lh-copy">© 2016-20 <a href=https://twitter.com/hurutoriya>@hurutoriya</a>.</p></div></footer><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.14.2/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>