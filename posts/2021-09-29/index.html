<!doctype html><html lang=ja><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>GCPのCloud Composer のDAGを素早く・簡単にデバッグする | Shunya Ueta</title><meta name=title content="GCPのCloud Composer のDAGを素早く・簡単にデバッグする"><meta name=description content="GCPでAirflow をマネージドサービスで使えるサービスで Cloud Composer が存在する。 BigQueryやBigTable, PubSub などGCPの各サービスをDAGとして定義してジョブを定期実行できるので非常に便利だが、その代わりDAGを実行するまで結果がわからないので、CloudComposer を一度実行するしか無いのでデバッグが困難になる傾向がある。
また、GitHubのリポジトリにDAGを保存して、CIでCloud Composerを更新するようしていると PRを都度作ってマージされないと確認できないという場合もある。
ローカルでDocker で走らせれば良いのじゃないかというツッコミがあると思いますが、結局 Cloud Composer 上での動作を確かめないといけないので、今回の記事を書くことにしました。
NOTE: 自分が使用しているComposerのversionはcomposer-1.8.0-Airflow-1.10.3 です。基本的にやれることは一緒だと思います。また、dev, prodで同一のDAGが走るCloud Composer を運用しているという前提です。
アプローチは２つ
logger.info() を仕込んで、DAGのなかで何が起こっているかを理解する import logging logger = logging.getLogger(__name__) logger.info() loggerをDAGを記述した Pythonファイルに仕込んで、内部で何が起こっているかを把握する。
各DAGのlogは、
GCPのCloud ComposerのページにアクセスしてAirflow webserver 列のボタンをクリックしてAirflowのWeb applicaiton にログイン 確認したいDAGをクリック DAG内のtask をクリックして表示されるモーダル内の View Logをクリックすると、loggerの情報が確認できる gstuil rsync コマンドでのGCSへのDAGの同期 gstuil rsyncコマンドを使うことで、リポジトリのDAGファイルをGCSに格納されている開発環境上のCloudComposer のDAGファイルに直接同期してPull Request マージ後のDAGの挙動を確認できる。
Cloud Composer のDAGは、自動作成されたGoogle Cloud Storage(GCS)に格納されており、GCSをCloud Composerが定期的に監視してCloud Composerを更新している。 つまり、GCS上のDAGファイルを直接更新してやるとそれがCloud Composerに反映される。 体感として2-3分に一度は監視されているので、ほぼ待ち状態がない状態で確認できて便利です。
以下のコマンドでリポジトリのDAGファイルをGCSに反映させます。
gsutil -m rsync -d -r dags &#34;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=&#34;get(config."><meta name=keywords content="airflow,gcp,"><meta property="og:title" content="GCPのCloud Composer のDAGを素早く・簡単にデバッグする"><meta property="og:description" content="GCPでAirflow をマネージドサービスで使えるサービスで Cloud Composer が存在する。 BigQueryやBigTable, PubSub などGCPの各サービスをDAGとして定義してジョブを定期実行できるので非常に便利だが、その代わりDAGを実行するまで結果がわからないので、CloudComposer を一度実行するしか無いのでデバッグが困難になる傾向がある。
また、GitHubのリポジトリにDAGを保存して、CIでCloud Composerを更新するようしていると PRを都度作ってマージされないと確認できないという場合もある。
ローカルでDocker で走らせれば良いのじゃないかというツッコミがあると思いますが、結局 Cloud Composer 上での動作を確かめないといけないので、今回の記事を書くことにしました。
NOTE: 自分が使用しているComposerのversionはcomposer-1.8.0-Airflow-1.10.3 です。基本的にやれることは一緒だと思います。また、dev, prodで同一のDAGが走るCloud Composer を運用しているという前提です。
アプローチは２つ
logger.info() を仕込んで、DAGのなかで何が起こっているかを理解する import logging logger = logging.getLogger(__name__) logger.info() loggerをDAGを記述した Pythonファイルに仕込んで、内部で何が起こっているかを把握する。
各DAGのlogは、
GCPのCloud ComposerのページにアクセスしてAirflow webserver 列のボタンをクリックしてAirflowのWeb applicaiton にログイン 確認したいDAGをクリック DAG内のtask をクリックして表示されるモーダル内の View Logをクリックすると、loggerの情報が確認できる gstuil rsync コマンドでのGCSへのDAGの同期 gstuil rsyncコマンドを使うことで、リポジトリのDAGファイルをGCSに格納されている開発環境上のCloudComposer のDAGファイルに直接同期してPull Request マージ後のDAGの挙動を確認できる。
Cloud Composer のDAGは、自動作成されたGoogle Cloud Storage(GCS)に格納されており、GCSをCloud Composerが定期的に監視してCloud Composerを更新している。 つまり、GCS上のDAGファイルを直接更新してやるとそれがCloud Composerに反映される。 体感として2-3分に一度は監視されているので、ほぼ待ち状態がない状態で確認できて便利です。
以下のコマンドでリポジトリのDAGファイルをGCSに反映させます。
gsutil -m rsync -d -r dags &#34;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=&#34;get(config."><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2021-09-29/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-09-29T22:20:23+09:00"><meta property="article:modified_time" content="2021-09-29T22:20:23+09:00"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="GCPのCloud Composer のDAGを素早く・簡単にデバッグする"><meta name=twitter:description content="GCPでAirflow をマネージドサービスで使えるサービスで Cloud Composer が存在する。 BigQueryやBigTable, PubSub などGCPの各サービスをDAGとして定義してジョブを定期実行できるので非常に便利だが、その代わりDAGを実行するまで結果がわからないので、CloudComposer を一度実行するしか無いのでデバッグが困難になる傾向がある。
また、GitHubのリポジトリにDAGを保存して、CIでCloud Composerを更新するようしていると PRを都度作ってマージされないと確認できないという場合もある。
ローカルでDocker で走らせれば良いのじゃないかというツッコミがあると思いますが、結局 Cloud Composer 上での動作を確かめないといけないので、今回の記事を書くことにしました。
NOTE: 自分が使用しているComposerのversionはcomposer-1.8.0-Airflow-1.10.3 です。基本的にやれることは一緒だと思います。また、dev, prodで同一のDAGが走るCloud Composer を運用しているという前提です。
アプローチは２つ
logger.info() を仕込んで、DAGのなかで何が起こっているかを理解する import logging logger = logging.getLogger(__name__) logger.info() loggerをDAGを記述した Pythonファイルに仕込んで、内部で何が起こっているかを把握する。
各DAGのlogは、
GCPのCloud ComposerのページにアクセスしてAirflow webserver 列のボタンをクリックしてAirflowのWeb applicaiton にログイン 確認したいDAGをクリック DAG内のtask をクリックして表示されるモーダル内の View Logをクリックすると、loggerの情報が確認できる gstuil rsync コマンドでのGCSへのDAGの同期 gstuil rsyncコマンドを使うことで、リポジトリのDAGファイルをGCSに格納されている開発環境上のCloudComposer のDAGファイルに直接同期してPull Request マージ後のDAGの挙動を確認できる。
Cloud Composer のDAGは、自動作成されたGoogle Cloud Storage(GCS)に格納されており、GCSをCloud Composerが定期的に監視してCloud Composerを更新している。 つまり、GCS上のDAGファイルを直接更新してやるとそれがCloud Composerに反映される。 体感として2-3分に一度は監視されているので、ほぼ待ち状態がない状態で確認できて便利です。
以下のコマンドでリポジトリのDAGファイルをGCSに反映させます。
gsutil -m rsync -d -r dags &#34;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=&#34;get(config."><meta itemprop=name content="GCPのCloud Composer のDAGを素早く・簡単にデバッグする"><meta itemprop=description content="GCPでAirflow をマネージドサービスで使えるサービスで Cloud Composer が存在する。 BigQueryやBigTable, PubSub などGCPの各サービスをDAGとして定義してジョブを定期実行できるので非常に便利だが、その代わりDAGを実行するまで結果がわからないので、CloudComposer を一度実行するしか無いのでデバッグが困難になる傾向がある。
また、GitHubのリポジトリにDAGを保存して、CIでCloud Composerを更新するようしていると PRを都度作ってマージされないと確認できないという場合もある。
ローカルでDocker で走らせれば良いのじゃないかというツッコミがあると思いますが、結局 Cloud Composer 上での動作を確かめないといけないので、今回の記事を書くことにしました。
NOTE: 自分が使用しているComposerのversionはcomposer-1.8.0-Airflow-1.10.3 です。基本的にやれることは一緒だと思います。また、dev, prodで同一のDAGが走るCloud Composer を運用しているという前提です。
アプローチは２つ
logger.info() を仕込んで、DAGのなかで何が起こっているかを理解する import logging logger = logging.getLogger(__name__) logger.info() loggerをDAGを記述した Pythonファイルに仕込んで、内部で何が起こっているかを把握する。
各DAGのlogは、
GCPのCloud ComposerのページにアクセスしてAirflow webserver 列のボタンをクリックしてAirflowのWeb applicaiton にログイン 確認したいDAGをクリック DAG内のtask をクリックして表示されるモーダル内の View Logをクリックすると、loggerの情報が確認できる gstuil rsync コマンドでのGCSへのDAGの同期 gstuil rsyncコマンドを使うことで、リポジトリのDAGファイルをGCSに格納されている開発環境上のCloudComposer のDAGファイルに直接同期してPull Request マージ後のDAGの挙動を確認できる。
Cloud Composer のDAGは、自動作成されたGoogle Cloud Storage(GCS)に格納されており、GCSをCloud Composerが定期的に監視してCloud Composerを更新している。 つまり、GCS上のDAGファイルを直接更新してやるとそれがCloud Composerに反映される。 体感として2-3分に一度は監視されているので、ほぼ待ち状態がない状態で確認できて便利です。
以下のコマンドでリポジトリのDAGファイルをGCSに反映させます。
gsutil -m rsync -d -r dags &#34;$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format=&#34;get(config."><meta itemprop=datePublished content="2021-09-29T22:20:23+09:00"><meta itemprop=dateModified content="2021-09-29T22:20:23+09:00"><meta itemprop=wordCount content="154"><meta itemprop=image content="https://shunyaueta.com/ogp.jpg"><meta itemprop=keywords content="airflow,gcp,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-JMJRQJT0Q3"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JMJRQJT0Q3")</script></head><body><header><a href=/ class=title><h2>Shunya Ueta</h2></a><nav><a href=/about/>著者について</a>
<a href=/index.xml>RSS</a></nav></header><main><h1>GCPのCloud Composer のDAGを素早く・簡単にデバッグする</h1><p><i><time datetime=2021-09-29 pubdate>2021-09-29</time></i></p><content><p>GCPでAirflow をマネージドサービスで使えるサービスで Cloud Composer が存在する。
BigQueryやBigTable, PubSub などGCPの各サービスをDAGとして定義してジョブを定期実行できるので非常に便利だが、その代わりDAGを実行するまで結果がわからないので、CloudComposer を一度実行するしか無いのでデバッグが困難になる傾向がある。</p><p>また、GitHubのリポジトリにDAGを保存して、CIでCloud Composerを更新するようしていると PRを都度作ってマージされないと確認できないという場合もある。</p><p>ローカルでDocker で走らせれば良いのじゃないかというツッコミがあると思いますが、結局 Cloud Composer 上での動作を確かめないといけないので、今回の記事を書くことにしました。</p><p><strong>NOTE:</strong> 自分が使用しているComposerのversionは<code>composer-1.8.0-Airflow-1.10.3</code> です。基本的にやれることは一緒だと思います。また、<code>dev</code>, <code>prod</code>で同一のDAGが走るCloud Composer を運用しているという前提です。</p><p>アプローチは２つ</p><h2 id=loggerinfo-を仕込んでdagのなかで何が起こっているかを理解する>logger.info() を仕込んで、DAGのなかで何が起こっているかを理解する</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> logging
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logger <span style=color:#f92672>=</span> logging<span style=color:#f92672>.</span>getLogger(__name__)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logger<span style=color:#f92672>.</span>info()
</span></span></code></pre></div><p>loggerをDAGを記述した Pythonファイルに仕込んで、内部で何が起こっているかを把握する。</p><p>各DAGのlogは、</p><ol><li>GCPのCloud Composerのページにアクセスして<code>Airflow webserver</code> 列のボタンをクリックしてAirflowのWeb applicaiton にログイン</li><li>確認したいDAGをクリック</li><li>DAG内のtask をクリックして表示されるモーダル内の <code>View Log</code>をクリックすると、loggerの情報が確認できる</li></ol><h2 id=gstuil-rsync-コマンドでのgcsへのdagの同期><code>gstuil rsync</code> コマンドでのGCSへのDAGの同期</h2><p><code>gstuil rsync</code>コマンドを使うことで、リポジトリのDAGファイルをGCSに格納されている開発環境上のCloudComposer のDAGファイルに直接同期してPull Request マージ後のDAGの挙動を確認できる。</p><p>Cloud Composer のDAGは、自動作成されたGoogle Cloud Storage(GCS)に格納されており、GCSをCloud Composerが定期的に監視してCloud Composerを更新している。
つまり、GCS上のDAGファイルを直接更新してやるとそれがCloud Composerに反映される。
体感として2-3分に一度は監視されているので、ほぼ待ち状態がない状態で確認できて便利です。</p><p>以下のコマンドでリポジトリのDAGファイルをGCSに反映させます。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gsutil -m rsync -d -r dags <span style=color:#e6db74>&#34;</span><span style=color:#66d9ef>$(</span>gcloud composer environments describe <span style=color:#f92672>{</span>COMPOSER_NAME<span style=color:#f92672>}</span> --project<span style=color:#f92672>={</span>GCP_PROJECT<span style=color:#f92672>}</span> --location<span style=color:#f92672>={</span>REGION<span style=color:#f92672>}</span>  --format<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;get(config.dagGcsPrefix)&#34;</span><span style=color:#66d9ef>)</span><span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div><p>{XXX} には使用する環境の情報を置換してください。</p><ul><li><code>"$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format="get(config.dagGcsPrefix)")"</code><ul><li>指定したGCP Project で動くCloud Composer のDAGが格納されているGCSのパスを取得できる。</li></ul></li><li><code>gsutil -m rsync -d -r dags</code><ul><li><code>-m</code> は並列処理</li><li><code>-d</code> は元のディレクトリに存在しないファイルがコピー先にあれば削除(ミラーリング)。これにより、GCS上でDAGを新たに作成して、デバッグしていたとしても、CIが走ればリポジトリにないDAGファイルは削除され、リポジトリのDAGと完全に同期される。</li><li><code>-r</code> はディレクトリとしてコピー</li><li>上記のオプションにより<code>dags</code> ディレクトリのDAGファイルをGCSにミラーリングで同期を行う。</li></ul></li></ul><h3 id=composer-のための正規のコマンドはあるが>Composer のための正規のコマンドはあるが&mldr;</h3><p>また、以下のように <code>gcloud composer environments storage dags import</code> コマンドで更新する方法もあるので、そちらを使っても大丈夫です。
実行内容自体はGCSのファイルを変更するのと変わりません。
ですが、ディレクトリを対象にしたファイルの同期には対応していないので、上記で説明したコマンドのほうが遥かに楽です。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gcloud composer environments storage dags import <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --environment <span style=color:#f92672>{</span>ENVIRONMENT_NAME<span style=color:#f92672>}</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --location <span style=color:#f92672>{</span>LOCATION<span style=color:#f92672>}</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --source <span style=color:#f92672>{</span>LOCAL_FILE_TO_UPLOAD<span style=color:#f92672>}</span>
</span></span></code></pre></div><p><strong>NOTE:</strong> プロダクションのDAGを直接書き換えるのは危険なのでやめましょう。</p><h2 id=reference>Reference</h2><ul><li><a href=https://cloud.google.com/composer/docs/how-to/using/managing-dags>Adding and Updating DAGs (workflows)</a></li><li><a href=https://cloud.google.com/storage/docs/gsutil/addlhelp/TopLevelCommandLineOptions>gsutil Top-Level Command-Line Options</a></li><li><a href=https://cloud.google.com/storage/docs/gsutil/commands/rsync>rsync - Synchronize content of two buckets/directories</a></li></ul></content>---<br>📮 📧 🐏: 記事への<a href="https://docs.google.com/forms/d/e/1FAIpQLScgZVDrjQiKLbQRovfs88oweCITzjtvt1PlgwL14JfWPOrpPQ/viewform?usp=pp_url&entry.838298670=https%3a%2f%2fshunyaueta.com%2fposts%2f2021-09-29%2f">感想</a>のおたよりをおまちしてます。
お気軽にお送りください。
メールアドレス入力があればメールで返信させていただきます。
もちろんお返事を希望せずに単なる感想だけでも大歓迎です。<br><br>このサイトの更新情報を<a href=/index.xml>RSS</a>で配信しています。
お好きなフィードリーダーで購読してみてください。<br><br>このウェブサイトの運営や著者の活動を支援していただける方を募集しています。
もしよろしければ、<a href=https://www.buymeacoffee.com/hurutoriya>Buy Me a Coffee</a> からサポート(投げ銭)していただけると、著者の活動のモチベーションに繋がります✨<br><br><p>関連しているかもしれない記事</p><ul><li><a href=/posts/2021-09-07/>gcloud commands で Pub/Sub に jsonファイルをメッセージとして公開 (Pusblish) する</a></li><li><a href=/posts/2021-03-03/>GKE 上にて Pythonで logger.info() を行うとCloud logging では stderr に保存され、すべてエラーになる問題への対処法</a></li><li><a href=/posts/2019-10-03/>遅すぎる `pandas.read_gbq` を使わずに、Google BigQueryから高速にデータを読み込む</a></li><li><a href=/posts/2019-09-24/>How to connect the Google Compute Engine via Visual Studio Code</a></li></ul><p>Tags:
<a href=https://shunyaueta.com/tags/airflow/>#airflow</a>
<a href=https://shunyaueta.com/tags/gcp/>#gcp</a></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>