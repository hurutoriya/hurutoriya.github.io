<!doctype html><html lang=ja><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる | hurutoriya</title><meta name=title content="Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる"><meta name=description content="2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1
Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow&rsquo;s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX)."><meta name=keywords content="apachebeam,machinelearning,python,"><meta property="og:title" content="Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる"><meta property="og:description" content="2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1
Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow&rsquo;s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX)."><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2022-08-18-1938/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-18T19:38:29+09:00"><meta property="article:modified_time" content="2022-08-18T19:38:29+09:00"><meta property="og:site_name" content="hurutoriya"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる"><meta name=twitter:description content="2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1
Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow&rsquo;s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX)."><meta itemprop=name content="Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる"><meta itemprop=description content="2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1
Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow&rsquo;s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX)."><meta itemprop=datePublished content="2022-08-18T19:38:29+09:00"><meta itemprop=dateModified content="2022-08-18T19:38:29+09:00"><meta itemprop=wordCount content="645"><meta itemprop=image content="https://shunyaueta.com/ogp.jpg"><meta itemprop=keywords content="apachebeam,machinelearning,python,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-JMJRQJT0Q3"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JMJRQJT0Q3")</script></head><body><header><a href=/ class=title><h2>hurutoriya</h2></a><nav><a href=/index.xml>RSS</a>
<a href=/about/>著者について</a></nav></header><main><h1>Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる</h1><p><i><time datetime=2022-08-18 pubdate>2022-08-18</time></i></p><content><p><code>2022-07-21</code> に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><blockquote><p><strong>Dataflow ML</strong> - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow&rsquo;s existing ML capabilities such as <a href=https://cloud.google.com/dataflow/docs/guides/using-gpus>GPU support</a> and the pre and post processing system for ML training, either directly or via frameworks such as <a href=https://www.tensorflow.org/tfx>Tensorflow Extended (TFX)</a>.</p></blockquote><p>DataFlow は Apache Beam で記述したプログラミングモデルの実行環境だ。
主に、バッチ処理・ストリーミング処理で使われており、機械学習に欠かせないデータ処理の観点からして非常に面白いと思っているソフトウェアなので、積極的に動向を追っている。</p><p>DataFlow ML は簡単に説明すると、PyTorch と scikit-learn が Dataflow のパイプライン内部で直接推論可能になった。
直近では Python SDK に限るが DataFlow で GPU の利用が可能になったりと、大規模データ処理や機械学習を行う際に魅力的な機能が続々とサポートされはじめた。
DataFlow ML の実態は <a href=https://beam.apache.org/blog/beam-2.40.0>Apache Beam 2.40.0</a>で追加された RunInference API だ。
RunInference API の DesignDocs <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>が公開されているので、それを見てみる。
機械学習システムの DesignDocs として非常に面白いので興味がある人は、見てみると面白いとおもう。</p><h2 id=designdocs-runinference-ml-inference-in-beam>DesignDocs: RunInference: ML Inference in Beam</h2><p>RunInference API が作られた背景として、TensorFlow だけが Beam 内部で、処理するデータ( Apache beam では<code>PCollection</code> と呼ばれる) に対して、推論を行うための <a href=https://github.com/tensorflow/tfx-bsl/blob/master/tfx_bsl/beam/run_inference.py>RunInference transformer</a> (Apache Beam では <code>PTransform</code>と呼ばれ、<code>PCollection</code> を入力として処理を行う) が存在しており、実装不要だった。</p><ul><li>問題点としては<ul><li>Beam のレポジトリに存在せず、<a href=https://github.com/tensorflow/tfx-bsl>tensorflow/tfx-bsl</a>のリポジトリに存在している。</li><li>TFX ライブラリに特化した API になっている</li><li>サンプルコードが充実しておらず、学習コストが高い</li></ul></li></ul><p>この DesignDocs の目的は、RunInference API を以下の 2 つの人気のある機械学習フレームワークで使えるように実装すること</p><ul><li>scikit-learn</li><li>PyTorch</li></ul><p>実装方法としては、</p><ul><li>内部の最適化が行われているべき</li><li>単純かつ統合されたインターフェイスで提供されるべき</li><li>入力と出力が、機械学習エンジニアに対して、直感的な型になっているべき<ul><li>scikit-learn なら numpy</li><li>PyTorch なら Tensors</li></ul></li></ul><p>最終的なゴールとしては、XGBoost や JAX など他の機械学習フレームワークにも適合したり、Vertex AI など外部のサービスも使えるようにしたい。</p><p>内部の実装方針は DesignDocs で詳細に議論されているので、そこは割愛して、まずは RunInference API のサンプルコードを動かしてみる。</p><h2 id=サンプルコードを通じて学ぶ>サンプルコードを通じて学ぶ</h2><p><a href=https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/inference>Example RunInference API pipelines</a>にサンプルコードが公開されていたので、動かしてみる。
まずは一番簡単そうなサンプルコードである scikit-learn による MNIST 分類を動かしてみる。
最初にドキュメントをじっくり読むよりも実際にコードを見たほうが理解が深まるので実際に動かしてみるのが近道。
サンプルコードはこちら<a href=https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/inference/sklearn_mnist_classification.py>beam/sdks/python/apache_beam/examples/inference/sklearn_mnist_classification.py</a>
公式のサンプルコードだと、推論対象の入力データと学習済みの scikit-learn のモデルを自前で用意する必要があるのでコマンド一発でサンプルコードを動かせるコードを以下のレポジトリに公開しました。
将来的には scikit-learn だけでなく、PyTorch にも対応したい。</p><p><a href=https://github.com/hurutoriya/beam-runinferenceapi-sample>https://github.com/hurutoriya/beam-runinferenceapi-sample</a></p><p>実際に何をやっているかの解説は、日本語のコメントを添えて解説してみます。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;このパイプラインは、RunInference APIを使って、MNISTデータの分類を行う。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>このパイプラインは、int 形式でCSV形式で保存されているテキストファイルを読み込む。CSVの１つ目の列はラベル、そのほかの列はMNISTのピクセルの値を格納している。データは学習済みのモデルで推論される。このパイプラインは output ファイルに推論結果の書き込みを行い、ラベルと推論結果の比較を行うことができる。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> argparse
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Iterable
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> List
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Tuple
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> apache_beam <span style=color:#66d9ef>as</span> beam
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.ml.inference.base <span style=color:#f92672>import</span> KeyedModelHandler
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.ml.inference.base <span style=color:#f92672>import</span> PredictionResult
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.ml.inference.base <span style=color:#f92672>import</span> RunInference
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.ml.inference.sklearn_inference <span style=color:#f92672>import</span> ModelFileType
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.ml.inference.sklearn_inference <span style=color:#f92672>import</span> SklearnModelHandlerNumpy
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.options.pipeline_options <span style=color:#f92672>import</span> PipelineOptions
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.options.pipeline_options <span style=color:#f92672>import</span> SetupOptions
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process_input</span>(row: str) <span style=color:#f92672>-&gt;</span> Tuple[int, List[int:
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;&#34;&#34;入力データをラベルとピクセルに分けて、返す
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>  data <span style=color:#f92672>=</span> row<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;,&#39;</span>)
</span></span><span style=display:flex><span>  label, pixels <span style=color:#f92672>=</span> int(data[<span style=color:#ae81ff>0</span>]), data[<span style=color:#ae81ff>1</span>:]
</span></span><span style=display:flex><span>  pixels <span style=color:#f92672>=</span> [int(pixel) <span style=color:#66d9ef>for</span> pixel <span style=color:#f92672>in</span> pixels]
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> label, pixels
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PostProcessor</span>(beam<span style=color:#f92672>.</span>DoFn):
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;&#34;&#34;予測ラベルを得るために予測結果を処理する。CSV形式で、真値と予測ラベルを返す。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process</span>(self, element: Tuple[int, PredictionResult]) <span style=color:#f92672>-&gt;</span> Iterable[str]:
</span></span><span style=display:flex><span>    label, prediction_result <span style=color:#f92672>=</span> element
</span></span><span style=display:flex><span>    prediction <span style=color:#f92672>=</span> prediction_result<span style=color:#f92672>.</span>inference
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> <span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>,</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(label, prediction)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>parse_known_args</span>(argv):
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;&#34;&#34;引数を定義&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>  parser <span style=color:#f92672>=</span> argparse<span style=color:#f92672>.</span>ArgumentParser()
</span></span><span style=display:flex><span>  parser<span style=color:#f92672>.</span>add_argument(
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#39;--input_file&#39;</span>,
</span></span><span style=display:flex><span>      dest<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;input&#39;</span>,
</span></span><span style=display:flex><span>      required<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>      help<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;text file with comma separated int values.&#39;</span>)
</span></span><span style=display:flex><span>  parser<span style=color:#f92672>.</span>add_argument(
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#39;--output&#39;</span>,
</span></span><span style=display:flex><span>      dest<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;output&#39;</span>,
</span></span><span style=display:flex><span>      required<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>      help<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Path to save output predictions.&#39;</span>)
</span></span><span style=display:flex><span>  parser<span style=color:#f92672>.</span>add_argument(
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#39;--model_path&#39;</span>,
</span></span><span style=display:flex><span>      dest<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model_path&#39;</span>,
</span></span><span style=display:flex><span>      required<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>      help<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Path to load the Sklearn model for Inference.&#39;</span>)
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> parser<span style=color:#f92672>.</span>parse_known_args(argv)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run</span>(argv<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, save_main_session<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;&#34;&#34;パイプラインを定義&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>  known_args, pipeline_args <span style=color:#f92672>=</span> parse_known_args(argv)
</span></span><span style=display:flex><span>  pipeline_options <span style=color:#f92672>=</span> PipelineOptions(pipeline_args)
</span></span><span style=display:flex><span>  pipeline_options<span style=color:#f92672>.</span>view_as(SetupOptions)<span style=color:#f92672>.</span>save_main_session <span style=color:#f92672>=</span> save_main_session
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># この例では、RunInference トランスフォームにキーとなる入力を渡している。それによって、SklearnModelHandlerNumpy のラッパーである KeyedModelHandler を使用している。</span>
</span></span><span style=display:flex><span>  model_loader <span style=color:#f92672>=</span> KeyedModelHandler(
</span></span><span style=display:flex><span>      SklearnModelHandlerNumpy(
</span></span><span style=display:flex><span>          model_file_type<span style=color:#f92672>=</span>ModelFileType<span style=color:#f92672>.</span>PICKLE,
</span></span><span style=display:flex><span>          model_uri<span style=color:#f92672>=</span>known_args<span style=color:#f92672>.</span>model_path))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>with</span> beam<span style=color:#f92672>.</span>Pipeline(options<span style=color:#f92672>=</span>pipeline_options) <span style=color:#66d9ef>as</span> p:
</span></span><span style=display:flex><span>	<span style=color:#75715e># 入力データを読み込む</span>
</span></span><span style=display:flex><span>    label_pixel_tuple <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        p
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#e6db74>&#34;ReadFromInput&#34;</span> <span style=color:#f92672>&gt;&gt;</span> beam<span style=color:#f92672>.</span>io<span style=color:#f92672>.</span>ReadFromText(
</span></span><span style=display:flex><span>            known_args<span style=color:#f92672>.</span>input, skip_header_lines<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#e6db74>&#34;PreProcessInputs&#34;</span> <span style=color:#f92672>&gt;&gt;</span> beam<span style=color:#f92672>.</span>Map(process_input))
</span></span><span style=display:flex><span>	<span style=color:#75715e># 推論して後処理を行う</span>
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        label_pixel_tuple
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#e6db74>&#34;RunInference&#34;</span> <span style=color:#f92672>&gt;&gt;</span> RunInference(model_loader)
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#e6db74>&#34;PostProcessOutputs&#34;</span> <span style=color:#f92672>&gt;&gt;</span> beam<span style=color:#f92672>.</span>ParDo(PostProcessor()))
</span></span><span style=display:flex><span>	<span style=color:#75715e># 後処理したデータを出力してファイルとして保存する</span>
</span></span><span style=display:flex><span>    _ <span style=color:#f92672>=</span> predictions <span style=color:#f92672>|</span> <span style=color:#e6db74>&#34;WriteOutput&#34;</span> <span style=color:#f92672>&gt;&gt;</span> beam<span style=color:#f92672>.</span>io<span style=color:#f92672>.</span>WriteToText(
</span></span><span style=display:flex><span>        known_args<span style=color:#f92672>.</span>output,
</span></span><span style=display:flex><span>        shard_name_template<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;&#39;</span>,
</span></span><span style=display:flex><span>        append_trailing_newlines<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>  run()
</span></span></code></pre></div><h2 id=runinference-api-についてドキュメントから学ぶ>RunInference API についてドキュメントから学ぶ</h2><p>RunInference API のドキュメント<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>が既に公開されているので、読み込むことで実際にどんな概念で作成されているのか理解できる。
Apache Beam の基礎的な概念や用語については、この記事を読む前に過去に書いた Apache beam Python 入門を読んでいただけると、この記事が分かりやすく読めると思います。</p><h3 id=なぜ-runinferenceapi-を使うのか>なぜ RunInferenceAPI を使うのか?</h3><p>そもそも僕の疑問は「今まで、Beam で PyTorch や scikit-learn のモデルを使って推論は可能だったが、 RunInferenceAPI は何が嬉しいのか?」への答えがドキュメントに書いてありました。</p><ul><li><code>BatchElements</code>トランスフォームや<code>Shared</code>クラスなど既存の Apache Beam の概念に沿って、機械学習の推論処理を最適化が可能になる。またマルチモデルパイプラインなど、複雑なパイプラインの構築も比較的簡単に構築できるようになる。</li></ul><h3 id=batchelements-ptransform-とは>BatchElements PTransform とは</h3><p>多くのモデルが実装しているベクトル化推論の最適化を利用するために、モデルの予測を行う前の中間段階として、BatchElements トランスフォームの追加した。このトランスフォームは、要素をバッチ処理する。そして、バッチされた要素は、RunInference の特定のフレームワークのための変換で適用される。例えば、numpy の ndarrays の場合は numpy.stack()を、torch の Tensor 要素の場合は torch.stack()を呼び出す。</p><p>beam.BatchElements の設定をカスタマイズするには、ModelHandler の中で、batch_elements_kwargs 関数をオーバーライドする必要がある。例えば、<code>min_batch_size</code> でバッチあたりの要素数の最小値を設定し、<code>max_batch_size</code> でバッチあたりの要素数の最大値を設定する。
詳しいドキュメントは<a href=https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html#apache_beam.transforms.util.BatchElements>こちら</a></p><h3 id=shared-helper-class>Shared helper class</h3><p>RunInference API で<code>Shared</code> クラスを使用することにより、プロセスごとに一度モデルを読み込んだ後に各プロセス内部でその読み込んだモデルを<code>DoFn</code>(全ての<code>PCollection</code>に適用される処理ロジックを保持する)内で共有することができる。このクラスを使えば、モデルの読み込み時間とメモリ消費を削減することができる。</p><p>詳しいドキュメント(ソースコード)は<a href=https://github.com/apache/beam/blob/master/sdks/python/apache_beam/utils/shared.py#L20>こちら</a>。内部の処理ロジック自体 100 行未満で書かれているので、読んで見るのもあり。</p><h3 id=機械学習モデルを使うためにパイプラインを構築する>機械学習モデルを使うためにパイプラインを構築する</h3><p>以下のコードを Apache Beam のパイプラインに追加すれば、RunInference トランスフォームを使用できる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.ml.inference.base <span style=color:#f92672>import</span> RunInference
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> pipeline <span style=color:#66d9ef>as</span> p:
</span></span><span style=display:flex><span>   predictions <span style=color:#f92672>=</span> ( p <span style=color:#f92672>|</span>  <span style=color:#e6db74>&#39;Read&#39;</span> <span style=color:#f92672>&gt;&gt;</span> beam<span style=color:#f92672>.</span>ReadFromSource(<span style=color:#e6db74>&#39;a_source&#39;</span>)
</span></span><span style=display:flex><span>                     <span style=color:#f92672>|</span> <span style=color:#e6db74>&#39;RunInference&#39;</span> <span style=color:#f92672>&gt;&gt;</span> RunInference(<span style=color:#f92672>&lt;</span>model_handler<span style=color:#f92672>&gt;</span>)
</span></span></code></pre></div><p><code>model_handler</code> は、モデルのハンドラーのためのセットアップコードで、モデルのインポートができる。
以下のような<code>ModelHandler</code>の例がある</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.ml.inference.sklearn_inference <span style=color:#f92672>import</span> SklearnModelHandlerNumpy
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.ml.inference.sklearn_inference <span style=color:#f92672>import</span> SklearnModelHandlerPandas
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.ml.inference.pytorch_inference <span style=color:#f92672>import</span> PytorchModelHandlerTensor
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> apache_beam.ml.inference.pytorch_inference <span style=color:#f92672>import</span> PytorchModelHandlerKeyedTensor
</span></span></code></pre></div><p>モデル A と モデル B の推論結果を並列して行ったり、</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>with</span> pipeline <span style=color:#66d9ef>as</span> p:
</span></span><span style=display:flex><span>   data <span style=color:#f92672>=</span> p <span style=color:#f92672>|</span> <span style=color:#e6db74>&#39;Read&#39;</span> <span style=color:#f92672>&gt;&gt;</span> beam<span style=color:#f92672>.</span>ReadFromSource(<span style=color:#e6db74>&#39;a_source&#39;</span>)
</span></span><span style=display:flex><span>   model_a_predictions <span style=color:#f92672>=</span> data <span style=color:#f92672>|</span> RunInference(<span style=color:#f92672>&lt;</span>model_handler_A<span style=color:#f92672>&gt;</span>)
</span></span><span style=display:flex><span>   model_b_predictions <span style=color:#f92672>=</span> data <span style=color:#f92672>|</span> RunInference(<span style=color:#f92672>&lt;</span>model_handler_B<span style=color:#f92672>&gt;</span>)
</span></span></code></pre></div><p>アンサンブルも柔軟に記述できる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>with</span> pipeline <span style=color:#66d9ef>as</span> p:
</span></span><span style=display:flex><span>   data <span style=color:#f92672>=</span> p <span style=color:#f92672>|</span> <span style=color:#e6db74>&#39;Read&#39;</span> <span style=color:#f92672>&gt;&gt;</span> beam<span style=color:#f92672>.</span>ReadFromSource(<span style=color:#e6db74>&#39;a_source&#39;</span>)
</span></span><span style=display:flex><span>   model_a_predictions <span style=color:#f92672>=</span> data <span style=color:#f92672>|</span> RunInference(<span style=color:#f92672>&lt;</span>model_handler_A<span style=color:#f92672>&gt;</span>)
</span></span><span style=display:flex><span>   model_b_predictions <span style=color:#f92672>=</span> model_a_predictions <span style=color:#f92672>|</span> beam<span style=color:#f92672>.</span>Map(some_post_processing) <span style=color:#f92672>|</span> RunInference(<span style=color:#f92672>&lt;</span>model_handler_B<span style=color:#f92672>&gt;</span>)
</span></span></code></pre></div><p>また驚きなのが、Apache Beam 2.41.0 移行は <a href=https://beam.apache.org/documentation/programming-guide/#multi-language-pipelines>Multi Language SDK</a>によって、 Java からも RunInference API を使うことができるらしい。
これってモデル構築は Python で行って、運用は安定した Java で実行可能ということなので凄い機能ですね。</p><p><a href=https://github.com/apache/beam/blob/master/sdks/java/extensions/python/src/main/java/org/apache/beam/sdk/extensions/python/transforms/RunInference.java>https://github.com/apache/beam/blob/master/sdks/java/extensions/python/src/main/java/org/apache/beam/sdk/extensions/python/transforms/RunInference.java</a></p><p>Multi Language SDK も非常に面白そうだが、まだ使ったことはないので、実際にコードをかきつつ理解を深めたい。</p><h2 id=まとめ>まとめ</h2><p>Apache Beam 2.40.0 から利用可能になった RunInference API についてサンプルコードと共に何をやっているかをまとめました。</p><p>Apache Beam はとても未来を感じるソフトウェアなので、この OSS にコントリビュートできる余地があれば積極的にやっていきたい。
そのため、これからはちょっとした Beam の勉強メモなども積極的に公開されていくと思います。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://cloud.google.com/blog/products/data-analytics/latest-dataflow-innovations-for-real-time-streaming-and-aiml>Latest Dataflow innovations for real-time streaming and AI/ML | Google Cloud Blog</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href="https://docs.google.com/document/d/1bVMU7Uo9Nzuu6aXR702j74nhQK4j6J1lkRVVBRySI0g/edit#heading=h.cqivojrr7lme">RunInference: ML Inference in Beam</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://beam.apache.org/documentation/sdks/python-machine-learning/>Apache Beam Python Machine Learning</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><h2>関連しているかもしれない記事</h2><ul><li><a href=/posts/2020-12-26/>PythonでApache beam 入門</a></li><li><a href=/posts/2017-11-14/>OpenCV 3.3から使えるDNNモジュールを使って物体検出</a></li><li><a href=/posts/2022-08-15-2335/>KDD2022 で気になった研究</a></li><li><a href=/posts/2022-08-10-1717/>poetry show でパッケージ名に (!) が付与されている意味</a></li><li><a href=/posts/2022-05-10-2200/>社内でデータ分析結果を可視化・共有する際に Google Colab が便利</a></li></ul><pre>
---

このサイトの更新情報を<a href=/index.xml>RSS</a>で配信しています。お好きなフィードリーダーで購読してみてください。

👏このウェブサイトの運営を支援していただける方を募集しています。
もしよろしければ、<a href=https://www.buymeacoffee.com/hurutoriya>Buy Me a Coffee</a> からサポート(投げ銭)していただけると、著者の活動のモチベーションに繋がります✨

📮おたより
記事への感想の<a href="https://docs.google.com/forms/d/e/1FAIpQLScgZVDrjQiKLbQRovfs88oweCITzjtvt1PlgwL14JfWPOrpPQ/viewform?usp=pp_url&entry.838298670=https%3a%2f%2fshunyaueta.com%2fposts%2f2022-08-18-1938%2f">おたより</a>をおまちしてます。
お気軽にお送りください。
お返事はメールアドレス入力があればメールでさせていただきます。
もちろんお返事を希望せずに単なる感想だけでも大歓迎です。
</pre></content><p><a href=https://shunyaueta.com/tags/apachebeam/>#apachebeam</a>
<a href=https://shunyaueta.com/tags/machinelearning/>#machinelearning</a>
<a href=https://shunyaueta.com/tags/python/>#python</a></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>