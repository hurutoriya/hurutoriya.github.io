<!doctype html><html lang=ja dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる | hurutoriya</title><meta name=keywords content="apachebeam,machinelearning,python"><meta name=description content="2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1
 Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow&rsquo;s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX)."><meta name=author content="Shunya Ueta"><link rel=canonical href=https://shunyaueta.com/posts/2022-08-18-1938/><link crossorigin=anonymous href=/assets/css/stylesheet.abc7c82c3d415a6df50430738d1cbcc4c76fea558bc5a0c830d3babf78167a35.css integrity="sha256-q8fILD1BWm31BDBzjRy8xMdv6lWLxaDIMNO6v3gWejU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://shunyaueta.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shunyaueta.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shunyaueta.com/favicon-32x32.png><link rel=apple-touch-icon href=https://shunyaueta.com/apple-touch-icon.png><link rel=mask-icon href=https://shunyaueta.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-JMJRQJT0Q3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-JMJRQJT0Q3');</script><meta property="og:title" content="Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる"><meta property="og:description" content="2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1
 Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow&rsquo;s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX)."><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2022-08-18-1938/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-18T19:38:29+09:00"><meta property="article:modified_time" content="2022-08-18T19:38:29+09:00"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる"><meta name=twitter:description content="2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1
 Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow&rsquo;s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる","item":"https://shunyaueta.com/posts/2022-08-18-1938/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる","name":"Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる","description":"2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1\n Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow\u0026rsquo;s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX).","keywords":["apachebeam","machinelearning","python"],"articleBody":"2022-07-21 に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。1\n Dataflow ML - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow’s existing ML capabilities such as GPU support and the pre and post processing system for ML training, either directly or via frameworks such as Tensorflow Extended (TFX).\n DataFlow は Apache Beam で記述したプログラミングモデルの実行環境だ。 主に、バッチ処理・ストリーミング処理で使われており、機械学習に欠かせないデータ処理の観点からして非常に面白いと思っているソフトウェアなので、積極的に動向を追っている。\nDataFlow ML は簡単に説明すると、PyTorch と scikit-learn が Dataflow のパイプライン内部で直接推論可能になった。 直近では Python SDK に限るが DataFlow で GPU の利用が可能になったりと、大規模データ処理や機械学習を行う際に魅力的な機能が続々とサポートされはじめた。 DataFlow ML の実態は Apache Beam 2.40.0で追加された RunInference API だ。 RunInference API の DesignDocs 2が公開されているので、それを見てみる。 機械学習システムの DesignDocs として非常に面白いので興味がある人は、見てみると面白いとおもう。\nDesignDocs: RunInference: ML Inference in Beam RunInference API が作られた背景として、TensorFlow だけが Beam 内部で、処理するデータ( Apache beam ではPCollection と呼ばれる) に対して、推論を行うための RunInference transformer (Apache Beam では PTransformと呼ばれ、PCollection を入力として処理を行う) が存在しており、実装不要だった。\n 問題点としては  Beam のレポジトリに存在せず、tensorflow/tfx-bslのリポジトリに存在している。 TFX ライブラリに特化した API になっている サンプルコードが充実しておらず、学習コストが高い    この DesignDocs の目的は、RunInference API を以下の 2 つの人気のある機械学習フレームワークで使えるように実装すること\n scikit-learn PyTorch  実装方法としては、\n 内部の最適化が行われているべき 単純かつ統合されたインターフェイスで提供されるべき 入力と出力が、機械学習エンジニアに対して、直感的な型になっているべき  scikit-learn なら numpy PyTorch なら Tensors    最終的なゴールとしては、XGBoost や JAX など他の機械学習フレームワークにも適合したり、Vertex AI など外部のサービスも使えるようにしたい。\n内部の実装方針は DesignDocs で詳細に議論されているので、そこは割愛して、まずは RunInference API のサンプルコードを動かしてみる。\nサンプルコードを通じて学ぶ Example RunInference API pipelinesにサンプルコードが公開されていたので、動かしてみる。 まずは一番簡単そうなサンプルコードである scikit-learn による MNIST 分類を動かしてみる。 最初にドキュメントをじっくり読むよりも実際にコードを見たほうが理解が深まるので実際に動かしてみるのが近道。 サンプルコードはこちらbeam/sdks/python/apache_beam/examples/inference/sklearn_mnist_classification.py 公式のサンプルコードだと、推論対象の入力データと学習済みの scikit-learn のモデルを自前で用意する必要があるのでコマンド一発でサンプルコードを動かせるコードを以下のレポジトリに公開しました。 将来的には scikit-learn だけでなく、PyTorch にも対応したい。\nhttps://github.com/hurutoriya/beam-runinferenceapi-sample\n実際に何をやっているかの解説は、日本語のコメントを添えて解説してみます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91  \"\"\"このパイプラインは、RunInference APIを使って、MNISTデータの分類を行う。 このパイプラインは、int 形式でCSV形式で保存されているテキストファイルを読み込む。CSVの１つ目の列はラベル、そのほかの列はMNISTのピクセルの値を格納している。データは学習済みのモデルで推論される。このパイプラインは output ファイルに推論結果の書き込みを行い、ラベルと推論結果の比較を行うことができる。 \"\"\" import argparse from typing import Iterable from typing import List from typing import Tuple import apache_beam as beam from apache_beam.ml.inference.base import KeyedModelHandler from apache_beam.ml.inference.base import PredictionResult from apache_beam.ml.inference.base import RunInference from apache_beam.ml.inference.sklearn_inference import ModelFileType from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpy from apache_beam.options.pipeline_options import PipelineOptions from apache_beam.options.pipeline_options import SetupOptions def process_input(row: str) - Tuple[int, List[int: \"\"\"入力データをラベルとピクセルに分けて、返す \"\"\" data = row.split(',') label, pixels = int(data[0]), data[1:] pixels = [int(pixel) for pixel in pixels] return label, pixels class PostProcessor(beam.DoFn): \"\"\"予測ラベルを得るために予測結果を処理する。CSV形式で、真値と予測ラベルを返す。 \"\"\" def process(self, element: Tuple[int, PredictionResult]) - Iterable[str]: label, prediction_result = element prediction = prediction_result.inference yield '{},{}'.format(label, prediction) def parse_known_args(argv): \"\"\"引数を定義\"\"\" parser = argparse.ArgumentParser() parser.add_argument( '--input_file', dest='input', required=True, help='text file with comma separated int values.') parser.add_argument( '--output', dest='output', required=True, help='Path to save output predictions.') parser.add_argument( '--model_path', dest='model_path', required=True, help='Path to load the Sklearn model for Inference.') return parser.parse_known_args(argv) def run(argv=None, save_main_session=True): \"\"\"パイプラインを定義\"\"\" known_args, pipeline_args = parse_known_args(argv) pipeline_options = PipelineOptions(pipeline_args) pipeline_options.view_as(SetupOptions).save_main_session = save_main_session # この例では、RunInference トランスフォームにキーとなる入力を渡している。それによって、SklearnModelHandlerNumpy のラッパーである KeyedModelHandler を使用している。 model_loader = KeyedModelHandler( SklearnModelHandlerNumpy( model_file_type=ModelFileType.PICKLE, model_uri=known_args.model_path)) with beam.Pipeline(options=pipeline_options) as p: # 入力データを読み込む label_pixel_tuple = ( p | \"ReadFromInput\"  beam.io.ReadFromText( known_args.input, skip_header_lines=1) | \"PreProcessInputs\"  beam.Map(process_input)) # 推論して後処理を行う predictions = ( label_pixel_tuple | \"RunInference\"  RunInference(model_loader) | \"PostProcessOutputs\"  beam.ParDo(PostProcessor())) # 後処理したデータを出力してファイルとして保存する _ = predictions | \"WriteOutput\"  beam.io.WriteToText( known_args.output, shard_name_template='', append_trailing_newlines=True) if __name__ == '__main__': run()   RunInference API についてドキュメントから学ぶ RunInference API のドキュメント3が既に公開されているので、読み込むことで実際にどんな概念で作成されているのか理解できる。 Apache Beam の基礎的な概念や用語については、この記事を読む前に過去に書いた Apache beam Python 入門を読んでいただけると、この記事が分かりやすく読めると思います。\nなぜ RunInferenceAPI を使うのか? そもそも僕の疑問は「今まで、Beam で PyTorch や scikit-learn のモデルを使って推論は可能だったが、 RunInferenceAPI は何が嬉しいのか?」への答えがドキュメントに書いてありました。\n BatchElementsトランスフォームやSharedクラスなど既存の Apache Beam の概念に沿って、機械学習の推論処理を最適化が可能になる。またマルチモデルパイプラインなど、複雑なパイプラインの構築も比較的簡単に構築できるようになる。  BatchElements PTransform とは 多くのモデルが実装しているベクトル化推論の最適化を利用するために、モデルの予測を行う前の中間段階として、BatchElements トランスフォームの追加した。このトランスフォームは、要素をバッチ処理する。そして、バッチされた要素は、RunInference の特定のフレームワークのための変換で適用される。例えば、numpy の ndarrays の場合は numpy.stack()を、torch の Tensor 要素の場合は torch.stack()を呼び出す。\nbeam.BatchElements の設定をカスタマイズするには、ModelHandler の中で、batch_elements_kwargs 関数をオーバーライドする必要がある。例えば、min_batch_size でバッチあたりの要素数の最小値を設定し、max_batch_size でバッチあたりの要素数の最大値を設定する。 詳しいドキュメントはこちら\nShared helper class RunInference API でShared クラスを使用することにより、プロセスごとに一度モデルを読み込んだ後に各プロセス内部でその読み込んだモデルをDoFn(全てのPCollectionに適用される処理ロジックを保持する)内で共有することができる。このクラスを使えば、モデルの読み込み時間とメモリ消費を削減することができる。\n詳しいドキュメント(ソースコード)はこちら。内部の処理ロジック自体 100 行未満で書かれているので、読んで見るのもあり。\n機械学習モデルを使うためにパイプラインを構築する 以下のコードを Apache Beam のパイプラインに追加すれば、RunInference トランスフォームを使用できる。\n1 2 3 4  from apache_beam.ml.inference.base import RunInference with pipeline as p: predictions = ( p | 'Read'  beam.ReadFromSource('a_source') | 'RunInference'  RunInference(model_handler)   model_handler は、モデルのハンドラーのためのセットアップコードで、モデルのインポートができる。 以下のようなModelHandlerの例がある\n1 2 3 4  from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpy from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerPandas from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerKeyedTensor   モデル A と モデル B の推論結果を並列して行ったり、\n1 2 3 4  with pipeline as p: data = p | 'Read'  beam.ReadFromSource('a_source') model_a_predictions = data | RunInference(model_handler_A) model_b_predictions = data | RunInference(model_handler_B)   アンサンブルも柔軟に記述できる。\n1 2 3 4 5  with pipeline as p: data = p | 'Read'  beam.ReadFromSource('a_source') model_a_predictions = data | RunInference(model_handler_A) model_b_predictions = model_a_predictions | beam.Map(some_post_processing) | RunInference(model_handler_B)   また驚きなのが、Apache Beam 2.41.0 移行は Multi Language SDKによって、 Java からも RunInference API を使うことができるらしい。 これってモデル構築は Python で行って、運用は安定した Java で実行可能ということなので凄い機能ですね。\nhttps://github.com/apache/beam/blob/master/sdks/java/extensions/python/src/main/java/org/apache/beam/sdk/extensions/python/transforms/RunInference.java\nMulti Language SDK も非常に面白そうだが、まだ使ったことはないので、実際にコードをかきつつ理解を深めたい。\nまとめ Apache Beam 2.40.0 から利用可能になった RunInference API についてサンプルコードと共に何をやっているかをまとめました。\nApache Beam はとても未来を感じるソフトウェアなので、この OSS にコントリビュートできる余地があれば積極的にやっていきたい。 そのため、これからはちょっとした Beam の勉強メモなども積極的に公開されていくと思います。\n  Latest Dataflow innovations for real-time streaming and AI/ML | Google Cloud Blog ↩︎\n RunInference: ML Inference in Beam ↩︎\n Apache Beam Python Machine Learning ↩︎\n   ","wordCount":"756","inLanguage":"ja","datePublished":"2022-08-18T19:38:29+09:00","dateModified":"2022-08-18T19:38:29+09:00","author":{"@type":"Person","name":"Shunya Ueta"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://shunyaueta.com/posts/2022-08-18-1938/"},"publisher":{"@type":"Organization","name":"hurutoriya","logo":{"@type":"ImageObject","url":"https://shunyaueta.com/favicon.ico"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href=https://shunyaueta.com/ accesskey=h title="hurutoriya (Alt + H)">hurutoriya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://shunyaueta.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://shunyaueta.com/about title=About><span>About</span></a></li><li><a href=https://searchengineeringnewsletter.substack.com/ title=Newsletter><span>Newsletter</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://shunyaueta.com/index.xml title=RSS><span>RSS</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Apache Beam 2.40 で導入された scikit-lean, Pytorch の効率的な推論が可能になる RunInference API を試してみる</h1><div class=post-meta><span title="2022-08-18 19:38:29 +0900 +0900">August 18, 2022</span>&nbsp;·&nbsp;Shunya Ueta</div></header><div class=post-content><p><code>2022-07-21</code> に Google Cloud が Cloud DataFlow の新機能として、DataFlow ML という新機能を発表した。<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><blockquote><p><strong>Dataflow ML</strong> - Speaking of ML transforms, Dataflow now has added out of the box support for running PyTorch and scikit-learn models directly within the pipeline. The new RunInference transform enables simplicity by allowing models to be used in production pipelines with very little code. These features are in addition to Dataflow&rsquo;s existing ML capabilities such as <a href=https://cloud.google.com/dataflow/docs/guides/using-gpus>GPU support</a> and the pre and post processing system for ML training, either directly or via frameworks such as <a href=https://www.tensorflow.org/tfx>Tensorflow Extended (TFX)</a>.</p></blockquote><p>DataFlow は Apache Beam で記述したプログラミングモデルの実行環境だ。
主に、バッチ処理・ストリーミング処理で使われており、機械学習に欠かせないデータ処理の観点からして非常に面白いと思っているソフトウェアなので、積極的に動向を追っている。</p><p>DataFlow ML は簡単に説明すると、PyTorch と scikit-learn が Dataflow のパイプライン内部で直接推論可能になった。
直近では Python SDK に限るが DataFlow で GPU の利用が可能になったりと、大規模データ処理や機械学習を行う際に魅力的な機能が続々とサポートされはじめた。
DataFlow ML の実態は <a href=https://beam.apache.org/blog/beam-2.40.0>Apache Beam 2.40.0</a>で追加された RunInference API だ。
RunInference API の DesignDocs <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>が公開されているので、それを見てみる。
機械学習システムの DesignDocs として非常に面白いので興味がある人は、見てみると面白いとおもう。</p><h2 id=designdocs-runinference-ml-inference-in-beam>DesignDocs: RunInference: ML Inference in Beam<a hidden class=anchor aria-hidden=true href=#designdocs-runinference-ml-inference-in-beam>#</a></h2><p>RunInference API が作られた背景として、TensorFlow だけが Beam 内部で、処理するデータ( Apache beam では<code>PCollection</code> と呼ばれる) に対して、推論を行うための <a href=https://github.com/tensorflow/tfx-bsl/blob/master/tfx_bsl/beam/run_inference.py>RunInference transformer</a> (Apache Beam では <code>PTransform</code>と呼ばれ、<code>PCollection</code> を入力として処理を行う) が存在しており、実装不要だった。</p><ul><li>問題点としては<ul><li>Beam のレポジトリに存在せず、<a href=https://github.com/tensorflow/tfx-bsl>tensorflow/tfx-bsl</a>のリポジトリに存在している。</li><li>TFX ライブラリに特化した API になっている</li><li>サンプルコードが充実しておらず、学習コストが高い</li></ul></li></ul><p>この DesignDocs の目的は、RunInference API を以下の 2 つの人気のある機械学習フレームワークで使えるように実装すること</p><ul><li>scikit-learn</li><li>PyTorch</li></ul><p>実装方法としては、</p><ul><li>内部の最適化が行われているべき</li><li>単純かつ統合されたインターフェイスで提供されるべき</li><li>入力と出力が、機械学習エンジニアに対して、直感的な型になっているべき<ul><li>scikit-learn なら numpy</li><li>PyTorch なら Tensors</li></ul></li></ul><p>最終的なゴールとしては、XGBoost や JAX など他の機械学習フレームワークにも適合したり、Vertex AI など外部のサービスも使えるようにしたい。</p><p>内部の実装方針は DesignDocs で詳細に議論されているので、そこは割愛して、まずは RunInference API のサンプルコードを動かしてみる。</p><h2 id=サンプルコードを通じて学ぶ>サンプルコードを通じて学ぶ<a hidden class=anchor aria-hidden=true href=#サンプルコードを通じて学ぶ>#</a></h2><p><a href=https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/inference>Example RunInference API pipelines</a>にサンプルコードが公開されていたので、動かしてみる。
まずは一番簡単そうなサンプルコードである scikit-learn による MNIST 分類を動かしてみる。
最初にドキュメントをじっくり読むよりも実際にコードを見たほうが理解が深まるので実際に動かしてみるのが近道。
サンプルコードはこちら<a href=https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/inference/sklearn_mnist_classification.py>beam/sdks/python/apache_beam/examples/inference/sklearn_mnist_classification.py</a>
公式のサンプルコードだと、推論対象の入力データと学習済みの scikit-learn のモデルを自前で用意する必要があるのでコマンド一発でサンプルコードを動かせるコードを以下のレポジトリに公開しました。
将来的には scikit-learn だけでなく、PyTorch にも対応したい。</p><p><a href=https://github.com/hurutoriya/beam-runinferenceapi-sample>https://github.com/hurutoriya/beam-runinferenceapi-sample</a></p><p>実際に何をやっているかの解説は、日本語のコメントを添えて解説してみます。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span><span class=lnt>87
</span><span class=lnt>88
</span><span class=lnt>89
</span><span class=lnt>90
</span><span class=lnt>91
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=s2>&#34;&#34;&#34;このパイプラインは、RunInference APIを使って、MNISTデータの分類を行う。
</span><span class=s2>
</span><span class=s2>このパイプラインは、int 形式でCSV形式で保存されているテキストファイルを読み込む。CSVの１つ目の列はラベル、そのほかの列はMNISTのピクセルの値を格納している。データは学習済みのモデルで推論される。このパイプラインは output ファイルに推論結果の書き込みを行い、ラベルと推論結果の比較を行うことができる。
</span><span class=s2>&#34;&#34;&#34;</span>

<span class=kn>import</span> <span class=nn>argparse</span>
<span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Iterable</span>
<span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span>
<span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Tuple</span>

<span class=kn>import</span> <span class=nn>apache_beam</span> <span class=kn>as</span> <span class=nn>beam</span>
<span class=kn>from</span> <span class=nn>apache_beam.ml.inference.base</span> <span class=kn>import</span> <span class=n>KeyedModelHandler</span>
<span class=kn>from</span> <span class=nn>apache_beam.ml.inference.base</span> <span class=kn>import</span> <span class=n>PredictionResult</span>
<span class=kn>from</span> <span class=nn>apache_beam.ml.inference.base</span> <span class=kn>import</span> <span class=n>RunInference</span>
<span class=kn>from</span> <span class=nn>apache_beam.ml.inference.sklearn_inference</span> <span class=kn>import</span> <span class=n>ModelFileType</span>
<span class=kn>from</span> <span class=nn>apache_beam.ml.inference.sklearn_inference</span> <span class=kn>import</span> <span class=n>SklearnModelHandlerNumpy</span>
<span class=kn>from</span> <span class=nn>apache_beam.options.pipeline_options</span> <span class=kn>import</span> <span class=n>PipelineOptions</span>
<span class=kn>from</span> <span class=nn>apache_beam.options.pipeline_options</span> <span class=kn>import</span> <span class=n>SetupOptions</span>

<span class=k>def</span> <span class=nf>process_input</span><span class=p>(</span><span class=n>row</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=n>List</span><span class=p>[</span><span class=nb>int</span><span class=p>:</span>
  <span class=s2>&#34;&#34;&#34;入力データをラベルとピクセルに分けて、返す
</span><span class=s2>  &#34;&#34;&#34;</span>
  <span class=n>data</span> <span class=o>=</span> <span class=n>row</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;,&#39;</span><span class=p>)</span>
  <span class=n>label</span><span class=p>,</span> <span class=n>pixels</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=n>data</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span>
  <span class=n>pixels</span> <span class=o>=</span> <span class=p>[</span><span class=nb>int</span><span class=p>(</span><span class=n>pixel</span><span class=p>)</span> <span class=k>for</span> <span class=n>pixel</span> <span class=ow>in</span> <span class=n>pixels</span><span class=p>]</span>
  <span class=k>return</span> <span class=n>label</span><span class=p>,</span> <span class=n>pixels</span>


<span class=k>class</span> <span class=nc>PostProcessor</span><span class=p>(</span><span class=n>beam</span><span class=o>.</span><span class=n>DoFn</span><span class=p>):</span>
  <span class=s2>&#34;&#34;&#34;予測ラベルを得るために予測結果を処理する。CSV形式で、真値と予測ラベルを返す。
</span><span class=s2>  &#34;&#34;&#34;</span>
  <span class=k>def</span> <span class=nf>process</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>element</span><span class=p>:</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=n>PredictionResult</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>str</span><span class=p>]:</span>
    <span class=n>label</span><span class=p>,</span> <span class=n>prediction_result</span> <span class=o>=</span> <span class=n>element</span>
    <span class=n>prediction</span> <span class=o>=</span> <span class=n>prediction_result</span><span class=o>.</span><span class=n>inference</span>
    <span class=k>yield</span> <span class=s1>&#39;{},{}&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>label</span><span class=p>,</span> <span class=n>prediction</span><span class=p>)</span>


<span class=k>def</span> <span class=nf>parse_known_args</span><span class=p>(</span><span class=n>argv</span><span class=p>):</span>
  <span class=s2>&#34;&#34;&#34;引数を定義&#34;&#34;&#34;</span>
  <span class=n>parser</span> <span class=o>=</span> <span class=n>argparse</span><span class=o>.</span><span class=n>ArgumentParser</span><span class=p>()</span>
  <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span>
      <span class=s1>&#39;--input_file&#39;</span><span class=p>,</span>
      <span class=n>dest</span><span class=o>=</span><span class=s1>&#39;input&#39;</span><span class=p>,</span>
      <span class=n>required</span><span class=o>=</span><span class=bp>True</span><span class=p>,</span>
      <span class=n>help</span><span class=o>=</span><span class=s1>&#39;text file with comma separated int values.&#39;</span><span class=p>)</span>
  <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span>
      <span class=s1>&#39;--output&#39;</span><span class=p>,</span>
      <span class=n>dest</span><span class=o>=</span><span class=s1>&#39;output&#39;</span><span class=p>,</span>
      <span class=n>required</span><span class=o>=</span><span class=bp>True</span><span class=p>,</span>
      <span class=n>help</span><span class=o>=</span><span class=s1>&#39;Path to save output predictions.&#39;</span><span class=p>)</span>
  <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span>
      <span class=s1>&#39;--model_path&#39;</span><span class=p>,</span>
      <span class=n>dest</span><span class=o>=</span><span class=s1>&#39;model_path&#39;</span><span class=p>,</span>
      <span class=n>required</span><span class=o>=</span><span class=bp>True</span><span class=p>,</span>
      <span class=n>help</span><span class=o>=</span><span class=s1>&#39;Path to load the Sklearn model for Inference.&#39;</span><span class=p>)</span>
  <span class=k>return</span> <span class=n>parser</span><span class=o>.</span><span class=n>parse_known_args</span><span class=p>(</span><span class=n>argv</span><span class=p>)</span>


<span class=k>def</span> <span class=nf>run</span><span class=p>(</span><span class=n>argv</span><span class=o>=</span><span class=bp>None</span><span class=p>,</span> <span class=n>save_main_session</span><span class=o>=</span><span class=bp>True</span><span class=p>):</span>
  <span class=s2>&#34;&#34;&#34;パイプラインを定義&#34;&#34;&#34;</span>
  <span class=n>known_args</span><span class=p>,</span> <span class=n>pipeline_args</span> <span class=o>=</span> <span class=n>parse_known_args</span><span class=p>(</span><span class=n>argv</span><span class=p>)</span>
  <span class=n>pipeline_options</span> <span class=o>=</span> <span class=n>PipelineOptions</span><span class=p>(</span><span class=n>pipeline_args</span><span class=p>)</span>
  <span class=n>pipeline_options</span><span class=o>.</span><span class=n>view_as</span><span class=p>(</span><span class=n>SetupOptions</span><span class=p>)</span><span class=o>.</span><span class=n>save_main_session</span> <span class=o>=</span> <span class=n>save_main_session</span>

  <span class=c1># この例では、RunInference トランスフォームにキーとなる入力を渡している。それによって、SklearnModelHandlerNumpy のラッパーである KeyedModelHandler を使用している。</span>
  <span class=n>model_loader</span> <span class=o>=</span> <span class=n>KeyedModelHandler</span><span class=p>(</span>
      <span class=n>SklearnModelHandlerNumpy</span><span class=p>(</span>
          <span class=n>model_file_type</span><span class=o>=</span><span class=n>ModelFileType</span><span class=o>.</span><span class=n>PICKLE</span><span class=p>,</span>
          <span class=n>model_uri</span><span class=o>=</span><span class=n>known_args</span><span class=o>.</span><span class=n>model_path</span><span class=p>))</span>

  <span class=k>with</span> <span class=n>beam</span><span class=o>.</span><span class=n>Pipeline</span><span class=p>(</span><span class=n>options</span><span class=o>=</span><span class=n>pipeline_options</span><span class=p>)</span> <span class=k>as</span> <span class=n>p</span><span class=p>:</span>
	<span class=c1># 入力データを読み込む</span>
    <span class=n>label_pixel_tuple</span> <span class=o>=</span> <span class=p>(</span>
        <span class=n>p</span>
        <span class=o>|</span> <span class=s2>&#34;ReadFromInput&#34;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>io</span><span class=o>.</span><span class=n>ReadFromText</span><span class=p>(</span>
            <span class=n>known_args</span><span class=o>.</span><span class=n>input</span><span class=p>,</span> <span class=n>skip_header_lines</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=o>|</span> <span class=s2>&#34;PreProcessInputs&#34;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>Map</span><span class=p>(</span><span class=n>process_input</span><span class=p>))</span>
	<span class=c1># 推論して後処理を行う</span>
    <span class=n>predictions</span> <span class=o>=</span> <span class=p>(</span>
        <span class=n>label_pixel_tuple</span>
        <span class=o>|</span> <span class=s2>&#34;RunInference&#34;</span> <span class=o>&gt;&gt;</span> <span class=n>RunInference</span><span class=p>(</span><span class=n>model_loader</span><span class=p>)</span>
        <span class=o>|</span> <span class=s2>&#34;PostProcessOutputs&#34;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>ParDo</span><span class=p>(</span><span class=n>PostProcessor</span><span class=p>()))</span>
	<span class=c1># 後処理したデータを出力してファイルとして保存する</span>
    <span class=n>_</span> <span class=o>=</span> <span class=n>predictions</span> <span class=o>|</span> <span class=s2>&#34;WriteOutput&#34;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>io</span><span class=o>.</span><span class=n>WriteToText</span><span class=p>(</span>
        <span class=n>known_args</span><span class=o>.</span><span class=n>output</span><span class=p>,</span>
        <span class=n>shard_name_template</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span>
        <span class=n>append_trailing_newlines</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>


<span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
  <span class=n>run</span><span class=p>()</span>
</code></pre></td></tr></table></div></div><h2 id=runinference-api-についてドキュメントから学ぶ>RunInference API についてドキュメントから学ぶ<a hidden class=anchor aria-hidden=true href=#runinference-api-についてドキュメントから学ぶ>#</a></h2><p>RunInference API のドキュメント<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>が既に公開されているので、読み込むことで実際にどんな概念で作成されているのか理解できる。
Apache Beam の基礎的な概念や用語については、この記事を読む前に過去に書いた Apache beam Python 入門を読んでいただけると、この記事が分かりやすく読めると思います。</p><h3 id=なぜ-runinferenceapi-を使うのか>なぜ RunInferenceAPI を使うのか?<a hidden class=anchor aria-hidden=true href=#なぜ-runinferenceapi-を使うのか>#</a></h3><p>そもそも僕の疑問は「今まで、Beam で PyTorch や scikit-learn のモデルを使って推論は可能だったが、 RunInferenceAPI は何が嬉しいのか?」への答えがドキュメントに書いてありました。</p><ul><li><code>BatchElements</code>トランスフォームや<code>Shared</code>クラスなど既存の Apache Beam の概念に沿って、機械学習の推論処理を最適化が可能になる。またマルチモデルパイプラインなど、複雑なパイプラインの構築も比較的簡単に構築できるようになる。</li></ul><h3 id=batchelements-ptransform-とは>BatchElements PTransform とは<a hidden class=anchor aria-hidden=true href=#batchelements-ptransform-とは>#</a></h3><p>多くのモデルが実装しているベクトル化推論の最適化を利用するために、モデルの予測を行う前の中間段階として、BatchElements トランスフォームの追加した。このトランスフォームは、要素をバッチ処理する。そして、バッチされた要素は、RunInference の特定のフレームワークのための変換で適用される。例えば、numpy の ndarrays の場合は numpy.stack()を、torch の Tensor 要素の場合は torch.stack()を呼び出す。</p><p>beam.BatchElements の設定をカスタマイズするには、ModelHandler の中で、batch_elements_kwargs 関数をオーバーライドする必要がある。例えば、<code>min_batch_size</code> でバッチあたりの要素数の最小値を設定し、<code>max_batch_size</code> でバッチあたりの要素数の最大値を設定する。
詳しいドキュメントは<a href=https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html#apache_beam.transforms.util.BatchElements>こちら</a></p><h3 id=shared-helper-class>Shared helper class<a hidden class=anchor aria-hidden=true href=#shared-helper-class>#</a></h3><p>RunInference API で<code>Shared</code> クラスを使用することにより、プロセスごとに一度モデルを読み込んだ後に各プロセス内部でその読み込んだモデルを<code>DoFn</code>(全ての<code>PCollection</code>に適用される処理ロジックを保持する)内で共有することができる。このクラスを使えば、モデルの読み込み時間とメモリ消費を削減することができる。</p><p>詳しいドキュメント(ソースコード)は<a href=https://github.com/apache/beam/blob/master/sdks/python/apache_beam/utils/shared.py#L20>こちら</a>。内部の処理ロジック自体 100 行未満で書かれているので、読んで見るのもあり。</p><h3 id=機械学習モデルを使うためにパイプラインを構築する>機械学習モデルを使うためにパイプラインを構築する<a hidden class=anchor aria-hidden=true href=#機械学習モデルを使うためにパイプラインを構築する>#</a></h3><p>以下のコードを Apache Beam のパイプラインに追加すれば、RunInference トランスフォームを使用できる。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>apache_beam.ml.inference.base</span> <span class=kn>import</span> <span class=n>RunInference</span>
<span class=k>with</span> <span class=n>pipeline</span> <span class=k>as</span> <span class=n>p</span><span class=p>:</span>
   <span class=n>predictions</span> <span class=o>=</span> <span class=p>(</span> <span class=n>p</span> <span class=o>|</span>  <span class=s1>&#39;Read&#39;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>ReadFromSource</span><span class=p>(</span><span class=s1>&#39;a_source&#39;</span><span class=p>)</span>
                     <span class=o>|</span> <span class=s1>&#39;RunInference&#39;</span> <span class=o>&gt;&gt;</span> <span class=n>RunInference</span><span class=p>(</span><span class=o>&lt;</span><span class=n>model_handler</span><span class=o>&gt;</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><code>model_handler</code> は、モデルのハンドラーのためのセットアップコードで、モデルのインポートができる。
以下のような<code>ModelHandler</code>の例がある</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>apache_beam.ml.inference.sklearn_inference</span> <span class=kn>import</span> <span class=n>SklearnModelHandlerNumpy</span>
<span class=kn>from</span> <span class=nn>apache_beam.ml.inference.sklearn_inference</span> <span class=kn>import</span> <span class=n>SklearnModelHandlerPandas</span>
<span class=kn>from</span> <span class=nn>apache_beam.ml.inference.pytorch_inference</span> <span class=kn>import</span> <span class=n>PytorchModelHandlerTensor</span>
<span class=kn>from</span> <span class=nn>apache_beam.ml.inference.pytorch_inference</span> <span class=kn>import</span> <span class=n>PytorchModelHandlerKeyedTensor</span>
</code></pre></td></tr></table></div></div><p>モデル A と モデル B の推論結果を並列して行ったり、</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>with</span> <span class=n>pipeline</span> <span class=k>as</span> <span class=n>p</span><span class=p>:</span>
   <span class=n>data</span> <span class=o>=</span> <span class=n>p</span> <span class=o>|</span> <span class=s1>&#39;Read&#39;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>ReadFromSource</span><span class=p>(</span><span class=s1>&#39;a_source&#39;</span><span class=p>)</span>
   <span class=n>model_a_predictions</span> <span class=o>=</span> <span class=n>data</span> <span class=o>|</span> <span class=n>RunInference</span><span class=p>(</span><span class=o>&lt;</span><span class=n>model_handler_A</span><span class=o>&gt;</span><span class=p>)</span>
   <span class=n>model_b_predictions</span> <span class=o>=</span> <span class=n>data</span> <span class=o>|</span> <span class=n>RunInference</span><span class=p>(</span><span class=o>&lt;</span><span class=n>model_handler_B</span><span class=o>&gt;</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>アンサンブルも柔軟に記述できる。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>with</span> <span class=n>pipeline</span> <span class=k>as</span> <span class=n>p</span><span class=p>:</span>
   <span class=n>data</span> <span class=o>=</span> <span class=n>p</span> <span class=o>|</span> <span class=s1>&#39;Read&#39;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>ReadFromSource</span><span class=p>(</span><span class=s1>&#39;a_source&#39;</span><span class=p>)</span>
   <span class=n>model_a_predictions</span> <span class=o>=</span> <span class=n>data</span> <span class=o>|</span> <span class=n>RunInference</span><span class=p>(</span><span class=o>&lt;</span><span class=n>model_handler_A</span><span class=o>&gt;</span><span class=p>)</span>
   <span class=n>model_b_predictions</span> <span class=o>=</span> <span class=n>model_a_predictions</span> <span class=o>|</span> <span class=n>beam</span><span class=o>.</span><span class=n>Map</span><span class=p>(</span><span class=n>some_post_processing</span><span class=p>)</span> <span class=o>|</span> <span class=n>RunInference</span><span class=p>(</span><span class=o>&lt;</span><span class=n>model_handler_B</span><span class=o>&gt;</span><span class=p>)</span>

</code></pre></td></tr></table></div></div><p>また驚きなのが、Apache Beam 2.41.0 移行は <a href=https://beam.apache.org/documentation/programming-guide/#multi-language-pipelines>Multi Language SDK</a>によって、 Java からも RunInference API を使うことができるらしい。
これってモデル構築は Python で行って、運用は安定した Java で実行可能ということなので凄い機能ですね。</p><p><a href=https://github.com/apache/beam/blob/master/sdks/java/extensions/python/src/main/java/org/apache/beam/sdk/extensions/python/transforms/RunInference.java>https://github.com/apache/beam/blob/master/sdks/java/extensions/python/src/main/java/org/apache/beam/sdk/extensions/python/transforms/RunInference.java</a></p><p>Multi Language SDK も非常に面白そうだが、まだ使ったことはないので、実際にコードをかきつつ理解を深めたい。</p><h2 id=まとめ>まとめ<a hidden class=anchor aria-hidden=true href=#まとめ>#</a></h2><p>Apache Beam 2.40.0 から利用可能になった RunInference API についてサンプルコードと共に何をやっているかをまとめました。</p><p>Apache Beam はとても未来を感じるソフトウェアなので、この OSS にコントリビュートできる余地があれば積極的にやっていきたい。
そのため、これからはちょっとした Beam の勉強メモなども積極的に公開されていくと思います。</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=https://cloud.google.com/blog/products/data-analytics/latest-dataflow-innovations-for-real-time-streaming-and-aiml>Latest Dataflow innovations for real-time streaming and AI/ML | Google Cloud Blog</a> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><a href="https://docs.google.com/document/d/1bVMU7Uo9Nzuu6aXR702j74nhQK4j6J1lkRVVBRySI0g/edit#heading=h.cqivojrr7lme">RunInference: ML Inference in Beam</a> <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p><a href=https://beam.apache.org/documentation/sdks/python-machine-learning/>Apache Beam Python Machine Learning</a> <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section><h2>See Also</h2><ul><li><a href=/posts/2020-12-26/>PythonでApache beam 入門</a></li><li><a href=/posts/2017-11-14/>OpenCV 3.3から使えるDNNモジュールを使って物体検出</a></li><li><a href=/posts/2022-08-15-2335/>KDD2022 で気になった研究</a></li><li><a href=/posts/2022-08-10-1717/>poetry show でパッケージ名に (!) が付与されている意味</a></li><li><a href=/posts/2022-05-10-2200/>社内でデータ分析結果を可視化・共有する際に Google Colab が便利</a></li></ul><h2>Support</h2>記事をお読みくださりありがとうございます。このウェブサイトの運営を支援していただける方を募集しています。
もしよろしければ、下のボタンからサポート(投げ銭)していただけると、ブログ執筆、情報発信のモチベーションに繋がります✨
<a href=https://www.buymeacoffee.com/hurutoriya><img src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&emoji=☕&slug=hurutoriya&button_colour=FFDD00&font_colour=000000&font_family=Inter&outline_colour=000000&coffee_colour=ffffff"></a></div><footer class=post-footer><ul class=post-tags><li><a href=https://shunyaueta.com/tags/apachebeam/>apachebeam</a></li><li><a href=https://shunyaueta.com/tags/machinelearning/>machinelearning</a></li><li><a href=https://shunyaueta.com/tags/python/>python</a></li></ul><nav class=paginav><a class=prev href=https://shunyaueta.com/posts/2022-08-22-1300/><span class=title>« 前のページ</span><br><span>Java の memory map を理解する</span></a>
<a class=next href=https://shunyaueta.com/posts/2022-08-15-2335/><span class=title>次のページ »</span><br><span>KDD2022 で気になった研究</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=hurutoriya/hurutoriya.github.io data-repo-id="MDEwOlJlcG9zaXRvcnk2MDgyNTY1Nw==" data-category=Comments data-category-id=DIC_kwDOA6AgOc4CAQTX data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=dark data-lang=ja crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://shunyaueta.com/>hurutoriya</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z" /></svg></a><script>let menu=document.getElementById('menu')
if(menu){menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script><script>document.querySelectorAll('pre > code').forEach((codeblock)=>{const container=codeblock.parentNode.parentNode;const copybutton=document.createElement('button');copybutton.classList.add('copy-code');copybutton.innerHTML='コピー';function copyingDone(){copybutton.innerHTML='コピーされました!';setTimeout(()=>{copybutton.innerHTML='コピー';},2000);}
copybutton.addEventListener('click',(cb)=>{if('clipboard'in navigator){navigator.clipboard.writeText(codeblock.textContent);copyingDone();return;}
const range=document.createRange();range.selectNodeContents(codeblock);const selection=window.getSelection();selection.removeAllRanges();selection.addRange(range);try{document.execCommand('copy');copyingDone();}catch(e){};selection.removeRange(range);});if(container.classList.contains("highlight")){container.appendChild(copybutton);}else if(container.parentNode.firstChild==container){}else if(codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"){codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);}else{codeblock.parentNode.appendChild(copybutton);}});</script></body></html>