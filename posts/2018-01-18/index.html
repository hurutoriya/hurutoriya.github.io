<!doctype html><html><head><script data-ad-client=ca-pub-8604812913439531 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Where To Look: Focus Regions for Visual Question Answering (CVPR2016)を読んだ - Shunya Ueta</title><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:image" content><meta property="og:title" content="Where To Look: Focus Regions for Visual Question Answering (CVPR2016)を読んだ"><meta property="og:description" content="Kevin J. Shih, Saurabh Singh, Derek Hoiem, “Where To Look: Focus Regions for Visual Question Answering”, in CVPR2016 link Summry を読んだので、軽くメモ。 VQA(Visual Question Answer) 画像に対する質問に対して応答するタスクに対し、その質問クエリに対して画像のどの領域に注目すべきかのモデルの学習方法について論じた論文。 Contribution VQA d"><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2018-01-18/"><meta property="article:published_time" content="2018-01-18T05:41:44+00:00"><meta property="article:modified_time" content="2019-06-16T18:17:50+09:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Where To Look: Focus Regions for Visual Question Answering (CVPR2016)を読んだ"><meta name=twitter:description content="Kevin J. Shih, Saurabh Singh, Derek Hoiem, “Where To Look: Focus Regions for Visual Question Answering”, in CVPR2016 link Summry を読んだので、軽くメモ。 VQA(Visual Question Answer) 画像に対する質問に対して応答するタスクに対し、その質問クエリに対して画像のどの領域に注目すべきかのモデルの学習方法について論じた論文。 Contribution VQA d"><script src=https://shunyaueta.com/js/feather.min.js></script><link href=https://shunyaueta.com/css/fonts.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://shunyaueta.com/css/main.css><link rel=stylesheet type=text/css href=https://shunyaueta.com/css/dark.css media="(prefers-color-scheme: dark)"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-39994406-11"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-39994406-11');</script></head><body><div class=content><header><div class=main><a href=https://shunyaueta.com/>Shunya Ueta</a></div><nav><a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a></nav></header><main><article><div class=title><h1 class=title>Where To Look: Focus Regions for Visual Question Answering (CVPR2016)を読んだ</h1><div class=meta>Posted on Jan 18, 2018</div></div><section class=body><p>Kevin J. Shih, Saurabh Singh, Derek Hoiem, “Where To Look: Focus Regions for Visual Question Answering”, in CVPR2016 <a href=http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shih_Where_to_Look_CVPR_2016_paper.pdf>link</a></p><p><img src=/posts/2018-01-17/images/1.png alt=image></p><p>Summry</p><p>を読んだので、軽くメモ。</p><p>VQA(Visual Question Answer) 画像に対する質問に対して応答するタスクに対し、その質問クエリに対して画像のどの領域に注目すべきかのモデルの学習方法について論じた論文。</p><h3 id=contribution>Contribution</h3><ul><li><a href=http://www.visualqa.org/>VQA dataset</a>に対して、提案手法を適用。従来手法を全て上回った。</li><li>画像に対して CNN を用いて物体領域の検出を行った後にベクトル化、質問クエリは<code>word2vec</code>を用いてベクトル化を行う。</li><li>その 2 つのベクトルを用いて内積計算により重み付けを行うことで、どの領域に注目すべきかを計算する。</li></ul><h3 id=comments>Comments</h3><p>引用文献の訳 9 割が 2014–2015(直近 2 年間)で発表された論文で、改めてこの分野の最先端を駆け抜けるのは凄まじい能力が必要になるなと思いました。<br>そして相変わらず CVPR の論文のネーミングセンスは良いですね。(ジャケ買いならぬジャケ読み)</p><p>単純な質問なら、人間でも瞬間的に解答可能な物が多いなと感じた。</p><p><img src=/posts/2018-01-17/images/2.png alt=image></p><p>fig. 1</p><p>セマンティックな疑問(Fig.1 雨は降っていますか?)の場合、人間に注目した場合は傘をさしているから雨と判断しても良いがもっと広い範囲で画像を見てみると空は快晴なので人間に注目するのは筋が悪く VQA はとても難しくチャレンジングな問題だと書かれていた。(それでも充分すごい領域に到達しているなと思うが)</p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/computer-vision>Computer Vision</a></li><li><a href=/tags/paper>Paper</a></li><li><a href=/tags/cvpr>CVPR</a></li></ul></nav></div></article></main><h3>See Also</h3><ul><li><a href=/posts/2018-01-16/>Slicing Convolutional Neural Network for Crowd Video Understanding (CVPR2016)を読んだ</a></li><li><a href=/posts/2018-01-17/>Data-driven Crowd Analysis in Videos (ICCV2011)を読んだ</a></li><li><a href=/posts/2018-01-14/>Analyzing Free-standing Conversational Groups: A Multimodal Approach (ACMMM15) を読んだ</a></li><li><a href=/posts/2018-01-12/>Call center stress recognition with person-specific models を読んだ</a></li><li><a href=/posts/2018-01-11/>FUSE: Full Spectral Clustering(KDD2016) を読んだ</a></li></ul><footer><hr><a class=soc href=https://github.com/hurutoriya title=GitHub><i data-feather=github></i></a>|<a class=soc href=https://twitter.com/hurutoriya/ title=Twitter><i data-feather=twitter></i></a>|⚡️
2021 © Shunya Ueta | <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></footer><script>feather.replace()</script></div></body></html>