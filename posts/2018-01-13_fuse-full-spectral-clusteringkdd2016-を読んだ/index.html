<!doctype html><html lang=ja prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=description content><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=keywords content="Machine Learning,
Paper,
KDD,"><meta property="og:type" content="article"><meta property="og:description" content><meta property="og:title" content="FUSE: Full Spectral Clustering(KDD2016) を読んだ"><meta property="og:site_name" content><meta property="og:image" content><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content><meta property="og:image:height" content><meta property="og:url" content="https://shunyaueta.com/posts/2018-01-13_fuse-full-spectral-clusteringkdd2016-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/"><meta property="og:locale" content="ja"><meta property="article:published_time" content="2018-01-13"><meta property="article:modified_time" content="2018-01-13"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Paper"><meta property="article:tag" content="KDD"><meta name=twitter:card content="summary"><meta name=twitter:site content="@hurutoriya"><meta name=twitter:creator content="@hurutoriya"><meta name=twitter:title content="FUSE: Full Spectral Clustering(KDD2016) を読んだ |"><meta name=twitter:description content="べき乗法と独立成分分析を用いたマルチスケールに頑強なクラスタリング手法の提案
 べき乗法では近似固有ベクトル(Pseudo-eigenvectors)は複数の真の固有ベクトルの線形結合からなる。有益でない固有ベクトルも含まれるのでいかにそれを除去するか Fig.1 で言ってることがいまいちわからない。ｃ,ｄも変化がないように見える。論文で言及してる stripe が何を指してるかが不明 分かった。赤・青・黒の３つのクラスタで分かれている。最初は黒色がクラスタ境界面に見えた。論文内での multi scale というのは、データの幾何的な分布を指している？ 固有値計算は O(n³）かかるから計算量が~~って毎回言われるが、行列の状態に依存するので毎回最悪の場合を主張されてるも困る ZP self-turining spectral clustering をなぜ対抗馬にして比較してるのかは謎。 ICA¹を行ったあとにその行列に対して回転処理を行い情報量の最小化を狙う クラスタ数が多い場合、PIC では良い結果が出づらい multi scale なデータの場合標準の spectral clustering では失敗することが多々ある fig.2 (a) 見ればわかるがk-means では分離が困難 fig.2(b) ４－５本目の固有ベクトルを基底ベクトルにもつ空間ではクラスタのオーバーラップが少ない つまり１－５本目の固有ベクトル全て含めてクラスタリングすれば結果が良好になるのではないか？（仮説） fig.2 © ４回べき乗法を行った。結果が似てる。（縦軸が何を表してるかは不明）。四回が上から四本求めたのか、一番上の固有ベクトルを４回求めたのか分からない fig. (d) 提案手法を適用。k-means で分離可能 行列Vをp回べき乗法を行って構築 E=MVの最小化を行う ICA を最小化するために Jacobi Rotation を用いる。探索には貪欲法を採用  Contribution  マルチスケール（多種多様な）データ分布に対してクラスタリングが可能 計算時間は従来（ncut)と同等  感想  PIC(Power Itetaion Clusterg)をまさか拡張してくるとは思わなかったなので、興味深い論文。(実装して再現実験したけど、Early stopping の段階で高確率でクラスタリングが失敗していて使い物にならなかった印象があるので…) データの分布が多様な場合、上位数本のベクトルではなく、そこから更に下の固有ベクトルに着目したほうが結果が良好になるのは面白い KDD の論文、相変わらず読みやすかった。  |"><meta name=twitter:image:src content><meta name=twitter:domain content="https://shunyaueta.com/posts/2018-01-13_fuse-full-spectral-clusteringkdd2016-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/"><title>FUSE: Full Spectral Clustering(KDD2016) を読んだ</title><link rel=canonical href=https://shunyaueta.com/posts/2018-01-13_fuse-full-spectral-clusteringkdd2016-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/><link rel=stylesheet href=https://unpkg.com/tachyons@4.11.1/css/tachyons.min.css><link rel=stylesheet href=https://shunyaueta.com/css/style.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/highlightjs@9.12.0/styles/github-gist.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-39994406-11','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon href=/apple-touch-icon.png></head><body lang=ja class="sans-serif w-90 w-60-ns center center-ns mv2 mv5-ns" itemscope itemtype=http://schema.org/Article><span class=b>/</span>
<a href=https://shunyaueta.com/ class="b bb bw1 pb1 no-underline black"></a><section id=main class=mt5><h1 itemprop=name id=title>FUSE: Full Spectral Clustering(KDD2016) を読んだ</h1><article itemprop=articleBody id=content class="w-90 lh-copy"><p>べき乗法と独立成分分析を用いたマルチスケールに頑強なクラスタリング手法の提案</p><p><img src=/posts/2018-01-13_fuse-full-spectral-clusteringkdd2016-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/1.png alt=image></p><ul><li>べき乗法では近似固有ベクトル(Pseudo-eigenvectors)は複数の真の固有ベクトルの線形結合からなる。有益でない固有ベクトルも含まれるのでいかにそれを除去するか</li><li>Fig.1 で言ってることがいまいちわからない。ｃ,ｄも変化がないように見える。論文で言及してる stripe が何を指してるかが不明
分かった。赤・青・黒の３つのクラスタで分かれている。最初は黒色がクラスタ境界面に見えた。論文内での multi scale というのは、データの幾何的な分布を指している？</li><li>固有値計算は O(n³）かかるから計算量が~~って毎回言われるが、行列の状態に依存するので毎回最悪の場合を主張されてるも困る</li><li>ZP self-turining spectral clustering をなぜ対抗馬にして比較してるのかは謎。</li><li>ICA<a href=http://www.kecl.ntt.co.jp/icl/signal/sawada/mypaper/subspace2010rev.pdf>¹</a>を行ったあとにその行列に対して回転処理を行い情報量の最小化を狙う</li><li>クラスタ数が多い場合、PIC では良い結果が出づらい
multi scale なデータの場合標準の spectral clustering では失敗することが多々ある</li><li>fig.2 (a) 見ればわかるが<em>k</em>-means では分離が困難</li><li>fig.2(b) ４－５本目の固有ベクトルを基底ベクトルにもつ空間ではクラスタのオーバーラップが少ない</li><li>つまり１－５本目の固有ベクトル全て含めてクラスタリングすれば結果が良好になるのではないか？（仮説）</li><li>fig.2 © ４回べき乗法を行った。結果が似てる。（縦軸が何を表してるかは不明）。四回が上から四本求めたのか、一番上の固有ベクトルを４回求めたのか分からない</li><li>fig. (d) 提案手法を適用。k-means で分離可能 行列<strong>V</strong>を<em>p</em>回べき乗法を行って構築 <strong>E=MV</strong>の最小化を行う</li><li>ICA を最小化するために Jacobi Rotation を用いる。探索には貪欲法を採用</li></ul><h3 id=contribution>Contribution</h3><ul><li>マルチスケール（多種多様な）データ分布に対してクラスタリングが可能</li><li>計算時間は従来（ncut)と同等</li></ul><h3 id=感想>感想</h3><ul><li>PIC(Power Itetaion Clusterg)をまさか拡張してくるとは思わなかったなので、興味深い論文。(実装して再現実験したけど、Early stopping の段階で高確率でクラスタリングが失敗していて使い物にならなかった印象があるので…)</li><li>データの分布が多様な場合、上位数本のベクトルではなく、そこから更に下の固有ベクトルに着目したほうが結果が良好になるのは面白い</li><li>KDD の論文、相変わらず読みやすかった。</li></ul></article></section><footer><div><p class="f6 gray mt6 lh-copy">© 2016-20 <a href=https://twitter.com/hurutoriya>@hurutoriya</a>.</p></div></footer><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.14.2/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>