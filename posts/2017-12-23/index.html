<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="自己符号化器と Spectral Clusteing の関連性を示した論文
 Paper link  Author  Fei Tian : http://home.ustc.edu.cn/~tianfei/ 中国科学技術大学 Ph.D １年生 MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。 この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%) MSRA @ ILSVRC2015 資料  Motivation (研究背景・動機)  Deep Learning が数多くの応用でめざましい成果をあげている。 音声認識 画像認識 自然言語処理 DL において Clustering に関する適切な調査が行われてない。 論文の目的として、DL における Clustering の調査を行う  概要  Graph Clustering はクラスタリングでも重要な手法の一つ Graph Clustering の応用 Image segmentation Community Detection VLSI Design 嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能 自己符号化器と Spectral Clustering の類似性 Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。 自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する 計算量 : 対象とするグラフは n 個のノードを持つ EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量 自己符号化器 : ノードがスパースな際は計算量は O(kn) スパース表現 : データが大きくなるならスパース性を有効活用したい EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない 自己符号化器 : スパース性を用いるのは容易  既存研究で未検証な事柄、何を解決・解明したいのか？  Graph Clustering のための Deep Learning の活用方法と調査 自己符号化器と Spectral Clustering の類似性とその比較・検証  Method (提案手法)  GraphEncoder(for graph clustering."><meta name=theme-color content="#d3381c"><meta property="og:title" content="“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ • Software Engineer as Data Scientist"><meta property="og:description" content="自己符号化器と Spectral Clusteing の関連性を示した論文
 Paper link  Author  Fei Tian : http://home.ustc.edu.cn/~tianfei/ 中国科学技術大学 Ph.D １年生 MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。 この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%) MSRA @ ILSVRC2015 資料  Motivation (研究背景・動機)  Deep Learning が数多くの応用でめざましい成果をあげている。 音声認識 画像認識 自然言語処理 DL において Clustering に関する適切な調査が行われてない。 論文の目的として、DL における Clustering の調査を行う  概要  Graph Clustering はクラスタリングでも重要な手法の一つ Graph Clustering の応用 Image segmentation Community Detection VLSI Design 嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能 自己符号化器と Spectral Clustering の類似性 Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。 自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する 計算量 : 対象とするグラフは n 個のノードを持つ EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量 自己符号化器 : ノードがスパースな際は計算量は O(kn) スパース表現 : データが大きくなるならスパース性を有効活用したい EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない 自己符号化器 : スパース性を用いるのは容易  既存研究で未検証な事柄、何を解決・解明したいのか？  Graph Clustering のための Deep Learning の活用方法と調査 自己符号化器と Spectral Clustering の類似性とその比較・検証  Method (提案手法)  GraphEncoder(for graph clustering."><meta property="og:url" content="https://shunyaueta.com/posts/2017-12-23/"><meta property="og:site_name" content="Software Engineer as Data Scientist"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Paper"><meta property="article:published_time" content="2017-12-23T17:38:19Z"><meta property="article:modified_time" content="2019-06-16T18:16:19+09:00"><meta name=twitter:card content="summary"><meta name=twitter:site content="@hurutoriya"><meta name=generator content="Hugo 0.67.1"><title>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ • Software Engineer as Data Scientist</title><link rel=canonical href=https://shunyaueta.com/posts/2017-12-23/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/assets/css/main.ab98e12b.css><link rel=stylesheet href=/css/custom.css><style>:root{--color-accent:#d3381c}</style><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-39994406-11','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class="page type-posts"><div class=site><a class=screen-reader-text href=#content>Skip to Content</a><div class=main><div class=header-widgets><div class=container><style>.widget-breadcrumbs li:after{content:'\2f '}</style><section class="widget widget-breadcrumbs sep-after"><nav id=breadcrumbs><ol><li><a href=/>Software Engineer as Data Scientist</a></li><li><a href=/posts/>Posts</a></li><li><span>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ</span></li></ol></nav></section></div></div><header id=header class="header site-header"><div class="container sep-after"><div class=header-info><p class="site-title title">Software Engineer as Data Scientist</p><p class="desc site-desc">Enjoy Software Engineering & Data Science!!</p></div></div></header><main id=content><article lang=ja class=entry><header class="header entry-header"><div class="container sep-after"><div class=header-info><h1 class=title>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ</h1></div><div class=entry-meta><span class=posted-on><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg><span class=screen-reader-text>Posted on</span>
<time class=entry-date datetime=2017-12-23T17:38:19Z>2017.12.23</time></span>
<span class=reading-time><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 15 15"/></svg>2 mins read</span></div></div></header><div class="container entry-content"><p>自己符号化器と Spectral Clusteing の関連性を示した論文</p><ul><li><a href=http://research.microsoft.com/pubs/226627/%5BAAAI2014%5D%20DNN%20for%20Graph%20Cut.pdf>Paper link</a></li></ul><h4 id=author>Author</h4><ul><li>Fei Tian : <a href=http://home.ustc.edu.cn/~tianfei/>http://home.ustc.edu.cn/~tianfei/</a></li><li>中国科学技術大学 Ph.D １年生</li><li>MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。</li><li>この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%)</li><li><a href=http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf>MSRA @ ILSVRC2015 資料</a></li></ul><h4 id=motivation-研究背景動機>Motivation (研究背景・動機)</h4><ul><li>Deep Learning が数多くの応用でめざましい成果をあげている。</li><li>音声認識</li><li>画像認識</li><li>自然言語処理</li><li>DL において Clustering に関する適切な調査が行われてない。</li><li>論文の目的として、DL における Clustering の調査を行う</li></ul><h4 id=概要>概要</h4><ul><li>Graph Clustering はクラスタリングでも重要な手法の一つ</li><li>Graph Clustering の応用</li><li>Image segmentation</li><li>Community Detection</li><li>VLSI Design</li><li>嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能</li><li>自己符号化器と Spectral Clustering の類似性</li><li>Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。</li><li>自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する</li><li>計算量 : 対象とするグラフは n 個のノードを持つ</li><li>EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量</li><li>自己符号化器 : ノードがスパースな際は計算量は O(kn)</li><li>スパース表現 : データが大きくなるならスパース性を有効活用したい</li><li>EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない</li><li>自己符号化器 : スパース性を用いるのは容易</li></ul><h3 id=既存研究で未検証な事柄何を解決解明したいのか>既存研究で未検証な事柄、何を解決・解明したいのか？</h3><ul><li>Graph Clustering のための Deep Learning の活用方法と調査</li><li>自己符号化器と Spectral Clustering の類似性とその比較・検証</li></ul><h3 id=method-提案手法>Method (提案手法)</h3><ul><li>GraphEncoder(for graph clustering.)</li><li>グラフラプラシアン(L=D−1S)に対してスパース自己符号化器(Sparse-AutoEncoders)を通した最終層に k-meas クラスタリングを行う</li><li>AutoEncoder は入力層・隠れ層・出力層の 3 層を Stacked する。</li><li>X(j)=D−1S の列ベクトルを各ユニットに入力し、隠れ層の活性化関数 h(j)を得て、X(j+1)=h(j)と Γ 回(層の階数分)更新する。</li></ul><p><img src=/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/1.png alt=image></p><h3 id=evaluation-検証方法評価方法優位性>Evaluation (検証方法・評価方法・優位性)</h3><ul><li>実世界のグラフデータに対して NMI によってクラスタリングの評価実験を行う。</li><li>データの種類</li><li>ワイン</li><li>ニュース記事</li><li>タンパク質構造グラフ</li></ul><p>以下の三種で比較</p><ul><li>Spectral Clustering</li><li>k-means</li><li>Sparse-AutoEncoders(Graph-Encoder)</li></ul><p><img src=/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/2.png alt=image></p><p><img src=/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/3.png alt=image></p><p>次元の減少推移</p><p><img src=/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/4.png alt=image></p><p>層を重ねる毎に NMI が向上している。</p><p><img src=/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/5.png alt=image></p><h3 id=conclusion-結論貢献>Conclusion (結論・貢献)</h3><ul><li>Deep Learn と Graph Clustering の関係性、結果を調査。</li><li>GraphEncoder の嬉しいところ</li><li>隠れ層の次元は入力層の次元より低い。これは全てのエッジが必須ではないことを直感的に示す。</li><li>エッジの除去を行いグラフ表現を更に明確にするために、浅い層から深い層へ。</li><li>EVD の計算量は最速でも O(n2.367)で、グラフは密なグラフ表現。(Toeplitz Matirix)</li><li>GraphEncoder は O(ncd)、d は隠れ層の最大次元、c はグラフの平均次元。(例: 各ノードが k 本のエッジを持つ類似度グラフの場合 c=k。ソーシャルグラフで表すと、c は友達の平均の数を示す。)</li><li>EVD は並列化が困難。確率的勾配降下法(SGD)は EVD と比べると並列化が容易である。</li></ul><h3 id=提案手法の限界残された課題>提案手法の限界(残された課題)</h3><p>実行時間の比較が行われていないが、あくまでこの論文の価値は DL と Graph Clusetering の関連性を示しているのが価値なのでそこは許して下さいって感じ。### Comments (疑問点・わからなかったところ・議論)</p><ul><li>トップカンファレンスを年 2 本、2nd tier を 1 本 1st で出せるのは、どうやればそのレベルに到達するんだ?</li><li>トレンドに乗った良い論文。</li><li>Good Writing. 内容もシンプルなので 90 分でサクッと気持よく読めた。論文読むより、スパース自己符号化の勉強に時間取られた。</li><li>Corollary2 で ~ symmetrix matrix D−1S って言ってる割に行列の対称性は保証されてないので 3.1 全般が怪しい、辻褄があってない。</li></ul><h3>See Also</h3><ul><li><a href=/posts/2017-12-04/>Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrier を読んだ</a></li><li><a href=/posts/2017-12-01/>Machine Learning that Matters (ICML2012) を読んだ</a></li><li><a href=/posts/2017-12-06/>CoreMLがTensorFlow Liteをサポート</a></li><li><a href=/posts/2017-11-14/>OpenCV 3.3から使えるDNNモジュールを使って物体検出</a></li></ul></div><footer class=entry-footer><div class="container sep-before"><div class=last-updated><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M20 14.66V20a2 2 0 01-2 2H4a2 2 0 01-2-2V6a2 2 0 012-2h5.34" /><polygon points="18 2 22 6 12 16 8 16 8 12 18 2" /></svg><span class=screen-reader-text>Last updated:</span>
<time class=entry-date datetime=2019-06-16T18:16:19+09:00>2019.06.16</time></div><div class=tags><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2H12l8.59 8.59A2 2 0 0120.59 13.41z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=screen-reader-text>Tags: </span><a class=tag href=/tags/deep-learning/>Deep Learning</a>, <a class=tag href=/tags/machine-learning/>Machine Learning</a>, <a class=tag href=/tags/paper/>Paper</a></div></div></footer></article><nav class=entry-nav><div class=container><div class="prev-entry sep-before"><a href=/posts/2017-12-22/><span aria-hidden=true><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><line x1="20" y1="12" x2="4" y2="12"/><polyline points="10 18 4 12 10 6"/></svg>Previous</span>
<span class=screen-reader-text>Previous post: </span>JupyterNotebookをリモートサーバー上で公開して、どこでも研究開発 & 講義でJupyterhubを利用する</a></div><div class="next-entry sep-before"><a href=/posts/2017-12-27/><span class=screen-reader-text>Next post: </span>HerokuのDBにpgadmin4で接続してローカルにデータをダウンロードする<span aria-hidden=true>Next<svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><line x1="4" y1="12" x2="20" y2="12"/><polyline points="14 6 20 12 14 18"/></svg></span></a></div></div></nav><section id=comments class=comments><div class="container sep-before"><div class=comments-area><script src=https://utteranc.es/client.js repo=hurutoriya/hurutoriya.github.io issue-term=url crossorigin=anonymous async></script></div></div></section></main><footer id=footer class=footer><div class="container sep-before"><section class="widget widget-about sep-after"><header><h2 class="title site-title"><a href=/>Shunya UETA</a></h2><div class=desc>Shunya UETA who is a working on Machine Leraning as Software Engineer at <a href=https://about.mercari.com/en/>Mercari, inc</a>. Opinions are my own. → <a href=/about>See more detail!</a></div></header></section><section class="widget widget-social_menu sep-after"><nav aria-label="Social Menu"><ul><li><a href=https://github.com/hurutoriya target=_blank rel=noopener><span class=screen-reader-text>Open Github account in new tab</span><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77a5.44 5.44.0 00-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li><a href=https://facebook.com/shunyaueta target=_blank rel=noopener><span class=screen-reader-text>Open Facebook account in new tab</span><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M18 2h-3a5 5 0 00-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 011-1h3z"/></svg></a></li><li><a href=https://twitter.com/hurutoriya target=_blank rel=noopener><span class=screen-reader-text>Open Twitter account in new tab</span><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></a></li><li><a href=https://linkedin.com/in/hurutoriya target=_blank rel=noopener><span class=screen-reader-text>Open Linkedin account in new tab</span><svg class="icon" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></li></ul></nav></section><div class=copyright><p>&copy; 2013-2020 Shunya UETA</p></div></div></footer></div></div><script>window.__assets_js_src="/assets/js/"</script><script src=/assets/js/main.c3bcf2df.js></script><script src=/js/custom.js></script></body></html>