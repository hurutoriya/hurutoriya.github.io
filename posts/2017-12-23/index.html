<!doctype html><html lang=ja dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ | Shunya Ueta</title><meta name=keywords content="machinelearning,paper"><meta name=description content="自己符号化器と Spectral Clusteing の関連性を示した論文
 Paper link  Author  Fei Tian : http://home.ustc.edu.cn/~tianfei/ 中国科学技術大学 Ph.D １年生 MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。 この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%) MSRA @ ILSVRC2015 資料  Motivation (研究背景・動機)  Deep Learning が数多くの応用でめざましい成果をあげている。 音声認識 画像認識 自然言語処理 DL において Clustering に関する適切な調査が行われてない。 論文の目的として、DL における Clustering の調査を行う  概要  Graph Clustering はクラスタリングでも重要な手法の一つ Graph Clustering の応用 Image segmentation Community Detection VLSI Design 嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能 自己符号化器と Spectral Clustering の類似性 Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。 自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する 計算量 : 対象とするグラフは n 個のノードを持つ EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量 自己符号化器 : ノードがスパースな際は計算量は O(kn) スパース表現 : データが大きくなるならスパース性を有効活用したい EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない 自己符号化器 : スパース性を用いるのは容易  既存研究で未検証な事柄、何を解決・解明したいのか？  Graph Clustering のための Deep Learning の活用方法と調査 自己符号化器と Spectral Clustering の類似性とその比較・検証  Method (提案手法)  GraphEncoder(for graph clustering."><meta name=author content="Shunya Ueta"><link rel=canonical href=https://shunyaueta.com/posts/2017-12-23/><link crossorigin=anonymous href=/assets/css/stylesheet.min.e21185e6c4b43ff34c81666f70aa4f80140274057866888c0a5c28addc9b7fd2.css integrity="sha256-4hGF5sS0P/NMgWZvcKpPgBQCdAV4ZoiMClwordybf9I=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://shunyaueta.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shunyaueta.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shunyaueta.com/favicon-32x32.png><link rel=apple-touch-icon href=https://shunyaueta.com/apple-touch-icon.png><link rel=mask-icon href=https://shunyaueta.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.67.1"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-39994406-11','auto');ga('send','pageview');}</script><meta property="og:title" content="“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ"><meta property="og:description" content="自己符号化器と Spectral Clusteing の関連性を示した論文
 Paper link  Author  Fei Tian : http://home.ustc.edu.cn/~tianfei/ 中国科学技術大学 Ph.D １年生 MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。 この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%) MSRA @ ILSVRC2015 資料  Motivation (研究背景・動機)  Deep Learning が数多くの応用でめざましい成果をあげている。 音声認識 画像認識 自然言語処理 DL において Clustering に関する適切な調査が行われてない。 論文の目的として、DL における Clustering の調査を行う  概要  Graph Clustering はクラスタリングでも重要な手法の一つ Graph Clustering の応用 Image segmentation Community Detection VLSI Design 嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能 自己符号化器と Spectral Clustering の類似性 Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。 自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する 計算量 : 対象とするグラフは n 個のノードを持つ EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量 自己符号化器 : ノードがスパースな際は計算量は O(kn) スパース表現 : データが大きくなるならスパース性を有効活用したい EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない 自己符号化器 : スパース性を用いるのは容易  既存研究で未検証な事柄、何を解決・解明したいのか？  Graph Clustering のための Deep Learning の活用方法と調査 自己符号化器と Spectral Clustering の類似性とその比較・検証  Method (提案手法)  GraphEncoder(for graph clustering."><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2017-12-23/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-12-23T17:38:19+00:00"><meta property="article:modified_time" content="2019-06-16T18:16:19+09:00"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ"><meta name=twitter:description content="自己符号化器と Spectral Clusteing の関連性を示した論文
 Paper link  Author  Fei Tian : http://home.ustc.edu.cn/~tianfei/ 中国科学技術大学 Ph.D １年生 MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。 この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%) MSRA @ ILSVRC2015 資料  Motivation (研究背景・動機)  Deep Learning が数多くの応用でめざましい成果をあげている。 音声認識 画像認識 自然言語処理 DL において Clustering に関する適切な調査が行われてない。 論文の目的として、DL における Clustering の調査を行う  概要  Graph Clustering はクラスタリングでも重要な手法の一つ Graph Clustering の応用 Image segmentation Community Detection VLSI Design 嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能 自己符号化器と Spectral Clustering の類似性 Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。 自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する 計算量 : 対象とするグラフは n 個のノードを持つ EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量 自己符号化器 : ノードがスパースな際は計算量は O(kn) スパース表現 : データが大きくなるならスパース性を有効活用したい EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない 自己符号化器 : スパース性を用いるのは容易  既存研究で未検証な事柄、何を解決・解明したいのか？  Graph Clustering のための Deep Learning の活用方法と調査 自己符号化器と Spectral Clustering の類似性とその比較・検証  Method (提案手法)  GraphEncoder(for graph clustering."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ","item":"https://shunyaueta.com/posts/2017-12-23/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ","name":"“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ","description":"自己符号化器と Spectral Clusteing の関連性を示した論文\n Paper link  Author  Fei Tian : http://home.ustc.edu.cn/~tianfei/ 中国科学技術大学 Ph.D １年生 MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。 この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%) MSRA @ ILSVRC2015 資料  Motivation (研究背景・動機)  Deep Learning が数多くの応用でめざましい成果をあげている。 音声認識 画像認識 自然言語処理 DL において Clustering に関する適切な調査が行われてない。 論文の目的として、DL における Clustering の調査を行う  概要  Graph Clustering はクラスタリングでも重要な手法の一つ Graph Clustering の応用 Image segmentation Community Detection VLSI Design 嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能 自己符号化器と Spectral Clustering の類似性 Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。 自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する 計算量 : 対象とするグラフは n 個のノードを持つ EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量 自己符号化器 : ノードがスパースな際は計算量は O(kn) スパース表現 : データが大きくなるならスパース性を有効活用したい EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない 自己符号化器 : スパース性を用いるのは容易  既存研究で未検証な事柄、何を解決・解明したいのか？  Graph Clustering のための Deep Learning の活用方法と調査 自己符号化器と Spectral Clustering の類似性とその比較・検証  Method (提案手法)  GraphEncoder(for graph clustering.","keywords":["machinelearning","paper"],"articleBody":"自己符号化器と Spectral Clusteing の関連性を示した論文\n Paper link  Author  Fei Tian : http://home.ustc.edu.cn/~tianfei/ 中国科学技術大学 Ph.D １年生 MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。 この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%) MSRA @ ILSVRC2015 資料  Motivation (研究背景・動機)  Deep Learning が数多くの応用でめざましい成果をあげている。 音声認識 画像認識 自然言語処理 DL において Clustering に関する適切な調査が行われてない。 論文の目的として、DL における Clustering の調査を行う  概要  Graph Clustering はクラスタリングでも重要な手法の一つ Graph Clustering の応用 Image segmentation Community Detection VLSI Design 嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能 自己符号化器と Spectral Clustering の類似性 Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。 自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する 計算量 : 対象とするグラフは n 個のノードを持つ EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量 自己符号化器 : ノードがスパースな際は計算量は O(kn) スパース表現 : データが大きくなるならスパース性を有効活用したい EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない 自己符号化器 : スパース性を用いるのは容易  既存研究で未検証な事柄、何を解決・解明したいのか？  Graph Clustering のための Deep Learning の活用方法と調査 自己符号化器と Spectral Clustering の類似性とその比較・検証  Method (提案手法)  GraphEncoder(for graph clustering.) グラフラプラシアン(L=D−1S)に対してスパース自己符号化器(Sparse-AutoEncoders)を通した最終層に k-meas クラスタリングを行う AutoEncoder は入力層・隠れ層・出力層の 3 層を Stacked する。 X(j)=D−1S の列ベクトルを各ユニットに入力し、隠れ層の活性化関数 h(j)を得て、X(j+1)=h(j)と Γ 回(層の階数分)更新する。  Evaluation (検証方法・評価方法・優位性)  実世界のグラフデータに対して NMI によってクラスタリングの評価実験を行う。 データの種類 ワイン ニュース記事 タンパク質構造グラフ  以下の三種で比較\n Spectral Clustering k-means Sparse-AutoEncoders(Graph-Encoder)  次元の減少推移\n層を重ねる毎に NMI が向上している。\nConclusion (結論・貢献)  Deep Learn と Graph Clustering の関係性、結果を調査。 GraphEncoder の嬉しいところ 隠れ層の次元は入力層の次元より低い。これは全てのエッジが必須ではないことを直感的に示す。 エッジの除去を行いグラフ表現を更に明確にするために、浅い層から深い層へ。 EVD の計算量は最速でも O(n2.367)で、グラフは密なグラフ表現。(Toeplitz Matirix) GraphEncoder は O(ncd)、d は隠れ層の最大次元、c はグラフの平均次元。(例: 各ノードが k 本のエッジを持つ類似度グラフの場合 c=k。ソーシャルグラフで表すと、c は友達の平均の数を示す。) EVD は並列化が困難。確率的勾配降下法(SGD)は EVD と比べると並列化が容易である。  提案手法の限界(残された課題) 実行時間の比較が行われていないが、あくまでこの論文の価値は DL と Graph Clusetering の関連性を示しているのが価値なのでそこは許して下さいって感じ。### Comments (疑問点・わからなかったところ・議論)\n トップカンファレンスを年 2 本、2nd tier を 1 本 1st で出せるのは、どうやればそのレベルに到達するんだ? トレンドに乗った良い論文。 Good Writing. 内容もシンプルなので 90 分でサクッと気持よく読めた。論文読むより、スパース自己符号化の勉強に時間取られた。 Corollary2 で ~ symmetrix matrix D−1S って言ってる割に行列の対称性は保証されてないので 3.1 全般が怪しい、辻褄があってない。  ","wordCount":"222","inLanguage":"ja","datePublished":"2017-12-23T17:38:19.179Z","dateModified":"2019-06-16T18:16:19+09:00","author":{"@type":"Person","name":"Shunya Ueta"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://shunyaueta.com/posts/2017-12-23/"},"publisher":{"@type":"Organization","name":"Shunya Ueta","logo":{"@type":"ImageObject","url":"https://shunyaueta.com/favicon.ico"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href=https://shunyaueta.com/ accesskey=h title="Shunya Ueta (Alt + H)">Shunya Ueta</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://shunyaueta.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://shunyaueta.com/about title=About><span>About</span></a></li><li><a href=https://www.getrevue.co/profile/hurutoriya title=Newsletter><span>Newsletter</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ</h1><div class=post-meta><span title="2017-12-23 17:38:19.179 +0000 UTC">December 23, 2017</span>&nbsp;·&nbsp;Shunya Ueta</div></header><div class=post-content><p>自己符号化器と Spectral Clusteing の関連性を示した論文</p><ul><li><a href=http://research.microsoft.com/pubs/226627/%5BAAAI2014%5D%20DNN%20for%20Graph%20Cut.pdf>Paper link</a></li></ul><h4 id=author>Author<a hidden class=anchor aria-hidden=true href=#author>#</a></h4><ul><li>Fei Tian : <a href=http://home.ustc.edu.cn/~tianfei/>http://home.ustc.edu.cn/~tianfei/</a></li><li>中国科学技術大学 Ph.D １年生</li><li>MSRA と共同研究、2014 年に AAAI2 本,COLING1 本を 1st で通してる。</li><li>この資料も面白い。ILSVRC2015 で 152 層の Deep Residual Learning を提案し優勝(Error Rate : 3.57%)</li><li><a href=http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf>MSRA @ ILSVRC2015 資料</a></li></ul><h4 id=motivation-研究背景動機>Motivation (研究背景・動機)<a hidden class=anchor aria-hidden=true href=#motivation-研究背景動機>#</a></h4><ul><li>Deep Learning が数多くの応用でめざましい成果をあげている。</li><li>音声認識</li><li>画像認識</li><li>自然言語処理</li><li>DL において Clustering に関する適切な調査が行われてない。</li><li>論文の目的として、DL における Clustering の調査を行う</li></ul><h4 id=概要>概要<a hidden class=anchor aria-hidden=true href=#概要>#</a></h4><ul><li>Graph Clustering はクラスタリングでも重要な手法の一つ</li><li>Graph Clustering の応用</li><li>Image segmentation</li><li>Community Detection</li><li>VLSI Design</li><li>嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能</li><li>自己符号化器と Spectral Clustering の類似性</li><li>Spectral Clustering : グラフラプラシアンに対して EVD を行い k 本の非零固有ベクトルを用いた空間に対して k-means を行ったもの。</li><li>自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する</li><li>計算量 : 対象とするグラフは n 個のノードを持つ</li><li>EVD : ナイーブに実装すると O(n3)の計算量、最速の実装は O(n2)の計算量</li><li>自己符号化器 : ノードがスパースな際は計算量は O(kn)</li><li>スパース表現 : データが大きくなるならスパース性を有効活用したい</li><li>EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない</li><li>自己符号化器 : スパース性を用いるのは容易</li></ul><h3 id=既存研究で未検証な事柄何を解決解明したいのか>既存研究で未検証な事柄、何を解決・解明したいのか？<a hidden class=anchor aria-hidden=true href=#既存研究で未検証な事柄何を解決解明したいのか>#</a></h3><ul><li>Graph Clustering のための Deep Learning の活用方法と調査</li><li>自己符号化器と Spectral Clustering の類似性とその比較・検証</li></ul><h3 id=method-提案手法>Method (提案手法)<a hidden class=anchor aria-hidden=true href=#method-提案手法>#</a></h3><ul><li>GraphEncoder(for graph clustering.)</li><li>グラフラプラシアン(L=D−1S)に対してスパース自己符号化器(Sparse-AutoEncoders)を通した最終層に k-meas クラスタリングを行う</li><li>AutoEncoder は入力層・隠れ層・出力層の 3 層を Stacked する。</li><li>X(j)=D−1S の列ベクトルを各ユニットに入力し、隠れ層の活性化関数 h(j)を得て、X(j+1)=h(j)と Γ 回(層の階数分)更新する。</li></ul><p><img loading=lazy src=/posts/2017-12-23/images/1.png alt=image></p><h3 id=evaluation-検証方法評価方法優位性>Evaluation (検証方法・評価方法・優位性)<a hidden class=anchor aria-hidden=true href=#evaluation-検証方法評価方法優位性>#</a></h3><ul><li>実世界のグラフデータに対して NMI によってクラスタリングの評価実験を行う。</li><li>データの種類</li><li>ワイン</li><li>ニュース記事</li><li>タンパク質構造グラフ</li></ul><p>以下の三種で比較</p><ul><li>Spectral Clustering</li><li>k-means</li><li>Sparse-AutoEncoders(Graph-Encoder)</li></ul><p><img loading=lazy src=/posts/2017-12-23/images/2.png alt=image></p><p><img loading=lazy src=/posts/2017-12-23/images/3.png alt=image></p><p>次元の減少推移</p><p><img loading=lazy src=/posts/2017-12-23/images/4.png alt=image></p><p>層を重ねる毎に NMI が向上している。</p><p><img loading=lazy src=/posts/2017-12-23/images/5.png alt=image></p><h3 id=conclusion-結論貢献>Conclusion (結論・貢献)<a hidden class=anchor aria-hidden=true href=#conclusion-結論貢献>#</a></h3><ul><li>Deep Learn と Graph Clustering の関係性、結果を調査。</li><li>GraphEncoder の嬉しいところ</li><li>隠れ層の次元は入力層の次元より低い。これは全てのエッジが必須ではないことを直感的に示す。</li><li>エッジの除去を行いグラフ表現を更に明確にするために、浅い層から深い層へ。</li><li>EVD の計算量は最速でも O(n2.367)で、グラフは密なグラフ表現。(Toeplitz Matirix)</li><li>GraphEncoder は O(ncd)、d は隠れ層の最大次元、c はグラフの平均次元。(例: 各ノードが k 本のエッジを持つ類似度グラフの場合 c=k。ソーシャルグラフで表すと、c は友達の平均の数を示す。)</li><li>EVD は並列化が困難。確率的勾配降下法(SGD)は EVD と比べると並列化が容易である。</li></ul><h3 id=提案手法の限界残された課題>提案手法の限界(残された課題)<a hidden class=anchor aria-hidden=true href=#提案手法の限界残された課題>#</a></h3><p>実行時間の比較が行われていないが、あくまでこの論文の価値は DL と Graph Clusetering の関連性を示しているのが価値なのでそこは許して下さいって感じ。### Comments (疑問点・わからなかったところ・議論)</p><ul><li>トップカンファレンスを年 2 本、2nd tier を 1 本 1st で出せるのは、どうやればそのレベルに到達するんだ?</li><li>トレンドに乗った良い論文。</li><li>Good Writing. 内容もシンプルなので 90 分でサクッと気持よく読めた。論文読むより、スパース自己符号化の勉強に時間取られた。</li><li>Corollary2 で ~ symmetrix matrix D−1S って言ってる割に行列の対称性は保証されてないので 3.1 全般が怪しい、辻褄があってない。</li></ul><h2>See Also</h2><ul><li><a href=/posts/2017-12-04/>Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrier を読んだ</a></li><li><a href=/posts/2017-12-01/>Machine Learning that Matters (ICML2012) を読んだ</a></li><li><a href=/posts/2017-12-06/>CoreMLがTensorFlow Liteをサポート</a></li><li><a href=/posts/2017-11-14/>OpenCV 3.3から使えるDNNモジュールを使って物体検出</a></li></ul><h2>Support</h2><a href=https://www.buymeacoffee.com/hurutoriya target=_blank>☕️ Buy me a cofee: お読みくださりありがとうございます。
こちらから ☕ を一杯支援していただけると、ブログ執筆のモチベーションに繋がります ✨</div><footer class=post-footer><ul class=post-tags><li><a href=https://shunyaueta.com/tags/machinelearning/>machinelearning</a></li><li><a href=https://shunyaueta.com/tags/paper/>paper</a></li></ul><nav class=paginav><a class=prev href=https://shunyaueta.com/posts/2017-12-27/><span class=title>« 前のページ</span><br><span>HerokuのDBにpgadmin4で接続してローカルにデータをダウンロードする</span></a>
<a class=next href=https://shunyaueta.com/posts/2017-12-22/><span class=title>次のページ »</span><br><span>JupyterNotebookをリモートサーバー上で公開して、どこでも研究開発 & 講義でJupyterhubを利用する</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=hurutoriya/hurutoriya.github.io data-repo-id="MDEwOlJlcG9zaXRvcnk2MDgyNTY1Nw==" data-category=Comments data-category-id=DIC_kwDOA6AgOc4CAQTX data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=dark data-lang=ja crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://shunyaueta.com/>Shunya Ueta</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z" /></svg></a><script>let menu=document.getElementById('menu')
if(menu){menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>