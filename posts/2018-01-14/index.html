<!doctype html><html lang=ja><head><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8604812913439531" crossorigin=anonymous></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme-color content="dark"><title>Analyzing Free-standing Conversational Groups: A Multimodal Approach (ACMMM15) を読んだ | Shunya Ueta</title><link rel=stylesheet href=/sass/main.min.1a3290acca8b3fc92df85ea9200859476d6c80d59009c89a749300f3ce7a7a67.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-39994406-11"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-39994406-11');</script></head><body class=dark><nav class=navbar><div class=container><div class=flex><div><a class=brand href=/><span class=emoji>🎰</span>
Shunya Ueta</a></div><div class=flex><a href=/articles/>All posts</a>
<button id=dark-mode-button></button></div></div></div></nav><main><div class=container><article><header class=article-header><div class=thumb><div><h1>Analyzing Free-standing Conversational Groups: A Multimodal Approach (ACMMM15) を読んだ</h1><div class=post-meta><div>By on <time>January 14, 2018</time></div><div class=tags><a href=/tags/machine-learning/>Machine Learning</a>
<a href=/tags/paper/>Paper</a>
<a href=/tags/computer-vision/>Computer Vision</a>
<a href=/tags/acmmm/>ACMMM</a></div></div></div></div></header></article><div class=article-post><p>スタンディングディスカッション形式での会話を評価した研究</p><p><img src=/posts/2018-01-14/images/1.png alt=image></p><p>Summary Slide</p><p>X Alameda-Pineda, Y Yan, E Ricci, O Lanz, N Sebe<br>“Analyzing Free-standing Conversational Groups: A Multimodal Approach”, in 2015 ACM Multimedia Conference</p><p><a href=http://xavirema.eu/wp-content/papercite-data/pdf/Alameda-ACMMM-2015.pdf>link</a></p><p>を読んだので、軽くメモ。</p><p>マルチモーダル系の論文初めて読んだんですが、まとめるとコントリビューションが 5 つあると主張。</p><h3 id=contribution>Contribution</h3><ul><li>音声・近接情報、そして監視カメラからの身体と頭の姿勢推定からマルチモーダルに解析</li><li>フリースタンディングディスカッションを身体・頭の姿勢推定から解析</li><li>カメラと音声・近接センサーからなるマルチモーダルな解析するためのフレームワークを提案</li><li>ラベリングされてないデータに対する行列補間問題の考案</li><li>SALSA(データ・セット)を公開・評価</li></ul><p>SALSA というポスターセッションの動画と音声のデータも公開されている</p><blockquote><p><a href=http://tev.fbk.eu/salsa><em>SALSA: Synergetic sociAL Scene Analysis</em></a></p></blockquote><p>動画は Google Drive で公開されていて時代の波を感じる。</p><ul><li>データセットを公開</li><li>論文も読みやすい</li><li>新しい行列補完計画法(アルゴリズム)を提案</li><li>実問題に取り組む</li></ul><p>と盛り沢山な内容で面白かった。</p><p>スライド内のリンクは Google Slide で共有しているのでこちらを参照すると便利です。</p><p><a href="https://docs.google.com/presentation/d/1G6zfzV4jIm7qj4LkHkm3Gk_qHEml0SWZExkNuqw0ef8/embed?start=true&loop=true&delayms=1000&slide=id.g1502801dfc_2_6">X Alameda-Pineda, Y Yan, E Ricci, O Lanz, N Sebe "Analyzing Free-standing Conversational Groups: A…</a></p></div><h2>See Also</h2><ul><li><a href=/posts/2018-01-12/>Call center stress recognition with person-specific models を読んだ</a></li><li><a href=/posts/2018-01-11/>FUSE: Full Spectral Clustering(KDD2016) を読んだ</a></li><li><a href=/posts/2017-12-23/>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ</a></li><li><a href=/posts/2017-12-04/>Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrier を読んだ</a></li><li><a href=/posts/2017-12-01/>Machine Learning that Matters (ICML2012) を読んだ</a></li></ul></div><div class=container><nav class="flex container suggested"><a rel=prev href=/posts/2018-01-13/ title="Previous post (older)"><span>Previous</span>
PythonでGaussian Kernelのアニメーションを作成</a>
<a rel=next href=/posts/2018-01-15/ title="Next post (newer)"><span>Next</span>
Jupyter Notebookの差分を明瞭に確認する事ができるpackage : nbdime</a></nav></div></main></main><footer class="footer flex"><section class=container><nav class=footer-links><a href=/index.xml>RSS</a></nav></section><script async src=/js/features.min.a94f58a30ad2560de728e080d87f75c60cf806fd1b3d5f4815f1a1a02c0d1859.js></script></footer></body></html>