<!doctype html><html lang=ja dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | hurutoriya</title><meta name=keywords content><meta name=description content="Posts - hurutoriya"><meta name=author content="Shunya Ueta"><link rel=canonical href=https://shunyaueta.com/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><link rel=icon href=https://shunyaueta.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shunyaueta.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shunyaueta.com/favicon-32x32.png><link rel=apple-touch-icon href=https://shunyaueta.com/apple-touch-icon.png><link rel=mask-icon href=https://shunyaueta.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://shunyaueta.com/posts/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-JMJRQJT0Q3"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JMJRQJT0Q3")</script><meta property="og:title" content="Posts"><meta property="og:description" content="Shunya Ueta's blog"><meta property="og:type" content="website"><meta property="og:url" content="https://shunyaueta.com/posts/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="Posts"><meta name=twitter:description content="Shunya Ueta's blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://shunyaueta.com/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://shunyaueta.com/ accesskey=h title="hurutoriya (Alt + H)">hurutoriya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://shunyaueta.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://shunyaueta.com/about title=About><span>About</span></a></li><li><a href=https://searchengineeringnewsletter.substack.com/ title=Newsletter><span>Newsletter</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://shunyaueta.com/index.xml title=RSS><span>RSS</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2>2021年に買って愛用しているもの</h2></header><div class=entry-content><p>2021 年に購入して、今も愛用しているものを記しておく。 買ったものだと、その後愛用しているか定かではないかつ良い品なのかわからないので、愛用しているもの限定です。
CORONA(コロナ) 衣類乾燥除湿機 除湿量 18L (木造 20 畳 / 鉄筋 40 畳まで) 日本製 Amazon
買ったきっかけは、2020 年に築浅の物件に引っ越したが新築の傾向なのか湿気が溜まりやすく、油断して、パントリーの床においていた旅行用のボストンバッグがカビてしまっていたことがあった。水取りぞうさんなどで対応していたことがあったが、知らぬ間に満水になり気づくのが遅れることが多かった。
そのため思い切って一番除湿力が高そうなこの機種を買ったところ大満足。梅雨の時期には一晩起動しておくと朝には 4.5L のタンクが必ず満タンになり、部屋の中に漂う嫌なモワモワ感が完全に消え去って住心地が圧倒的に向上した。また副次的に良かったのは衣類乾燥機能がすごく良かった。雨で部屋干しをせざるを得ないときもこの機能を使えば全く臭わずに乾燥させることができて、一粒で二度美味しい家電だった。またフィルターも 10 年は交換しなくても良いのが良い。1 年経った今でも全く臭わない。タンク自体も特有の臭さが全くないので最高。
AfterShokz Aeropex 骨伝導イヤホン Amazon
リモートワークの開始に伴い、最初は 2019 年頃にオフィスでの騒音対策として購入したWH-1000XM3を使っていたのだが、長時間つけると耳が痛くなることが多かった。またイヤーインタイプのイヤホンも自分は耳の形が合わなくてフィット感がいまいちだったりモゾモゾして 1 時間のミーティングでも嫌なことが多かった。AirPods も検討したが、自分は iPhone ではなく Pixel を使っているし、気づかずに落として紛失しそうで怖かったのでなにか良いイヤホンがないか探していた。Shokz はその悩みを完全に解決してくれた。耳は防がないし、1-2h つけても全く痛くならないし、何より軽い。Shokz はいろんなシリーズがあるが、
ミドルレンジのものを買ってなんか違うなと思うのは嫌だったので思い切ってハイエンドモデルを買ったがとても満足している。もしかしたらOpenMove でも良いかもしれないが、試したことがないのでわからん。ブランド名も After・Shokz から Shokz に変わって、(OpenRun Pro)が出たのでちょっと気になっている。バッテリーが更に長持ちなのと小型化、低音が効くようになったらしい。
Shokz いいですよね！自分も使ってます。新しく出る OpenRun 系統は 29g と、Aeropex と比べると 3g 重いようです。小型化の方がより適切かと感じました。 by @tatsuokundayo
2022/01/08: @tatsuokundayo さんにご指摘いただいた点を修正
WH-1000XM3はノイズキャンセリング機能は感動するレベルだったが、オフィスに行くことがなくなった今、用済みなのでメルカリで売った。速攻で売れたので気持ちよかった。
Xiaomi Mi ハンディクリーナー ミニ Mi Vacuum Cleaner Mini コードレス ミニ掃除機 ハンディ掃除機 Amazon...</p></div><footer class=entry-footer><span title='2022-01-07 21:14:25 +0900 +0900'>1月 7, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 2021年に買って愛用しているもの" href=https://shunyaueta.com/posts/2022-01-07/></a></article><article class=post-entry><header class=entry-header><h2>Python で DeepL API Free を利用してテキストファイルを翻訳する</h2></header><div class=entry-content><p>機械翻訳サービスの DeepL はアプリだけでなく API 提供も行っている。 今回は DeepL が公開している free API を利用して、テキストファイルを英日翻訳して、翻訳結果をテキストファイルとして保存する方法について説明する。
無料 API は1か月あたり500,000文字の上限ありの制限があるが、Pro version と変わらない品質の翻訳を行うことができる。 個人利用する分にはこの文字数制限は特に大きな問題にはならないと思われる。
https://www.deepl.com/ja/pro#developer
まずアカウントを作成して、DeepL API Free のAPI_KEYを入手する。 その後、以下のスクリプトを実行すれば、翻訳元のファイル名にJA_という接頭辞がついたファイルが保存される。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import requests # NOTE: put API KEY API_KEY:str = '' # NOTE: put target file path target_file:str = "" with open(target_file) as f: txt = f....</p></div><footer class=entry-footer><span title='2022-01-05 23:02:06 +0900 +0900'>1月 5, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Python で DeepL API Free を利用してテキストファイルを翻訳する" href=https://shunyaueta.com/posts/2022-01-05/></a></article><article class=post-entry><header class=entry-header><h2>2022 年の目標</h2></header><div class=entry-content><p>Objective 1 検索エンジニアの領域でシニアレベルのスキルを身につける
KR 1 2022 年に国際会議・インダストリー会議のどちらでも良いので、対外発表できる成果を出す 成果を提出するのは 2023 年で OK
KR 2 Python, Go の両方を自信を持って書けるように。 TODO: 定量的に測定して数値化する
Write Code Every Day とか良いのかもしれない? wakatime とかでどれくらい書いたかを算出しても面白いかもしれない。 もしくは Atcoder とか?
Objective 2 有限な自分の時間の管理を生産的に行って、手を動かしまくる習慣を身につける
KR 1 Toggl を積極的に使って、1 年間どれだけ生産的な時間を創出できてか可視化する KR 2 継続して、SNS は断つ(ダラダラと見ない) 2021 年も設定していたが、2022 年も引き続き。連絡手段と周知を行うことのみに特化して使うようにする。 SNS での情報取得は辞めて、RSS や News letter などで情報を取得するようにしはじめた。今の所凄く良い。 自分も News letter を過去にやって続かなかったりしたけど、なんかやってみたいなと思うようになりつつも Blog でいいのではと思ったり…
KR 3 2021 年の 5 月から開始している翻訳プロジェクトを一段落させて出版可能な状態まで持っていく 詳細はまで伏せますが、自分が発起人の翻訳プロジェクトがあるんですが、今年の年末までには出版可能な状態まで仕上げていきたい。 友人 2 名を誘って始めたプロジェクトだが、初めての経験ということもあり学びが多い。
KR 4 (Advance) 自前で検索エンジンを運用した個人開発のサービスを作る。 同僚の@yomo さんが、個人で検索サービスを運営しているのですが、この取組が素晴らしいなと思ったので自分も真似したい。...</p></div><footer class=entry-footer><span title='2022-01-01 21:34:30 +0900 +0900'>1月 1, 2022</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 2022 年の目標" href=https://shunyaueta.com/posts/2022-01-01/></a></article><article class=post-entry><header class=entry-header><h2>2021 年振り返り</h2></header><div class=entry-content><p>うなすけさんの wakatime を利用した振り返り方が面白かったので、来年は真似したいと思い導入してみた。
サーバーサイドエンジニアとして 2021 年に使った技術と来年の目標
なので先週くらいから wakatime を使って、VSCode での利用統計をとってみることにした。
https://wakatime.com/
Markdown が圧倒的に多いのは現在 Markdown で執筆活動をしてるからですね。
仕事をほぼ納めてから導入して執筆しかまともにしてないからこうなってるな。。。 詳細は今はお話できないのですが、再来年くらいには形になっていることを祈る。
利用した技術一覧 Language New: Go Backend 開発ではメインで使っている。今までは Python がほぼメインだったが、検索チームに異動したことで Go が必要不可欠になったので頑張って習得中。久々に新しい言語に触れるけど新鮮な気持ち。明確に型があると、エディタがガンガンサジェストしてくれて楽しい。間違ってるとすぐ知らせてくれる。Go も Python までのレベルまで引き上げて書けるようにしておきたい所存。 New: Java (code reading) 主に Apache Lucene と Apache Beam の code reading をしていたのがメイン。同僚からは VSCode ではなく、IntelliJ IDEA 入れたほうがめちゃくちゃ捗るよと言われつつもまだ使いこなせていない…。Lucene, Solr, Elasticsearch のどれかに来年は contribute してみたい。 Python Google BigQuery と組み合わせたデータ分析や可視化、Airflow で利用。あとは機械学習サービスの改修でも書いていた。なんだかんだ手に馴染んでいるのがやはり Python で、2022 年は一段階上のコードを書けるようになりたい。1/4 ほど読んで積ん読になってしまっている Fluent Python を読みきらないと… StandardSQL Google BigQuery でお世話になっている。まだまだ「え、こんな便利関数あったんだ」となる。ちょっとした前処理は BQ に投げたほうが遥かに効率が良いので、BQ→Python で何をどこまでやるかはバランス感覚がやはり大事。 Software New: Apache Beam Java は code reading, Python は自分で入門がてら形態素解析する Beam model を書いていた。Apache Beam Go SDK が GA になったので、なにか作りたい。原著論文も勉強会で今度話したいな。ストリーミングで処理を行いたい際には、選択肢の第一候補に入るソフトウェアかつ動いている仕組みがめちゃくちゃおもしろいので、もっと深堀りして書いていきたい。 New: Apache Airflow (CloudComposer) GCP の各サービスを組み合わせてゴニョゴニョしたいときにものすごく楽。なれるまではデバッグが辛かった。そんなにこなれたことやっていなかったとしても、ピタゴラスイッチ的なデバッグが必要になることが多いので、最初は辛かったけど、慣れたらめっちゃ便利。 New: Apache Lucene 社内の code readning 勉強会で、近似近傍探索のロジックを眺めていた。 Amazon が e コマース検索を Lucene により、どうスケールさせているか at Berlin Buzzwords 2019 の記事でも Lucene 自体の特性を表層的に理解できてスゲーッ!...</p></div><footer class=entry-footer><span title='2021-12-29 22:42:08 +0900 +0900'>12月 29, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 2021 年振り返り" href=https://shunyaueta.com/posts/2021-12-29/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2021-12-28/images/1.png alt="DataFrame 内部にURLから参照して画像表示"></figure><header class=entry-header><h2>Jupyter Notebook で画像をダウンロードすることなく、URLから参照してPandas DataFrame内部に表示させる</h2></header><div class=entry-content><p>データ分析などをしていると、画像はダウンロードせずに特定の CDN (GCP なら GCS, AWS なら S3 など)で提供されている画像を参照して、 Jupyter Notebook 上で良い感じに表示させたいときがありませんか?
例えば、画像と説明文がペアになっているデータを画像自体はダウンロードせずに Jupyter 上で画像と説明文を DataFrame として表示させたいときが多々ある。 元の画像自体は CDN に格納されていて、画像をダウンロードする必要はなく参照するだけのときにはすごく便利。 毎度画像を CDN からダウンロードするのも無駄なので、画像を加工せずに Jupyter 上で表示するだけなら、この方法がベストですね。
url からとってきた画像を jupyter に表示する でも同じような課題に取り組んでいるが、今回紹介する方法なら余計なパッケージを入れずに最小構成で Jupyter 上で表示できるのが利点。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import pandas as pd from IPython.display import HTML # NOTE: https://www.irasutoya.com/2021/01/onepiece.html から画像を参照 onepiece = { "モンキー・D・ルフィ" : "https://1.bp.blogspot.com/-uxIsaN0S5lQ/X-FcrvAAInI/AAAAAAABdD4/6uw_qNUh9dQrG0aUzIExybt84yTEmXOPwCNcBGAsYHQ/s200/onepiece01_luffy.png", "ロロノア・ゾロ" : "https://1....</p></div><footer class=entry-footer><span title='2021-12-28 23:04:19 +0900 +0900'>12月 28, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Jupyter Notebook で画像をダウンロードすることなく、URLから参照してPandas DataFrame内部に表示させる" href=https://shunyaueta.com/posts/2021-12-28/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2021-12-26/images/1.png alt="Amazon の検索改善事例"></figure><header class=entry-header><h2>Amazon検索ランキングの奥深さ at MLconf SF 2016</h2></header><div class=entry-content><p>1 日遅れてしまいましたが、情報検索・検索技術 Advent Calendar 2021 25 日目の記事です。
ついにアドベントカレンダー最終日を迎えました! 今年はまだ検索領域のアドベントカレンダーが作られていないからということで、勢いで情報検索・検索技術 Advent Calendar 2021を作りましたが、多くの方に投稿に協力していただきありがとうございました。
社内勉強会の発表でネタを探しており、2016 年と少し昔の情報ですが、Amazon の製品検索において、どのようにランキングを行っているかの公演動画が非常に面白かったので、勉強がてら残したメモを記事として公開します。
今回の口頭発表は MLconf という開発者会議(非学会・非アカデミック)で発表されています。 自分が知る限り、MLconf は機械学習黎明期から高品質な発表が継続されて発信されており、非常に素晴らしいカンファレンスの一つ。 国際会議には投稿されていないが、実応用の観点からしてとても学びの多い発表がとても多いです。 機械学習の応用を考えている場合、世界の最先端事例を知ることができるので非常におすすめです。
Referemces Sorokina, D., & Cantu-Paz, E. (2016, July). Amazon search: The joy of ranking products. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval (pp. 459-460).
Amazon Search: The joy of ranking products in amazon science Youtube メモ 自分の私的な意見は NOTE: で書いておきます。...</p></div><footer class=entry-footer><span title='2021-12-26 22:52:06 +0900 +0900'>12月 26, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Amazon検索ランキングの奥深さ at MLconf SF 2016" href=https://shunyaueta.com/posts/2021-12-26/></a></article><article class=post-entry><header class=entry-header><h2>Offers Magazine というメディアに、エンジニアによる業務実績の論文化について寄稿した</h2></header><div class=entry-content><p>少し前のことだが、Offers Magazine さんに、「エンジニアによる業務実績の論文化」をテーマとして寄稿した。
機械学習エンジニアの学会での論文発表のススメ。応募から査読通過までの流れ
メディアに寄稿するのは初めてなので良い経験になった。
会社によっては業務成果を論文提出まで持っていくまでに難しい会社もあるかと思いますが、それに見合った対外的な成果を得れたので本当にやってよかったと思える。 USENIX は、MLOps に限らずシステムやセキュリティ、SRE 領域など、ソフトウェアエンジニア領域で幅広く投稿できるので、自分の成果を引用可能な形式で残したい人にはぜひ投稿してみたほしい。 素晴らしい仕組みを USENIX は提供していると思う。
何より自分にとって、この分野のエキスパートのレビュアーからこの成果を対外発表するのは非常にリスペクトするとコメントをいただけたりして、良い刺激になった。
論文公開して一年と少しが経過して、先日 Google Scholar を確認すると引用数が 1 になっていて非常に嬉しい!!
Auto Content Moderation in C2C e-Commerce citation
2021 年には、機械学習エンジニアから検索エンジニアになったが、この分野も論文化できそうなネタが無限にあるので時間はかかるだろうが 2023 年には論文提出ができる段階になりたい(否、なる)。</p></div><footer class=entry-footer><span title='2021-12-07 22:58:10 +0900 +0900'>12月 7, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Offers Magazine というメディアに、エンジニアによる業務実績の論文化について寄稿した" href=https://shunyaueta.com/posts/2021-12-07/></a></article><article class=post-entry><header class=entry-header><h2>投げ銭サービスのBuy me a cofee をBlog に導入してみた</h2></header><div class=entry-content><p>@potato4d さんや @takuti さんがBlog に Buy me a cofeeを導入していて、自分も導入したいと思いたち、導入してみた。
やったこととして、Blog記事の末尾に、サポート機能として自分のbuy me a cofeeページへのリンクが表示されるようにした。
導入経緯 以下のポストに強く共感と興味を持った。特に良い文章は抜粋しておく
情報に対して価値を感じてくれた人に還元してもらえるのは素直に嬉しい
ブログに Buy Me a Coffee の投げ銭導線を設置した @shu223 さんの とにかく、「技術の発信」でも収入が得られる時代が到来しつつある
技術書でご飯は食べられるのか？ #技術書典 技術で稼げるようになった今、内発的動機（興味）があっても外発的動機（お金）が伴わないと優先度が上がりにくいということは起きていて、だからこそ勉強自体でも稼げるようよう仕組み化したいと思っていて、それが去年から試行錯誤してる「技術情報発信のマネタイズ」です。 tweet
の方針に凄く興味と共感が持てた。
拝金主義というわけではなく、投げ銭文化自体が凄く良い文化なので自分もその波に乗ってみたさがあり、導入してみた。
zenn やnote でも良い記事だなと思った際には、投げ銭をしたりするが、個人Blog でも簡単にその機能が作れるのは良い時代になった。
もし自分が価値ある情報を提供することで、誰かの助けになり、その人達が還元してくれたなら凄くよいなと思った。業界全体がその流れになると面白いと思う。
早速、この前書いた記事 を友人に送ったら、メンテナンスお疲れ様の意をこめて Coffee ☕️ を奢ってくれた。
へんたい運用お疲れ様でした link
手探りで作った記憶が蘇る。開発、メンテとありがとう link</p></div><footer class=entry-footer><span title='2021-12-04 22:24:38 +0900 +0900'>12月 4, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 投げ銭サービスのBuy me a cofee をBlog に導入してみた" href=https://shunyaueta.com/posts/2021-12-04/></a></article><article class=post-entry><header class=entry-header><h2>2013年4月に友人とリリースした高専からの大学編入体験談投稿サービスが8年の時を経て成仏した</h2></header><div class=entry-content><p>高専 5 年生の卒業前の春休みに友人 3 人と集まって、2013 年 4 月にリリースした個人開発の Web サービスが完全に終わりを迎えた。
2021 年の現在から換算すると 8 年以上前の出来事であり、時の流れは凄まじい。
先程アプリをデプロイしていた Heroku 上からも完全に削除をしたのだが、色々と昔のことを思い出したので筆を執ってみる。
開発当時 当時仲の良かった同じクラスの友人 3 人で、春休みに友達の家に泊まりつつ 2 泊３日で、Rails 3 でロジック部分を作り上げた。 Twitter ログインや DB 定義なども本を読んだり、ネットの記事を参考にひいこら言いつつ実装していった。
Heroku を使って、 http://kosen.herokuapp.com/ という URL でサービスを提供しており、Twitter ログインで編入体験談投稿、ログイン無しで掲示板で編入情報について交換できるような機能を提供していた。(注: サイト名のネーミングセンスが若気の至りすぎるので言及しません)
最終的に Bootstrap で見た目を整えて、編入後の授業が本格的に始まる前の 2013/04/10 にはリリースしていた模様。
共同開発した友人の 2013/04/28 に書かれた当時のブログ記事が残っていた。 8 年経過しても、その時の状況がブログ記事として残っているの凄い。 自分はブログのホスティングサービスをひたすら変遷して、現在の GitHub pages に落ち着いたので尊敬する。
ついに動き出しました!! 編入体験談のまとめサービス 「HenTai ~編入体験談~」 http://kosen.herokuapp.com 午前 0:15 · 2013 年 4 月 10 日 tweet
当時全く Web サービスの運用方法も全く知らない自分が、インターネット上に Rails アプリを公開できたので間違いなく Heroku のおかげだと思う。GitHub repository と連携して Push すれば Rails アプリがデプロイされる体験はとても簡単だった。...</p></div><footer class=entry-footer><span title='2021-12-03 23:28:52 +0900 +0900'>12月 3, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 2013年4月に友人とリリースした高専からの大学編入体験談投稿サービスが8年の時を経て成仏した" href=https://shunyaueta.com/posts/2021-12-03/></a></article><article class=post-entry><header class=entry-header><h2>kubernetes デプロイ時に `MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable` エラーが出た際に対処方法</h2></header><div class=entry-content><p>k8s で manifest file を編集して実行したら以下のようなエラーが出て実行できなかった。
Exception ( Monitor Deploy ) Deploy failed: The Deployment “—” is invalid: spec.selector: Invalid value: v1.LabelSelector{MatchLabels:map[string]string{“app”:“—”}, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable
調べてみたところ、
once deployed the label selectors of kubernetes service can not be updated until you decide to delete the existing deployment
ref: MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutabl #508
らしく、一度デプロイされたk8s service の label selector は、既存のdeployment を削除しないとアップデートは不可能らしい。
なので、field is immutableというエラー文は正しいわけですね。
そのdeployment を削除しても良い状態なら、以下のコマンドを実行後再度デプロイすれば、k8s service のlabel selector は実行されます。
1 kubectl delete deploy &lt;deployment-name> もしくは、k8s service のlabel selectorの変更を諦めて既存のまま運用するのがもう一つの正解でしょうか。...</p></div><footer class=entry-footer><span title='2021-12-02 16:48:54 +0900 +0900'>12月 2, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to kubernetes デプロイ時に `MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable` エラーが出た際に対処方法" href=https://shunyaueta.com/posts/2021-12-02/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2021-11-26/images/1.png alt="Amazon search アーキテクチャ"></figure><header class=entry-header><h2>Amazonがeコマース検索を Lucene により、どうスケールさせているか at Berlin Buzzwords 2019</h2></header><div class=entry-content><p>情報検索・検索技術 Advent Calendar 2021 1 日目の記事です。 早めに書き終えたので、カレンダー登録日の 2021/12/01 よりもはやめですが、記事を公開してしまいます。
Berlin Buzzwords はドイツで毎年開催されている OSS を利用した検索、データ処理、データベースに焦点をあてたカンファレンスです。
検索関係のシステムに携わっている場合、毎年面白い内容が目白押しなのでぜひとも見てほしい。
今回は Berlin Buzzwords 2019 で発表された「Amazon では Lucene をどう活用して e コマース検索をスケールさせているか」の講演動画を社内勉強会で紹介するために視聴したので、そのメモを公開する。
E-Commerce search at scale on Apache Lucene YouTube Web page PDF 自分の所感などを切り分けるため、自分の意見は IMO ではじめた文にして、メモっています。
Overview クエリの p999 latency に対して非常に厳しい制限を行っている IMO このクエリの p999 latency 定義は、Lucene+(おそらく内製で今も開発している、response を返すための Lucene server?)が返す検索のレスポンスを指していると思われる p99.9 latency を SLA として、監視しているのはたしかにとてもシビアな基準だと感じる。 Amazon の query rate はめちゃくちゃピーキー (daily, weekly, yearly) Why Lucene? Lucene は成熟しており、豊富な検索エンジンの機能が揃っている 情熱を持ったコミュニティが存在している Uber, Airbnb, Linkedin 全部 Lucene を使っている maxscore scoring , Weak AND, Lucene 8....</p></div><footer class=entry-footer><span title='2021-11-26 20:59:21 +0900 +0900'>11月 26, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Amazonがeコマース検索を Lucene により、どうスケールさせているか at Berlin Buzzwords 2019" href=https://shunyaueta.com/posts/2021-11-26/></a></article><article class=post-entry><header class=entry-header><h2>Standard SQLのCOALESCEで、時間経過によってカラム名が変化したデータを柔軟に抽出する</h2></header><div class=entry-content><p>データの蓄積帰還が長くなってくると、例えば JSON 形式でログを取っているが、同じデータでもマイグレーションやロギングロジックの更新などでkey の名前が変化したりする場合がある。
その場合取り扱いに困るのが、古い key と新しい key をどのように併合するかだ。 例えば特定の日次できれいにデータが入れ替わっているのなら、色々やりようがあるが、クライアントなどのログの場合データの変化も均一ではないので、徐々に変化していることが大半なので、日次で別々の抽出をして結合するというアプローチも難しい。
その際に役立つのが Standard SQL 条件付き構文の COALESCE だ。
COALECSCE は、引数の最初の非 NULL の値を返す関数で、
1 COALESCE(NULL, 'B', 'C') だと Bが返される。この関数を使うことで、複数カラムを一つに併合することができる。
具体例を交えつつ実践してみる 例えば、以下のように昔のカラム名が title で、全く同じデータが新しいカラムの title_v2 に入ってきているとする。
NOTE: json を例題に key の抽出にしたほうが実際の状況に沿いますが、カラムのみで表現したほうが説明が簡単なので今回はそちらを採用。
用意したデータ 1 2 3 4 5 6 7 8 WITH menues AS (SELECT "うどん" as title, NULL as title_v2, "2021/10/06" as created UNION ALL SELECT "ラーメン", NULL, "2021/10/07" UNION ALL SELECT NULL, "そば", "2021/10/08" UNION ALL SELECT "カツ丼", NULL, "2021/10/09" UNION ALL SELECT "カツ丼", "カツ丼", "2021/10/10" UNION ALL SELECT NULL, "カレー", "2021/10/11") SELECT * FROM menues title title_v2 created うどん 2021/10/06 ラーメン 2021/10/07 そば 2021/10/08 カツ丼 2021/10/09 カツ丼 カツ丼 2021/10/10 カレー 2021/10/11 2021/10/10 のデータなどは旧カラムと新カラムにダブルライトされています。...</p></div><footer class=entry-footer><span title='2021-11-06 22:40:02 +0900 +0900'>11月 6, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Standard SQLのCOALESCEで、時間経過によってカラム名が変化したデータを柔軟に抽出する" href=https://shunyaueta.com/posts/2021-11-06/></a></article><article class=post-entry><header class=entry-header><h2>Google Cloud Pub/Sub に公開された結果はDataflow template を使えばめちゃくちゃ簡単に確認できる</h2></header><div class=entry-content><p>PubSub に出力された結果を確認するのって、なかなか手間がかかりませんか?
最近同僚に簡単な確認方法を教えてもらい、感動したのでそれを記事にしました。
確認方法 PubSub のメッセージを出力する Google Cloud Storage bucket を同一 GCP プロジェクトで作成する。 GCP の Pub/Sub ページに移動する 確認したい Pub/Sub topic をクリックする ページ下部にある CREATE SUBSCRIPTION ボタンを押すと選択肢で、Create subscription, Export to BigQuery, Export to Cloud Storageがあり、 Export to Cloud Storageを選択する。 BigQuery、 Google Cloud Storage への吐き出しを行い際に、自動的に subscription が生成される。 Export to Cloud Storage を選択すると、Text 形式か Abro 形式での出力にするかを選択できる。基本的には簡単に確認できる Text 形式を選ぶと良さげ。 選択後、下記のような設定画面が出てくるので情報を埋めていく。基本的には、どこの Google Cloud Storage に出力するかを埋めれば完了。 10m ほどすると Streaming job の Dataflow の起動が完了して、一定期間ごとに Pub/Sub の topic に公開されたデータがテキスト形式で出力され始めます。 出力された GCS の結果を眺めるには、 gsutil コマンドなどを使うのが簡単です。自分はgsutil cat の結果をコピーして VS Code で確認しています。 Cloud Dataflow のテンプレート機能については、端的に説明すると、GUI でパラメータを設定するだけで、Dataflow によるデータ処理が簡単に実行できるようになる機能です。...</p></div><footer class=entry-footer><span title='2021-11-05 23:05:22 +0900 +0900'>11月 5, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Google Cloud Pub/Sub に公開された結果はDataflow template を使えばめちゃくちゃ簡単に確認できる" href=https://shunyaueta.com/posts/2021-11-05/></a></article><article class=post-entry><header class=entry-header><h2>Airflow でDAGを任意のタイミングで一度だけ実行する方法</h2></header><div class=entry-content><p>Airflow で作成したDAGを自動で定期実行せずに、あえて手動実行で一度だけ実行したい場合もある。
DAGのオプションを以下のように設定する。
schedule_interval を “@once” に設定することで、一度だけDAGが実行される is_paused_upon_creation を True に設定することで、DAGが作成時に自動的に実行されず、DAGが停止状態で作成される。 デフォルトではFalseとなっており、自動実行される。 1 2 3 4 5 6 7 8 9 from airflow import DAG with DAG( dag_id="sellerscore_initial_batch", # NOTE: dosen't need to repeat schedule_interval="@once", # NOTE: we have to manually start the this DAG is_paused_upon_creation=True, ) as dag: Reference Airflow: schedule_interval = ‘@once’ Docs - airflow.models.dag</p></div><footer class=entry-footer><span title='2021-10-12 00:07:05 +0900 +0900'>10月 12, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Airflow でDAGを任意のタイミングで一度だけ実行する方法" href=https://shunyaueta.com/posts/2021-10-12/></a></article><article class=post-entry><header class=entry-header><h2>クエリ分類(Query Classification) について社内の勉強会で話してきた</h2></header><div class=entry-content><p>今年の 10 月から、新しく入社した同僚とともに、検索領域の論文や技術ブログを定期的に紹介する社内勉強会をはじめてみた。 定常的に開催されることが一番大事だよねという方針になったので、以下のような仕組みで、可能な限り低コストで継続できるような仕組みにした。
参加者は何も準備をしなくても大丈夫で、勉強会中に紹介された論文をみたり話を聞くだけで良い 発表者は凝った資料は用意するのは必須ではなく、極論論文を画面共有で見せながらしゃべるだけでも問題なし 当面の目標としては、来年の年末まで継続されているように気長に続けていきたい。
第一回は、発起人の一人である自分がクエリ分類について発表を行った。
Query Understanding for Search Engines (The Information Retrieval Series, 46) の第二章を主にテーマとして取り上げて紹介した。
メイントピックは KDDCup2005 として開催されたクエリ分類コンペの優勝者の手法について紹介を行ったので、気になる方はスライドを公開しているので御覧ください。
このコンペの特徴として、
データセットが生データ特有の問題として汚い そしてラベルデータの規模がとても少ない という鬼畜仕様だった。 だがコンペ参加者はそんな状態を物と物せずにありとあらゆる手段で精度向上に努めていてそれらの手法と姿勢がとても参考になった。
Query Understanding の包括的な解説は 晋策さんが書かれた 検索体験を向上する Query Understanding とは がわかりやすいのでおすすめです。
検索領域は本当に奥深い…</p></div><footer class=entry-footer><span title='2021-10-09 00:42:40 +0900 +0900'>10月 9, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to クエリ分類(Query Classification) について社内の勉強会で話してきた" href=https://shunyaueta.com/posts/2021-10-09/></a></article><article class=post-entry><header class=entry-header><h2>Hugo で記事の更新日をgitと連携して自動的に取得して表示させる</h2></header><div class=entry-content><p>最近昔書いていた技術記事の情報が古くなりすぎて不正確なこともあったので、書き直すときがあったのだが、そのときに自動的に最終更新日を記事に表記できないか探していたら、実現方法があったのでメモしておきます。
やっていることは Last Modified Date with Hugo の記事をと完全に一緒だが、日本語での情報が無かったので備忘録がてら記録を残す。
Hugo は各ページに関する情報をFront Matter Variables という仕組みで Markdown 上に定義します。 主に YAML 形式で記述されていることが多いです。
lastmod という変数が更新日を表す変数であり、この変数に対して更新日の情報を与えてやれば記事の最終更新日を表現することができる。
Front Matter に lastmod: "2021-03-31" の形式で与えておけば、以下の形式で記事作成日と最終更新日を表記できる。
1 2 3 4 5 {{ $date := .Date.Format "02.01.2006" }} {{ $lastmod := .Lastmod.Format "02.01.2006" }} &lt;p>Published on: {{ $date }}&lt;/p> &lt;p>Edited on: {{ $lastmod }}&lt;/p> だが、毎回記事を編集するたびに lastmod 変数を追記するのは面倒なので自動化できるなら自動化したい。
config.yaml で、以下の設定を行う。
1 enableGitInfo: true enableGitInfoを trueにすることで、各ページに対してGit 情報を更新日として付与してくれる。
最後にconfig.yaml で以下の設定を行えば、 Front Matter の lastmod 変数に対して、 Front Matter で定義されているlastmod、もしその情報がなければ各ページの gitの最終コミット日を返すという設定がされる。...</p></div><footer class=entry-footer><span title='2021-10-06 20:38:58 +0900 +0900'>10月 6, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Hugo で記事の更新日をgitと連携して自動的に取得して表示させる" href=https://shunyaueta.com/posts/2021-10-06/></a></article><article class=post-entry><header class=entry-header><h2>CloudComposer のDAGをCircleCIで更新する</h2></header><div class=entry-content><p>Cloud Composer(Airflow) の DAG を GitHub リポジトリで管理して、CI によりリポジトリで管理している DAG を Pull Request がマージされると Cloud Composer の DAG へ同期する方法について説明する。
DAG は、ルートディレクトリ直下の dags/ というディレクトリに格納されている状態を前提とする。
以下の２つのコマンドラインツールを利用して実現できる。
Service Account の認証のために gcloud DAG の同期のために gsutil CircleCI によるワークフローの記述例は以下のとおり
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 version: 2.1 jobs: rsync-dags: working_directory: ~/workspace docker: - image: gcr.io/google.com/cloudsdktool/cloud-sdk:alpine environment: GOOGLE_APPLICATION_CREDENTIALS: /gcp-service-key.json steps: - checkout - run: name: Sync DAG folder to GCS's DAG folder command: | echo "${CLOUD_COMPOSER_CREDENTIALS_JSON}" > ${GOOGLE_APPLICATION_CREDENTIALS} gcloud auth activate-service-account --key-file ${GOOGLE_APPLICATION_CREDENTIALS} gsutil -m rsync -d -r dags \ "$(gcloud composer environments describe {COMPOSER_NAME} --project={GCP_PROJECT} --location={REGION} --format="get(config....</p></div><footer class=entry-footer><span title='2021-10-04 22:23:24 +0900 +0900'>10月 4, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to CloudComposer のDAGをCircleCIで更新する" href=https://shunyaueta.com/posts/2021-10-04/></a></article><article class=post-entry><header class=entry-header><h2>CircleCI アプリ内の設定ファイルエディターを利用して、CI上の環境変数などローカルCLIでは確認できない挙動を素早く確認して修正する</h2></header><div class=entry-content><p>TL;DR; Pull Requestのコミット履歴を汚さずにCircleCIのConfigmap をWeb上で編集して、CIの挙動をすぐ試せる機能がすごく便利 課題点 CircleCI の上のCIの挙動のデバッグをする際、ブランチにプッシュをせずに挙動が確認できる方法としてローカルCLIを利用する方法がある。 しかし、CIのマシン上で定義されている環境変数などは、ローカルCLIを使用しても確認することができない。
そのためコミットでCIが失敗している際には、
Rerun Job with SSHを利用してSSHで接続して、環境変数を確認 再度 config.yml を修正して、コミットをプッシュ だが、上手くいかないので1→2を繰り返す を繰り返してしまい、コミット履歴が不用意に汚れてしまう。
解決方法 この問題点を解決する方法として、CircleCI アプリ内の設定ファイル エディターを利用するのがすごく便利だった。
この機能は、ジョブのページの右上からアクセスできる。 Rerun ボタンの右に... ボタンがあり、そのボタンをクリックすると
Project Settings Configuration File の選択項目があり、Configuration File をクリックすると、config.yml のウェブエディターが起動する。 エディターでYAMLファイルを編集後、右上のSave and Runボタンをクリックすれば、PRで作成されているブランチと別のリモートブランチがCircleCIによって新たに作成されるので、もとのPRのコミット履歴を汚さずにCIの問題を修正できる。
Reference CircleCI アプリ内の設定ファイル エディターの使用</p></div><footer class=entry-footer><span title='2021-10-01 20:39:06 +0900 +0900'>10月 1, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to CircleCI アプリ内の設定ファイルエディターを利用して、CI上の環境変数などローカルCLIでは確認できない挙動を素早く確認して修正する" href=https://shunyaueta.com/posts/2021-10-01/></a></article><article class=post-entry><header class=entry-header><h2>GCPのCloud Composer のDAGを素早く・簡単にデバッグする</h2></header><div class=entry-content><p>GCPでAirflow をマネージドサービスで使えるサービスで Cloud Composer が存在する。 BigQueryやBigTable, PubSub などGCPの各サービスをDAGとして定義してジョブを定期実行できるので非常に便利だが、その代わりDAGを実行するまで結果がわからないので、CloudComposer を一度実行するしか無いのでデバッグが困難になる傾向がある。
また、GitHubのリポジトリにDAGを保存して、CIでCloud Composerを更新するようしていると PRを都度作ってマージされないと確認できないという場合もある。
ローカルでDocker で走らせれば良いのじゃないかというツッコミがあると思いますが、結局 Cloud Composer 上での動作を確かめないといけないので、今回の記事を書くことにしました。
NOTE: 自分が使用しているComposerのversionはcomposer-1.8.0-Airflow-1.10.3 です。基本的にやれることは一緒だと思います。また、dev, prodで同一のDAGが走るCloud Composer を運用しているという前提です。
アプローチは２つ
logger.info() を仕込んで、DAGのなかで何が起こっているかを理解する 1 2 3 4 5 import logging logger = logging.getLogger(__name__) logger.info() loggerをDAGを記述した Pythonファイルに仕込んで、内部で何が起こっているかを把握する。
各DAGのlogは、
GCPのCloud ComposerのページにアクセスしてAirflow webserver 列のボタンをクリックしてAirflowのWeb applicaiton にログイン 確認したいDAGをクリック DAG内のtask をクリックして表示されるモーダル内の View Logをクリックすると、loggerの情報が確認できる gstuil rsync コマンドでのGCSへのDAGの同期 gstuil rsyncコマンドを使うことで、リポジトリのDAGファイルをGCSに格納されている開発環境上のCloudComposer のDAGファイルに直接同期してPull Request マージ後のDAGの挙動を確認できる。
Cloud Composer のDAGは、自動作成されたGoogle Cloud Storage(GCS)に格納されており、GCSをCloud Composerが定期的に監視してCloud Composerを更新している。 つまり、GCS上のDAGファイルを直接更新してやるとそれがCloud Composerに反映される。 体感として2-3分に一度は監視されているので、ほぼ待ち状態がない状態で確認できて便利です。
以下のコマンドでリポジトリのDAGファイルをGCSに反映させます。...</p></div><footer class=entry-footer><span title='2021-09-29 22:20:23 +0900 +0900'>9月 29, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to GCPのCloud Composer のDAGを素早く・簡単にデバッグする" href=https://shunyaueta.com/posts/2021-09-29/></a></article><article class=post-entry><header class=entry-header><h2>Pandoc を使って抽出できなかったWord内部の画像をGoogle Driveを使って抽出する</h2></header><div class=entry-content><p>先日の記事 では、pandoc の--extract-media オプションをオンにしても word 内部の画像を抽出することができなかった。
だが、Google Drive を使うことで Word 内部の画像を抽出することができたのでここに記しておく。
対象の Word ファイルを Google Drive にアップロードする そのファイルを Google Docs で開く File → Download → Web Page (.html, zippted) でウェブページとして zip ファイルをダウンロードする zip ファイルを解凍後、その中にあるimages フォルダに Word 内部の画像が格納されている</p></div><footer class=entry-footer><span title='2021-09-27 00:18:26 +0900 +0900'>9月 27, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Pandoc を使って抽出できなかったWord内部の画像をGoogle Driveを使って抽出する" href=https://shunyaueta.com/posts/2021-09-27/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://shunyaueta.com/posts/page/3/>«&nbsp;前へ&nbsp;</a>
<a class=next href=https://shunyaueta.com/posts/page/5/>次へ&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2023 <a href=https://shunyaueta.com/>hurutoriya</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>