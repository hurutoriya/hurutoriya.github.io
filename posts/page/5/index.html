<!doctype html><html lang=ja dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | hurutoriya</title><meta name=keywords content><meta name=description content="Posts - hurutoriya"><meta name=author content="Shunya Ueta"><link rel=canonical href=https://shunyaueta.com/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><link rel=icon href=https://shunyaueta.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shunyaueta.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shunyaueta.com/favicon-32x32.png><link rel=apple-touch-icon href=https://shunyaueta.com/apple-touch-icon.png><link rel=mask-icon href=https://shunyaueta.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://shunyaueta.com/posts/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-JMJRQJT0Q3"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JMJRQJT0Q3")</script><meta property="og:title" content="Posts"><meta property="og:description" content="Shunya Ueta's blog"><meta property="og:type" content="website"><meta property="og:url" content="https://shunyaueta.com/posts/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="og:site_name" content="Shunya Ueta"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="Posts"><meta name=twitter:description content="Shunya Ueta's blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://shunyaueta.com/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://shunyaueta.com/ accesskey=h title="hurutoriya (Alt + H)">hurutoriya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://shunyaueta.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://shunyaueta.com/about title=About><span>About</span></a></li><li><a href=https://searchengineeringnewsletter.substack.com/ title=Newsletter><span>Newsletter</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://shunyaueta.com/index.xml title=RSS><span>RSS</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2>Pandoc で特定のディレクトリ直下にある複数のWordをMarkdown形式に一括変換する</h2></header><div class=entry-content><p>表題の通り、Pandoc を使って、特定ディレクトリ配下にある複数の Wordファイル(*.docx) を Markdownファイル(*.md) へ一括変換したい。
単一Wordファイルの変換コマンド 単一の変換である場合は、@tomo-makes さんのWordファイル(.docx)をMarkdownへ変換する を参考に実行すると良いと思います。
自分は特に困らなかったので、despan の処理は省いた形にしました。 また、--extract-media をオンにして指定しても Wordファイル内の画像を上手く抜き出せなかったです。 WordファイルからMarkdownファイルへの完全変換って難しい。まさに餅をもち米に戻す行為に近い…
1 pandoc -s {input}.docx --wrap=none -t gfm -o {output}.md 複数Wordファイルの変換コマンド ワンライナーのシェルスクリプトを組んで実行する。 実行時には、変換元のWordファイルが配置されているディレクトリで実行する。
1 for f in *.docx; do pandoc -s "$f" --wrap=none -t gfm -o "${f}.md"; done "${f}.md" の部分を "../../docs/${f}.md" のような形で修正してやれば、所定のディレクトリへ変換されたMarkdownファイルが生成される。
Reference How can I convert a whole directory of files from Markdown to RTF?</p></div><footer class=entry-footer><span title='2021-09-19 23:52:45 +0900 +0900'>9月 19, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Pandoc で特定のディレクトリ直下にある複数のWordをMarkdown形式に一括変換する" href=https://shunyaueta.com/posts/2021-09-19/></a></article><article class=post-entry><header class=entry-header><h2>gcloud commands で PubSub に jsonファイルをメッセージとして公開 (Pusblish) する</h2></header><div class=entry-content><p>gcloud commands で PubSub に json ファイルをメッセージとして公開 (Pusblish) する
jq コマンドが必要になるが、一番簡単に実現できるのは
1 $ gcloud pubsub topics publish {PUBSUB_TOPIC_NAME} --message "$(cat {FILE_NAME} | jq -c)" jq コマンドの -c オプションは compact-output を意味している。デフォルトだと pretty-prints になってしまう。 それを避けるために-cオプションを使用している。
ref Publishing messages to topics Read a txt file JSON data to publish the messages in Cloud Pub Sub</p></div><footer class=entry-footer><span title='2021-09-07 12:22:16 +0900 +0900'>9月 7, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to gcloud commands で PubSub に jsonファイルをメッセージとして公開 (Pusblish) する" href=https://shunyaueta.com/posts/2021-09-07/></a></article><article class=post-entry><header class=entry-header><h2>gRPC client evans で portforward 先のリモートサーバーにリクエストを行う</h2></header><div class=entry-content><p>evans
evans は対象のサーバーの gRPC のリフレクション機能が起動されていれば、proto ファイルを参照せずに便利な [REPL mode](gRPC のリフレクション機能) を使用できます。
If your server is enabling gRPC reflection, you can launch Evans with only -r (–reflection) option.
gRPC のリフレクション機能については evans 作者の ktr0731 さんが解説している記事が非常にわかりやすいです。
gRPC リフレクションはなにをしているか？
ローカルの 5000 番のポートをリモートサーバの 5000 番ポートにフォワード (port-forward)しているとします。 例えば、kubectl だと以下のような実行コマンドになります。
Forward a local port to a port on the Pod
1 kubectl port-forward pods/hoge-asas32s 5000:5000 そして、ポートフォワードのシェルは保持した上で、別にシェルを起動します。
この際に 対象となるlocalhost:5000 に対して、--host, --port オプションで指定してやれば evans の REPL モードが起動します。
REPL 1 2 3 4 5 6 7 8 9 10 > evans -r --host localhost --port 5000 ______ | ____| | |__ __ __ __ _ _ __ ___ | __| \ \ / / / _....</p></div><footer class=entry-footer><span title='2021-08-19 16:51:05 +0900 +0900'>8月 19, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to gRPC client evans で portforward 先のリモートサーバーにリクエストを行う" href=https://shunyaueta.com/posts/2021-08-19/></a></article><article class=post-entry><header class=entry-header><h2>システムの応答速度は本質的な価値提供であることを示す A/B テストの実例</h2></header><div class=entry-content><p>内容 システム提供において、基本的に高速であればあるほど顧客は嬉しいものだが、実際のところ高速なシステムを提供して、どの程度の価値が発生するのかが気になったので、調べてみた。
2021/08/14 追記 A/Bテスト実践ガイド　真のデータドリブンへ至る信用できる実験とは の書籍で同様な事例が紹介されているとのこと。情報提供ありがとうございます。 実務でA/Bテストに向き合った人間であれば必ず一度は考えたことのあるトピックについて、アメリカのテックカンパニー（Airbnb, Google, LinkedInなど）勤務の著者らが国際会議で発表された研究もちゃんと引用して見解を述べており説得力がある。 従って、現時点における最高レベルの意思決定をデータ（A/Bテスト）に基づいて行いたいと思うなら、一度は目を通しておくべきであり関係者必携だと思う。 ※個人的には”Webサービスのレイテンシーと利益の関係（５章や”多くのスピード問題”の節）”がお気に入りで、サイトのレイテンシー改善がいかに収益に貢献し得るか、つまりCodeの実行速度というエンジニアのアウトプットがダイレクトに収益に貢献できるか？をデータに基づいてきちんと測っているのが印象的で興味深かった内容でした。 Amazon review
Three Challenges in Building Industrial-Scale Recommender Systems" - Keynote for ORSUM@RecSys'20 3rd Workshop on Online Recommender Systems and User Modeling でのkeynote session で発表された内容
講演者は Sebastian Schelter さんという方で、アカデミックもインダストリーもどちらもバリバリにこなしている人だった。日本だとこういう経歴の人ってかなり珍しい気がするので、やはり層が厚い
ふと@hagino3000 さんのツイートが印象に残っていたので、記録のためにこちらに。1年くらい前のやり取りだけど、印象に残っていて今回この記事を書いたきっかけでもある。
推薦システムのレイテンシが15msと32msで差が出るかA/B Testしたって。推薦結果は同じで片方はあえて遅らせたって事だよな、はじめて聴く実験だ。15msの方がrevenueが良かったとの事。 twitter
公開されている動画はこちら
Three Challenges in Building Industrial-Scale Recommender Systems" - Keynote for ORSUM@RecSys'20
19,20枚目のスライド
要約すると、
既存の研究では、検索エンジン上で人工的に応答速度を遅らせた際にネガティブな影響が発生した。
では、逆に応答速度を早めた場合どのような影響になるのだろうか? とてもおもしろい事例があるので是非紹介したい、
オンプレのシステムからGoogle Cloud に移行するイベントを利用した実験を行った。マイグレーション時にサービングシステムの最適化などを行い、マイグレーション後のシステム性能向上した。この最適化により、モデルやシステム構成は全く同じだが、p90 の応答速度がオンプレのシステムでは 32ms だったものが、GCPでは15ms に向上した。 これにより生じた差異を活用して、以下のA/B テストを行った。 32ms をcontroll, 15ms をtest 群に分けてA....</p></div><footer class=entry-footer><span title='2021-08-13 23:41:08 +0900 +0900'>8月 13, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to システムの応答速度は本質的な価値提供であることを示す A/B テストの実例" href=https://shunyaueta.com/posts/2021-08-13/></a></article><article class=post-entry><header class=entry-header><h2>子供が1歳児を迎えるまでに、育児で役に立ったもの</h2></header><div class=entry-content><p>去年の秋頃に子供を授かり、色々と役に立った情報や製品、サービスなどがあったので備忘録がてら残しておく。 ベビーウォール、プレイマット、服や抱っこひもなど鉄板系で買えば良いものなどは記す必要が無いのであえて書いてない。
書籍・情報源 ★★★ 赤ちゃん寝かしつけの新常識 科学的な見地に基づいた、赤ちゃんの睡眠に関する情報を纏めた書籍。データやメタ分析、また著者の経験に基づいて、どう寝かしつけに取り組めばいいのか説明してくれている。 ★★ 小児科医のママが教える 離乳食は作らなくてもいいんです。 簡単にまとめると、離乳食をわざわざ作るのではなく販売されている商品で離乳食をカバーしたほうが栄養素、準備に係る労力もなくなってみんな幸せという書籍 ★ 厚生労働省が公開している情報 ac.jp, go.jp の資料の信憑性は高いので、優先して閲覧するようにしている。 ★ 赤ちゃんとママ・パパのための情報 by 花王 花王がお医者さんと連携して作成している F&amp;Q サイト。 モノ ★★★ 食洗機 哺乳瓶を電子レンジで殺菌するものもあるが、こちらのほうが食器も合わせて洗えるのでめちゃくちゃ楽。変にやけどする心配もない。哺乳瓶を 8 本程度買っておいて、食洗機で毎日 4 本程度洗っていく運用が非常に楽だった。 ★★★ 防水敷き布団カバー 子供がよる咳がひどくて飲んだミルクを吐いてしまっている場合があるのだが、その際に布団をすべて洗う必要がなくなるので非常に助かる。自分は西松屋で買ったけど、Amazon でも打っているので紹介 ★★★ iHerb で購入する栄養満点ベビーフード Gerber のライスシリアル など、日本の離乳食と比較して味付けが素材そのままの味で、必須栄養素が添加されている Gerber を愛用して買っている。離乳食にミルクと混ぜて会えると簡単に離乳食が作れるので非常に便利。小児科医のママが教える 離乳食は作らなくてもいいんです。で Gerber の存在を知れた。 ★★★ スワドルアップ 2-5 ヶ月ごろまで愛用していた。抱っこしている状態から、ベッドに置くと一種で起きて泣き出す確率が 9 割から 1 割程度に減る神の道具。途中で起き出すこともかなり少なくなった。 ★★★ ニトリの引っ張るだけで取り込めるハンガー わざわざベビーハンガーなどを買う必要は無い。引っ張って回収できるのが便利。Amazon でも同様のものが売られている ★★★ Google Home Mini ホワイトノイズを再生するのに使っている。「OK Google ホワイトノイズを流して」で 11 時間程度連続再生してくれるので子供の就寝時間中ほぼカバーできる。 赤ちゃん寝かしつけの新常識 で紹介されていたので導入してみたが、寝ている部屋の近くで音を立ててしまったり、一緒の部屋で寝ていて自分のベッドの軋む音で泣かなくなったので非常にありがたい。Alexa とかでも同等の機能はあるんじゃないのでしょうか。昼寝のときもちょっとした音で起きなくなるのでありがたし 2022-04-26: 追記: 子供の就寝時に使っているホワイトノイズマシンを Google Home から Dreamegg に変更 ★★ Xiaomi Mi スマートバンド 子供が深夜に起きた際にスマホで時計を確認すると光が強すぎるので、適切な光量で確認できる。また、料理しているときにタイマーとしても秀逸です。アラームも振動で起床できるので他の人を起こさずに起床できる点も秀逸。何よりも安いので気軽に買えるのが良い ★★ クリップライト 赤ちゃん寝かしつけの新常識でも紹介されている、レッドライトを買うと高いので、クリップライトを買って、百均で買った赤色の透明の下敷きを当ててレッドライトを即席で作成したがなんの問題もなく使えている。実際に深夜に起きて作業をしていると赤色のライトだと眩しいという感覚が非常に和らいでいる気がする ★ オムツ替え防水シート おむつ交換時にいきなりおしっこが発射されることもあるので、防水シートを買った。購入後いざという時何度も助かったので便利 サービス・アプリ ★★★ ぴよログ 睡眠やミルク、排泄の回数などを夫婦間で共有して管理できる。細かな使い勝手が洗練されていて感動するレベル。できた 🚩 という項目があり、これをこまめに付けておくと見返すときにニヤつきながら成長を振り替えれる。 ★★★ メルカリ 元値が数千円台の子供服やおもちゃを 300 円、送料込みで買える。数十着は買ったのではなかろうか。ここでしか見つからないようなかわいい服も多くて、見てての楽しい ★★ ベビーカレンダー このアプリは仕組みが面白い。こどもの生後の日数に合わせて、生後 N 日だと~ができるようになりますとか、こういうことをこころがけましょうと日めくり的に毎日記事が見れて、書籍を一度にまとめて読むよりも今必要なことを適宜教えてくれる感じで助かった。 ★★ ジモティー 4 万円相当のベビーベッドを無料でいただけた。車で 2 時間程度の場所だったが、旅行がてら受け取りに行った。感謝。 ★ とりあえず登録しておくといいサービス 楽天ママ割 Amazon ファミリー ★ Amazon、ベビー用品クーポン おむつやおしりふきなど時々割引クーポンで、ドラッグストアより安くなるときがあり、そのときに購入している。通常時はドラッグストアのほうがやすいので使い分けている おむつカテゴリ おしりふき まとめ 可能な限り効率化できるような、仕組みづくりに投資したほうが幸せになれる。あと体力が一番大事、健康が最強!...</p></div><footer class=entry-footer><span title='2021-07-23 22:22:56 +0900 +0900'>7月 23, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 子供が1歳児を迎えるまでに、育児で役に立ったもの" href=https://shunyaueta.com/posts/2021-07-23/></a></article><article class=post-entry><header class=entry-header><h2>mvn archetype:generate でJavaのプロジェクト雛形を作成する際のオプションの解説</h2></header><div class=entry-content><p>最近、Java を業務で触っている。 門外漢の自分からすると Maven のお作法が分からなかったので、備忘録がてら残しておく。
mvn archetype:generate コマンドのオプションの意味 mvn archetype:generate コマンドを使えば任意のテンプレートに沿ったプロジェクトを一発で作成することができる。
具体例として、Apache Beam でプロジェクト管理ツールである Maven を使って、mvn archetype:generate コマンドを用いて、プロジェクト作成を行う場合、公式サイトでは以下のように指定されている
1 2 3 4 5 6 7 8 9 $ mvn archetype:generate \ -DarchetypeGroupId=org.apache.beam \ -DarchetypeArtifactId=beam-sdks-java-maven-archetypes-examples \ -DarchetypeVersion=2.31.0 \ -DgroupId=org.example \ -DartifactId=word-count-beam \ -Dversion="0.1" \ -Dpackage=org.apache.beam.examples \ -DinteractiveMode=false オプション名 意味 archetypeGroupId archetypeの groupId つまり、テンプレートを提供している作成元の識別子 archetypeArtifactId archetype のテンプレート。個々では beam-sdk に対応したプロジェクトテンプレートを作成している。 archetypeVersion テンプレートのバージョン groupId Java のパッケージ名のルールに則ったすべてのプロジェクトで唯一に識別可能な識別子。今回は org.example が使われており、実際は識別子として機能指定なさそうな名前ではある。(実際昔のプロジェクトでは重複可能な単語が使われていることが多いが、その場合は marven に登録する際に名前が衝突して登録ができないとのこと artifactId 任意の名前が使用可能であり、jar ファイルのバージョン抜きの名前を指定する。プロジェクトのパッケージ名と考えたら良さそう。これが作成されるロートディレクトリのフォルダ名 version プロジェクトのバージョン情報 package クラスやインターフェースの名前空間を指す。基本的に groupid と同一だが、groupid を接頭辞にして、独自に付け足すこともある。 interactiveMode ウィザード形式の生成をするかしないか Reference Guide to naming conventions on groupId, artifactId, and version maven プロジェクトの作成 archtypeArtifactId を指定する package 宣言 | java-code</p></div><footer class=entry-footer><span title='2021-07-18 00:05:06 +0900 +0900'>7月 18, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to mvn archetype:generate でJavaのプロジェクト雛形を作成する際のオプションの解説" href=https://shunyaueta.com/posts/2021-07-18/></a></article><article class=post-entry><header class=entry-header><h2>eコマースの検索と推薦についてのサーベイ論文である 'Challenges and research opportunities in eCommerce search and recommendations' を社内勉強会で発表した</h2></header><div class=entry-content><p>SIGIR eCom を探索していたら発見したサーベイ論文の “Challenges and research opportunities in eCommerce search and recommendations"が面白かったので、社内の勉強会で発表してきた。
和訳すると、「e コマースの検索と推薦における挑戦と研究トピック」で、e コマースにおける検索と推薦の課題が明瞭に書かれていて非常に面白い論文でした。 自分もまだ検索エンジニアとして日が浅いので、手持ちのパターンを増やせるように日々勉強していますが、この論文のおかげでかなり解像度が上がった。
個人的に面白かったのは、
そもそも、顧客が商品を検索するというタスクの奥深さと面白さが知れる Query Understanding は、非構造なクエリを構造化されたクエリに変換するのが究極的な目標 Learn to Rank(LtR)の実践的な課題点として、LtR 適用時に、Native Ranker とのギャップが発生して非連続な検索結果を返してしまうことがある 実際のクエリから、購入される商品はクエリと商品が関連性が高いとは限らないのでモデルを学習させる際には要注意 Amazon での実例として クエリ「ダイヤモンドリング」に対して LtR を適用すると、実際のクエリとそれに紐づくランキングシグナルから学習すると、「ダイヤモンドリング」というクエリで、「ジルコニウムリング」が大量に購買されていたので LtR では、「ダイヤモンドリング」というクエリに対して、「ジルコニウムリング」を表示するようになってしまった これは、学習データを全く見ないで適用するとそうなりそうだけど、広範囲に影響を及ぼす LtR の QA は非常に骨が折れそう Ref: Amazon Search: The Joy of Ranking Products スライド作成元の Markdown ファイルはhurutoriya/deckはこちらです。 スライド内のリンクに簡単にアクセスできます。
e コマースでの検索に改善したいけど何したらいいかわからんという人は、とりあえずこれ見れば OK という論文だったので読めてよかった
余談 Matching & Ranking の章までを解説したけど、それでも 45m 喋りっぱなしで最後のほうがかなり駆け足になってしまった。 また、英語での発表になったけど、やはり熟れたわかりやすい発表レベルに達するには、まだまだだなぁ感じた。精進せねば
今回スライド作成に Marp を使いましたが、VS Code 上でスラスラとかけつつ読みやすくテンションの上がるデザインに簡単にできて感動しました。これからも愛用したいなと思います。
年末くらいに、検索エンジニアとして 9 ヶ月経過するので、役になった学習リソースなどをまとめたい</p></div><footer class=entry-footer><span title='2021-07-10 23:20:58 +0900 +0900'>7月 10, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to eコマースの検索と推薦についてのサーベイ論文である 'Challenges and research opportunities in eCommerce search and recommendations' を社内勉強会で発表した" href=https://shunyaueta.com/posts/2021-07-10/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2021-07-09/images/1.png alt="Streamlit app screen shot"></figure><header class=entry-header><h2>How to get the uploaded file path and processing its file in Streamlit</h2></header><div class=entry-content><p>Motivation Streamlit is a powerful tools to quickliy build the demo application. If we use Streamlit file upload feature via WebBrowser then we need to its file path to process the uploaded file. So I will introduce how to get uploaed file path in Streamlit.
Example We buid the PDF File upload feature in Streamlit and its PDF file convert to image. We use Belval/pdf2image which is a populer PDF converting tool....</p></div><footer class=entry-footer><span title='2021-07-09 22:40:37 +0900 +0900'>7月 9, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to How to get the uploaded file path and processing its file in  Streamlit" href=https://shunyaueta.com/posts/2021-07-09/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2021-07-08/images/1.png alt="Streamlit スクリーンショット"></figure><header class=entry-header><h2>Streamlit でアップロードしたファイルのパスを取得して、特定の処理をする</h2></header><div class=entry-content><p>モチベーション Streamlit は Python code のみで簡単かつ高速に Web アプリを作成できる強力なパッケージ。 Streamplit で作られた Web アプリ経由でファイルをアップロードして、そのファイルを処理したい際の具体的な実現方法がなかったので備忘録がてら残しておく。
PDF ファイルをアップロードして、画像に変換する Web アプリ 具体的に例を交えつつ説明する。 Streamlit を使って、PDF ファイルをアップロードしてアップロードされた PDF ファイルを画像化するアプリを作成する。 今回は、Belval/pdf2image という PDF パッケージを使用する。 このパッケージは処理したい PDF のファイルパスを要求するインターフェースなので今回の実例に沿っていてわかりやすい。 ローカルマシンは MacOS を想定しており、pdf2image はpoppler の事前インストールが必須。
完成形のスクリーンショット GitHub でもコードを公開しておきました。
hurutoriya/streamlist-file-uploader-example
デモ動画はこちら
Demo Movie in Youtube
Makefile Makefile は依存パッケージを事前インストールするために採用
1 2 3 4 5 install: brew install poppler poetry install run: poetry run streamlit run streamlit_pdf_uploader/main.py Poetry for package management 環境構築は poetry を使っています。...</p></div><footer class=entry-footer><span title='2021-07-08 22:40:37 +0900 +0900'>7月 8, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Streamlit でアップロードしたファイルのパスを取得して、特定の処理をする" href=https://shunyaueta.com/posts/2021-07-08/></a></article><article class=post-entry><header class=entry-header><h2>2021年05月時点で自分が実践しているMLOpsの情報収集方法</h2></header><div class=entry-content><p>先日、同僚に「機械学習プロジェクトに興味があるんだけど、おすすめの資料があったら教えてほしい」と言われたので、Blog 記事に現時点でのおすすめの資料としてまとめておいたら、数年後見返したら面白そうだと思ったので記事として公開しておく。
おすすめの資料 プロジェクトマネジメントや考え方、思想 How Google does Machine Learning これは機械学習を実応用する人たちにはぜひ見てほしいビデオ講義。前半が、機械学習プロジェクトの計画や、優先順位、よくあるアンチパターンについて GCP で機械学習について多く関わってきたエンジニアが解説してくれていて、非常に勉強になる。 感想記事 リーン・スタートアップ　ムダのない起業プロセスでイノベーションを生みだす 顧客が求めるものを作ろう。機械学習にこだわったらまずだめなので… (詳しくは後述の Rules of ML を呼んでみよう。) 関連する良いフレームワークとして @nishio さんの機械学習キャンバス もおすすめです。 Make something people want. by Paul Graham 人によって意見が別れるところではありますが、機械学習エンジニアとして、これがなぜ機械学習で必要なのかの「なぜ」を説明できないとたいてい上手く行かない経験がある。つまるところ、必要とされるものを見つけ出して作っていこうぜということですね Netflix がカスタマーを誰よりも理解するためのデータ分析プロセス、コンシューマー・サイエンスの紹介 カスタマーオブセッションの考え方を、常に心のなかに秘めつつ世の中を良くするプロダクトを作りたい MLOps, 機械学習エンジニアリング Rules of Machine Learning 全員これを毎日読もう。聖書 仕事ではじめる機械学習 第 2 版 MLCT 創始者の @chezou さんが筆頭に書き上げた実践的な機械学習本。日本人で機械学習をやりたいならまずこれを買うべし。 AI アルゴリズムマーケティング 自動化のための機械学習/経済モデル、ベス トプラクティス、アーキテクチャ 邦訳だとべらぼうに怪しい感じになってしまっているが、内容はとんでもなく素晴らしい。マーケティングのために機械学習を適用することが多いと思うが、かなり網羅的に適用例を解説してくれている。原著の英語は無料なので、中身が気になる人はそちらをおすすめする。無料公開偉大すぎる MLOps: 機械学習における継続的デリバリーと自動化のパイプライン GCP による MLOps の解説。人によって、MLOps の定義って差異がありますが、自分はここで語られている ML システム構築のすべてのステップで自動化とモニタリングを推進できます こそが、 MLOps の骨子だなと思っています。クラウドサービスは、開発に関係する知識をパターン化して、資料を公開してくれるのでありがたいですね。 Google Cloud で機械学習を実装するためのベスト プラクティス この資料なんかは、GCP で機械学習を実践したい場合にはまず見ておけば困ることはなさそうですね 各クラウドサービスの MLOps の white paper AWS, Azure は普段使わないので深く言及しませんが、同様の資料は公開されたりしています。 Practitioner Guide to MLOps by GCP MLOps: Continuous Delivery for Machine Learning on AWS Azure Best practices for MLOps - DevOps for machine learning....</p></div><footer class=entry-footer><span title='2021-05-29 22:32:58 +0900 +0900'>5月 29, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 2021年05月時点で自分が実践しているMLOpsの情報収集方法" href=https://shunyaueta.com/posts/2021-05-29/></a></article><article class=post-entry><header class=entry-header><h2>Poetry からsetup.py を自動生成する</h2></header><div class=entry-content><p>現状の Poetry では、pyproject.toml を基にした setup.py の直接的な自動生成をサポートしていない。
Support generation of poetry manged setup.py file #761
え？なんで setup.py が必要なんですか? poetry build で生成される source と wheels で事足りるんじゃないですかというツッコミがあると思います。
PyPI や Jflog などでホストせずに、GitHub のリポジトリでパッケージを管理したり、特定のサブディレクトリをパッケージとして扱う際には、未だ setup.py での依存関係の記述が必要です。
Poetry による実現方法 poetry build コマンドと Makefile を組み合わせることで、pyproject.toml に対応した setup.py の自動生成ができるのでそれを採用します。 コマンドはGitHub のissues でのコメントを参考にしました。 1
1 2 3 4 5 6 7 8 9 10 11 12 # package name PACKAGE = lib .PHONY: build-package build-package: ## Generate setup.py by poetry command for shared package poetry build # source ....</p></div><footer class=entry-footer><span title='2021-05-23 23:42:28 +0900 +0900'>5月 23, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Poetry からsetup.py を自動生成する" href=https://shunyaueta.com/posts/2021-05-23/></a></article><article class=post-entry><header class=entry-header><h2>KyTeaをPythonで扱えるMykyteaを使うために必要なこと</h2></header><div class=entry-content><p>テキスト解析機 KyTea KyTeaを業務で使う機会があり、Python wrapper である Mykytea を使ってみたのですが、poetry や pip で Mykytea をインストールするだけでは、
Library not loaded: /usr/local/lib/libkytea.0.dylib in version = "0.1.5"
上記のエラーが出力され、KyTea を使うことができませんでした。 Mykytea のリポジトリに issue 1 を立てて、@chezou さんにお聞きしてみたところ、
Good point. Mykytea wheel assumes that kytea is installed under /usr/local/lib, while your kytea exists another place. This should be Mykytea issue and there are two options we can avoid it like:
Use delocate, like Linux’s audit-wheel https://realpython.com/python-wheels/ Install from source using wheel instead....</p></div><footer class=entry-footer><span title='2021-05-19 22:04:41 +0900 +0900'>5月 19, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to KyTeaをPythonで扱えるMykyteaを使うために必要なこと" href=https://shunyaueta.com/posts/2021-05-19/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://shunyaueta.com/posts/2021-05-12/images/1.png alt=検索チームの成熟度ピラミッド></figure><header class=entry-header><h2>[抄訳] 検索エンジンの達成度と検索チームの成熟度モデル</h2></header><div class=entry-content><p>@rilmayer_jp さんのツイート をきっかけに、検索チームの成熟度モデルの存在を知りました。ありがとうございます!
Eric Pugh さんが、検索エンジンに関する会議で公演した内容で、検索チームがどのように成熟していくかをモデル化しており、それが面白かったので備忘録として残しておく
更新 2021/05/13 : 原著者のEric Pugh さんから、抄訳のご快諾いただけました。ありがとうございます 翻訳元資料 Search Relevance Organizational Maturity Model slide Haystack LIVE! 2020 Search Relevance Organizational Maturity Model 検索エンジンのレベル 検索エンジンへの要求をどれだけ満たしているかをピラミッド構造でわかりやすく説明している
検索チームの成熟度モデル 7 項目の検索チームの評価項目を考え、3 段階で評価を行う
ビジネス 顧客の要求の理解 検索技術 実験駆動 UX コンテンツ強化 データ保有 発展 ステークホルダーがリアルタイム KPI を使用している データ解析から質的なデータを得ている カスタムプラグインを作成している A/B テスト、オフラインテストをサポートしている 革新的な発見性を提供している(chatbot, 等) NLP やデータサイエンティストの専任チームが取り組んでいる 多種多様な、複雑かつ大規模なデータを扱っている 実践 不定期にレポートを行っている いくつかのユーザーテスト、基礎的な分析を行っている 関連性のための複雑な設定、プラグインの使用をしている 実験は適用可能だが、A/B テストなどはできない 発見しやすくするための UI を提供している 分類学や概念体型の適用をしている データの複雑度の監視している 基礎 ビジネスインパクトが測定されていない クエリログは存在しない、またはユーザーテストを行っていない 技術スタックを適度に調整している 検索のテストは手作業で行い、デプロイは低頻度 1 ページに 10 個の検索結果がある 僅かな取り組み(シノニムなど) とても単純なデータモデル 感想 ひと目で...</p></div><footer class=entry-footer><span title='2021-05-12 22:33:23 +0900 +0900'>5月 12, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to [抄訳] 検索エンジンの達成度と検索チームの成熟度モデル" href=https://shunyaueta.com/posts/2021-05-12/></a></article><article class=post-entry><header class=entry-header><h2>Pythonで、変数を挿入して柔軟にSQLクエリを構築する</h2></header><div class=entry-content><p>データ処理のタスクをこなしていると、Python で SQL に変数を挿入し柔軟に SQL クエリを構築したくなる。 例えば、
中間テーブルを作るために Airflow などで定期的なジョブを実行し、SQL の createdの時間を当日のものに変更する training, dev, test でデータを分割する際に、createdの条件を変更して 3 パターンのデータを取得する などが考えられる。
変数を SQL に組み込んで実行したい際には、kayak/pypikaのような SQL builder もあるが、個人的に可読性が悪くなったり、SQL クエリの作成のためだけに余計なパッケージをいれたくない。そのためパッケージを入れずにシンプルに完結する方法をここでは紹介する。
編集履歴 2021/05/12: twitter で docstring ではなく string literal ですよという指摘をいただき修正 ref 2021/05/12: twitter での意見を反映 1. 単なる文字列として SQL クエリを構築 1 2 3 def get_guery(num: int, category: str): sql=f"SELECT field1, field2, field3, field4 FROM TABLE WHERE condition1={num} AND condition2={category}" return sql f-string で文字列に変数を挿入して、SQL クエリを構築 だが、
SQL が長くなると PEP8 に準拠せず、E501 line too longに抵触する 視認性が低く、SQL クエリの実行内容を理解しづらい 2....</p></div><footer class=entry-footer><span title='2021-04-29 22:52:25 +0900 +0900'>4月 29, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Pythonで、変数を挿入して柔軟にSQLクエリを構築する" href=https://shunyaueta.com/posts/2021-04-29/></a></article><article class=post-entry><header class=entry-header><h2>機械学習エンジニアから検索エンジニアに転生</h2></header><div class=entry-content><p>2018 年 2 月に株式会社メルカリに機械学習エンジニアとして入社したが、来月から同社で検索エンジニアに転向する。
機械学習エンジニアとして 入社してからは主に、機械学習により同社での Cusotmer Service 分野の業務効率化を推進してきた。 この３年間で、実応用を前提にした機械学習プロジェクトの1→100と０→１の両者を体験できたのは得難い経験だった。
主に 2 つのプロジェクトを取り組んできたが、そのうちの一つである「機械学習による商品監視プロジェクト」の成果を論文として公開 できたのは、自分としても嬉しい。
この領域はTrust and Safaty とよばれており、少し聞き慣れない言葉かもしれないが、サービスを提供するにあたって必須となるサービスの信頼と安全性を高めることを指す。 少し前に Pinterest がこの領域でのカンファレンスを開催していて、この領域での機械学習の需要がなくなることは無いだろうなと改めて確信を得た。
Trust & Safety Machine Learning Summit なぜ検索エンジニア? キャリアで新しいことに挑戦したい時、鶏と卵問題は必ず発生すると思っている。 例えば機械学習エンジニアが募集されているが、実務経験を重視されるので未経験だとそもそもポジションに就くことができない。
データとエンジニアリングの重なる領域でもう一段スキルと経験を深めたいなと検討した領域が検索領域だったが、一般的な募集では検索領域での経験 N 年以上を求むというものが多く、未経験での転職の壁はなかなかに厳しい。 なら内部での異動はどうだろうと去年の年末に希望を出してみたところ、まずは実験的にチーム異動してみようという話になった。 2 月の半ばから 1 ヶ月程度検証期間を経て問題なしということで、ひとまず正式に検索チームに異動できることになった。
身内贔屓というわけではないが、世界的に見ても同社の検索チームは領域的にもチームとしても凄くエキサイティングだと思っている。 まさに情熱プログラマーでも提唱されている、一番の下手くそでいようが実践できるので凄くワクワクしている。
一番の下手くそでいよう by 情熱プログラマー ソフトウェア開発者の幸せな生き方 扱う技術スタックなどは変わりますが、僕の中では根本的にデータとエンジニアリングにどっぷり浸かる点では機械学習エンジニアだろうと検索エンジニアだろうと変わりはしないので、着実に経験を積んでいきたいなと思っています。
直近の目標としては 2 年以内に、検索領域で自分の興味とマッチする査読付き国際会議
KDD CHIIR SIGIR ecom ECNLP OpML MLSys に採択されるような、Novel な(今までにない)成果を出したい</p></div><footer class=entry-footer><span title='2021-03-27 23:54:15 +0900 +0900'>3月 27, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 機械学習エンジニアから検索エンジニアに転生" href=https://shunyaueta.com/posts/2021-03-27/></a></article><article class=post-entry><header class=entry-header><h2>pipenv でローカルパッケージが正常にインストールされないときの対処法</h2></header><div class=entry-content><p>TL; DR; pip install pipenv==2018.11.26 をすれば直った!!!!! 実行環境 1 2 $pipenv --version pipenv, version 2020.11.15 直面した問題 1 2 3 4 5 ./app/ ├── model │ └── setup.py └── serving └── Pipfile のような構成で、modelというローカルパッケージを作成しており、serving 直下の Pipfile は、model を読み込んで setup.py に記述されている依存パッケージもインストールするようにしたい。
serving ディレクトリで、以下のコマンドを入力すればローカルパッケージが pipenv によりインストールされるはずだが
1 pipenv install --editable ../model 依存関係をすべて記述するはずの Pipenv.lock には、modelのパスのみが記述され、ローカルパッケージが要求する依存パッケージが記述されていない。
原因を探してみたところ、
Installing a local package with pipenv install ‘-e .’ doesn’t save dependencies #1024
同じ GitHub issue を発見しダメ元で pipenv を以下のコマンドでダウングレードして見たところ
1 pip install pipenv==2018....</p></div><footer class=entry-footer><span title='2021-03-13 21:43:44 +0900 +0900'>3月 13, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to pipenv でローカルパッケージが正常にインストールされないときの対処法" href=https://shunyaueta.com/posts/2021-03-13/></a></article><article class=post-entry><header class=entry-header><h2>2021年の目標</h2></header><div class=entry-content><p>ちょっと遅れましたが、最近これからどうするかについて色々と考えていて、その過程で 2021 年をどう過ごすかが決まったのでメモ。
3 つの目標 Senior Software Engineer として確固たる実力を身につけることに集中 英語にふれることを習慣化 SNS を断ち自分にとって後悔の無い時間を歩む Senior Software Engineer として確固たる実力を身につけることに集中 まず Senior Software Engineer と自分で胸を張って宣言できる実力を身につける。
最近は自分のキャリアを専門性を深堀りしていくとして、Software Engineer としての道を進んでいくことを決めた。
上記の理由として自分は Data Scientist になれるほど専門性は無いと思っている。Ph.D は持っていないし、再び博士課程に行くとしても研究は辛い。また Machine Learning Engineer や Data Scientist に特化していっても将来 30 年間働いていく中でそれらのレッドオーシャンで自分が生き抜いていけるとは思えない。 Software Engineer という職に飽きるまでは Product Manager のキャリアも考えなくていいと思った。その葛藤を同僚の人にも相談したが、輝かしい経歴(アメリカの有名大学で CS 専攻 →SWE→PM と天上人)の人でも、手を動かせなくなることに焦りを感じるよとおっしゃられていて非常に共感した。
何よりも今の会社に入社して 3 年間働いてきた経験で身に染みたのは、Software Engineer という職業が楽しいなと思えた。もちろんデータ分析や機械学習などにも楽しさがあるが、それらを最大限レバレッジをきかせるためには Software Enginner としてのスキルが必須であり、最終的に物事を実現するためも避けて通れないと考えている。
2020/03 からは新しいキャリアを見据えて少し方向転換をしてみることした。機械学習やデータ分析領域からは離れないが、チームが変わって扱う領域も異なるので非常に楽しみだ。弱くてニューゲームを楽しもう
英語にふれることを習慣化 最近親しい人と雑談する際に言葉に出すようにしているんですが、国外で働くことに憧れを持っています。業務でも英語を使ったコミュニケーションはムラがあるが 4-7 割ほどになっている。今の段階はコミュニケーションはできるが、細かいミスや相手にネイティブレベルのスピードで喋られると脳みそがストップして働くのをやめてしまうのでなんとかしたい。社内の語学学習強化トレーナーの人に相談したところ、やはり毎日時間を積み重ねていくしかないと言われたので、今の気合でしゃべる英会話から自分でも自信を持てる英語の力を持っていきたいなと思っている。
SNS を断ち自分にとって後悔の無い時間を歩む これは自戒なのですが、何気なくボーッとして SNS を見たりする時間が多く、学生の頃にあったひたむきに何かを学び積み重ねていくという習慣が消え去ってます。やばい。集中力も継続できないしで焦燥感だけが高まり行動ができていないという悪循環です。
人生を楽しむためにも、貴重な時間を自分自身に自信を持って過ごせるように後悔のない時間を歩んでいきたい。
これらの目標は toggl で追跡しておいて、毎月どれくらいの時間を費やせたかを振り替えれるようにする 学び続けるということを習慣化したいので、学びの内容が可視化されるように、これからは学んだことを文章化して気軽に Blog 記事としてアウトプットしていく</p></div><footer class=entry-footer><span title='2021-03-05 20:42:15 +0900 +0900'>3月 5, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to 2021年の目標" href=https://shunyaueta.com/posts/2021-03-05/></a></article><article class=post-entry><header class=entry-header><h2>GKE 上にて Pythonで logger.info() を行うとCloud logging では stderr に保存され、すべてエラーになる問題への対処法</h2></header><div class=entry-content><p>Python のアプリケーションで、Cloud logger にログを出力したいときに
標準の Python logging モジュールを利用して、ログを出力する Python Cloud Logging package を使用する 上記の２つの方法があります。
不必要にパッケージを増やしたくはないので、1 の標準モジュールで Cloud Logger へ出力できないか試してみました。
標準の Python logging モジュールを試す 標準の logging モジュールでログを出力したいときに
1 2 3 4 5 6 import logging logger = logging.getLogger(__name__) def hoge(): logger.info('logging Start 2021') と、logging.info() を仕込んで、Cloud logger にログを出力してみると、logger.info() で出しているはずなのに、Cloud logger 上ではすべてエラーとして扱われてしまっています。
原因を特定するために、logger のログを見てみると logger.info() がすべて stderr標準エラーストリームへ出力されてしまっています。
1 2 3 4 5 6 7 8 9 10 11 12 { "textPayload": "2021-02-20 21:26:51,012 - root:predict:36 - INFO: logging Start\n", ....</p></div><footer class=entry-footer><span title='2021-03-03 00:29:03 +0900 +0900'>3月 3, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to GKE 上にて Pythonで logger.info() を行うとCloud logging では stderr に保存され、すべてエラーになる問題への対処法" href=https://shunyaueta.com/posts/2021-03-03/></a></article><article class=post-entry><header class=entry-header><h2>GKE でローリングアップデート後、ローカルからポートフォワードでリクエストを投げるとcurl: (52) Empty reply from server と返ってくるときの対処方法</h2></header><div class=entry-content><p>前提 ローカルからkubectlでポートフォワードして、GKEにリクエストを投げて確認を行っている
発生した問題 deployment のローリングアップデート前は問題なくポートフォワードを通してリクエストが返っていた。コードに変更を加えてGKE上でも確認をしたかったので、まずローカルで確認をして問題がなかった変更が、ローリングアップデート後ポートフォワード でGKE にリクエストを投げると curl: (52) Empty reply from server と返ってくる。
TL; DR; ポートフォワードはrolling update が終わったら貼り直そう。なぜなら、ポートフォワードの接続先はローリングアップデート前後で変化するため。 エラーメッセージ curl でリクエストした際のメッセージ
1 curl: (52) Empty reply from server Port foward の出力
1 uid : Error: No such container: xxxxx まとめ 発生していた問題は、ローリングアップデートを行うと、ポートフォワードの接続先が変更され、その際にローリングアップデート前後でkubectl でのポートフォワードは固定されたままなのでリクエストはサーバーから返ってこないという説明するのも恥ずかしい問題でした。
理由は単純だけど、気づくのに時間がかかってしまった。k8s の動きを理解していないからこういうので時間を溶かしてしまった。反省
Appendix Performing a Rolling Update</p></div><footer class=entry-footer><span title='2021-02-21 23:46:30 +0900 +0900'>2月 21, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to GKE でローリングアップデート後、ローカルからポートフォワードでリクエストを投げるとcurl: (52) Empty reply from server と返ってくるときの対処方法" href=https://shunyaueta.com/posts/2021-02-21/></a></article><article class=post-entry><header class=entry-header><h2>Standard SQLで 列と列の組み合わせの数を集計したい</h2></header><div class=entry-content><p>group by は集計作業において根幹となる処理ですが、少し手の混んだ集計をしたいときに毎回調べていることが多かったのでここに学んだことをまとめておく
今回やりたいことは
A 列が α になっている行の B 列の種類を集計したい
です。
はじめに 実際のデータを用意したほうが、理解が深まるので擬似的なテーブルを作成する。 テーブルのデータの概略として、何日に sender (送信者) が receiver (受信者) にいくら送金(price)したかを格納しているテーブルとする。
StandardSQL は WITH を使って簡単にモックテーブルを作れるのが良いところ。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 #standardSQL WITH `transactions` AS ( SELECT 'A' AS sender, 'B' AS receiver, 600 AS price, '2020-01-01' AS day UNION ALL SELECT 'A', 'B', 1200, '2020-01-01' UNION ALL SELECT 'A', 'B', 600, '2020-01-01' UNION ALL SELECT 'A', 'C', 2000, '2020-01-01' UNION ALL SELECT 'A', 'D', 3000, '2020-01-01' UNION ALL SELECT 'A', 'D', 2000, '2020-01-01' UNION ALL SELECT 'B', 'C', 700, '2020-01-01' UNION ALL SELECT 'B', 'C', 300, '2020-01-01' UNION ALL SELECT 'B', 'D', 250, '2020-01-01' UNION ALL SELECT 'A', 'B', 400, '2020-01-02' UNION ALL SELECT 'A', 'B', 1000, '2020-01-02' UNION ALL SELECT 'A', 'B', 1200, '2020-01-02' UNION ALL SELECT 'A', 'B', 2000, '2020-01-02' UNION ALL SELECT 'B', 'C', 450, '2020-01-02' UNION ALL SELECT 'B', 'C', 500, '2020-01-02' ) SELECT * FROM transactions sender receiver price day A B 600 2020-01-01 A B 1200 2020-01-01 A B 1800 2020-01-01 A C 2000 2020-01-01 A D 3000 2020-01-01 A D 2000 2020-01-01 B C 700 2020-01-01 B C 300 2020-01-01 B D 250 2020-01-01 A B 400 2020-01-02 A B 1000 2020-01-02 A B 1200 2020-01-02 A B 2000 2020-01-02 B C 450 2020-01-02 B C 500 2020-01-02 列と列の組み合わせの数を集計する 日次ごとに送金者が何人に送ったかを集計したい、つまり(sender, receiver)のペアを考えて、sender を固定した上で何人に送金したいかを集計したとする。 上記のデータだと...</p></div><footer class=entry-footer><span title='2021-02-09 23:27:37 +0900 +0900'>2月 9, 2021</span>&nbsp;·&nbsp;Shunya Ueta</footer><a class=entry-link aria-label="post link to Standard SQLで 列と列の組み合わせの数を集計したい" href=https://shunyaueta.com/posts/2021-02-09/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://shunyaueta.com/posts/page/4/>«&nbsp;前へ&nbsp;</a>
<a class=next href=https://shunyaueta.com/posts/page/6/>次へ&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2023 <a href=https://shunyaueta.com/>hurutoriya</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>