<!doctype html><html lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme-color content="dark"><title>[抄訳] What’s your ML test score? A rubric for ML production systems | Shunya Ueta</title><link rel=stylesheet href=/sass/main.min.1a3290acca8b3fc92df85ea9200859476d6c80d59009c89a749300f3ce7a7a67.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-39994406-11','auto');ga('send','pageview');}</script><meta property="og:title" content="[抄訳] What’s your ML test score? A rubric for ML production systems"><meta property="og:description" content="NIPS206にて開催された Reliable Machine Learning in the Wild - NIPS 2016 Workshop (2016) という、現実世界でどうやって信頼性の高い機械学習に取り組んでいくかについてのワークショップがある
ここで Google から発表された What’s your ML test score? A rubric for ML production systems がとても面白く、身になるものが多かったのでメモがてら抄訳を残しておく
 PDF Slide 発表動画もワークショップページにて公開されています。  change logs  2021-04-25  この原著論文の完全版になっている論文の抄訳を新たに公開しています。 [抄訳]: The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction     概略  現実世界のプロダクションシステムで機械学習を使う際に、機械学習の研究実験と異なる、小さなもしくは小さくない問題がある テストとモニタリングはプロダクションレディの機械学習システムにとって必要不可欠 しかし、どれくらいのテストとモニタリングをすれば十分と言えるのだろうか? この論文では、それらの問題を解決する ML Test Score という基準を提案する  Introduciton   Google 内で積み重ねたベストプラクティスをもとに、実行可能なテスト、そしてその機械学習システムがどのていどプロダクションレディなのかを示すスコアシステムを提案"><meta property="og:type" content="article"><meta property="og:url" content="https://shunyaueta.com/posts/2020-04-19/"><meta property="og:image" content="https://shunyaueta.com/ogp.jpg"><meta property="article:published_time" content="2020-04-19T22:18:10+09:00"><meta property="article:modified_time" content="2021-11-26T23:07:54+09:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shunyaueta.com/ogp.jpg"><meta name=twitter:title content="[抄訳] What’s your ML test score? A rubric for ML production systems"><meta name=twitter:description content="NIPS206にて開催された Reliable Machine Learning in the Wild - NIPS 2016 Workshop (2016) という、現実世界でどうやって信頼性の高い機械学習に取り組んでいくかについてのワークショップがある
ここで Google から発表された What’s your ML test score? A rubric for ML production systems がとても面白く、身になるものが多かったのでメモがてら抄訳を残しておく
 PDF Slide 発表動画もワークショップページにて公開されています。  change logs  2021-04-25  この原著論文の完全版になっている論文の抄訳を新たに公開しています。 [抄訳]: The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction     概略  現実世界のプロダクションシステムで機械学習を使う際に、機械学習の研究実験と異なる、小さなもしくは小さくない問題がある テストとモニタリングはプロダクションレディの機械学習システムにとって必要不可欠 しかし、どれくらいのテストとモニタリングをすれば十分と言えるのだろうか? この論文では、それらの問題を解決する ML Test Score という基準を提案する  Introduciton   Google 内で積み重ねたベストプラクティスをもとに、実行可能なテスト、そしてその機械学習システムがどのていどプロダクションレディなのかを示すスコアシステムを提案"></head><body class=dark><nav class=navbar><div class=container><div class=flex><div><a class=brand href=/><span class=emoji>🦅</span>
Shunya Ueta</a></div><div class=flex><a href=/articles/>All posts</a>
<button id=dark-mode-button></button></div></div></div></nav><main><div class=container><article><header class=article-header><div class=thumb><div><h1>[抄訳] What’s your ML test score? A rubric for ML production systems</h1><div class=post-meta><div>By on <time>2020.04.19</time> (Last updated:<time>2021.11.26</time>)</div><div class=tags><a href=/tags/machinelearning/>machinelearning</a>
<a href=/tags/mlops/>mlops</a>
<a href=/tags/translation/>translation</a>
<a href=/tags/paper/>paper</a></div></div></div></div></header></article><div class=article-post><p>NIPS206にて開催された <a href=https://sites.google.com/site/wildml2016nips/>Reliable Machine Learning in the Wild - NIPS 2016 Workshop (2016)</a> という、<code>現実世界でどうやって信頼性の高い機械学習に取り組んでいくか</code>についてのワークショップがある</p><p>ここで Google から発表された <code>What’s your ML test score? A rubric for ML production systems</code> がとても面白く、身になるものが多かったのでメモがてら抄訳を残しておく</p><ul><li><a href=https://research.google/pubs/pub45742/>PDF</a></li><li><a href="https://0586f9b3-a-62cb3a1a-s-sites.googlegroups.com/site/wildml2016nips/SculleySlides1.pdf?attachauth=ANoY7crr9fea9TxDl1EIyOScNG1yJSo3gBkHKOq5exLlPtetuZ_EiAKL3rJq7h17Nwcx82i0CSwmLWc97IQiQdTBODUaMhMm7CBrbpU7uQXx55Km8JFTLlOMIgFflPev0_chmj41VydTan6OjwAOfeQ_U5tRz7m-HHi0rKvumbALo74j5XV0NbPaXcQDbwInz3CdFaDamzvwOH7nn0V-DADxKf6Kqkv-SA%3D%3D&attredirects=0">Slide</a></li><li>発表動画もワークショップページにて公開されています。</li></ul><h2 id=change-logs>change logs</h2><ul><li>2021-04-25<ul><li>この原著論文の完全版になっている論文の抄訳を新たに公開しています。</li><li><a href=https://shunyaueta.com/posts/2020-04-25/>[抄訳]: The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction</a></li></ul></li></ul><hr><h2 id=概略>概略</h2><ul><li>現実世界のプロダクションシステムで機械学習を使う際に、機械学習の研究実験と異なる、小さなもしくは小さくない問題がある</li><li>テストとモニタリングはプロダクションレディの機械学習システムにとって必要不可欠</li><li>しかし、どれくらいのテストとモニタリングをすれば十分と言えるのだろうか?</li><li>この論文では、それらの問題を解決する <code>ML Test Score</code> という基準を提案する</li></ul><h2 id=introduciton>Introduciton</h2><ul><li><p>Google 内で積み重ねたベストプラクティスをもとに、実行可能なテスト、そしてその機械学習システムがどのていどプロダクションレディなのかを示すスコアシステムを提案</p></li><li><p>このスコアは、機械学習を始めたばかるのチームからエキスパートがあつまるチームまで幅広く適用可能</p></li><li><p>注意: 一般的なSofware Engineering のベストプラクティスは含んでいない</p></li><li><p>そのかわり、学習とサービングのためのUnit Test Coverage の計算方法など機械学習に必要不可欠な点を抑えている</p></li></ul><h3 id=ml-test-score-の計算方法>ML Test Score の計算方法</h3><ul><li>各テストの加点基準<ul><li>1pt: 手動で実行し、その結果を文章として共有済</li><li>2pt: CIに組み込まれ、自動的に反復実行済</li></ul></li><li>最終的なML Score は以下の基準となる<ul><li><code>0pt</code> : プロダクション向けというよりも研究プロジェクト</li><li><code>1-2pt</code> : テストが少しはされているが、プロダクションではもっと深刻な罠がある可能性あり</li><li><code>3-4pt</code> : 最初のプロダクションレディへの第一歩。しかし、さらなる投資が必要</li><li><code>5-6pt</code> : 適切なテストがされているが、もっと自動化してみよう</li><li><code>7-10pt</code> : 自動化されたテストとモニタリングが整備されている。重要なシステムでも適切なレベルに達している</li><li><code>12+pt</code> : 卓越した自動化されたテストとモニタリング</li></ul></li></ul><h4 id=前提条件>前提条件</h4><ul><li>システムアーキテクチャの前提として、生データから特徴量を抽出し、学習システムに流し込まれる。そして推論のためにサービングされ、その機能は顧客に影響を与える。また、ソースリポジトリやCIを通したテスト、実験のバージョン管理なども可能</li></ul><h3 id=特徴量とデータセット>特徴量とデータセット</h3><ul><li>各特徴量の分布が期待した値になっているか?<ul><li>一例として、特徴Aが1から５の値を持つ、または特徴Bの最も普遍期な値として「Harry」と「Potter」が全体の10%を占める特徴に対して、上記の分布を検証するようなテストを書き、このテストが失敗すれば外部環境が変化しモデルの変更が余儀なくされていることが把握できる</li></ul></li><li>各特徴と目的変数の関係性そして、ペアの相関が独立してあらわれるか?<ul><li>個の特徴がモデルに使用される際に、</li></ul></li><li>各特徴量のコストは検討されているか?<ul><li>特徴量のコストとは、各特徴を追加した際の推論速度やメモリ使用率、データ依存やその特徴に依存することによるシステムの不安定性などの検証</li></ul></li><li>手動作業により適切でない特徴量は使用されていないか?</li><li>データパイプラインの中で、プライバシーコントロールが考慮されているか?<ul><li>Access Controllの権限は機械学習システムにも適切に付与されているか検証を行う</li></ul></li><li>プロダクションモデルへ新しい特徴量を開発・追加するのに必要な日数はいくらか?<ul><li>ちょっとしたアイデアをプロダクションで素早く検証できるチームは強い。速さはシステムの改善と外的変化の対応するために重要</li></ul></li><li>学習とサービングで使用される特徴量生成に関わるコードすべてに対するテスト<ul><li>軽視されがちだが、非常に重要</li></ul></li></ul><h3 id=モデル開発>モデル開発</h3><ul><li>全てのモデルはコードレビューされリポジトリに格納されているか?<ul><li>コードレビューは素晴らしい仕組みで、つまらないミスやインシデントに関わるような潜在的なものまで顕在化される</li></ul></li><li>オフラインの疑似指標とオンライン指標の関係性<ul><li>1%の精度がお客さまの満足度にどうつながるのだろうか? (例: CTR) この検証は意図的に劣化させたモデルなどで小規模のABテストを通して検証可能</li></ul></li><li>チューニング可能な各ハイパーパラメータのインパクト<ul><li>グリッドサーチやベイズ最適化は、予測性能を向上させるだけではなく潜在的な信頼性の問題を明らかにできる。例えば、データ並列によるモデルの精度の変化は大きなインパクトである</li></ul></li><li>モデルのフレッシュネス<ul><li>各モデルが、<code>昨日</code> VS <code>先週</code> VS <code>過去一年</code> で学習されたとしてオンラインメトリクスへの影響がどう異なるか? 全てのモデルは外部要求により更新を余儀なくされる。この評価は注意深く行い、決定していくこと</li></ul></li><li>単純なモデルをベースラインとした検証<ul><li>線形モデルで少ない特徴量のモデルでベースラインを作成しよう。これは、大規模なパイプライン構築やコストのトレードオフの検証にとても効果的である</li></ul></li><li>重要なデータを分割した際のモデル品質の検証<ul><li>TBD: ちょっと理解しきれない</li></ul></li><li>暗黙的バイアスへの検証<ul><li>例えば学習データの規模が小規模だと暗黙的なバイアスが掛かってしまう。(例：靴の認識で、男物の靴しか学習データが収集できず、女性の靴のデータがほぼ存在しないなど)</li></ul></li></ul><h3 id=機械学習インフラストラクチャ>機械学習インフラストラクチャ</h3><ul><li>モデルの学習の再現性検証<ul><li>完全に同一データから、２つのモデル作成する。両者のスコア、サンプルに対する予測結果など大きな違いがないか検証を行う</li></ul></li><li>モデルの仕様に関する検証<ul><li>有用な Assertion として、ロスの低下や学習ジョブがクラッシュしても復元可能など</li></ul></li><li>完全な機械学習パイプラインテストのための結合テスト<ul><li>良い結合テストとは、もとのデータソースから特徴生成→学習 またはサービングまでをカバーする。結合テストはCIを通じて実行し、潜在的な問題をリリース前に発見するために新規リリースの前に検証される</li></ul></li><li>サービング前のモデル品質テスト<ul><li>サンプルデータを収集して、既知の正しい出力がされるように検証を行い、集計する。前バージョンの機械学習モデルの予測結果を用いたバックテストも良い取り組みの一つ</li></ul></li><li>一つのサンプル、もしくはバッチ学習をモデルに送り、学習から予測までの内部状態の変化の監視<ul><li>少量のデータを流し、モデルの状態がどう変化するか監視するのは有効なアプローチである</li></ul></li><li>本番環境前のカナリアリリースによるテスト<ul><li>モデリングのコードはサービングコードと違って高頻度に変化する。それにより以前のサービングコードが新しいモデルでは動かない危険性なども発生する。この検証は、モデルがプロダクション環境下で正常に読み込まれるか、正常に推論が稼働するかを確認する。新しいモデルは常に、小規模のトラフィックで検証されるべきである</li></ul></li><li>新しい機械学習モデルから、安全かつ素早く前の機械学習モデルに戻せるか?<ul><li>モデルのローロバック手続きは、モデルの品質がリリースして良くなかった場合にとても便利である。以前の高性能なモデルに安全かつ素早く戻せる仕組みは非常に重要</li></ul></li></ul><h3 id=機械学習のためのモニタリング指標>機械学習のためのモニタリング指標</h3><ul><li>学習とサービングの両者にて、イベント発生時の不安定性の検証<ul><li>モデルの学習時間の不安定性<ul><li>モデルが頻繁に再学習されるような状況だと、特に問題である</li></ul></li><li>サービング時間の不安定性<ul><li>client側の通信が停止したりすると、サービングされなくなる</li></ul></li></ul></li><li>学習時とサービング時でデータの不変性が保持されているかの検証<ul><li>例: 特徴量AとBは、常に同一の非零の要素数を持つ、特徴量Cは常に(0,100)の範囲内かなどを検証する</li></ul></li><li>学習したモデルと、サービングの特徴が完全に同一か<ul><li>Training/ Serviing Skewとも呼ばれている (Concept Drift とも呼ばれています)</li><li>学習したデータと本番のデータが時間経過、もしくは何らかの要因で異なってしまうことが原因です</li><li>Training/Serving skew は <a href=https://github.com/tensorflow/tfx/blob/master/docs/guide/tfdv.md>TFX</a> で説明されている</li></ul></li><li>モデルは枯れているか?<ul><li>使い古されたモデルの場合、パイプラインが停止しても何が原因か把握できる。例えば、重要なテーブルを作成する毎日実行されるジョブがストップしたとして、どんなアラートが良いだろうか? *TBD</li></ul></li><li>学習、もしくはサービング時に<code>NaN</code> または、 無限の値が発生への検証</li><li>学習速度、サービング速度、スループットまたはRAM使用率の検証<ul><li>スケールする際に機械学習システムの計算機コストは悩みの種である。回帰テストなどで検証されるべきである</li></ul></li><li>予測品質に関する回帰テスト<ul><li>外部要因により、品質が下がる場合もあるが多くの場合は現実の問題を洗い出すことができる</li></ul></li></ul><hr><p>翻訳でもし怪しい部分やもっとこうしたほうが良いというところがあれば、コメントか <a href=https://twitter.com/hurutoriya>@hurutoriya</a> までご連絡ください</p><p>実際読み込んで見ると、信頼性の高いモデルを届けるための秘訣が山盛りで、流石Google 内のベストプラクティスをまとめたチェックリストですね
実際に開発している機械学習システムがあれば、このTest Scoreを当てはめてみてどの程度できているのか計測してみるが良さそう</p><p>機械学習エンジニアとして働く自分として、興味のど真ん中にあるようなワークショップだが、2016,2017年の二回しか開催されておらず、非常に悲しい
逆に言えば、すごく先進的なワークショップでこのワークショップが開催されていたのは凄いことだ</p><p>自分が業務を通じて得た経験やベストプラクティスがすでに論文として世の中に公開されているのはすごく経験になるし、経験から学ぶのではなく歴史から学べるようにしていきたい！
なので、このWorkshop 論文の発展形として、<a href=https://research.google/pubs/pub46555/>The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction</a> が発表されているので読んでみる</p><p>いや、実装寄りの勉強ばかりしていたけど気分転換にこういう知識体系に触れるのすごく楽しい</p></div><h2>See Also</h2><ul><li><a href=/posts/2018-04-09/>Google, Facebookが提供する機械学習基盤まとめ</a></li><li><a href=/posts/2018-04-24/>[抄訳] Data engineers vs. data scientists</a></li><li><a href=/posts/2018-01-14/>Analyzing Free-standing Conversational Groups: A Multimodal Approach (ACMMM15) を読んだ</a></li><li><a href=/posts/2018-01-12/>Call center stress recognition with person-specific models を読んだ</a></li><li><a href=/posts/2018-01-11/>FUSE: Full Spectral Clustering(KDD2016) を読んだ</a></li></ul></div><div class=container><nav class="flex container suggested"><a rel=prev href=/posts/2020-04-18/ title="Previous post (older)"><span>Previous</span>
CourseraでHow Google does Machine Learning の講義を修了した</a>
<a rel=next href=/posts/2020-04-25/ title="Next post (newer)"><span>Next</span>
[抄訳] The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction</a></nav></div></main></main><footer class="footer flex"><section class=container><nav class=footer-links><a href=/index.xml>RSS</a></nav></section><script async src=/js/features.min.a94f58a30ad2560de728e080d87f75c60cf806fd1b3d5f4815f1a1a02c0d1859.js></script></footer></body></html>