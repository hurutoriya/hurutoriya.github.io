<!doctype html><html lang=ja prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=description content><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=keywords content="Deep Learning,
Clustering,
日本語,
Machine Learning,"><meta property="og:type" content="article"><meta property="og:description" content><meta property="og:title" content="“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ"><meta property="og:site_name" content="Shunya UETA"><meta property="og:image" content><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content><meta property="og:image:height" content><meta property="og:url" content="https://shunyaueta.com/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/"><meta property="og:locale" content="ja"><meta property="article:published_time" content="2017-12-23"><meta property="article:modified_time" content="2017-12-23"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Clustering"><meta property="article:tag" content="日本語"><meta property="article:tag" content="Machine Learning"><meta name=twitter:card content="summary"><meta name=twitter:site content="@hurutoriya"><meta name=twitter:creator content="@hurutoriya"><meta name=twitter:title content="“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ | Shunya UETA"><meta name=twitter:description content="自己符号化器とSpectral Clusteingの関連性を示した論文
 Paper link  Author  Fei Tian : http://home.ustc.edu.cn/~tianfei/ 中国科学技術大学Ph.D １年生 MSRAと共同研究、2014年にAAAI2本,COLING1本を1stで通してる。 この資料も面白い。ILSVRC2015で152層のDeep Residual Learningを提案し優勝(Error Rate : 3.57%) MSRA @ ILSVRC2015 資料  Motivation (研究背景・動機)  Deep Learningが数多くの応用でめざましい成果をあげている。 音声認識 画像認識 自然言語処理 DLにおいて Clustering に関する適切な調査が行われてない。 論文の目的として、DLにおける Clusteringの調査を行う  概要  Graph Clusteringはクラスタリングでも重要な手法の一つ Graph Clusteringの応用 Image segmentation Community Detection VLSI Design 嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能 自己符号化器とSpectral Clusteringの類似性 Spectral Clustering : グラフラプラシアンに対してEVDを行いk本の非零固有ベクトルを用いた空間に対してk-meansを行ったもの。 自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する 計算量 : 対象とするグラフはn個のノードを持つ EVD : ナイーブに実装するとO(n3)の計算量、最速の実装はO(n2)の計算量 自己符号化器 : ノードがスパースな際は計算量はO(kn) スパース表現 : データが大きくなるならスパース性を有効活用したい EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない 自己符号化器 : スパース性を用いるのは容易  既存研究で未検証な事柄、何を解決・解明したいのか？  Graph Clusteringのための Deep Learning の活用方法と調査 自己符号化器とSpectral Clusteringの類似性とその比較・検証  Method (提案手法)  GraphEncoder(for graph clustering.|"><meta name=twitter:image:src content><meta name=twitter:domain content="https://shunyaueta.com/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/"><title>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ</title><link rel=canonical href=https://shunyaueta.com/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/><link rel=stylesheet href=https://unpkg.com/tachyons@4.11.1/css/tachyons.min.css><link rel=stylesheet href=https://shunyaueta.com/css/style.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/highlightjs@9.12.0/styles/github-gist.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-39994406-11','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon href=/apple-touch-icon.png></head><body lang=ja class="sans-serif w-90 w-60-ns center center-ns mv2 mv5-ns" itemscope itemtype=http://schema.org/Article><span class=b>/</span>
<a href=https://shunyaueta.com/ class="b bb bw1 pb1 no-underline black">Shunya UETA</a><section id=main class=mt5><h1 itemprop=name id=title>“Learning Deep Representations for Graph Clustering (AAAI2014)” を読んだ</h1><article itemprop=articleBody id=content class="w-90 lh-copy"><p>自己符号化器とSpectral Clusteingの関連性を示した論文</p><ul><li><a href=http://research.microsoft.com/pubs/226627/%5BAAAI2014%5D%20DNN%20for%20Graph%20Cut.pdf>Paper link</a></li></ul><h4 id=author>Author</h4><ul><li>Fei Tian : <a href=http://home.ustc.edu.cn/~tianfei/>http://home.ustc.edu.cn/~tianfei/</a></li><li>中国科学技術大学Ph.D １年生</li><li>MSRAと共同研究、2014年にAAAI2本,COLING1本を1stで通してる。</li><li>この資料も面白い。ILSVRC2015で152層のDeep Residual Learningを提案し優勝(Error Rate : 3.57%)</li><li><a href=http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf>MSRA @ ILSVRC2015 資料</a></li></ul><h4 id=motivation-研究背景動機>Motivation (研究背景・動機)</h4><ul><li>Deep Learningが数多くの応用でめざましい成果をあげている。</li><li>音声認識</li><li>画像認識</li><li>自然言語処理</li><li>DLにおいて Clustering に関する適切な調査が行われてない。</li><li>論文の目的として、DLにおける Clusteringの調査を行う</li></ul><h4 id=概要>概要</h4><ul><li>Graph Clusteringはクラスタリングでも重要な手法の一つ</li><li>Graph Clusteringの応用</li><li>Image segmentation</li><li>Community Detection</li><li>VLSI Design</li><li>嬉しい点 : ベクトル空間におけるクラスタリングの問題 → データの類似度グラフ問題への変換が可能</li><li>自己符号化器とSpectral Clusteringの類似性</li><li>Spectral Clustering : グラフラプラシアンに対してEVDを行いk本の非零固有ベクトルを用いた空間に対してk-meansを行ったもの。</li><li>自己符号化器 : 入力データを低次元化、情報が最大限になるようにデータの次元を再構築する</li><li>計算量 : 対象とするグラフはn個のノードを持つ</li><li>EVD : ナイーブに実装するとO(n3)の計算量、最速の実装はO(n2)の計算量</li><li>自己符号化器 : ノードがスパースな際は計算量はO(kn)</li><li>スパース表現 : データが大きくなるならスパース性を有効活用したい</li><li>EVD : 固有ベクトルが高い確率で密になるため、スパース性が保証されない</li><li>自己符号化器 : スパース性を用いるのは容易</li></ul><h3 id=既存研究で未検証な事柄何を解決解明したいのか>既存研究で未検証な事柄、何を解決・解明したいのか？</h3><ul><li>Graph Clusteringのための Deep Learning の活用方法と調査</li><li>自己符号化器とSpectral Clusteringの類似性とその比較・検証</li></ul><h3 id=method-提案手法>Method (提案手法)</h3><ul><li>GraphEncoder(for graph clustering.)</li><li>グラフラプラシアン(L=D−1S)に対してスパース自己符号化器(Sparse-AutoEncoders)を通した最終層にk-measクラスタリングを行う</li><li>AutoEncoderは入力層・隠れ層・出力層の3層をStacked する。</li><li>X(j)=D−1Sの列ベクトルを各ユニットに入力し、隠れ層の活性化関数h(j)を得て、X(j+1)=h(j)とΓ回(層の階数分)更新する。</li></ul><p><img src=/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/1.png alt=image></p><h3 id=evaluation-検証方法評価方法優位性>Evaluation (検証方法・評価方法・優位性)</h3><ul><li>実世界のグラフデータに対してNMIによってクラスタリングの評価実験を行う。</li><li>データの種類</li><li>ワイン</li><li>ニュース記事</li><li>タンパク質構造グラフ</li></ul><p>以下の三種で比較</p><ul><li>Spectral Clustering</li><li>k-means</li><li>Sparse-AutoEncoders(Graph-Encoder)</li></ul><p><img src=/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/2.png alt=image></p><p><img src=/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/3.png alt=image></p><p>次元の減少推移</p><p><img src=/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/4.png alt=image></p><p>層を重ねる毎にNMIが向上している。</p><p><img src=/posts/2017-12-23_learning-deep-representations-for-graph-clustering-aaai2014-%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/images/5.png alt=image></p><h3 id=conclusion-結論貢献>Conclusion (結論・貢献)</h3><ul><li>Deep Learn と Graph Clusteringの関係性、結果を調査。</li><li>GraphEncoderの嬉しいところ</li><li>隠れ層の次元は入力層の次元より低い。これは全てのエッジが必須ではないことを直感的に示す。</li><li>エッジの除去を行いグラフ表現を更に明確にするために、浅い層から深い層へ。</li><li>EVDの計算量は最速でもO(n2.367)で、グラフは密なグラフ表現。(Toeplitz Matirix)</li><li>GraphEncoderはO(ncd)、dは隠れ層の最大次元、cはグラフの平均次元。(例: 各ノードがk本のエッジを持つ類似度グラフの場合 c=k。ソーシャルグラフで表すと、cは友達の平均の数を示す。)</li><li>EVDは並列化が困難。確率的勾配降下法(SGD)はEVDと比べると並列化が容易である。</li></ul><h3 id=提案手法の限界残された課題>提案手法の限界(残された課題)</h3><p>実行時間の比較が行われていないが、あくまでこの論文の価値はDLとGraph Cluseteringの関連性を示しているのが価値なのでそこは許して下さいって感じ。### Comments (疑問点・わからなかったところ・議論)</p><ul><li>トップカンファレンスを年2本、2nd tier を1本1stで出せるのは、どうやればそのレベルに到達するんだ?</li><li>トレンドに乗った良い論文。</li><li>Good Writing. 内容もシンプルなので90分でサクッと気持よく読めた。論文読むより、スパース自己符号化の勉強に時間取られた。</li><li>Corollary2で ~ symmetrix matrix D−1S って言ってる割に行列の対称性は保証されてないので3.1全般が怪しい、辻褄があってない。</li></ul></article></section><footer><div><p class="f6 gray mt6 lh-copy">© 2016-20 <a href=https://twitter.com/hurutoriya>@hurutoriya</a>.</p></div></footer><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.14.2/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>