<!doctype html><html lang=ja><head><script data-ad-client=ca-pub-8604812913439531 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme-color content="dark"><title>Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrier を読んだ | Shunya Ueta</title><link rel=stylesheet href=/sass/main.min.1a3290acca8b3fc92df85ea9200859476d6c80d59009c89a749300f3ce7a7a67.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-39994406-11"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-39994406-11');</script></head><body class=dark><nav class=navbar><div class=container><div class=flex><div><a class=brand href=/><span class=emoji>🎰</span>
Shunya Ueta</a></div><div class=flex><a href=/articles/>All posts</a>
<button id=dark-mode-button></button></div></div></div></nav><main><div class=container><article><header class=article-header><div class=thumb><div><h1>Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrier を読んだ</h1><div class=post-meta><div>By on <time>December 04, 2017</time></div><div class=tags><a href=/tags/machine-learning/>Machine Learning</a>
<a href=/tags/paper/>Paper</a>
<a href=/tags/kdd/>KDD</a></div></div></div></div></header></article><div class=article-post><p>応用数理研究者が機械学習界に進出していく研究</p><p><img src=/posts/2017-12-04/images/1.png alt=image></p><p>youtube clip</p><p>応用数理界隈ではクラシックな解き方で PageRank が解かれているので最新の数値計算手法に置き換えることで劇的にパフォーマンスが向上して 10 年前のパフォーマンスの鎖を解き放った論文</p><ul><li>KDD2015 Best student paper award</li><li><a href=http://www.cs.cornell.edu/~bindel/present/2015-08-kdd-talk_kdd-aug15.pdf>Slide(PDF)</a></li><li><a href=http://www.cs.cornell.edu/~bindel/present/2015-08-kdd-poster_poster-kdd-pr.pdf>Poster(PDF)</a>
<a href=http://www.cs.cornell.edu/~bindel//blurbs/graphspec.html>Spectral network analysis</a></li></ul><p><a href=https://github.com/wenleix/EdgePPR>wenleix/EdgePPR</a></p><ul><li>Presentation Movie is uploaded in Youtube.</li></ul><p>Author</p><ul><li>W. Xie</li><li>Ph.D Candidate at Cornell University</li><li><a href=http://wenleix.github.io/>http://wenleix.github.io/</a></li><li>iterative computation on big graph data</li><li>D. Bindel</li><li><a href=http://www.cs.cornell.edu/~bindel/>http://www.cs.cornell.edu/~bindel/</a></li><li><a href=http://www.cs.cornell.edu/~bindel/talks.html>http://www.cs.cornell.edu/~bindel/talks.html</a></li><li>He is frequently research activ like ideal young researcher.</li><li>Nonlinear eigenvalue problem.</li><li>Alan J. Demers</li><li>Prof.</li><li>Johannes Gehrke</li><li><a href=http://www.cs.cornell.edu/johannes/>http://www.cs.cornell.edu/johannes/</a></li><li>Prof.</li><li>VLDB,SIGMOD,KDD</li></ul><h3 id=motivation>Motivation</h3><ul><li>ページランクは重要な指標。遷移確率を求めるにはランダムサーファーモデル(ランダムウォーク)が必要。</li><li>様々な高速解法が提案されている。</li></ul><h3 id=reseatch-question>Reseatch Question</h3><ul><li>しかし 10 年以上前、PageRank の黎明期から Personalization based の手法は問題がある。</li><li>一般的な PageRank の解法の説明、その後に Model Reduction をベースにした約 5 倍の性能を誇る提案手法を説明。</li><li>提案手法の性能によって、パフォーマンス上のボトムネックは消えた。</li></ul><h3 id=proposed-method>Proposed Method</h3><ul><li>PageRank</li><li>初期:WEBPage のランクに使用 → 現在:推薦、ソーシャルネットワーク</li><li>Random Suffer Model</li><li>Transition : α の確率で Random Walk(滞在ノードから無作為に遷移)</li><li>Teleporting : 1−α Random Junmp(滞在ノードに依存せずに全てのノードを対象に無作為遷移)</li><li>x(t+1)=αPxt+(1−α)v,where P=AD−1</li><li>v is represents telepotation probabilitie</li><li>xt Walker の確率分布</li><li>サーファーが来る確率 = RandomWalk の確率_RandomWalk による遷移確率 + RandomJump の確率_RandomJump の遷移確率</li><li>= α× RandomWalk による遷移確率 + (1−α)×RandomJump の遷移確率</li><li>= α×+(1−α)×v</li><li>x: Stationary vector(不動ベクトル、定常ベクトル、不動点定理など)</li><li>x が定常状態になった際に、x の確率分布が PageRank を表す。</li><li>x(t),x(t+1)が同一(残差が無い)だと仮定することで、次式の線形方程式を解くことで Pagerank を求める。</li><li>Mx=b,where M=(I−αP),b=(1−α)v.</li></ul><h4 id=standard-pagerank>Standard PageRank</h4><ul><li>グラフの構造(幾何性)を利用</li><li>多くのグラフでは、ノードやエッジに重みが存在する</li></ul><h4 id=personalized-pagerank>Personalized PageRank</h4><ul><li>PageRank を特定のユーザやクエリに合わせたランク</li><li>Node wighted graph</li><li>Mx(w)=(1−α)v(w),w∈ℝ𝕕</li><li>w: 特定の個人、クエリに合わせたパラメータ</li><li>Edge weighted graph</li><li>M(w)x(w)=(I−α)v,w∈ℝ𝕕</li><li>Node Weight, Edge Weight 共に Personalized PageRank は重要だが、２つの問題がある。</li><li>Node Weight に関する論文:多い Edge Weight:少ない</li><li>計算コストが高い</li><li>Edge weighted Personalized PageRank における x(w)を求める高速手法を提案</li><li>提案手法は計算量が準線形的。</li><li>予備知識</li><li>xt+1:べき乗法</li><li>Mx=b:ヤコビ法</li><li>x(w)=(1−α)M−1Vw.</li><li>v≈Vwwhere V∈ℝ𝕟×𝕕</li><li>M−1V:O(n) 、x(w)の再構築に O(d)、PageRank を求めるには O(nd)が要求される。</li><li>v が疎なら計算量は準線形へ。</li></ul><h3 id=model-resuction-method>MODEL RESUCTION METHOD</h3><ol><li>k 次元の次元の削減された空間を構築</li><li>k 次元を近似するために k 個の等式が必要</li><li>次元削減の問題を解く(PageRank のベクトルは一部分が重要だという仮説)</li></ol><ul><li>Reduced Space Construction</li><li>{w(j)}rj=1(ベクトルの集合)に対応する PageRank {x(j)}rj=1(ベクトルの集合)を求める。</li><li>w は乱数によって決定する</li><li>k 次元の空間 υ を探索する</li></ul></div><h2>See Also</h2><ul><li><a href=/posts/2017-12-01/>Machine Learning that Matters (ICML2012) を読んだ</a></li></ul></div><div class=container><nav class="flex container suggested"><a rel=prev href=/posts/2017-12-01/ title="Previous post (older)"><span>Previous</span>
Machine Learning that Matters (ICML2012) を読んだ</a>
<a rel=next href=/posts/2017-12-05/ title="Next post (newer)"><span>Next</span>
Visualized Approximate Eigen Vector by Power Iteration on 3 dimensions.</a></nav></div></main></main><footer class="footer flex"><section class=container><nav class=footer-links><a href=/index.xml>RSS</a></nav></section><script async src=/js/features.min.a94f58a30ad2560de728e080d87f75c60cf806fd1b3d5f4815f1a1a02c0d1859.js></script></footer></body></html>