---
title: "The ML Test Score:
A Rubric for ML Production Readiness and Technical Debt Reduction を読んでみる"
date: 2020-04-25T01:35:20+09:00
lang: ja
draft: True
tags:
  - Paper
  - Machine Learning
  - MLOps
---

[[抄訳] What’s your ML test score? A rubric for ML production systems](https://shunyaueta.com/posts/2020-04-19/)で紹介した論文の続編があったので読んでみた

- [The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction](https://research.google/pubs/pub46555/)

大規模なオフラインの機械学習実験は注目されているが、反対にオンラインでの信頼性がある機械学習システムの開発は難しく、技術的負債が溜まりやすい
 
`図1` では、左側が伝統的なシステムのテストとモニタリング、右側は、機械学習システムのテストとモニタリングである
機械学習システムのテストとモニタリングが複雑になる要因として、コードだけではなく、動的に決定されるデータの質とモデルの多種多様な設定に依存するからである

![Figure 1 in paper](/posts/2020-04-25/images/fig-1.png)

28の実行可能なテスト項目を提案する
各項目は前回抄訳した[論文](https://shunyaueta.com/posts/2020-04-19/) が基となっている

Google内部の調査では、調査対象の80%のチームが機械学習システムへのテストをされていなかった。(エンジニアリングに長けているGoogle内部でさえも)

有名な[Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)もこの論文内で数多く引用されている(著者の組織は同じGoogleなので当たり前ですね)

## TEST FOR DATA AND FEATURES

1. 期待する特徴量は全てスキーマで管理され、読み込み可能か?
   1. How: このスキーマは学習データの統計値を計算することに有用である。これらを可視化することで事前のバイアスを検知したり、Fasets[^1]などのツールを用いた統計値の可視化もとても有用である
2. 全ての特徴量は有用か?
   1. 全ての特徴量は、追加すればするほどコストになる。独立した全ての特料料は有用だろうか?
   2. How: 特徴量削除や目的変数と特徴量の相関などを見てみよう
3. 特徴量のコストは高くないか?
   1. How: 推論速度やRAMの使用率だけを見るのではなく、その特徴量が依存先や不安定性なども考慮する
4. 特徴量は要件に準拠しているか?
   1. 一般的には、機械学習システムにデータが送られる際には単一のリソースからデータを取得することが前提ですが、実験の際には知らず識らずのうちに精度を向上させるために特徴量をどんどん追加してしまいがちである
   2. How: 学習データのリソースがプロダクション環境から逸脱していないか監視する
5. データパイプラインは適切なプライバシーコントロールはされているか?
   1. How: データパイプライン構築の時には権限管理を厳密に行う
6. 新しい特徴量は素早く追加可能か?
   1. 新しいアイデアを素早く試せるチームは強い。とても効率的なチームは、1-2ヶ月でトラフィックが多くとも新しい特徴量を追加することが可能である。第5の法則と相反する形にはなっています。 
7. 全ての特徴量生成コードはテストされているか?
   1. 特徴量生成のコードは一見シンプルでユニットテストなどは必要そうに見えないかもしれないが、ここで発生するバグは発見することはとても難しい


## TESTS FOR MODEL DEVELOPMENT

8. 全てのモデルはレビューされ、リポジトリに格納されている
   1. バージョン管理され、過去の実験との比較や再現実験などを可能にする
9. オフラインの疑似指標はオンラインメトリクスに相関しているか?
   1.  How: A/Bテストで意図的にモデルのスコアを劣化させてテストする
10. 全てのハイパーパラメータはチューニングされているか?
    1.  学習率・NNの層の数・次元数など数多くのハイパーパラメータがあり、予測精度に大きな影響を与える
    2.  How: グリッドサーチや確率的なパラメータチューニングをおこなう
11. 古くなった(陳腐)モデルの影響は把握されているか?
    1.  多くのプロダクションの機械学習システムは素早く変更される。例えば学習パイプラインが失敗し、更新されなかったモデルをここでは陳腐なモデルと呼ぶ。モデルが更新されないことでどれくらい予測精度に影響を与えるか理解することで、再学習の適切な期間も決定できる。ほとんどのモデルは外部世界の要因により更新しなければならない。
    2.  How: 小規模なA/Bテストにより、新モデルと旧モデルを比較することでモデルのリフレッシュネスがどれくらい重要か計測することができる
12. 単純なモデルは必ずしも良くはない
    1.  少ない特徴量で単純な線形モデルがベースラインとして使われることがあるが、パイプライン構築のコストとその利益のトレードオフを常に考えていくべきである
13. TODO
    1.  How: 
14. 包括性を考慮したモデルになっているか?
    1.  機械学習システムではFairnessなどが最近課題となっている。例えば、Bolukbasi らは文字埋め込みにおいて性別の差が予測において不適切な影響を発生させていることがわかった。
    2.  How: 特定のグループ間で、実験をおこない結果が異なっていないか確認する。

![Figure 2 in paper](/posts/2020-04-25/images/fig-2.png)
![Figure 3 in paper](/posts/2020-04-25/images/fig-3.png)
![Figure 4 in paper](/posts/2020-04-25/images/fig-4.png)

[^1]: https://pair-code.github.io/facets/

